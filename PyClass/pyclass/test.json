{"Repository": "Unpaired-Sentiment-Translation", "input": "Class representing a minibatch of train/val/test examples for text summarization. className Batch(object) Method __init__ Method init_encoder_seq Attribute pad_id", "label": "class Batch(object):\n  def __init__(self, example_list, hps, vocab):\n    self.pad_id = vocab.word2id(data.PAD_TOKEN) # id of the PAD token used to pad sequences\n    self.init_encoder_seq(example_list, hps)  # initialize the input to the encoder\n\n\n\n  def init_encoder_seq(self, example_list, hps):\n\n    #print (example_list)\n\n    #max_enc_seq_len = max(ex.enc_len for ex in example_list)\n    for ex in example_list:\n      ex.pad_encoder_input(hps.max_dec_steps, self.pad_id)\n\n    self.enc_batch = np.zeros((hps.batch_size,hps.max_dec_steps), dtype=np.int32)\n    #self.enc_word_padding_mask = np.zeros((hps.batch_size,hps.max_enc_sen_num, max_enc_seq_len), dtype=np.int32)\n    self.enc_lens = np.zeros((hps.batch_size), dtype=np.int32)\n    #self.enc_sen_lens=np.zeros((hps.batch_size*hps.max_enc_sen_num), dtype=np.int32)\n    self.enc_padding_mask = np.zeros((hps.batch_size, hps.max_dec_steps), dtype=np.float32)\n    self.labels = np.zeros((hps.batch_size), dtype=np.int32)\n    self.original_reviews = [ex.original_reivew for ex in example_list]\n\n    # Fill in the numpy arrays\n    for i, ex in enumerate(example_list):\n      self.labels[i] = ex.label\n      self.enc_batch[i,:] = np.array(ex.enc_input)[:]\n      self.enc_lens[i] = ex.enc_len\n      for j in range(ex.enc_len):\n          self.enc_padding_mask[i][j] = 1\n\n\n\n      '''for j in range(ex.enc_len):\n            self.enc_padding_mask[i][j] = 1\n            for k in range(max_enc_seq_len):\n              if self.enc_batch[i][j][k] != self.pad_id:\n                self.enc_word_padding_mask[i][j][k] =1'''"}
{"Repository": "NoisyNaturalGradient", "input": "Implements Noisy Adam algorithm. className NoisyAdam(Optimizer) Method check_bayesian_and_option Method __init__ Method step Method get_delta_dicts", "label": "class NoisyAdam(Optimizer):\n    @staticmethod\n    def check_bayesian_and_option(network):\n        name = network.__class__.__name__\n        # TODO : replace this hard-coded area\n        if 'Sequential' in name:\n            return\n        for network_name in COMPLEX_BAYESIAN_NETWORKS:\n            if network_name in name:\n                return\n        for network_name in BASIC_BAYESIAN_NETWORKS:\n            if network_name not in name and len(list(network.parameters())) > 0:\n                raise RuntimeError(\"This implementation only supports BayesianLinear module.\")\n            elif network_name in name and network.option != 'FFG':\n                raise RuntimeError(\"Noisy Adam optimizer only supports fully factorized gaussian option.\")\n\n    def __init__(self, networks, lr=1e-3, betas=(0.9, 0.999), eps=1e-8, weight_decay=0):\n        # TODO : supports Bayesian Convolution?\n        for network in networks:\n            network.apply(self.check_bayesian_and_option)\n        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)\n        params = chain(*[network.parameters() for network in networks])\n        super(NoisyAdam, self).__init__(params, defaults)\n\n    def step(self, closure=None):\n        loss = None\n        if closure is not None:\n            loss = closure()\n\n        for group in self.param_groups:\n            for p in group['params']:\n                if p.grad is None:\n                    continue\n                # This is different part between Adam and Noisy Adam.\n                grad = p.grad.data - group['eps'] * p.data\n                if grad.is_sparse:\n                    raise RuntimeError('Does not support sparse gradients.')\n\n                state = self.state[p]\n\n                # State initialization\n                if len(state) == 0:\n                    state['step'] = 0\n                    # Exponential moving average of gradient values\n                    state['exp_avg'] = torch.zeros_like(p.data)\n                    # Exponential moving average of squared gradient values\n                    state['exp_avg_sq'] = torch.zeros_like(p.data)\n\n                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n                beta1, beta2 = group['betas']\n\n                state['step'] += 1\n\n                if group['weight_decay'] != 0:\n                    grad = grad.add(group['weight_decay'], p.data)\n\n                # Decay the first and second moment running average coefficient\n                exp_avg.mul_(beta1).add_(1 - beta1, grad)\n                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n\n                denom = exp_avg_sq.sqrt().add_(group['eps'])\n\n                bias_correction1 = 1 - beta1 ** state['step']\n                bias_correction2 = 1 - beta2 ** state['step']\n                step_size = group['lr'] * math.sqrt(bias_correction2) / bias_correction1\n\n                p.data = torch.addcdiv(torch.zeros(1), -step_size, exp_avg, denom)\n\n        return loss\n\n    def get_delta_dicts(self):\n        u_delta_dict = dict()\n        f_dict = dict()\n        for group in self.param_groups:\n            for p in group['params']:\n                u_delta_dict[p] = p.data\n                f_dict[p] = self.state[p]['exp_avg_sq']\n        return u_delta_dict, f_dict"}
{"Repository": "bitcoinlib", "input": "Handle Key class Exceptions className BKeyError(Exception) Method __init__ Method __str__ Attribute msg", "label": "class BKeyError(Exception):\n    def __init__(self, msg=''):\n        self.msg = msg\n        _logger.error(msg)\n\n    def __str__(self):\n        return self.msg"}
{"Repository": "dd-agent", "input": "A metric to track the distribution of a set of values. className Histogram(Metric) Method __init__ Method sample Method flush Attribute formatter Attribute name Attribute count Attribute samples Attribute aggregates Attribute percentiles Attribute tags Attribute hostname Attribute device_name Attribute last_sample_time", "label": "class Histogram(Metric):\n    def __init__(self, formatter, name, tags, hostname, device_name, extra_config=None):\n        self.formatter = formatter\n        self.name = name\n        self.count = 0\n        self.samples = []\n        self.aggregates = extra_config['aggregates'] if\\\n            extra_config is not None and extra_config.get('aggregates') is not None\\\n            else DEFAULT_HISTOGRAM_AGGREGATES\n        self.percentiles = extra_config['percentiles'] if\\\n            extra_config is not None and extra_config.get('percentiles') is not None\\\n            else DEFAULT_HISTOGRAM_PERCENTILES\n        self.tags = tags\n        self.hostname = hostname\n        self.device_name = device_name\n        self.last_sample_time = None\n\n    def sample(self, value, sample_rate, timestamp=None):\n        self.count += int(1 / sample_rate)\n        self.samples.append(value)\n        self.last_sample_time = time()\n\n    def flush(self, ts, interval):\n        if not self.count:\n            return []\n\n        self.samples.sort()\n        length = len(self.samples)\n\n        min_ = self.samples[0]\n        max_ = self.samples[-1]\n        med = self.samples[int(round(length/2 - 1))]\n        sum_ = sum(self.samples)\n        avg = sum_ / float(length)\n\n        aggregators = [\n            ('min', min_, MetricTypes.GAUGE),\n            ('max', max_, MetricTypes.GAUGE),\n            ('median', med, MetricTypes.GAUGE),\n            ('avg', avg, MetricTypes.GAUGE),\n            ('sum', sum_, MetricTypes.GAUGE),\n            ('count', self.count/interval, MetricTypes.RATE),\n        ]\n\n        metric_aggrs = [\n            (agg_name, agg_func, m_type)\n            for agg_name, agg_func, m_type in aggregators\n            if agg_name in self.aggregates\n        ]\n\n        metrics = [self.formatter(\n            hostname=self.hostname,\n            device_name=self.device_name,\n            tags=self.tags,\n            metric='%s.%s' % (self.name, suffix),\n            value=value,\n            timestamp=ts,\n            metric_type=metric_type,\n            interval=interval) for suffix, value, metric_type in metric_aggrs\n        ]\n\n        for p in self.percentiles:\n            val = self.samples[int(round(p * length - 1))]\n            name = '%s.%spercentile' % (self.name, int(p * 100))\n            metrics.append(self.formatter(\n                hostname=self.hostname,\n                tags=self.tags,\n                metric=name,\n                value=val,\n                timestamp=ts,\n                metric_type=MetricTypes.GAUGE,\n                interval=interval,\n            ))\n\n        # Reset our state.\n        self.samples = []\n        self.count = 0\n\n        return metrics"}
{"Repository": "schnetpack", "input": "Organic Materials Database (OMDB) of bulk organic crystals. className OrganicMaterialsDatabase(AtomsDataModule) Method prepare_data Method _convert", "label": "class OrganicMaterialsDatabase(AtomsDataModule):\n    BandGap = \"band_gap\"\n\n    def __init__(\n        self,\n        datapath: str,\n        batch_size: int,\n        num_train: Optional[int] = None,\n        num_val: Optional[int] = None,\n        num_test: Optional[int] = None,\n        split_file: Optional[str] = \"split.npz\",\n        format: Optional[AtomsDataFormat] = AtomsDataFormat.ASE,\n        load_properties: Optional[List[str]] = None,\n        val_batch_size: Optional[int] = None,\n        test_batch_size: Optional[int] = None,\n        transforms: Optional[List[torch.nn.Module]] = None,\n        train_transforms: Optional[List[torch.nn.Module]] = None,\n        val_transforms: Optional[List[torch.nn.Module]] = None,\n        test_transforms: Optional[List[torch.nn.Module]] = None,\n        num_workers: int = 2,\n        num_val_workers: Optional[int] = None,\n        num_test_workers: Optional[int] = None,\n        property_units: Optional[Dict[str, str]] = None,\n        distance_unit: Optional[str] = None,\n        raw_path: Optional[str] = None,\n        **kwargs\n    ):\n        super().__init__(\n            datapath=datapath,\n            batch_size=batch_size,\n            num_train=num_train,\n            num_val=num_val,\n            num_test=num_test,\n            split_file=split_file,\n            format=format,\n            load_properties=load_properties,\n            val_batch_size=val_batch_size,\n            test_batch_size=test_batch_size,\n            transforms=transforms,\n            train_transforms=train_transforms,\n            val_transforms=val_transforms,\n            test_transforms=test_transforms,\n            num_workers=num_workers,\n            num_val_workers=num_val_workers,\n            num_test_workers=num_test_workers,\n            property_units=property_units,\n            distance_unit=distance_unit,\n            **kwargs\n        )\n        self.raw_path = raw_path\n\n    def prepare_data(self):\n        if not os.path.exists(self.datapath):\n            property_unit_dict = {OrganicMaterialsDatabase.BandGap: \"eV\"}\n\n            dataset = create_dataset(\n                datapath=self.datapath,\n                format=self.format,\n                distance_unit=\"Ang\",\n                property_unit_dict=property_unit_dict,\n            )\n\n            self._convert(dataset)\n        else:\n            dataset = load_dataset(self.datapath, self.format)\n\n    def _convert(self, dataset):\n        if self.raw_path is None or not os.path.exists(self.raw_path):\n            # TODO: can we download here automatically like QM9?\n            raise AtomsDataModuleError(\n                \"The path to the raw dataset is not provided or invalid and the db-file does \"\n                \"not exist!\"\n            )\n        logging.info(\"Converting %s to a .db file..\" % self.raw_path)\n        tar = tarfile.open(self.raw_path, \"r:gz\")\n        names = tar.getnames()\n        tar.extractall()\n        tar.close()\n\n        structures = read(\"structures.xyz\", index=\":\")\n        Y = np.loadtxt(\"bandgaps.csv\")\n        [os.remove(name) for name in names]\n\n        atoms_list = []\n        property_list = []\n        for i, at in enumerate(structures):\n            atoms_list.append(at)\n            property_list.append({OrganicMaterialsDatabase.BandGap: np.array([Y[i]])})\n        dataset.add_systems(atoms_list=atoms_list, property_list=property_list)"}
{"Repository": "MC-BERT", "input": "Source: https://stackoverflow.com/a/54128391 className NumpyExtension(Extension) Method __init__ Method include_dirs Method include_dirs Attribute __include_dirs", "label": "class NumpyExtension(Extension):\n    def __init__(self, *args, **kwargs):\n        self.__include_dirs = []\n        super().__init__(*args, **kwargs)\n\n    @property\n    def include_dirs(self):\n        import numpy\n        return self.__include_dirs + [numpy.get_include()]\n\n    @include_dirs.setter\n    def include_dirs(self, dirs):\n        self.__include_dirs = dirs"}
{"Repository": "CVSE", "input": "Simple vocabulary wrapper. className Vocabulary(object) Method __init__ Method add_word Method __call__ Method __len__ Attribute word2idx Attribute idx2word Attribute idx", "label": "class Vocabulary(object):\n    def __init__(self):\n        self.word2idx = {}\n        self.idx2word = {}\n        self.idx = 0\n\n    def add_word(self, word):\n        if word not in self.word2idx:\n            self.word2idx[word] = self.idx\n            self.idx2word[self.idx] = word\n            self.idx += 1\n\n    def __call__(self, word):\n        if word not in self.word2idx:\n            return self.word2idx['<unk>']\n        return self.word2idx[word]\n\n    def __len__(self):\n        return len(self.word2idx)"}
{"Repository": "subliminal", "input": "Subclass of :class:`ProviderPool` with asynchronous support for :meth:`~ProviderPool. className AsyncProviderPool(ProviderPool) Method __init__ Method list_subtitles_provider Method list_subtitles Attribute max_workers", "label": "class AsyncProviderPool(ProviderPool):\n    def __init__(self, max_workers=None, *args, **kwargs):\n        super(AsyncProviderPool, self).__init__(*args, **kwargs)\n\n        #: Maximum number of threads to use\n        self.max_workers = max_workers or len(self.providers)\n\n    def list_subtitles_provider(self, provider, video, languages):\n        return provider, super(AsyncProviderPool, self).list_subtitles_provider(provider, video, languages)\n\n    def list_subtitles(self, video, languages):\n        subtitles = []\n\n        with ThreadPoolExecutor(self.max_workers) as executor:\n            for provider, provider_subtitles in executor.map(self.list_subtitles_provider, self.providers,\n                                                             itertools.repeat(video, len(self.providers)),\n                                                             itertools.repeat(languages, len(self.providers))):\n                # discard provider that failed\n                if provider_subtitles is None:\n                    logger.info('Discarding provider %s', provider)\n                    self.discarded_providers.add(provider)\n                    continue\n\n                # add subtitles\n                subtitles.extend(provider_subtitles)\n\n        return subtitles"}
{"Repository": "AISHELL-4", "input": "Location aware attention described in \"Attention-Based Models for Speech Recognition\" className LocAttention(Attention) Method clear", "label": "class LocAttention(Attention):\n    def __init__(self,\n                 enc_dim: int,\n                 dec_dim: int,\n                 att_dim: int = 512,\n                 conv_channels: int = 10,\n                 loc_context: int = 64):\n        super(LocAttention, self).__init__()\n        self.enc_proj = nn.Linear(enc_dim, att_dim)\n        self.dec_proj = nn.Linear(dec_dim, att_dim, bias=False)\n        # N x D_conv x T => N x D_att x T\n        self.att = nn.Conv1d(conv_channels, att_dim, 1, bias=False)\n        # N x 1 x T => N x D_att x T\n        self.F = nn.Conv1d(1,\n                           conv_channels,\n                           loc_context * 2 + 1,\n                           stride=1,\n                           padding=loc_context)\n        self.w = nn.Linear(att_dim, 1, bias=False)\n        # clear variables\n        self.clear()\n\n    def clear(self):\n        self.enc_part = None\n        self.pad_mask = None\n\n    def forward(self, enc_pad: th.Tensor, enc_len: Optional[th.Tensor],\n                dec_prev: th.Tensor,\n                ali_prev: th.Tensor) -> Tuple[th.Tensor, th.Tensor]:\n        N, T, _ = enc_pad.shape\n        # prepare variable\n        if self.enc_part is None:\n            # N x Ti x D_att\n            self.enc_part = self.enc_proj(enc_pad)\n            # init padding mask\n            if enc_len is not None:\n                self.pad_mask = padding_mask(enc_len, enc_pad.device)\n        if ali_prev is None:\n            # initialize attention\n            ali_prev = th.ones(N, T, device=enc_pad.device)\n            if enc_len is not None:\n                ali_prev = ali_prev.masked_fill(self.pad_mask, 0)\n                ali_prev = ali_prev / enc_len[..., None]\n            else:\n                ali_prev = ali_prev / T\n        # N x 1 x T => N x D_conv x Ti\n        att_part = self.F(ali_prev[:, None])\n        # N x D_conv x Ti => N x D_att x Ti\n        att_part = self.att(att_part)\n        # N x D_att x Ti => N x Ti x D_att\n        att_part = th.transpose(att_part, 1, 2)\n        # N x D_dec =>  N x D_att\n        dec_part = self.dec_proj(dec_prev)\n        # N x Ti x D_att\n        sum_part = th.tanh(att_part + dec_part[:, None] + self.enc_part)\n        # N x Ti\n        score = self.w(sum_part).squeeze(-1)\n        # ali: N x Ti\n        ali = self.softmax(score, enc_len, self.pad_mask)\n        # ctx: N x D_enc\n        ctx = th.sum(ali[..., None] * enc_pad, 1)\n        # return alignment weight & context\n        return ali, ctx"}
{"Repository": "nba_api", "input": "runs a user-specified function whenever member is accessed className OnAccess(EnumMeta) Method __getattribute__ Method __getitem__", "label": "class OnAccess(EnumMeta):\n    def __getattribute__(cls, name):\n        obj = super().__getattribute__(name)\n        if isinstance(obj, Enum) and obj._on_access:\n            obj._on_access()\n        return obj\n\n    def __getitem__(cls, name):\n        member = super().__getitem__(name)\n        if member._on_access:\n            member._on_access()\n        return member\n\n    def __call__(\n        cls, value, names=None, *, module=None, qualname=None, type=None, start=1\n    ):\n        obj = super().__call__(\n            value, names, module=module, qualname=qualname, type=type, start=start\n        )\n        if isinstance(obj, Enum) and obj._on_access:\n            obj._on_access()\n        return obj"}
{"Repository": "PoseFromShape", "input": "Computes and stores the average and current value className AverageValueMeter(object) Method __init__ Method reset Method update", "label": "class AverageValueMeter(object):\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0.\n        self.avg = 0.\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.avg = self.avg * (self.count / (self.count + n)) + val * (n / (self.count + n))\n        self.count += n"}
{"Repository": "census-api", "input": "Parser for the main topics page. className TopicsParser(HTMLParser) Method __init__ Method handle_starttag Method handle_endtag Method handle_data Attribute in_dt_tag Attribute topic_buffer Attribute base_url Attribute topics", "label": "class TopicsParser(HTMLParser):\n    def __init__(self):\n        super().__init__()\n        self.in_dt_tag = False\n        self.topic_buffer = {'name': '', 'url': ''}\n        self.base_url = \"https://censusreporter.org\"\n        self.topics = []\n\n    def handle_starttag(self, tag, attrs):\n        if tag == 'dt':\n            self.in_dt_tag = True\n\n        if self.in_dt_tag and tag == 'a':\n            topic_url = self.base_url + attrs[0][1]\n            self.topic_buffer['url'] = topic_url\n\n    def handle_endtag(self, tag):\n        if tag == 'dt':\n            self.in_dt_tag = False\n\n    def handle_data(self, data):\n        if self.in_dt_tag:\n            self.topic_buffer['name'] = data\n            self.topics.append(self.topic_buffer.copy())"}
{"Repository": "Moodle-DL", "input": "An Exception which gets thrown if a Downloader is already running. className LockError(Exception) Method lock Method unlock", "label": "class LockError(Exception):\n        pass\n\n    @staticmethod\n    def lock(dir_path: str):\n        path = Path(dir_path) / 'running.lock'\n        if Path(path).exists():\n            raise ProcessLock.LockError(\n                f'A downloader is already running. Delete {str(path)} if you think this is wrong.'\n            )\n        Path(path).touch()\n\n    @staticmethod\n    def unlock(dir_path: str):\n        path = Path(dir_path) / 'running.lock'\n        try:\n            Path(path).unlink()\n        except OSError:\n            pass"}
{"Repository": "ceres", "input": "A :class:`CeresNode` represents a single time-series metric of a given `timeStep` (its seconds-per-point resolution) and containing arbitrary key-value metadata. className CeresNode(object) Method __init__ Method __repr__ Method create Attribute tree Attribute nodePath Attribute fsPath Attribute metadataFile Attribute timeStep Attribute aggregationMethod Attribute sliceCache Attribute sliceCachingBehavior", "label": "class CeresNode(object):\n  __slots__ = ('tree', 'nodePath', 'fsPath',\n               'metadataFile', 'timeStep', 'aggregationMethod',\n               'sliceCache', 'sliceCachingBehavior')\n\n  def __init__(self, tree, nodePath, fsPath):\n    self.tree = tree\n    self.nodePath = nodePath\n    self.fsPath = fsPath\n    self.metadataFile = join(fsPath, '.ceres-node')\n    self.timeStep = None\n    self.aggregationMethod = 'average'\n    self.sliceCache = None\n    self.sliceCachingBehavior = DEFAULT_SLICE_CACHING_BEHAVIOR\n\n  def __repr__(self):\n    return \"<CeresNode[0x%x]: %s>\" % (id(self), self.nodePath)\n  __str__ = __repr__\n\n  @classmethod\n  def create(cls, tree, nodePath, **properties):\n    \"\"\"Create a new :class:`CeresNode` on disk with the specified properties.\n\n    :param tree: The :class:`CeresTree` this node is associated with\n    :param nodePath: The name of the metric this node represents\n    :param \\*\\*properties: A set of key-value properties to be associated with this node"}
{"Repository": "iceberg", "input": "A playbook is an easy way to create linear animations. className Playbook(ABC) Method __init__ Method combined_scene Method frozen Method freeze Method add_scene Method play Method _make_frame Method timeline", "label": "class Playbook(ABC):\n    def __init__(self) -> None:\n        self._cursor = 0\n        self._scenes = []\n\n        self.timeline()\n\n    @property\n    def combined_scene(self) -> Scene:\n        assert len(self._scenes) > 0, \"No scenes have been added to the playbook.\"\n\n        # Add all the scenes together.\n        scene = self._scenes[0]\n        for next_scene in self._scenes[1:]:\n            scene += next_scene\n\n        return scene\n\n    def frozen(self, drawable: Drawable, t: float = None) -> Drawable:\n        if t is None:\n            t = self._get_drawable_duration()\n\n        return self._get_drawable_at_t(t)\n\n    def freeze(self, duration: float):\n        if len(self._scenes) == 0:\n            raise ValueError(\n                \"No scenes have been added to the playbook. So cannot freeze.\"\n            )\n\n        self._scenes.append(self._scenes[-1].freeze(duration))\n        self._cursor += duration\n\n    def add_scene(self, scene: Scene):\n        self._scenes.append(scene)\n        self._cursor += scene.duration\n\n    def play(self, drawable: Drawable, duration: float = None) -> None:\n        # Find the duration of the animation, by finding all the animated objects.\n\n        if duration is None:\n            duration = _get_drawable_duration(drawable)\n\n        # Build the scene.\n        def _make_frame(t: float) -> Drawable:\n            drawable.set_time(t)\n            return drawable\n\n        scene = Scene(duration, _make_frame)\n        self._scenes.append(scene)\n        self._cursor += duration\n\n    @abstractmethod\n    def timeline(self):\n        pass\n\n    def ipython_display(\n        self, fps: int = 60, loop: bool = True, display_format: str = \"mp4\"\n    ) -> None:\n        self.combined_scene.ipython_display(\n            fps=fps, loop=loop, display_format=display_format\n        )"}
{"Repository": "python-youtube", "input": "This is a return demo: {'error': {'errors': [{'domain': 'youtube. className PyYouTubeException(Exception) Method __init__ Method error_handler Method __repr__ Method __str__", "label": "class PyYouTubeException(Exception):\n    def __init__(self, response: Optional[Union[ErrorMessage, Response]]):\n        self.status_code: Optional[int] = None\n        self.error_type: Optional[str] = None\n        self.message: Optional[str] = None\n        self.response: Optional[Union[ErrorMessage, Response]] = response\n        self.error_handler()\n\n    def error_handler(self):\n        if isinstance(self.response, ErrorMessage):\n            self.status_code = self.response.status_code\n            self.message = self.response.message\n            self.error_type = \"PyYouTubeException\"\n        elif isinstance(self.response, Response):\n            res_data = self.response.json()\n            if \"error\" in res_data:\n                error = res_data[\"error\"]\n                if isinstance(error, dict):\n                    self.status_code = res_data[\"error\"][\"code\"]\n                    self.message = res_data[\"error\"][\"message\"]\n                else:\n                    self.status_code = self.response.status_code\n                    self.message = error\n                self.error_type = \"YouTubeException\"\n\n    def __repr__(self):\n        return (\n            f\"{self.error_type}(status_code={self.status_code},message={self.message})\"\n        )\n\n    def __str__(self):\n        return self.__repr__()"}
{"Repository": "parslepy", "input": "Called when building abstract Parsley trees and when etracting object values during the actual parsing of documents This should be subclassed to implement the selector processing logic you need for your Parsley handling. className SelectorHandler(object) Method __init__ Method make Method select Method extract Attribute DEBUG", "label": "class SelectorHandler(object):\n    DEBUG = False\n\n    def __init__(self, debug=False):\n        if debug:\n            self.DEBUG = True\n\n    def make(self, selection_string):\n        raise NotImplementedError\n\n    def select(self, document, selector):\n        raise NotImplementedError\n\n    def extract(self, document, selector):\n        raise NotImplementedError"}
{"Repository": "wavenet_vocoder", "input": "Partially randomized sampler 1. className PartialyRandomizedSimilarTimeLengthSampler(Sampler) Method __init__ Method __iter__ Method __len__ Attribute sorted_indices Attribute batch_size Attribute batch_group_size", "label": "class PartialyRandomizedSimilarTimeLengthSampler(Sampler):\n    def __init__(self, lengths, batch_size=8, batch_group_size=None):\n        self.lengths, self.sorted_indices = torch.sort(torch.LongTensor(lengths))\n\n        self.batch_size = batch_size\n        if batch_group_size is None:\n            batch_group_size = min(batch_size * 8, len(self.lengths))\n            if batch_group_size % batch_size != 0:\n                batch_group_size -= batch_group_size % batch_size\n\n        self.batch_group_size = batch_group_size\n        assert batch_group_size % batch_size == 0\n\n    def __iter__(self):\n        indices = self.sorted_indices.numpy()\n        batch_group_size = self.batch_group_size\n        s, e = 0, 0\n        bins = []\n        for i in range(len(indices) // batch_group_size):\n            s = i * batch_group_size\n            e = s + batch_group_size\n            group = indices[s:e]\n            random.shuffle(group)\n            bins += [group]\n\n        # Permutate batches\n        random.shuffle(bins)\n        binned_idx = np.stack(bins).reshape(-1)\n\n        # Handle last elements\n        s += batch_group_size\n        if s < len(indices):\n            last_bin = indices[len(binned_idx):]\n            random.shuffle(last_bin)\n            binned_idx = np.concatenate([binned_idx, last_bin])\n\n        return iter(torch.tensor(binned_idx).long())\n\n    def __len__(self):\n        return len(self.sorted_indices)"}
{"Repository": "ASAP", "input": "Plots a 2D clustering plot given x,y coordinates and a label z for every data point. className Plot_Function_Cluster(Plot_Function_Base) Method __init__ Method create Attribute acronym Attribute p_spec", "label": "class Plot_Function_Cluster(Plot_Function_Base):\n    def __init__(self, p_spec):\n\n        self.acronym = \"cluster\"\n\n        self.p_spec = {\n        'w_label': False,\n        'circle_size': 20, \n        'facecolor': 'none',\n        'edgecolor': 'gray',\n        'fontsize': 16,\n        'cmap': 'gnuplot',\n        'alpha': 1.0 # color transparency\n        }\n\n        # fill in the values\n        for k, v in p_spec.items():\n            if k in self.p_spec.keys():\n                self.p_spec[k] = v\n\n        print(\"Using cluster plot ...\")\n\n    def create(self, fig, ax, X, z=[], y=[], tags=[]):\n        # get the cluster size and mean position\n        from ..cluster import get_cluster_size, get_cluster_properties\n        y_unique = np.unique(y)\n        [_, cluster_mx] = get_cluster_properties(y, X[:, 0], 'mean')\n        [_, cluster_my] = get_cluster_properties(y, X[:, 1], 'mean')\n        [_, cluster_size] = get_cluster_size(y)\n        s = {}\n        for k in y_unique:\n            s[k] = np.log(cluster_size[k]) # default is using log(frequency)\n\n        for k in y_unique:\n            ax.plot(cluster_mx[k], cluster_my[k], 'o', \n                    markerfacecolor=self.p_spec['facecolor'],\n                    markeredgecolor=self.p_spec['edgecolor'], \n                    markersize=self.p_spec['circle_size'] * s[k])\n\n        if self.p_spec['w_label'] is True:\n            for k in y_unique:\n                # Position of each label.\n                txt = ax.annotate(str(k), xy=(cluster_mx[k], cluster_my[k]),\n                              xytext=(0, 0), textcoords='offset points',\n                              fontsize=self.p_spec['fontsize'], \n                              horizontalalignment='center', verticalalignment='center')\n                txt.set_path_effects([\n                              PathEffects.Stroke(linewidth=5, foreground='none'),\n                              PathEffects.Normal()])\n\n        return fig, ax"}
{"Repository": "owyl", "input": "A dict that defaults values to None. className Blackboard(defaultdict) Method __init__ Attribute __dict__", "label": "class Blackboard(defaultdict):\n    _name_dict = defaultdict(dict)  # For a twist on the Borg idiom\n\n    def __init__(self, name, **kwargs):\n        self.__dict__ = Blackboard._name_dict[name]\n\n        default = lambda: None\n        super(Blackboard, self).__init__(default, **kwargs)"}
{"Repository": "datatree", "input": "Base class for included renderers. className InternalRenderer(Renderer) Method data_only Method instruction_only Method __filter Method default_options Method get_options", "label": "class InternalRenderer(Renderer):\n    friendly_names = []\n\n    ### Node Methods ###\n\n    def data_only(self, node):\n        return self.__filter(node, Vertex)\n\n    def instruction_only(self, node):\n        return self.__filter(node, InstructionNode)\n\n    def __filter(self, node, node_type):\n        return [x for x in node.__children__ if isinstance(x, node_type)]\n\n    ### Option Methods ###\n\n    @property\n    def default_options(self):\n        raise NotImplementedError()\n\n    def get_options(self, user_options):\n        if user_options is None: user_options = {}\n        options = deepcopy(self.default_options)\n        options.update(user_options)\n        return options"}
{"Repository": "scaleformer", "input": "Minute of hour encoded as value between [-0.5, 0.5] className MinuteOfHour(TimeFeature) Method __call__", "label": "class MinuteOfHour(TimeFeature):\n    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n        return index.minute / 59.0 - 0.5"}
{"Repository": "Sec-Tools", "input": "A state machine to verify a byte sequence for a particular encoding. className CodingStateMachine(object) Method __init__ Method reset Method next_state Method get_current_charlen Method get_coding_state_machine Method language Attribute _model Attribute _curr_byte_pos Attribute _curr_char_len Attribute _curr_state Attribute logger", "label": "class CodingStateMachine(object):\n    def __init__(self, sm):\n        self._model = sm\n        self._curr_byte_pos = 0\n        self._curr_char_len = 0\n        self._curr_state = None\n        self.logger = logging.getLogger(__name__)\n        self.reset()\n\n    def reset(self):\n        self._curr_state = MachineState.START\n\n    def next_state(self, c):\n        # for each byte we get its class\n        # if it is first byte, we also get byte length\n        byte_class = self._model[\"class_table\"][c]\n        if self._curr_state == MachineState.START:\n            self._curr_byte_pos = 0\n            self._curr_char_len = self._model[\"char_len_table\"][byte_class]\n        # from byte's class and state_table, we get its next state\n        curr_state = self._curr_state * self._model[\"class_factor\"] + byte_class\n        self._curr_state = self._model[\"state_table\"][curr_state]\n        self._curr_byte_pos += 1\n        return self._curr_state\n\n    def get_current_charlen(self):\n        return self._curr_char_len\n\n    def get_coding_state_machine(self):\n        return self._model[\"name\"]\n\n    @property\n    def language(self):\n        return self._model[\"language\"]"}
{"Repository": "lightning-bolts", "input": "Weight update rule from Bootstrap Your Own Latent (BYOL). className BYOLMAWeightUpdate(Callback) Method __init__ Method update_tau Method update_weights", "label": "class BYOLMAWeightUpdate(Callback):\n    def __init__(self, initial_tau: float = 0.996) -> None:\n        if not 0.0 <= initial_tau <= 1.0:\n            raise ValueError(f\"initial tau should be between 0 and 1 instead of {initial_tau}.\")\n\n        super().__init__()\n        self.initial_tau = initial_tau\n        self.current_tau = initial_tau\n\n    def on_train_batch_end(\n        self,\n        trainer: Trainer,\n        pl_module: LightningModule,\n        outputs: Sequence,\n        batch: Sequence,\n        batch_idx: int,\n    ) -> None:\n        # get networks\n        online_net = pl_module.online_network\n        target_net = pl_module.target_network\n\n        # update target network weights\n        self.update_weights(online_net, target_net)\n\n        # update tau after\n        self.update_tau(pl_module, trainer)\n\n    def update_tau(self, pl_module: LightningModule, trainer: Trainer) -> None:\n        max_steps = len(trainer.train_dataloader) * trainer.max_epochs\n        self.current_tau = 1 - (1 - self.initial_tau) * (math.cos(math.pi * pl_module.global_step / max_steps) + 1) / 2\n\n    def update_weights(self, online_net: Union[nn.Module, Tensor], target_net: Union[nn.Module, Tensor]) -> None:\n        for online_p, target_p in zip(online_net.parameters(), target_net.parameters()):\n            target_p.data = self.current_tau * target_p.data + (1.0 - self.current_tau) * online_p.data"}
{"Repository": "learning-to-learn", "input": "Adam algorithm (https://arxiv.org/pdf/1412.6980v8.pdf). className Adam(Network) Method _build Method initial_state_for_inputs", "label": "class Adam(Network):\n  def __init__(self, learning_rate=1e-3, beta1=0.9, beta2=0.999, epsilon=1e-8,\n               name=\"adam\"):\n    super(Adam, self).__init__(name=name)\n    self._learning_rate = learning_rate\n    self._beta1 = beta1\n    self._beta2 = beta2\n    self._epsilon = epsilon\n\n  def _build(self, g, prev_state):\n    b1 = self._beta1\n    b2 = self._beta2\n\n    g_shape = g.get_shape().as_list()\n    g = tf.reshape(g, (-1, 1))\n\n    t, m, v = prev_state\n\n    t_next = t + 1\n\n    m_next = _update_adam_estimate(m, g, b1)\n    m_hat = _debias_adam_estimate(m_next, b1, t_next)\n\n    v_next = _update_adam_estimate(v, tf.square(g), b2)\n    v_hat = _debias_adam_estimate(v_next, b2, t_next)\n\n    update = -self._learning_rate * m_hat / (tf.sqrt(v_hat) + self._epsilon)\n    return tf.reshape(update, g_shape), (t_next, m_next, v_next)\n\n  def initial_state_for_inputs(self, inputs, dtype=tf.float32, **kwargs):\n    batch_size = int(np.prod(inputs.get_shape().as_list()))\n    t = tf.zeros((), dtype=dtype)\n    m = tf.zeros((batch_size, 1), dtype=dtype)\n    v = tf.zeros((batch_size, 1), dtype=dtype)\n    return (t, m, v)"}
{"Repository": "sqlfmt", "input": "Abstract class for a SQL dialect. className Dialect(ABC) Method get_rules Method initialize_analyzer", "label": "class Dialect(ABC):\n    RULES: List[Rule]\n    case_sensitive_names = False\n\n    @abstractmethod\n    def get_rules(self) -> List[Rule]:\n        return sorted(self.RULES, key=lambda rule: rule.priority)\n\n    def initialize_analyzer(self, line_length: int) -> Analyzer:\n        analyzer = Analyzer(\n            line_length=line_length,\n            rules=self.get_rules(),\n            node_manager=NodeManager(self.case_sensitive_names),\n        )\n        return analyzer"}
{"Repository": "yark", "input": "Invalid timestamp inputted for note className TimestampException(Exception) Method __init__", "label": "class TimestampException(Exception):\n    def __init__(self, *args: object) -> None:\n        super().__init__(*args)"}
{"Repository": "TextClassificationBenchmark", "input": "Translates index lookups into attribute lookups. className AttrProxy(object) Method __init__ Method __getitem__ Attribute module Attribute prefix", "label": "class AttrProxy(object):\n    def __init__(self, module, prefix):\n        self.module = module\n        self.prefix = prefix\n\n    def __getitem__(self, i):\n        return getattr(self.module, self.prefix + str(i))"}
{"Repository": "HexViewer", "input": "Checksum. className Checksum(object) Method __init__ Method update Method threaded_update Method chunk_thread Method reset_thread Method display Attribute hash Attribute name", "label": "class Checksum(object):\n    thread = None\n\n\n\n    def __init__(self, hash_algorithm=None, data=b\"\"):\n        if hash_algorithm is None or hash_algorithm not in VALID_HASH:\n\n            hash_algorithm = common.hv_settings(\"hash_algorithm\", DEFAULT_CHECKSUM)\n\n        if hash_algorithm not in VALID_HASH:\n\n            hash_algorithm = DEFAULT_CHECKSUM\n\n        self.hash = getattr(hashlib, hash_algorithm)(data)\n\n        self.name = hash_algorithm\n\n\n\n    def update(self, data=\"\"):\n        if isinstance(data, str):\n\n            self.hash.update(data)\n\n\n\n    def threaded_update(self, data_buffer=None, fmt_callback=None, count=None):\n        global active_thread\n\n        if data_buffer is None:\n\n            data_buffer = []\n\n        self.thread = HashThread(data_buffer, self.hash, fmt_callback, count)\n\n        self.thread.start()\n\n        self.chunk_thread()\n\n        active_thread = self.thread\n\n\n\n    def chunk_thread(self):\n        ratio = float(self.thread.chunk) / float(self.thread.chunks)\n\n        percent = int(ratio * 10)\n\n        leftover = 10 - percent\n\n        message = \"[\" + \"-\" * percent + \">\" + \"-\" * leftover + (\"] %3d%%\" % int(ratio * 100)) + \" chunks hashed\"\n\n        sublime.status_message(message)\n\n        if not self.thread.is_alive():\n\n            if self.thread.abort is True:\n\n                notify(\"Hash calculation aborted!\")\n\n                sublime.set_timeout(self.reset_thread, 500)\n\n            else:\n\n                sublime.set_timeout(self.display, 500)\n\n        else:\n\n            sublime.set_timeout(self.chunk_thread, 500)\n\n\n\n    def reset_thread(self):\n        self.thread = None\n\n\n\n    def display(self, window=None):\n        if window is None:\n\n            window = sublime.active_window()\n\n        if common.use_hex_lowercase():\n\n            digest = str(self.hash.hexdigest())\n\n        else:\n\n            digest = str(self.hash.hexdigest()).upper()\n\n        window.show_input_panel(self.name + \":\", digest, None, None, None)"}
{"Repository": "audiostream", "input": "A data source for float32 mono binary data, as loaded by libROSA/soundfile. className MonoAmplitudeSource(ThreadSource) Method __init__ Method get_bytes Attribute chunksize Attribute data Attribute cursor", "label": "class MonoAmplitudeSource(ThreadSource):\n    def __init__(self, stream, data, *args, **kwargs):\n        super().__init__(stream, *args, **kwargs)\n        self.chunksize = kwargs.get('chunksize', 64)\n        self.data = data\n        self.cursor = 0\n\n    def get_bytes(self):\n        chunk = self.data[self.cursor:self.cursor+self.chunksize]\n        self.cursor += self.chunksize\n\n        if not isinstance(chunk, np.ndarray):\n            chunk = np.array(chunk)\n        assert len(chunk.shape) == 1 and chunk.dtype == np.dtype('float32')\n\n        # Convert to 16 bit format.\n        return (chunk * 2**15).astype('int16').tobytes()"}
{"Repository": "MCNS", "input": "Aggregates via max-pooling over MLP functions. className MaxPoolingAggregator(Layer) Method _call", "label": "class MaxPoolingAggregator(Layer):\n    def __init__(self, input_dim, output_dim, model_size=\"small\", neigh_input_dim=None,\n            dropout=0., bias=False, act=tf.nn.relu, name=None, concat=False, **kwargs):\n        super(MaxPoolingAggregator, self).__init__(**kwargs)\n\n        self.dropout = dropout\n        self.bias = bias\n        self.act = act\n        self.concat = concat\n\n        if neigh_input_dim is None:\n            neigh_input_dim = input_dim\n\n        if name is not None:\n            name = '/' + name\n        else:\n            name = ''\n\n        if model_size == \"small\":\n            hidden_dim = self.hidden_dim = 512\n        elif model_size == \"big\":\n            hidden_dim = self.hidden_dim = 1024\n\n        self.mlp_layers = []\n        self.mlp_layers.append(Dense(input_dim=neigh_input_dim,\n                                 output_dim=hidden_dim,\n                                 act=tf.nn.relu,\n                                 dropout=dropout,\n                                 sparse_inputs=False,\n                                 logging=self.logging))\n\n        with tf.variable_scope(self.name + name + '_vars'):\n            self.vars['neigh_weights'] = glorot([hidden_dim, output_dim],\n                                                        name='neigh_weights')\n           \n            self.vars['self_weights'] = glorot([input_dim, output_dim],\n                                                        name='self_weights')\n            if self.bias:\n                self.vars['bias'] = zeros([self.output_dim], name='bias')\n\n        if self.logging:\n            self._log_vars()\n\n        self.input_dim = input_dim\n        self.output_dim = output_dim\n        self.neigh_input_dim = neigh_input_dim\n\n    def _call(self, inputs):\n        self_vecs, neigh_vecs = inputs\n        neigh_h = neigh_vecs\n\n        dims = tf.shape(neigh_h)\n        batch_size = dims[0]\n        num_neighbors = dims[1]\n        # [nodes * sampled neighbors] x [hidden_dim]\n        h_reshaped = tf.reshape(neigh_h, (batch_size * num_neighbors, self.neigh_input_dim))\n\n        for l in self.mlp_layers:\n            h_reshaped = l(h_reshaped)\n        neigh_h = tf.reshape(h_reshaped, (batch_size, num_neighbors, self.hidden_dim))\n        neigh_h = tf.reduce_max(neigh_h, axis=1)\n        \n        from_neighs = tf.matmul(neigh_h, self.vars['neigh_weights'])\n        from_self = tf.matmul(self_vecs, self.vars[\"self_weights\"])\n        \n        if not self.concat:\n            output = tf.add_n([from_self, from_neighs])\n        else:\n            output = tf.concat([from_self, from_neighs], axis=1)\n\n        # bias\n        if self.bias:\n            output += self.vars['bias']\n       \n        return self.act(output)"}
{"Repository": "UCL_LIU", "input": "Class that represents a single 'keyboard' action It represents either a PAUSE action (not really keyboard) or a keyboard action (press or release or both) of a particular key. className KeyAction(object) Method __init__ Method _get_key_info Method GetInput Method Run Method _get_down_up_string Method key_description Method __str__ Attribute key Attribute key Attribute down Attribute up", "label": "class KeyAction(object):\n    def __init__(self, key, down = True, up = True):\n        self.key = key\n        if isinstance(self.key, str_class):\n            self.key = enforce_unicode(key)\n        self.down = down\n        self.up = up\n\n    def _get_key_info(self):\n        return 0, ord(self.key), KEYEVENTF_UNICODE\n\n    def GetInput(self):\n        \"Build the INPUT structure for the action\"\n        actions = 1\n        # if both up and down\n        if self.up and self.down:\n            actions = 2\n\n        inputs = (INPUT * actions)()\n\n        vk, scan, flags = self._get_key_info()\n\n        for inp in inputs:\n            inp.type = INPUT_KEYBOARD\n\n            inp._.ki.wVk = vk\n            inp._.ki.wScan = scan\n            inp._.ki.dwFlags |= flags\n\n        # if we are releasing - then let it up\n        if self.up:\n            inputs[-1]._.ki.dwFlags |= KEYEVENTF_KEYUP\n\n        return inputs\n\n    def Run(self):\n        \"Execute the action\"\n        inputs = self.GetInput()\n        return SendInput(\n            len(inputs),\n            ctypes.byref(inputs),\n            ctypes.sizeof(INPUT))\n\n    def _get_down_up_string(self):\n        down_up = \"\"\n        if not (self.down and self.up):\n            if self.down:\n                down_up = \"down\"\n            elif self.up:\n                down_up = \"up\"\n        return down_up\n    \n    def key_description(self):\n        \"Return a description of the key\"\n        vk, scan, flags = self._get_key_info()\n        desc = ''\n        if vk:\n            if vk in CODE_NAMES:\n                desc = CODE_NAMES[vk]\n            else:\n                desc = \"VK %d\"% vk\n        else:\n            desc = \"%s\"% self.key\n        \n        return desc\n\n    def __str__(self):\n        parts = []\n        parts.append(self.key_description())\n        up_down = self._get_down_up_string()\n        if up_down:\n            parts.append(up_down)\n\n        return \"<%s>\"% (\" \".join(parts))\n    __repr__ = __str__"}
{"Repository": "EpubToPdf", "input": "This class contains the methods needed to get the files, to help make the pdf file. className GetEngine(object) Method __init__ Method get_html Method get_pdf Method get_css Method get_images Method get_all Attribute html_files Attribute css_files Attribute image_files Attribute directory Attribute files Attribute pdf_files", "label": "class GetEngine(object):\n\tdef __init__(self, directory):\n\t\tself.html_files = []\n\t\tself.css_files = []\n\t\tself.image_files = []\n\t\tself.directory = directory\n\t\tself.files = []\n\t\tself.pdf_files = []\n\n\tdef get_html(self):\n\n\t\tfor file in self.files:\n\t\t\tif file.endswith(\".xhtml\") or file.endswith(\".html\"):\n\t\t\t\tself.html_files.append(file)\n\n\tdef get_pdf(self):\n\n\t\tfor file in self.html_files:\n\t\t\tself.pdf_files.append(\"{}.pdf\".format(self.html_files.index(file)))\n\n\tdef get_css(self):\n\n\t\tfor file in self.files:\n\t\t\tif file.endswith(\".css\"):\n\t\t\t\tself.css_files.append(file)\n\n\tdef get_images(self):\n\n\t\tfor file in self.files:\n\t\t\tif file.endswith((\".png\", \".jpg\", \".gif\")):\n\t\t\t\tself.image_files.append(file)\n\n\tdef get_all(self):\n\t\tfile = None\n\t\tdirectory_paths = []\n\t\tfor root, dirs, files in os.walk(self.directory):\n\t\t\t#This traverses the directory passed in as an argument,\n\t\t\t#returns the current directory, the sub directories and all the files\n\t\t\tdirectory_paths.append(root)\n\t\t\tif file:\n\t\t\t\tcontinue\n\t\t\tfor each in files:\n\t\t\t\tif each.endswith(\".opf\"):\n\t\t\t\t\tfile = os.path.join(root, each)\n\t\t\t\t\tcontinue\n\t\tif not file:\n\t\t\treturn\n\n\t\txml_content = open(file, \"r\").read()\n\n\t\txml_tree = bs(xml_content, features = \"xml\")\n\n\t\tfile_names = xml_tree.package.manifest.findAll('item')\n\n\t\t# Gets the name of all the documents in order\n\t\t# from the opf file, then saves the file name with its path\n\t\t# The file path in the opf file can't be relied upon\n\t\t# Hence, the need to extract file name and get its path\n\n\t\tfor file in file_names:\n\t\t\tfile_path_match = re.match(r'.+\\.[a-zA-Z]+', file.get('href', ''))\n\t\t\tif not file_path_match:\n\t\t\t\tcontinue\n\t\t\tfile_name = ntpath.basename(file_path_match.group())\n\t\t\tfor path in directory_paths:\n\t\t\t\tfilepath = path + '/' + file_name\n\t\t\t\tif os.path.exists(filepath):\n\t\t\t\t\tself.files.append(filepath)"}
{"Repository": "lennoxs30", "input": "Represents Lennox Outdoor Unit className S30OutdoorUnit(Device) Method unique_name Method device_model Method register_device", "label": "class S30OutdoorUnit(Device):\n    def __init__(\n        self,\n        hass: HomeAssistant,\n        config_entry: ConfigEntry,\n        system: lennox_system,\n        s30_device: S30ControllerDevice,\n    ):\n        super().__init__(system.get_outdoor_unit_equipment())\n        self._hass = hass\n        self._system = system\n        self._config_entry = config_entry\n        self._s30_controller_device: S30ControllerDevice = s30_device\n\n    @property\n    def unique_name(self) -> str:\n        return self._system.unique_id + \"_ou\"\n\n    @property\n    def device_model(self):\n        if self.equipment is not None:\n            return self.equipment.unit_model_number\n        return self._system.outdoorUnitType\n\n    def register_device(self):\n        device_registry = dr.async_get(self._hass)\n        if self.equipment is not None and self.equipment.equipment_type_name is not None:\n            name = f\"{self._system.name} {self.equipment.equipment_type_name}\"\n        elif self._system.outdoorUnitType is not None:\n            name = f\"{self._system.name} {self._system.outdoorUnitType}\"\n        else:\n            name = f\"{self._system.name} outdoor unit\"\n\n        device_registry.async_get_or_create(\n            config_entry_id=self._config_entry.entry_id,\n            identifiers={(LENNOX_DOMAIN, self.unique_name)},\n            manufacturer=LENNOX_MFG,\n            suggested_area=\"outside\",\n            name=name,\n            model=self.device_model,\n            hw_version=self.hw_version,\n            via_device=(LENNOX_DOMAIN, self._s30_controller_device.unique_name),\n        )"}
{"Repository": "pyshaders", "input": "Represent a shader object. className ShaderObject(object) Method __init__ Method __alloc Method vertex Method fragment Method source Method source Method logs Method compile Method valid Method __bool__ Method __eq__ Method __repr__ Method __del__ Attribute sid Attribute owned", "label": "class ShaderObject(object):\n    __slots__ = ['sid', 'owned']\n    \n    type = GetShaderObject(GL_SHADER_TYPE)    \n    delete_status = GetShaderObject(GL_DELETE_STATUS)\n    compiled = GetShaderObject(GL_COMPILE_STATUS) \n    log_length = GetShaderObject(GL_INFO_LOG_LENGTH)\n    source_length = GetShaderObject(GL_SHADER_SOURCE_LENGTH)\n    \n    def __init__(self, shader_id, owned=False):\n        self.sid = c_uint(getattr(shader_id, 'value', shader_id))\n        self.owned = owned\n    \n    @staticmethod\n    def __alloc(cls, shader_type): \n        sobj = super().__new__(cls)\n        sobj.sid = c_uint(glCreateShader(shader_type))\n        sobj.owned = True\n        return sobj\n    \n    @classmethod\n    def vertex(cls):\n        return ShaderObject.__alloc(cls, GL_VERTEX_SHADER)\n        \n    @classmethod\n    def fragment(cls):\n        return ShaderObject.__alloc(cls, GL_FRAGMENT_SHADER)\n        \n    @property\n    def source(self):\n        src = read_opengl_array(self.sid, self.source_length, glGetShaderSource)\n        return bytes(src).decode('UTF-8')\n        \n    @source.setter\n    def source(self, src):\n        glShaderSource(self.sid, 1, shader_source(src.encode('UTF-8')), null_c_int)\n        \n    @property\n    def logs(self):\n        \" Return the compilation log of the shader \"   \n        logs = read_opengl_array(self.sid, self.log_length, glGetShaderInfoLog)   \n        return bytes(logs).decode('UTF-8')\n        \n    def compile(self):\n        glCompileShader(self.sid)\n        return self.compiled == GL_TRUE\n        \n    def valid(self):\n        return glIsShader(self.sid) == GL_TRUE\n\n    def __bool__(self):\n        return self.valid()\n\n    def __eq__(self, other):\n        if not isinstance(other, ShaderObject):\n            return False\n        \n        return self.sid.value == other.sid.value\n        \n    def __repr__(self):\n        return \"ShaderObject {}\".format(self.sid)\n        \n    def __del__(self):\n        if self.owned and self.valid():\n            glDeleteShader(self.sid)"}
{"Repository": "example-code-2e", "input": "SlowHTTPRequestHandler adds delays and errors to test HTTP clients. className SlowHTTPRequestHandler(SimpleHTTPRequestHandler) Method __init__ Method do_GET Attribute error_rate", "label": "class SlowHTTPRequestHandler(SimpleHTTPRequestHandler):\n    def __init__(self, *args, error_rate=0.0, **kwargs):\n        self.error_rate = error_rate\n        super().__init__(*args, **kwargs)\n\n    def do_GET(self):\n        delay = uniform(MIN_DELAY, MAX_DELAY)\n        cc = self.path[-6:-4].upper()\n        print(f'{cc} delay: {delay:0.2}s')\n        time.sleep(delay)\n        if random() < self.error_rate:\n            # HTTPStatus.IM_A_TEAPOT requires Python >= 3.9\n            try:\n                self.send_error(HTTPStatus.IM_A_TEAPOT, \"I'm a Teapot\")\n            except BrokenPipeError as exc:\n                print(f'{cc} *** BrokenPipeError: client closed')\n        else:\n            f = self.send_head()\n            if f:\n                try:\n                    self.copyfile(f, self.wfile)\n                except BrokenPipeError as exc:\n                    print(f'{cc} *** BrokenPipeError: client closed')\n                finally:\n                    f.close()"}
{"Repository": "sparrow", "input": "Provides a simplified RPC-style interface for HTTP requests. className HttpRpcServer(AbstractRpcServer) Method _Authenticate Method _GetOpener", "label": "class HttpRpcServer(AbstractRpcServer):\n  def _Authenticate(self):\n    super(HttpRpcServer, self)._Authenticate()\n    if self.save_cookies:\n      StatusUpdate(\"Saving authentication cookies to %s\" % self.cookie_file)\n      self.cookie_jar.save()\n\n  def _GetOpener(self):\n    opener = urllib2.OpenerDirector()\n    opener.add_handler(urllib2.ProxyHandler())\n    opener.add_handler(urllib2.UnknownHandler())\n    opener.add_handler(urllib2.HTTPHandler())\n    opener.add_handler(urllib2.HTTPDefaultErrorHandler())\n    opener.add_handler(urllib2.HTTPSHandler())\n    opener.add_handler(urllib2.HTTPErrorProcessor())\n    if self.save_cookies:\n      self.cookie_file = os.path.expanduser(\"~/.codereview_upload_cookies\")\n      self.cookie_jar = cookielib.MozillaCookieJar(self.cookie_file)\n      if os.path.exists(self.cookie_file):\n        try:\n          self.cookie_jar.load()\n          self.authenticated = True\n          StatusUpdate(\"Loaded authentication cookies from %s\" %\n                       self.cookie_file)\n        except (cookielib.LoadError, IOError):\n          # Failed to load cookies - just ignore them.\n          pass\n      else:\n        # Create an empty cookie file with mode 600\n        fd = os.open(self.cookie_file, os.O_CREAT, 0600)\n        os.close(fd)\n      # Always chmod the cookie file\n      os.chmod(self.cookie_file, 0600)\n    else:\n      # Don't save cookies across runs of update.py.\n      self.cookie_jar = cookielib.CookieJar()\n    opener.add_handler(urllib2.HTTPCookieProcessor(self.cookie_jar))\n    return opener"}
{"Repository": "django-booking", "input": "Custom authentication backend that allows login via email and booking ID. className BookingIDBackend(ModelBackend) Method authenticate", "label": "class BookingIDBackend(ModelBackend):\n    def authenticate(self, username=None, password=None, **kwargs):\n        try:\n            booking = Booking.objects.get(\n                booking_id=password, user__email=username)\n        except Booking.DoesNotExist:\n            return None\n        return booking.user"}
{"Repository": "hobbit-core", "input": "EnumSetMeta is a metaclass that can be used to auto generate load and dump func for EnumField. className EnumSetMeta(SQLAlchemyAutoSchemaMeta) Method gen_func Method wrapper Method __new__", "label": "class EnumSetMeta(SQLAlchemyAutoSchemaMeta):\n    @classmethod\n    def gen_func(cls, decorator, field_name, enum, verbose=True):\n\n        @decorator\n        def wrapper(self, data, many, **kwargs):\n            if data.get(field_name) is None:\n                return data\n\n            if decorator is pre_load:\n                data[field_name] = enum.load(data[field_name])\n            elif decorator is post_dump:\n                data[field_name] = enum.dump(data[field_name], verbose)\n            else:\n                raise Exception(\n                    'hobbit_core: decorator `{}` not support'.format(\n                        decorator))\n\n            return data\n        return wrapper\n\n    def __new__(cls, name, bases, attrs):\n        schema = SQLAlchemyAutoSchemaMeta.__new__(\n            cls, name, tuple(bases), attrs)\n        verbose = getattr(schema.Meta, 'verbose', True)\n\n        setattr(schema.Meta, 'dateformat', '%Y-%m-%d %H:%M:%S')\n\n        for field_name, declared in schema._declared_fields.items():\n            if not isinstance(declared, EnumField):\n                continue\n\n            setattr(schema, 'load_{}'.format(field_name), cls.gen_func(\n                pre_load, field_name, declared.enum))\n            setattr(schema, 'dump_{}'.format(field_name), cls.gen_func(\n                post_dump, field_name, declared.enum, verbose=verbose))\n\n        return schema"}
{"Repository": "napalm-logs", "input": "Memory buffer class. className MemoryBuffer(object) Method __init__ Method __setitem__ Method __contains__ Method __getitem__ Method items Attribute expire_time Attribute expire_time_delta Attribute _cache", "label": "class MemoryBuffer(object):\n    def __init__(self, expire_time, **kwargs):\n        self.expire_time = expire_time\n        self.expire_time_delta = datetime.timedelta(0, expire_time, 0)\n        # expire_time is assumed to be in seconds\n        self._cache = {}\n\n    def __setitem__(self, key, val):\n        self._cache[key] = {\"data\": val, \"timestamp\": datetime.datetime.utcnow()}\n\n    def __contains__(self, key):\n        return True if key in self._cache else False\n\n    def __getitem__(self, key):\n        try:\n            item = self._cache[key]\n        except KeyError:\n            return None\n        if datetime.datetime.utcnow() - item[\"timestamp\"] < self.expire_time_delta:\n            return item[\"data\"]\n        else:\n            del self._cache[key]\n            return None\n\n    def items(self):\n        keys = list(self._cache)\n        for key in keys:\n            val = self[key]\n            if val:\n                yield key"}
{"Repository": "language-quantized-autoencoders", "input": "A convenient stateful Jax RNG wrapper. Can be used to wrap RNG inside pure function. className JaxRNG(object) Method from_seed Method __init__ Method __call__ Attribute rng", "label": "class JaxRNG(object):\n    @classmethod\n    def from_seed(cls, seed):\n        return cls(jax.random.PRNGKey(seed))\n\n    def __init__(self, rng):\n        self.rng = rng\n\n    def __call__(self, keys=None):\n        if keys is None:\n            self.rng, split_rng = jax.random.split(self.rng)\n            return split_rng\n        elif isinstance(keys, int):\n            split_rngs = jax.random.split(self.rng, num=keys + 1)\n            self.rng = split_rngs[0]\n            return tuple(split_rngs[1:])\n        else:\n            split_rngs = jax.random.split(self.rng, num=len(keys) + 1)\n            self.rng = split_rngs[0]\n            return {key: val for key, val in zip(keys, split_rngs[1:])}"}
{"Repository": "pybindgen", "input": "generates a getter, for use in a PyGetSetDef table className PyGetter(ForwardWrapperBase) Method generate Method generate_call", "label": "class PyGetter(ForwardWrapperBase):\n    def generate(self, code_sink):\n        raise NotImplementedError\n    def generate_call(self):\n        raise AssertionError"}
{"Repository": "Traffic-Rule-Violation-Detection-System", "input": "Abstract base class for box coder. className BoxCoder(object) Method code_size Method encode Method decode Method _encode Method _decode", "label": "class BoxCoder(object):\n  __metaclass__ = ABCMeta\n\n  @abstractproperty\n  def code_size(self):\n    pass\n\n  def encode(self, boxes, anchors):\n    with tf.name_scope('Encode'):\n      return self._encode(boxes, anchors)\n\n  def decode(self, rel_codes, anchors):\n    with tf.name_scope('Decode'):\n      return self._decode(rel_codes, anchors)\n\n  @abstractmethod\n  def _encode(self, boxes, anchors):\n    pass\n\n  @abstractmethod\n  def _decode(self, rel_codes, anchors):\n    pass"}
{"Repository": "pyflux", "input": "Inverse Gamma Distribution ---- This class contains methods relating to the inverse gamma distribution for time series. className InverseGamma(Family) Method __init__ Method logpdf Method pdf Attribute covariance_prior Attribute alpha Attribute beta", "label": "class InverseGamma(Family):\n    def __init__(self, alpha, beta, transform=None, **kwargs):\n        super(InverseGamma, self).__init__(transform)\n        self.covariance_prior = False\n        self.alpha = alpha\n        self.beta = beta\n\n    def logpdf(self, x):\n        if self.transform is not None:\n            x = self.transform(x)       \n        return (-self.alpha-1)*np.log(x) - (self.beta/float(x))\n\n    def pdf(self, x):\n        if self.transform is not None:\n            x = self.transform(x)               \n        return (x**(-self.alpha-1))*np.exp(-(self.beta/float(x)))"}
{"Repository": "ICLRec", "input": "Randomly mask k items given a sequence className Mask(object) Method __init__ Method __call__ Attribute gamma", "label": "class Mask(object):\n    def __init__(self, gamma=0.7):\n        self.gamma = gamma\n\n    def __call__(self, sequence):\n        # make a deep copy to avoid original sequence be modified\n        copied_sequence = copy.deepcopy(sequence)\n        mask_nums = int(self.gamma * len(copied_sequence))\n        mask = [0 for i in range(mask_nums)]\n        mask_idx = random.sample([i for i in range(len(copied_sequence))], k=mask_nums)\n        for idx, mask_value in zip(mask_idx, mask):\n            copied_sequence[idx] = mask_value\n        return copied_sequence"}
{"Repository": "Instance-Search", "input": "Background subtraction for region proposals. className BackgroundSubtraction(BaseLocalizer) Method __init__ Method reset Method detect Attribute fgbg Attribute scale Attribute top_proposal Attribute kernel", "label": "class BackgroundSubtraction(BaseLocalizer):\n    def __init__(self, gpus, _cfg):\n\n        os.environ['CUDA_VISIBLE_DEVICES'] = str(gpus)\n        BaseLocalizer.__init__(self, gpus, _cfg)\n        self.fgbg = cv2.bgsegm.createBackgroundSubtractorMOG()\n        self.scale = 0.25\n        self.top_proposal = self._cfg.INFERENCE.LOCALIZER_TOPK\n        self.kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (2, 2))\n\n    def reset(self):\n        self.fgbg = cv2.bgsegm.createBackgroundSubtractorMOG()\n\n    # pylint: disable=arguments-differ\n    def detect(self, img_bgr):\n        img_bgr = cv2.resize(img_bgr, dsize=(0, 0), fx=self.scale, fy=self.scale)\n        fgmask = self.fgbg.apply(img_bgr)\n        draw1 = cv2.threshold(fgmask, 25, 255, cv2.THRESH_BINARY)[1]\n        draw1 = cv2.dilate(draw1, self.kernel, iterations=1)\n        contours_m, _ = cv2.findContours(draw1.copy(),\\\n                cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n\n        result = []\n        for cnt in contours_m:\n            area = cv2.contourArea(cnt)\n            if area < 50:\n                continue\n            rects = cv2.boundingRect(cnt)\n            rects = list(rects)\n            rects[2] += rects[0]\n            rects[3] += rects[1]\n            fg_region = fgmask[rects[1]: rects[3], rects[0]: rects[2]].copy()\n            score = np.mean(fg_region) / 255.0\n            rects.append(score)\n            new_bbox = np.array(rects)\n            new_bbox[:4] = new_bbox[:4] / self.scale\n            result.append(new_bbox)\n        return np.array(result)[:self.top_proposal]"}
{"Repository": "opencompass", "input": "Check the language of the entire response. className ResponseLanguageChecker(Instruction) Method build_description Method get_instruction_args Method get_instruction_args_keys Method check_following", "label": "class ResponseLanguageChecker(Instruction):\n    def build_description(self, *, language=None):\n        self._language = language\n        if self._language is None:\n            self._language = random.choice(list(_LANGUAGES.keys()))\n        # TODO(tianjianlu): opens the description generation to more choices.\n        self._description_pattern = (\n            'Your ENTIRE response should be in {language} language, no other '\n            + 'language is allowed.')\n        return self._description_pattern.format(\n            language=_LANGUAGES[self._language])\n\n    def get_instruction_args(self):\n        return {'language': self._language}\n\n    def get_instruction_args_keys(self):\n        return ['language']\n\n    def check_following(self, value):\n        assert isinstance(value, str)\n\n        try:\n            return langdetect.detect(value) == self._language\n        except langdetect.LangDetectException as e:\n            # Count as instruction is followed.\n            logging.error('Unable to detect language for text %s due to %s',\n                          value, e)  # refex: disable=pytotw.037\n            return True"}
{"Repository": "pvse", "input": "Composes several transforms together. className Compose(object) Method __init__ Method __call__ Method __repr__ Attribute transforms", "label": "class Compose(object):\n    def __init__(self, transforms):\n        self.transforms = transforms\n\n    def __call__(self, img):\n        for t in self.transforms:\n            img = t(img)\n        return img\n\n    def __repr__(self):\n        format_string = self.__class__.__name__ + '('\n        for t in self.transforms:\n            format_string += '\\n'\n            format_string += '    {0}'.format(t)\n        format_string += '\\n)'\n        return format_string"}
{"Repository": "MTFL-For-Personalised-DNNs", "input": "Optimizer to use for pFedMe simulations. className pFedMeOptimizer(Optimizer) Method __init__ Method step Attribute device", "label": "class pFedMeOptimizer(Optimizer):\n    def __init__(self, params, device, lr=0.01, lamda=0.1 , mu = 0.001):\n        if lr < 0.0:\n\n            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n\n        \n\n        defaults = dict(lr=lr, lamda=lamda, mu = mu)\n\n        super(pFedMeOptimizer, self).__init__(params, defaults)\n\n        self.device = device\n\n    \n\n    def step(self, omega, closure=None):\n        loss = None\n\n        if closure is not None:\n\n            loss = closure\n\n        \n\n        # apply pFedMe update rule \n\n        for group in self.param_groups:\n\n            for p, localweight in zip( group['params'], omega):\n\n                w = torch.tensor(localweight).to(self.device)\n\n                p.data = p.data - group['lr'] * (  p.grad.data \n\n                                                 + group['lamda'] * (p.data - w)\n\n                                                 + group['mu'] * p.data)\n\n        \n\n        return  group['params'], loss"}
{"Repository": "dara", "input": "Any component that's primary role is to aid in laying out a document should inherit from this class. className LayoutComponent(BaseDashboardComponent) Method append Method pop", "label": "class LayoutComponent(BaseDashboardComponent):\n    position: str = 'relative'\n    justify: Optional[JustifyContent] = None\n    align: Optional[AlignItems] = None\n\n    def append(self, component: ComponentInstance):\n        if isinstance(component, ComponentInstance) is False:\n            name = self.__class__.__name__\n            raise TypeError(f'You may only append other components to a {name} component. Not: {component}')\n        self.children.append(component)   # type: ignore\n\n    def pop(self):\n        if len(self.children) == 0:   # type: ignore\n            raise IndexError(f'{self.__class__.__name__} is empty')\n        return self.children.pop()   # type: ignore"}
{"Repository": "openvasreporting", "input": "Host information className Host(object) Method __init__ Method addvulnerability Method nv_total Method __eq__ Attribute ip Attribute host_name Attribute num_vulns Attribute nv Attribute sum_cvss Attribute higher_cvss Attribute vuln_list", "label": "class Host(object):\n    def __init__(self, ip:str, host_name:str=\"\"):\n        if not isinstance(ip, str):\n            raise TypeError(\"Expected basestring, got '{}' instead\".format(type(ip)))\n        if not isinstance(host_name, str):\n            raise TypeError(\"Expected basestring, got '{}' instead\".format(type(host_name)))\n \n        self.ip = ip\n        self.host_name = host_name\n        self.num_vulns = 0\n        self.nv = {'critical': 0,\n                   'high': 0,\n                   'medium': 0,\n                   'low': 0,\n                   'none': 0\n                  }\n        self.sum_cvss = 0\n        self.higher_cvss = 0\n        self.vuln_list = []\n \n    def addvulnerability(self, parsed_vuln: ParseVulnerability):\n        if not isinstance(parsed_vuln, ParseVulnerability):\n            raise TypeError(\"Expected ParseVulnerability, got '{}' instead\".format(type(parsed_vuln)))\n\n        # check if a vulnerability with the same vuln_id (nvt-oid) already exists in the vuln_list\n        for v in self.vuln_list:\n            if v.vuln_id == parsed_vuln.vuln_id:\n                return\n            \n        v = Vulnerability(parsed_vuln.vuln_id,\n                          name=parsed_vuln.vuln_name,\n                          version=parsed_vuln.vuln_version,\n                          threat=parsed_vuln.vuln_threat,\n                          tags=parsed_vuln.vuln_tags,\n                          cvss=parsed_vuln.vuln_cvss,\n                          cves=parsed_vuln.vuln_cves,\n                          references=parsed_vuln.vuln_references,\n                          family=parsed_vuln.vuln_family,\n                          level=parsed_vuln.vuln_level)\n        try:\n            # added results to port function as will ne unique per port on each host.\n            port = Port.string2port(parsed_vuln.vuln_port, parsed_vuln.vuln_result)\n        except ValueError:\n            port = Port(\"\", \"\", \"\")\n        v.add_vuln_host(self, port)\n        self.vuln_list.append(v)\n        self.num_vulns += 1\n        self.nv[v.level] += 1\n        self.sum_cvss += v.cvss\n        if v.cvss > self.higher_cvss:\n            self.higher_cvss = v.cvss\n    \n    def nv_total(self):\n        return self.nv['critical'] + self.nv['high'] + self.nv['medium'] + self.nv['low']\n                   \n    def __eq__(self, other:'Host'):\n        return (\n                isinstance(other, Host) and\n                other.ip == self.ip and\n                other.host_name == self.host_name\n        )"}
{"Repository": "scratchattach", "input": "Class that calls events on Scratch or Turbowarp cloud variable updates. Data fetched using the provided CloudConnection or TwCloudConnection object. className WsCloudEvents(CloudEvents) Method __init__ Method _update Attribute data Attribute _thread Attribute running Attribute _events Attribute connection", "label": "class WsCloudEvents(CloudEvents):\n    def __init__(self, project_id, connection, **entries):\n        self.data = []\n        self._thread = None\n        self.running = False\n        self._events = {}\n        self.connection = connection\n        self.__dict__.update(entries)\n\n    def _update(self):\n        if isinstance(self.connection, CloudConnection):\n            log_data = get_cloud(project_id = self.connection.project_id)\n        else:\n            log_data = {}\n        while True:\n            try:\n                data = self.connection.websocket.recv().split('\\n')\n                result = []\n                for i in data:\n                    try:\n                        result.append(json.loads(i))\n                    except Exception:\n                        pass\n                for activity in result:\n                    if \"on_\"+activity[\"method\"] in self._events:\n                        self._events[\"on_\"+activity[\"method\"]](self.Event(user=None, var=activity[\"name\"][2:], name=activity[\"name\"][2:], value=activity[\"value\"], timestamp=time.time()*10000))\n            except Exception:\n                try:\n                    self.connection._connect(cloud_host=self.connection.cloud_host)\n                    self.connection._handshake()\n                    if isinstance(self.connection, CloudConnection):\n                        log_data = get_cloud(project_id = self.connection.project_id)\n                    else:\n                        log_data = {}\n                except Exception:\n                    if \"on_disconnect\" in self._events:\n                        self._events[\"on_disconnect\"]()"}
{"Repository": "python-fuse", "input": "Auxiliary class which can be filled up stat attributes. className Stat(FuseStruct) Method __init__ Attribute st_mode Attribute st_ino Attribute st_dev Attribute st_nlink Attribute st_uid Attribute st_gid Attribute st_size Attribute st_atime Attribute st_mtime Attribute st_ctime", "label": "class Stat(FuseStruct):\n    def __init__(self, **kw):\n        self.st_mode  = None\n        self.st_ino   = 0\n        self.st_dev   = 0\n        self.st_nlink = None\n        self.st_uid   = 0\n        self.st_gid   = 0\n        self.st_size  = 0\n        self.st_atime = 0\n        self.st_mtime = 0\n        self.st_ctime = 0\n\n        FuseStruct.__init__(self, **kw)"}
{"Repository": "python-samples", "input": "Unit test for create Slide  snippet className TestCreateSlide(BaseTest) Method test_create_slide", "label": "class TestCreateSlide(BaseTest):\n  def test_create_slide(self):\n    presentation_id = self.create_test_presentation()\n    self.add_slides(presentation_id, 3)\n    page_id = \"my_page_id\"\n    response = slides_create_slide.create_slide(presentation_id, page_id)\n    self.assertEqual(\n        page_id,\n        response.get(\"replies\")[0].get(\"createSlide\").get(\"objectId\"),\n    )"}
{"Repository": "gps", "input": "Collection of iteration variables. className IterationData(BundleType) Method __init__", "label": "class IterationData(BundleType):\n    def __init__(self):\n        variables = {\n            'sample_list': None,  # List of samples for the current iteration.\n            'traj_info': None,  # Current TrajectoryInfo object.\n            'pol_info': None,  # Current PolicyInfo object.\n            'traj_distr': None,  # Initial trajectory distribution.\n            'new_traj_distr': None, # Updated trajectory distribution.\n            'cs': None,  # Sample costs of the current iteration.\n            'step_mult': 1.0,  # KL step multiplier for the current iteration.\n            'eta': 1.0,  # Dual variable used in LQR backward pass.\n        }\n        BundleType.__init__(self, variables)"}
{"Repository": "sverchok", "input": "Property of a node(s) inside a group tree was changed className GroupPropertyEvent(GroupTreeEvent) Method __init__ Attribute updated_nodes", "label": "class GroupPropertyEvent(GroupTreeEvent):\n    updated_nodes: Iterable[SvNode]\n\n    def __init__(self, tree, update_path, update_nodes):\n        super().__init__(tree, update_path)\n        self.updated_nodes = update_nodes"}
{"Repository": "gdscript-docs-maker", "input": "Represents a constant className Constant(Element) Method summarize Method from_dict", "label": "class Constant(Element):\n    type: str\n    default_value: str\n\n    def summarize(self) -> List[str]:\n        return [self.type, self.name]\n\n    @staticmethod\n    def from_dict(data: dict) -> \"Constant\":\n        return Constant(\n            data[\"signature\"],\n            data[\"name\"],\n            data[\"description\"],\n            data[\"data_type\"],\n            data[\"value\"],\n        )"}
{"Repository": "A-ViT", "input": "## Regularization loss $$L_{Reg} = \\mathop{KL} \\Big(p_n \\Vert p_G(\\lambda_p) \\Big)$$ $\\mathop{KL}$ is the [KullbackLeibler divergence](https://en. className RegularizationLoss() Method __init__ Method forward Attribute p_g Attribute p_g Attribute kl_div", "label": "class RegularizationLoss():\n    def __init__(self, lambda_p: float, max_steps: int = 12, args=None):\n        super().__init__()\n\n        # Empty vector to calculate $p_G(\\lambda_p)$\n        p_g = torch.zeros((max_steps,))\n        # $(1 - \\lambda_p)^k$\n        not_halted = 1.\n        # Iterate upto `max_steps`\n        for k in range(max_steps):\n            # $$Pr_{p_G(\\lambda_p)}(X = k) = (1 - \\lambda_p)^k \\lambda_p$$\n            p_g[k] = not_halted * lambda_p\n            # Update $(1 - \\lambda_p)^k$\n            not_halted = not_halted * (1 - lambda_p)\n\n        # Save $Pr_{p_G(\\lambda_p)}$\n        self.p_g = nn.Parameter(p_g, requires_grad=False).cuda()\n        self.p_g = self.p_g.expand(args.batch_size, max_steps).permute(1,0)\n\n        # KL-divergence loss\n        self.kl_div = nn.KLDivLoss(reduction='batchmean').cuda()\n\n\n    def forward(self, p):\n        p = torch.clamp(torch.stack(p), 0.01, 0.99)\n\n        return self.kl_div(p.log(), self.p_g)"}
{"Repository": "fonttools", "input": "Compile *. className cython_build_ext(_build_ext) Method finalize_options Method build_extensions", "label": "class cython_build_ext(_build_ext):\n    def finalize_options(self):\n        from Cython.Build import cythonize\n\n        # optionally enable line tracing for test coverage support\n        linetrace = os.environ.get(\"CYTHON_TRACE\") == \"1\"\n\n        self.distribution.ext_modules[:] = cythonize(\n            self.distribution.ext_modules,\n            force=linetrace or self.force,\n            annotate=os.environ.get(\"CYTHON_ANNOTATE\") == \"1\",\n            quiet=not self.verbose,\n            compiler_directives={\n                \"linetrace\": linetrace,\n                \"language_level\": 3,\n                \"embedsignature\": True,\n            },\n        )\n\n        _build_ext.finalize_options(self)\n\n    def build_extensions(self):\n        try:\n            _build_ext.build_extensions(self)\n        except Exception as e:\n            if with_cython:\n                raise\n            from distutils.errors import DistutilsModuleError\n\n            # optional compilation failed: we delete 'ext_modules' and make sure\n            # the generated wheel is 'pure'\n            del self.distribution.ext_modules[:]\n            try:\n                bdist_wheel = self.get_finalized_command(\"bdist_wheel\")\n            except DistutilsModuleError:\n                # 'bdist_wheel' command not available as wheel is not installed\n                pass\n            else:\n                bdist_wheel.root_is_pure = True\n            log.error(\"error: building extensions failed: %s\" % e)"}
{"Repository": "peepdf", "input": "Uncompresses a stream of lzw code points, as created by L{Encoder}. className Decoder(object) Method __init__ Method code_size Method decode Method _decode_codepoint Method _clear_codes Attribute remainder", "label": "class Decoder(object):\n    def __init__(self):\n       self._clear_codes()\n       self.remainder = []\n\n\n    def code_size(self):\n       return len(self._codepoints)\n\n\n    def decode(self, codepoints):\n        codepoints = [ cp for cp in codepoints ]\n\n        for cp in codepoints:\n            decoded = self._decode_codepoint(cp)\n            for character in decoded:\n                yield character\n\n\n\n    def _decode_codepoint(self, codepoint):\n        ret = \"\"\n\n        if codepoint == CLEAR_CODE:\n            self._clear_codes()\n        elif codepoint == END_OF_INFO_CODE:\n            pass\n            #raise ValueError(\"End of information code not supported directly by this Decoder\")\n        else:\n            if codepoint in self._codepoints:\n                ret = self._codepoints[ codepoint ]\n                if None != self._prefix:\n                    self._codepoints[ len(self._codepoints) ] = self._prefix + ret[0]\n\n            else:\n                ret = self._prefix + self._prefix[0]\n                self._codepoints[ len(self._codepoints) ] = ret\n\n            self._prefix = ret\n\n        return ret\n\n\n    def _clear_codes(self):\n        self._codepoints = dict( (pt, struct.pack(\"B\", pt)) for pt in range(256) )\n        self._codepoints[CLEAR_CODE] = CLEAR_CODE\n        self._codepoints[END_OF_INFO_CODE] = END_OF_INFO_CODE\n        self._prefix = None"}
{"Repository": "vedadet", "input": "Memcached storage backend. className MemcachedBackend(BaseStorageBackend) Method __init__ Method get Method get_text Attribute server_list_cfg Attribute client_cfg Attribute _client Attribute _mc_buffer", "label": "class MemcachedBackend(BaseStorageBackend):\n    def __init__(self, server_list_cfg, client_cfg, sys_path=None):\n        if sys_path is not None:\n            import sys\n            sys.path.append(sys_path)\n        try:\n            import mc\n        except ImportError:\n            raise ImportError(\n                'Please install memcached to enable MemcachedBackend.')\n\n        self.server_list_cfg = server_list_cfg\n        self.client_cfg = client_cfg\n        self._client = mc.MemcachedClient.GetInstance(self.server_list_cfg,\n                                                      self.client_cfg)\n        # mc.pyvector servers as a point which points to a memory cache\n        self._mc_buffer = mc.pyvector()\n\n    def get(self, filepath):\n        filepath = str(filepath)\n        import mc\n        self._client.Get(filepath, self._mc_buffer)\n        value_buf = mc.ConvertBuffer(self._mc_buffer)\n        return value_buf\n\n    def get_text(self, filepath):\n        raise NotImplementedError"}
{"Repository": "importance-sampling", "input": "Make sure that the in memory dataset has 4 dimensions and is normalized className InMemoryImageDataset(InMemoryDataset) Method __init__", "label": "class InMemoryImageDataset(InMemoryDataset):\n    def __init__(self, X_train, y_train, X_test, y_test, categorical=True):\n        # Expand the dims and make sure the shapes are correct image shapes\n        if len(X_train.shape) < 4:\n            X_train = np.expand_dims(X_train, axis=-1)\n            X_test = np.expand_dims(X_test, axis=-1)\n        assert X_train.shape[1:] == X_test.shape[1:]\n        assert len(X_train.shape) == 4\n\n        # Normalize to [0, 1]\n        X_train = X_train.astype(np.float32) / X_train.max()\n        X_test = X_test.astype(np.float32) / X_test.max()\n\n        super(InMemoryImageDataset, self).__init__(\n            X_train,\n            y_train,\n            X_test,\n            y_test,\n            categorical=categorical\n        )"}
{"Repository": "MedTagger", "input": "Sign in endpoint. className SignIn(Resource) Method post", "label": "class SignIn(Resource):\n    @staticmethod\n    @api.expect(serializers.sign_in)\n    @api.doc(responses={200: 'Signed in', 400: 'User does not exist or wrong password was provided'})\n    def post() -> Any:\n        sign_in = request.json\n        token = sign_in_user(sign_in['email'], sign_in['password'])\n        return {\"token\": token}, 200"}
{"Repository": "two-stream-pytorch", "input": "Computes and stores the average and current value className AverageMeter(object) Method __init__ Method reset Method update", "label": "class AverageMeter(object):\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count"}
{"Repository": "webdataset", "input": "An iterable dataset yielding a list of urls. className SimpleShardList(IterableDataset) Method __init__ Method __len__ Method __iter__ Attribute urls Attribute seed", "label": "class SimpleShardList(IterableDataset):\n    def __init__(self, urls, seed=None):\n        super().__init__()\n        if isinstance(urls, str):\n            urls = expand_urls(urls)\n        else:\n            urls = list(urls)\n        self.urls = urls\n        assert isinstance(self.urls[0], str)\n        if seed is True:\n            seed = time.time()\n        self.seed = seed\n\n    def __len__(self):\n        return len(self.urls)\n\n    def __iter__(self):\n        urls = self.urls.copy()\n        if self.seed is not None:\n            random.Random(self.seed).shuffle(urls)\n        for url in urls:\n            yield dict(url=url)"}
{"Repository": "bert_textMatching", "input": "Base class for data converters for sequence classification data sets. className DataProcessor(object) Method get_train_examples Method get_dev_examples Method get_test_examples Method get_labels Method _read_tsv", "label": "class DataProcessor(object):\n  def get_train_examples(self, data_dir):\n    raise NotImplementedError()\n\n  def get_dev_examples(self, data_dir):\n    raise NotImplementedError()\n\n  def get_test_examples(self, data_dir):\n    raise NotImplementedError()\n\n  def get_labels(self):\n    raise NotImplementedError()\n\n  @classmethod\n  def _read_tsv(cls, input_file, quotechar=None):\n    with tf.gfile.Open(input_file, \"r\") as f:\n      reader = csv.reader(f, delimiter=\"\\t\", quotechar=quotechar)\n      lines = []\n      for line in reader:\n        lines.append(line)\n      return lines"}
{"Repository": "crosscat", "input": "A client interface that gives a singue interface to the various engines. className CrossCatClient(object) Method __init__ Method __getattribute__ Attribute engine", "label": "class CrossCatClient(object):\n    def __init__(self, engine):\n        self.engine = engine\n\n    def __getattribute__(self, name):\n        engine = object.__getattribute__(self, 'engine')\n        attr = None\n        if hasattr(engine, name):\n            attr = getattr(engine, name)\n        else:\n            attr = object.__getattribute__(self, name)\n        return attr"}
{"Repository": "textsum-gan", "input": "Vocabulary class for mapping between words and ids (integers) className Vocab(object) Method __init__ Method word2id Method id2word Method size Method write_metadata Attribute _word_to_id Attribute _id_to_word Attribute _count", "label": "class Vocab(object):\n    def __init__(self, vocab_file, max_size):\n        self._word_to_id = {}\n        self._id_to_word = {}\n        self._count = 0  # keeps track of total number of words in the Vocab\n\n        # [UNK], [PAD], [START] and [STOP] get the ids 0,1,2,3.\n        for w in [UNKNOWN_TOKEN, PAD_TOKEN, START_DECODING, STOP_DECODING]:\n            self._word_to_id[w] = self._count\n            self._id_to_word[self._count] = w\n            self._count += 1\n\n        # Read the vocab file and add words up to max_size\n        with open(vocab_file, 'r', encoding='utf-8') as vocab_f:\n            for line in vocab_f:\n                pieces = line.split()\n                if len(pieces) != 2:\n                    print('Warning: incorrectly formatted line in vocabulary file: %s\\n' % line)\n                    continue\n                w = pieces[0]\n                if w in [SENTENCE_START, SENTENCE_END, UNKNOWN_TOKEN, PAD_TOKEN, START_DECODING, STOP_DECODING]:\n                    raise Exception(\n                        '<s>, </s>, [UNK], [PAD], [START] and [STOP] shouldn\\'t be in the vocab file, but %s is' % w)\n                if w in self._word_to_id:\n                    raise Exception('Duplicated word in vocabulary file: %s' % w)\n                self._word_to_id[w] = self._count\n                self._id_to_word[self._count] = w\n                self._count += 1\n                if max_size != 0 and self._count >= max_size:\n                    print(\"max_size of vocab was specified as %i; we now have %i words. Stopping reading.\"\n                          % (max_size, self._count))\n                    break\n\n        print(\"Finished constructing vocabulary of %i total words. Last word added: %s\"\n              % (self._count, self._id_to_word[self._count - 1]))\n\n    def word2id(self, word):\n        if word not in self._word_to_id:\n            return self._word_to_id[UNKNOWN_TOKEN]\n        return self._word_to_id[word]\n\n    def id2word(self, word_id):\n        if word_id not in self._id_to_word:\n            raise ValueError('Id not found in vocab: %d' % word_id)\n        return self._id_to_word[word_id]\n\n    def size(self):\n        return self._count\n\n    def write_metadata(self, fpath):\n        print(\"Writing word embedding metadata file to %s...\" % (fpath))\n        with open(fpath, \"w\") as f:\n             fieldnames = ['word']\n             writer = csv.DictWriter(f, delimiter=\"\\t\", fieldnames=fieldnames)\n             for i in range(self.size()):\n                 writer.writerow({\"word\": self._id_to_word[i]})"}
{"Repository": "pegasus", "input": "Public C4 dataset. className CommonCrawlDataset(PublicPretrainingTFDSDataset) Method override_build Method load", "label": "class CommonCrawlDataset(PublicPretrainingTFDSDataset):\n  def override_build(self, build):\n    return \"c4/en\" + build.lstrip(\"common_crawl\")\n\n  def load(self, build, split, shuffle):\n    return self._split_validation_50_50(build, split, shuffle)"}
{"Repository": "PytorchInsight", "input": "Registers a signal handler that calls finish on SIGINT className SigIntMixin(object) Method __init__ Method _sigint_handler", "label": "class SigIntMixin(object):\n    def __init__(self, *args, **kwargs):\n        super(SigIntMixin, self).__init__(*args, **kwargs)\n        signal(SIGINT, self._sigint_handler)\n\n    def _sigint_handler(self, signum, frame):\n        self.finish()\n        exit(0)"}
{"Repository": "django-ecommerce-project-v2", "input": "Inventory Category table implimented with MPTT className Category(MPTTModel) Method __str__", "label": "class Category(MPTTModel):\n    name = models.CharField(\n        max_length=100,\n        null=False,\n        unique=False,\n        blank=False,\n        verbose_name=_(\"category name\"),\n        help_text=_(\"format: required, max-100\"),\n    )\n    slug = models.SlugField(\n        max_length=150,\n        null=False,\n        unique=False,\n        blank=False,\n        verbose_name=_(\"category safe URL\"),\n        help_text=_(\"format: required, letters, numbers, underscore, or hyphens\"),\n    )\n    is_active = models.BooleanField(\n        default=True,\n    )\n\n    parent = TreeForeignKey(\n        \"self\",\n        on_delete=models.PROTECT,\n        related_name=\"children\",\n        null=True,\n        blank=True,\n        unique=False,\n        verbose_name=_(\"parent of category\"),\n        help_text=_(\"format: not required\"),\n    )\n\n    class MPTTMeta:\n        order_insertion_by = [\"name\"]\n\n    class Meta:\n        verbose_name = _(\"product category\")\n        verbose_name_plural = _(\"product categories\")\n\n    def __str__(self):\n        return self.name"}
{"Repository": "LGSC-for-FAS", "input": "Preprocess an image. className ImageTransform(object) Method __call__", "label": "class ImageTransform(object):\n    def __init__(self,\n                 mean=(0, 0, 0),\n                 std=(1, 1, 1),\n                 to_rgb=True):\n        self.mean = np.array(mean, dtype=np.float32)\n        self.std = np.array(std, dtype=np.float32)\n        self.to_rgb = to_rgb\n\n    def __call__(self, img, scale, mask=None, flip=False):\n        img = cv2.resize(img, scale, interpolation=cv2.INTER_LINEAR)\n        img = np.float32(img) if img.dtype != np.float32 else img.copy()\n        if self.to_rgb:\n            cv2.cvtColor(img, cv2.COLOR_BGR2RGB, img)\n        cv2.subtract(img, np.float64(self.mean.reshape(1, -1)), img)\n        cv2.multiply(img, 1 / np.float64(self.std.reshape(1, -1)), img)\n\n        if flip:\n            img = np.flip(img, axis=1)\n        img = img.transpose(2, 0, 1)\n        \n        if mask is not None:\n            mask = cv2.resize(mask, scale, interpolation=cv2.INTER_LINEAR).astype(np.float32)\n            if flip:\n                mask = np.flip(mask, axis=1)\n            mask = mask.transpose(2, 0, 1)\n            return img, mask\n        else:\n            return img"}
{"Repository": "Cyberbrain-Deprecated", "input": "Class that represents a call site. className Call(Computation) Method to_dict Method create", "label": "class Call(Computation):\n    def __init__(\n        self,\n        *,\n        callsite_ast: ast.AST,\n        source_location: SourceLocation,\n        arg_values: inspect.ArgInfo,\n        func_name: str,\n        vars: Vars,\n        event_type: str,\n        frame_id: FrameID,\n        callee_frame_id: FrameID,\n        surrounding: Surrounding,\n    ):\n        self.callsite_ast = callsite_ast\n        self.source_location = source_location\n        self.arg_values = arg_values\n        self.func_name = func_name\n        self.vars = vars\n        self.event_type = event_type\n        self.frame_id = frame_id\n        self.callee_frame_id = callee_frame_id\n        self.code_str = utils.ast_to_str(self.callsite_ast)\n        self.vars_before_return = None\n        self.surrounding = surrounding\n\n    def to_dict(self):\n        return {\n            **super().to_dict(),\n            \"caller_frame_id\": str(self.frame_id),\n            \"callee_frame_id\": str(self.callee_frame_id),\n        }\n\n    @staticmethod\n    def create(frame):\n        caller_frame = frame.f_back\n        _, surrounding = utils.get_code_str_and_surrounding(caller_frame)\n        callsite_ast = executing.Source.executing(caller_frame).node\n        # If it's not ast.Call, like ast.ListComp, ignore for now.\n        if not isinstance(callsite_ast, ast.Call):\n            return None\n        frame_id = FrameID.create(\"call\")\n        frame_id.co_name = caller_frame.f_code.co_name\n        return Call(\n            callsite_ast=callsite_ast,\n            source_location=SourceLocation(\n                filepath=caller_frame.f_code.co_filename, surrounding=surrounding\n            ),\n            arg_values=inspect.getargvalues(frame),\n            func_name=frame.f_code.co_name,\n            vars=Vars(caller_frame),\n            event_type=\"call\",\n            frame_id=frame_id,\n            callee_frame_id=FrameID.current(),\n            surrounding=surrounding,\n        )"}
{"Repository": "pyfailsafe", "input": "A CircuitBreaker which is always closed allowing all executions. className AlwaysClosedCircuitBreaker(CircuitBreaker) Method __init__ Method allows_execution Method record_success Method record_failure", "label": "class AlwaysClosedCircuitBreaker(CircuitBreaker):\n    def __init__(self):\n        pass\n\n    def allows_execution(self):\n        return True\n\n    def record_success(self):\n        pass\n\n    def record_failure(self):\n        pass"}
{"Repository": "navrep", "input": "Generic class to encode the observations of an environment into a single 1d vector className FlatLidarAndStateEncoder(object) Method __init__ Method reset Method close Method _encode_obs Attribute _N Attribute observation_space", "label": "class FlatLidarAndStateEncoder(object):\n    def __init__(self):\n        self._N = _L + _RS\n        self.observation_space = spaces.Box(low=-np.inf, high=np.inf,\n                                            shape=(self._N,1), dtype=np.float32)\n\n    def reset(self):\n        pass\n\n    def close(self):\n        pass\n\n    def _encode_obs(self, obs, action):\n        lidar, state = obs\n        e2e_obs = np.concatenate([lidar, state]).reshape(self._N,1)\n        return e2e_obs"}
{"Repository": "FADNet", "input": "Crop the image at center Args: int or tuple. className CenterCrop(object) Method __init__ Method __call__ Attribute output_size Attribute output_size Attribute augment Attribute transform", "label": "class CenterCrop(object):\n    def __init__(self, output_size, augment=False):\n        assert isinstance(output_size, (int, tuple))\n        if isinstance(output_size, int):\n            self.output_size = (output_size, output_size)\n        else:\n            assert len(output_size) == 2\n            self.output_size = output_size\n        self.augment = augment\n        self.transform = ColorJitter() \n\n    def __call__(self, sample):\n        image_left, image_right, gt_disp = sample['img_left'], sample['img_right'], sample['gt_disp']\n\n        h, w = image_left.shape[1:3]\n        new_h, new_w = self.output_size\n\n        top = int((h - new_h) / 2)\n        left = int((w - new_w) / 2)\n        # top = 0\n        # left = 0\n\n        image_left = image_left[:, top: top + new_h, left: left + new_w]\n        image_right = image_right[:, top: top + new_h, left: left + new_w]\n        gt_disp = gt_disp[:, top: top + new_h, left: left + new_w]\n        if self.augment:\n            rd = np.random.randint(0,2)\n            if rd == 0:\n                image_left = self.transform(image_left)\n                #imgtmp = image_left.cpu().numpy()\n                #imgtmp = np.transpose(imgtmp, [2, 1, 0])\n                #print('lighted shape:', imgtmp.shape)\n                #io.imsave('test.png', imgtmp)\n                image_right = self.transform(image_right)\n        new_sample = sample\n        new_sample.update({'img_left': image_left, \n                      'img_right': image_right, \n                      'gt_disp': gt_disp})\n\n        return new_sample"}
{"Repository": "nsf", "input": "Reference: > Mller et al. className PiecewiseLinearCouplingTransform(PiecewiseCouplingTransform) Method _transform_dim_multiplier Method _piecewise_cdf", "label": "class PiecewiseLinearCouplingTransform(PiecewiseCouplingTransform):\n    def __init__(self,\n                 mask,\n                 transform_net_create_fn,\n                 num_bins=10,\n                 tails=None,\n                 tail_bound=1.,\n                 apply_unconditional_transform=False,\n                 img_shape=None):\n        self.num_bins = num_bins\n        self.tails = tails\n        self.tail_bound = tail_bound\n\n        if apply_unconditional_transform:\n            unconditional_transform = lambda features: transforms.PiecewiseLinearCDF(\n                shape=[features] + (img_shape if img_shape else []),\n                num_bins=num_bins,\n                tails=tails,\n                tail_bound=tail_bound\n            )\n        else:\n            unconditional_transform = None\n\n        super().__init__(mask, transform_net_create_fn,\n                         unconditional_transform=unconditional_transform)\n\n    def _transform_dim_multiplier(self):\n        return self.num_bins\n\n    def _piecewise_cdf(self, inputs, transform_params, inverse=False):\n        unnormalized_pdf = transform_params\n\n        if self.tails is None:\n            return splines.linear_spline(\n                inputs=inputs,\n                unnormalized_pdf=unnormalized_pdf,\n                inverse=inverse\n            )\n        else:\n            return splines.unconstrained_linear_spline(\n                inputs=inputs,\n                unnormalized_pdf=unnormalized_pdf,\n                inverse=inverse,\n                tails=self.tails,\n                tail_bound=self.tail_bound\n            )"}
{"Repository": "open-ucn", "input": "Computes and stores the average and current value className AverageMeter(object) Method __init__ Method reset Method update", "label": "class AverageMeter(object):\n  def __init__(self):\n    self.reset()\n\n  def reset(self):\n    self.val = 0\n    self.avg = 0\n    self.sum = 0.0\n    self.sq_sum = 0.0\n    self.count = 0\n\n  def update(self, val, n=1):\n    if not np.isnan(val):\n      self.val = val\n      self.sum += val * n\n      self.count += n\n      self.avg = self.sum / self.count\n      self.sq_sum += val**2 * n\n      self.var = self.sq_sum / self.count - self.avg**2"}
{"Repository": "meliad", "input": "Creates and configures the Adam optimizer. className AdamConfig(OptimizerConfig) Method create_optimizer_def", "label": "class AdamConfig(OptimizerConfig):\n  # Adam does not use parameter scale, and thus requires a smaller lrate.\n  # This will be multiplied by the learning rate schedule.\n  learning_rate: float = 0.05\n\n  beta1: float = 0.9               # For moving average of gradient.\n  beta2: float = 0.98              # For moving average of gradient magnitude.\n  weight_decay_rate: float = 0.0   # Relative to learning rate.\n\n  def create_optimizer_def(self) -> optim.OptimizerDef:\n    logging.info(\"Using Adam Optimizer. lr=%f, b1=%f, b2=%f\",\n                 self.learning_rate, self.beta1, self.beta2)\n    return optim.Adam(beta1=self.beta1,\n                      beta2=self.beta2,\n                      weight_decay=self.weight_decay_rate)"}
{"Repository": "ftw", "input": "This class holds test and stage information from a YAML test file className Ruleset(object) Method __init__ Method extract_tests Attribute yaml_file Attribute meta Attribute author Attribute description Attribute enabled Attribute tests", "label": "class Ruleset(object):\n    def __init__(self, yaml_file):\n        self.yaml_file = yaml_file\n        self.meta = yaml_file['meta']\n        self.author = self.meta['author']\n        self.description = self.meta['description']\n        self.enabled = self.meta['enabled']\n        self.tests = self.extract_tests() if self.enabled else []\n\n    def extract_tests(self):\n        try:\n            return [Test(test_dict, index, self.meta)\n                    for index, test_dict in enumerate(self.yaml_file['tests'])]\n        except errors.TestError as e:\n            e.args[1]['meta'] = self.meta\n            raise e\n        except Exception as e:\n            raise Exception(\n                'Caught error. Message: %s on test with metadata: %s'\n                % (str(e), str(self.meta))\n            )"}
{"Repository": "sqllineage", "input": "egginfo is a hook both for 1) building source code distribution (python setup. className EggInfoWithJS(egg_info) Method run", "label": "class EggInfoWithJS(egg_info):\n    def run(self) -> None:\n        static_path = os.path.join(NAME, STATIC_FOLDER)\n        if os.path.exists(static_path) or \"READTHEDOCS\" in os.environ:\n            pass\n        else:\n            js_path = \"sqllineagejs\"\n            use_shell = True if platform.system() == \"Windows\" else False\n            subprocess.check_call(\n                shlex.split(\"npm install\"), cwd=js_path, shell=use_shell\n            )\n            subprocess.check_call(\n                shlex.split(\"npm run build\"), cwd=js_path, shell=use_shell\n            )\n            shutil.move(os.path.join(js_path, STATIC_FOLDER), static_path)\n        super().run()"}
{"Repository": "penrose", "input": "A class representing a \"B_L\" Penrose tile in the P3 tiling scheme as a \"large\" Robinson triangle (sides in ratio 1:1:phi). className BtileL(RobinsonTriangle) Method inflate", "label": "class BtileL(RobinsonTriangle):\n    def inflate(self):\n        # D and E divide sides AC and AB respectively\n        D = psi2 * self.A + psi * self.C\n        E = psi2 * self.A + psi * self.B\n        # Take care to order the vertices here so as to get the right\n        # orientation for the resulting triangles.\n        return [BtileL(D, E, self.A),\n                BtileS(E, D, self.B),\n                BtileL(self.C, D, self.B)]"}
{"Repository": "Global-Encoding", "input": "Set the learning rate of each parameter group to the initial lr decayed by gamma every epoch. className ExponentialLR(_LRScheduler) Method __init__ Method get_lr Attribute gamma", "label": "class ExponentialLR(_LRScheduler):\n    def __init__(self, optimizer, gamma, last_epoch=-1):\n\n        self.gamma = gamma\n\n        super(ExponentialLR, self).__init__(optimizer, last_epoch)\n\n\n\n    def get_lr(self):\n\n        return [base_lr * self.gamma ** self.last_epoch\n\n                for base_lr in self.base_lrs]"}
{"Repository": "stream.py", "input": "Apply a accelerator recursively. className recur_transform(Stream) Method __init__ Method __call__ Method transform Attribute accelerator", "label": "class recur_transform(Stream):\n\tdef __init__(self, accelerator):\n\t\tsuper(recur_transform, self).__init__()\n\t\tself.accelerator = accelerator\n\t\n\tdef __call__(self, series):\n\t\tdef transform():\n\t\t\ts = series\n\t\t\twhile 1:\n\t\t\t\tyield next(s)\n\t\t\t\ts = iter(s >> self.accelerator)\n\t\treturn transform()"}
{"Repository": "pandas", "input": "Custom command subclassed from Cython. className CythonCommand(build_ext) Method build_extension", "label": "class CythonCommand(build_ext):\n    def build_extension(self, ext) -> None:\n        pass"}
{"Repository": "dcs_liberation", "input": "A combo box for selecting a squadrons home air base. className SquadronBaseSelector(QComboBox) Method set_aircraft_type", "label": "class SquadronBaseSelector(QComboBox):\n    def __init__(\n        self,\n        bases: Iterable[ControlPoint],\n        selected_base: Optional[ControlPoint],\n        aircraft_type: Optional[AircraftType],\n    ) -> None:\n        super().__init__()\n        self.setSizeAdjustPolicy(QComboBox.SizeAdjustPolicy.AdjustToContents)\n        self.bases = list(bases)\n        self.set_aircraft_type(aircraft_type)\n\n        if selected_base:\n            self.setCurrentText(selected_base.name)\n        # TODO can we get a prefered base if none is selected?\n\n    def set_aircraft_type(self, aircraft_type: Optional[AircraftType]):\n        self.clear()\n        if aircraft_type:\n            for base in self.bases:\n                if not base.can_operate(aircraft_type):\n                    continue\n                self.addItem(base.name, base)\n            self.model().sort(0)\n            self.setEnabled(True)\n        else:\n            self.addItem(\"Select aircraft type first\", None)\n            self.setEnabled(False)\n        self.update()"}
{"Repository": "sagecell", "input": "This handles the websocket-ZMQ bridge to an IPython kernel. className ZMQChannelsHandler(object) Method _json_msg Method connect Method disconnect Method kernel_stopped Method on_recv Method send", "label": "class ZMQChannelsHandler(object):\n    def _json_msg(self, msg):\n        # can't encode buffers, so let's get rid of them if they exist\n        msg.pop(\"buffers\", None)\n        # sage_json handles things like encoding dates and sage types\n        return jsonapi.dumps(msg, default=misc.sage_json)\n\n    def connect(self, kernel):\n        self.kernel = kernel\n        self.msg_from_kernel_callbacks = []\n        self.msg_to_kernel_callbacks = []\n        for channel in [\"iopub\", \"shell\"]:\n            kernel.channels[channel].on_recv_stream(self.on_recv)\n        kernel.on_stop(self.kernel_stopped)\n\n    def disconnect(self):\n        self.kernel.stop()\n\n    def kernel_stopped(self):\n        msg = {\n            \"channel\": \"iopub\",\n            'header': {\n                'msg_type': 'status',\n                'session': self.kernel.id,\n                'msg_id': str(uuid.uuid4()),\n                'username': ''\n            },\n            'parent_header': {},\n            'metadata': {},\n            'content': {'execution_state': 'dead'}\n        }\n        self.output_message(msg)\n\n    def on_recv(self, stream, msg_list):\n        kernel = self.kernel\n        msg_list = kernel.session.feed_identities(msg_list)[1]\n        msg = kernel.session.unserialize(msg_list)\n        msg[\"channel\"] = stream.channel\n        # Useful but may be way too verbose even for debugging\n        #logger.debug(\"received from kernel %s\", msg)\n        msg_type = msg[\"msg_type\"]\n        if msg_type == \"status\":\n            kernel.status = msg[\"content\"][\"execution_state\"]\n        if msg_type in (\"execute_reply\",\n                        \"sagenb.interact.update_interact_reply\"):\n            kernel.executing -= 1\n            logger.debug(\"decreased execution counter for %s to %s\",\n                         kernel.id, kernel.executing)\n        if msg_type == \"kernel_timeout\":\n            timeout = float(msg[\"content\"][\"timeout\"])\n            logger.debug(\"reset timeout for %s to %f\", kernel.id, timeout)\n            if timeout >= 0:\n                kernel.timeout = min(timeout, config.get(\"max_timeout\"))\n        else:\n            for callback in self.msg_from_kernel_callbacks:\n                callback(msg)\n            self.output_message(msg)\n        if kernel.timeout > 0:\n            kernel.deadline = time.time() + kernel.timeout\n        elif kernel.executing == 0 and kernel.status == \"idle\":\n            logger.debug(\"stopping on %s, %s\", stream.channel, msg_type)\n            kernel.stop()\n\n    def send(self, msg):\n        # Useful but may be way too verbose even for debugging\n        #logger.debug(\"sending to kernel %s\", msg)\n        for f in self.msg_to_kernel_callbacks:\n            f(msg)\n        kernel = self.kernel\n        if msg['header']['msg_type'] in ('execute_request',\n                                         'sagenb.interact.update_interact'):\n            kernel.executing += 1\n            logger.debug(\"increased execution counter for %s to %s\",\n                kernel.id, kernel.executing)\n        kernel.session.send(kernel.channels[\"shell\"], msg)"}
{"Repository": "COCON_ICLR2021", "input": "This is the configuration class to store the configuration of an :class:`~transformers. className OpenAIGPTConfig(PretrainedConfig) Method max_position_embeddings Method hidden_size Method num_attention_heads Method num_hidden_layers", "label": "class OpenAIGPTConfig(PretrainedConfig):\n    pretrained_config_archive_map = OPENAI_GPT_PRETRAINED_CONFIG_ARCHIVE_MAP\n    model_type = \"openai-gpt\"\n\n    def __init__(\n        self,\n        vocab_size=40478,\n        n_positions=512,\n        n_ctx=512,\n        n_embd=768,\n        n_layer=12,\n        n_head=12,\n        afn=\"gelu\",\n        resid_pdrop=0.1,\n        embd_pdrop=0.1,\n        attn_pdrop=0.1,\n        layer_norm_epsilon=1e-5,\n        initializer_range=0.02,\n        predict_special_tokens=True,\n        summary_type=\"cls_index\",\n        summary_use_proj=True,\n        summary_activation=None,\n        summary_proj_to_labels=True,\n        summary_first_dropout=0.1,\n        **kwargs\n    ):\n        super().__init__(**kwargs)\n\n        self.vocab_size = vocab_size\n        self.n_ctx = n_ctx\n        self.n_positions = n_positions\n        self.n_embd = n_embd\n        self.n_layer = n_layer\n        self.n_head = n_head\n        self.afn = afn\n        self.resid_pdrop = resid_pdrop\n        self.embd_pdrop = embd_pdrop\n        self.attn_pdrop = attn_pdrop\n        self.layer_norm_epsilon = layer_norm_epsilon\n        self.initializer_range = initializer_range\n        self.predict_special_tokens = predict_special_tokens\n        self.summary_type = summary_type\n        self.summary_use_proj = summary_use_proj\n        self.summary_activation = summary_activation\n        self.summary_first_dropout = summary_first_dropout\n        self.summary_proj_to_labels = summary_proj_to_labels\n\n    @property\n    def max_position_embeddings(self):\n        return self.n_positions\n\n    @property\n    def hidden_size(self):\n        return self.n_embd\n\n    @property\n    def num_attention_heads(self):\n        return self.n_head\n\n    @property\n    def num_hidden_layers(self):\n        return self.n_layer"}
{"Repository": "neural-gpu", "input": "Inputs where a single operation can appear many times className MultiOpGenerator(DataGenerator) Method __init__ Method is_valid_length Method _rand_inputs Method rand_pair Attribute base Attribute f Attribute sep Attribute num Attribute zero_pad Attribute min_length Attribute zero_chance", "label": "class MultiOpGenerator(DataGenerator):\n  def __init__(self, base, f, sep, num, zero_chance=1, zero_pad=True):\n    self.base = base\n    self.f = f\n    self.sep = sep\n    self.num = num\n    self.zero_pad = zero_pad\n    self.min_length = 1 if num is None else 2*num - 1\n    self.zero_chance = zero_chance\n\n  def is_valid_length(self, l):\n    return l >= self.min_length\n\n  def _rand_inputs(self, k, num, allow_zero):\n    k = int(k)\n    return [random.randint(0 if allow_zero else 1, self.base**k-1) for i in range(num)]\n\n  def rand_pair(self, l):\n    num = self.num\n    if num is None:\n      num = random.randint(1, (l+1)//2)\n    k = int((l+1)//num-1)\n    allow_zero = random.random() < self.zero_chance\n    ns = self._rand_inputs(k, num, allow_zero)\n    result = functools.reduce(self.f, ns)\n    input_arrays = []\n    for i, n in enumerate(ns):\n      if i:\n        input_arrays.append([self.sep])\n      input_arrays.append(to_base(n, self.base, k if self.zero_pad else 1)+1)\n    inp = np.concatenate(input_arrays)\n    outp = np.concatenate([\n            to_base(result, self.base, (k+1)*num-1 if self.zero_pad else 1) + 1,\n    ])\n    return inp, outp"}
{"Repository": "CLMR", "input": "Support setup.py upload. className UploadCommand(Command) Method status Method initialize_options Method finalize_options Method run", "label": "class UploadCommand(Command):\n    description = \"Build and publish the package.\"\n    user_options = []\n\n    @staticmethod\n    def status(s):\n        print(\"\\033[1m{0}\\033[0m\".format(s))\n\n    def initialize_options(self):\n        pass\n\n    def finalize_options(self):\n        pass\n\n    def run(self):\n        try:\n            self.status(\"Removing previous builds\")\n            rmtree(os.path.join(here, \"dist\"))\n        except OSError:\n            pass\n\n        self.status(\"Building Source and Wheel (universal) distribution\")\n        os.system(\"{0} setup.py sdist bdist_wheel --universal\".format(sys.executable))\n\n        self.status(\"Uploading the package to PyPI via Twine\")\n        os.system(\"twine upload dist/*\")\n\n        self.status(\"Pushing git tags\")\n        os.system(\"git tag v{0}\".format(about[\"__version__\"]))\n        os.system(\"git push --tags\")\n\n        sys.exit()"}
{"Repository": "yolo-for-k210", "input": "No-op context manager. className NoOpScope(object) Method __enter__ Method __exit__", "label": "class NoOpScope(object):\n    def __enter__(self):\n        return None\n\n    def __exit__(self, exc_type, exc_value, traceback):\n        return False"}
{"Repository": "capreolus", "input": "Post-installation for installation mode. className PostInstallCommand(install) Method run", "label": "class PostInstallCommand(install):\n    def run(self):\n        install.run(self)"}
{"Repository": "Mathics", "input": "Opens a stream This can be used in a context_manager like this: with Stream(pypath, \"r\") as f: . className Stream(object) Method __init__ Method __enter__ Method __exit__ Attribute name Attribute mode Attribute encoding Attribute io Attribute n", "label": "class Stream(object):\n    def __init__(self, name: str, mode=\"r\", encoding=None, io=None, channel_num=None):\n        if channel_num is None:\n            channel_num = stream_manager.next\n        if mode is None:\n            mode = \"r\"\n        self.name = name\n        self.mode = mode\n        self.encoding = encoding\n        self.io = io\n        self.n = channel_num\n\n        if mode not in [\"r\", \"w\", \"a\", \"rb\", \"wb\", \"ab\"]:\n            raise ValueError(\"Can't handle mode {0}\".format(mode))\n\n    def __enter__(self):\n        # find path\n        path = path_search(self.name)\n        if path is None and self.mode in [\"w\", \"a\", \"wb\", \"ab\"]:\n            path = self.name\n        if path is None:\n            raise IOError\n\n        # determine encoding\n        if \"b\" not in self.mode:\n            encoding = self.encoding\n        else:\n            encoding = None\n\n        # open the stream\n        fp = io_open(path, self.mode, encoding=encoding)\n        stream_manager.add(name=path, mode=self.mode, encoding=encoding, io=fp)\n        return fp\n\n    def __exit__(self, type, value, traceback):\n        if self.io is not None:\n            self.io.close()\n        # Leave around self.io so we can call closed() to query its status.\n        stream_manager.delete(self.n)"}
{"Repository": "pyfor", "input": "ImportedGrid is used to normalize a parent cloud object with an arbitrary raster file. className ImportedGrid(Grid) Method __init__ Method _update Attribute in_raster Attribute cloud Attribute cell_size Attribute m Attribute n Attribute cells", "label": "class ImportedGrid(Grid):\n    def __init__(self, path, cloud):\n        import rasterio\n\n        self.in_raster = rasterio.open(path)\n\n        # Check cell size\n        cell_size_x, cell_size_y = (\n            self.in_raster.transform[0],\n            abs(self.in_raster.transform[4]),\n        )\n        if cell_size_x != cell_size_y:\n            print(\"Cell sizes not equal of input raster, not supported.\")\n            raise ValueError\n        else:\n            cell_size = cell_size_x\n\n        self.cloud = cloud\n        self.cell_size = cell_size\n\n        min_x, max_x = self.in_raster.bounds[0], self.in_raster.bounds[2]\n        min_y, max_y = self.in_raster.bounds[1], self.in_raster.bounds[3]\n\n        self.m = self.in_raster.height\n        self.n = self.in_raster.width\n\n        # Create bins\n        bins_x = np.searchsorted(\n            np.linspace(min_x, max_x, self.n), self.cloud.data.points[\"x\"]\n        )\n        bins_y = np.searchsorted(\n            np.linspace(min_y, max_y, self.m), self.cloud.data.points[\"y\"]\n        )\n\n        self.cloud.data.points[\"bins_x\"] = bins_x\n        self.cloud.data.points[\"bins_y\"] = bins_y\n        self.cells = self.cloud.data.points.groupby([\"bins_x\", \"bins_y\"])\n\n    def _update(self):\n        self.cloud.data._update()"}
{"Repository": "Aini_Medic", "input": "Exception raised for errors in the input. className ShapeError(Error) Method __init__ Attribute message", "label": "class ShapeError(Error):\n    def __init__(self, message):\n        self.message = message"}
{"Repository": "pyexchange", "input": "Connection to Exchange that uses NTLM authentication className ExchangeNTLMAuthConnection(ExchangeBaseConnection) Method __init__ Method build_password_manager Method build_session Method send Attribute url Attribute username Attribute password Attribute verify_certificate Attribute handler Attribute session Attribute password_manager", "label": "class ExchangeNTLMAuthConnection(ExchangeBaseConnection):\n  def __init__(self, url, username, password, verify_certificate=True, **kwargs):\n    self.url = url\n    self.username = username\n    self.password = password\n    self.verify_certificate = verify_certificate\n    self.handler = None\n    self.session = None\n    self.password_manager = None\n\n  def build_password_manager(self):\n    if self.password_manager:\n      return self.password_manager\n\n    log.debug(u'Constructing password manager')\n\n    self.password_manager = HttpNtlmAuth(self.username, self.password)\n\n    return self.password_manager\n\n  def build_session(self):\n    if self.session:\n      return self.session\n\n    log.debug(u'Constructing opener')\n\n    self.password_manager = self.build_password_manager()\n\n    self.session = requests.Session()\n    self.session.auth = self.password_manager\n\n    return self.session\n\n  def send(self, body, headers=None, retries=2, timeout=30, encoding=u\"utf-8\"):\n    if not self.session:\n      self.session = self.build_session()\n\n    try:\n      response = self.session.post(self.url, data=body, headers=headers, verify = self.verify_certificate)\n      response.raise_for_status()\n    except requests.exceptions.RequestException as err:\n      log.debug(err.response.content)\n      raise FailedExchangeException(u'Unable to connect to Exchange: %s' % err)\n\n    log.info(u'Got response: {code}'.format(code=response.status_code))\n    log.debug(u'Got response headers: {headers}'.format(headers=response.headers))\n    log.debug(u'Got body: {body}'.format(body=response.text))\n\n    return response.text"}
{"Repository": "ha-dyson", "input": "hashable dict implementation, suitable for use as a key into other dicts. className hashdict(dict) Method __key Method __repr__ Method __hash__ Method __setitem__ Method __delitem__ Method clear Method pop Method popitem Method setdefault Method update Method __add__", "label": "class hashdict(dict):\n    def __key(self):\n        return tuple(sorted(self.items()))\n\n    def __repr__(self):  # noqa: D105 no docstring\n        return \", \".join(f\"{i[0]!s}={i[1]!r}\" for i in self.__key())\n\n    def __hash__(self):  # noqa: D105 no docstring\n        return hash(self.__key())\n\n    def __setitem__(self, key, value):  # noqa: D105 no docstring\n        raise TypeError(f\"{self.__class__.__name__} does not support item assignment\")\n\n    def __delitem__(self, key):  # noqa: D105 no docstring\n        raise TypeError(f\"{self.__class__.__name__} does not support item assignment\")\n\n    def clear(self):  # noqa: D102 no docstring\n        raise TypeError(f\"{self.__class__.__name__} does not support item assignment\")\n\n    def pop(self, *args, **kwargs):  # noqa: D102 no docstring\n        raise TypeError(f\"{self.__class__.__name__} does not support item assignment\")\n\n    def popitem(self, *args, **kwargs):  # noqa: D102 no docstring\n        raise TypeError(f\"{self.__class__.__name__} does not support item assignment\")\n\n    def setdefault(self, *args, **kwargs):  # noqa: D102 no docstring\n        raise TypeError(f\"{self.__class__.__name__} does not support item assignment\")\n\n    def update(self, *args, **kwargs):  # noqa: D102 no docstring\n        raise TypeError(f\"{self.__class__.__name__} does not support item assignment\")\n\n    # update is not ok because it mutates the object\n    # __add__ is ok because it creates a new object\n    # while the new object is under construction, it's ok to mutate it\n    def __add__(self, right):  # noqa: D105 no docstring\n        result = hashdict(self)\n        dict.update(result, right)\n        return result"}
{"Repository": "deep-learning-containers", "input": "A child class of ScanVulnerabilityList that is specifically made to deal with ECR Basic Scans. className ECRBasicScanVulnerabilityList(ScanVulnerabilityList) Method get_vulnerability_package_name_from_allowlist_formatted_vulnerability Method construct_allowlist_from_file Method construct_allowlist_from_ecr_scan_result Method are_vulnerabilities_equivalent", "label": "class ECRBasicScanVulnerabilityList(ScanVulnerabilityList):\n    def get_vulnerability_package_name_from_allowlist_formatted_vulnerability(self, vulnerability):\n        for attribute in vulnerability[\"attributes\"]:\n            if attribute[\"key\"] == \"package_name\":\n                return attribute[\"value\"]\n        return None\n\n    def construct_allowlist_from_file(self, file_path):\n        with open(file_path, \"r\") as f:\n            file_allowlist = json.load(f)\n        for package_name, package_vulnerability_list in file_allowlist.items():\n            for vulnerability in package_vulnerability_list:\n                if CVESeverity[vulnerability[\"severity\"]] >= self.minimum_severity:\n                    if package_name not in self.vulnerability_list:\n                        self.vulnerability_list[package_name] = []\n                    self.vulnerability_list[package_name].append(vulnerability)\n        return self.vulnerability_list\n\n    def construct_allowlist_from_allowlist_formatted_vulnerabilities(\n        self, allowlist_formatted_vulnerability_list\n    ):\n        for vulnerability in allowlist_formatted_vulnerability_list:\n            package_name = (\n                self.get_vulnerability_package_name_from_allowlist_formatted_vulnerability(\n                    vulnerability\n                )\n            )\n            if package_name not in self.vulnerability_list:\n                self.vulnerability_list[package_name] = []\n            if CVESeverity[vulnerability[\"severity\"]] >= self.minimum_severity:\n                self.vulnerability_list[package_name].append(vulnerability)\n        return self.vulnerability_list\n\n    def construct_allowlist_from_ecr_scan_result(self, ecr_format_vulnerability_list):\n        return self.construct_allowlist_from_allowlist_formatted_vulnerabilities(\n            ecr_format_vulnerability_list\n        )\n\n    def are_vulnerabilities_equivalent(self, vulnerability_1, vulnerability_2):\n        if (vulnerability_1[\"name\"], vulnerability_1[\"severity\"]) == (\n            vulnerability_2[\"name\"],\n            vulnerability_2[\"severity\"],\n        ):\n            # Do not compare package_version, because this may have been obtained at the time the CVE was first observed\n            # on the ECR Scan, which would result in unrelated version updates causing a mismatch while the CVE still\n            # applies on both vulnerabilities.\n            if all(\n                attribute in vulnerability_2[\"attributes\"]\n                for attribute in vulnerability_1[\"attributes\"]\n                if not attribute[\"key\"] == \"package_version\"\n            ):\n                return True\n        return False"}
{"Repository": "SSL_CR_Histo", "input": "BreastPathQ consistency training / validation className DatasetBreastPathQ_SSLtrain(Dataset) Method __init__ Method __len__ Method __getitem__ Attribute datalist Attribute transform", "label": "class DatasetBreastPathQ_SSLtrain(Dataset):\n    def __init__(self, dataset_path, transform=None):\n\n        self.datalist = []\n        self.transform = transform\n\n        data_paths = glob.glob(dataset_path + \"*.h5\")\n        with tqdm(enumerate(sorted(data_paths)), disable=True) as t:\n            for wj, data_path in t:\n                data = h5py.File(data_path)\n                data_patches = data['x'][:]\n                cls_id = data['y'][:]\n                for idx in range(len(data_patches)):\n                    self.datalist.append((data_patches[idx], cls_id[idx]))\n\n    def __len__(self):\n        return len(self.datalist)\n\n    def __getitem__(self, index):\n\n        np_data = self.datalist[index][0]\n        np_data = np.transpose(np_data, (1, 2, 0))\n        img = Image.fromarray((np_data * 255).astype(np.uint8))\n        label = self.datalist[index][1]\n\n        if self.transform:\n            image = self.transform(img)\n\n            if isinstance(image, tuple):\n                img = image[0]\n                target = image[1]\n\n                # Numpy to torch\n                img = np.array(img)\n                img = torch.from_numpy(img)\n                target = np.array(target)\n                target = torch.from_numpy(target)\n\n                # Change Tensor Dimension to N x C x H x W\n                img = img.permute(2, 0, 1)\n                target = target.permute(2, 0, 1)\n\n            else:\n                # Numpy to torch\n                img = np.array(image)\n                img = torch.from_numpy(img)\n\n                target = np.array(label)\n                target = torch.from_numpy(target)\n\n                # Change Tensor Dimension to N x C x H x W\n                img = img.permute(2, 0, 1)\n\n        return img, target"}
{"Repository": "exercises-in-programming-style", "input": "Models the stop word filter className StopWordManager(ActiveWFObject) Method _dispatch Method _init Method _filter", "label": "class StopWordManager(ActiveWFObject):\n    _stop_words = []\n\n    def _dispatch(self, message):\n        if message[0] == 'init':\n            self._init(message[1:])\n        elif message[0] == 'filter':\n            return self._filter(message[1:])\n        else:\n            # forward\n            send(self._word_freqs_manager, message)\n \n    def _init(self, message):\n        with open('../stop_words.txt') as f:\n            self._stop_words = f.read().split(',')\n        self._stop_words.extend(list(string.ascii_lowercase))\n        self._word_freqs_manager = message[0]\n\n    def _filter(self, message):\n        word = message[0]\n        if word not in self._stop_words:\n            send(self._word_freqs_manager, ['word', word])"}
{"Repository": "Quantify", "input": "docstring for stock className stock(object) Method __init__ Method get_price Method update_price Attribute price Attribute name Attribute code", "label": "class stock(object):\n\t\tdef __init__(self, first):\n\t\t\tself.price = first\n\t\t\tself.name = ''\n\t\t\tself.code = '002292'\n\n\t\tdef get_price(self):\n\t\t\treturn self.price\n\n\t\tdef update_price(self,p):\n\t\t\tself.price = p"}
{"Repository": "restler-fuzzer", "input": "Class used for encapsulating data about a specific rendered request and its response. className RenderedRequestStats(object) Method __init__ Method set_request_stats Method set_response_stats Attribute request_sent_timestamp Attribute response_received_timestamp Attribute request_verb Attribute request_uri Attribute request_headers Attribute request_body Attribute response_status_code Attribute response_status_text Attribute response_headers Attribute response_body Attribute request_str Attribute response_str", "label": "class RenderedRequestStats(object):\n    def __init__(self):\n        self.request_sent_timestamp = None\n        self.response_received_timestamp = None\n\n        self.request_verb = None\n        self.request_uri = None\n        self.request_headers = None\n        self.request_body = None\n\n        self.response_status_code = None\n        self.response_status_text = None\n        self.response_headers = None\n        self.response_body = None\n\n        self.request_str = None\n        self.response_str = None\n\n    def set_request_stats(self, request_text):\n        self.request_str = request_text\n        try:\n            split_body = request_text.split(messaging.DELIM)\n            split_headers = split_body[0].split(\"\\r\\n\")\n            verb_and_uri = split_headers[0].split(\" \")\n            self.request_verb = verb_and_uri[0]\n            self.request_uri = verb_and_uri[1]\n            self.request_headers = split_headers[1:]\n\n            # Remove the value of the Authorization header,\n            # so it is not persisted in logs\n            if Settings().no_tokens_in_logs:\n                for idx, h in enumerate(self.request_headers):\n                    if h.startswith(\"Authorization:\"):\n                        self.request_headers[idx] = \"Authorization: _OMITTED_AUTH_TOKEN_\"\n\n            if len(split_body) > 0 and split_body[1]:\n                self.request_body = split_body[1]\n        except:\n            logger.write_to_main(\n                            f\"Error setting request stats for text: {request_text}\",\n                            print_to_console=True\n                        )\n            pass\n\n    def set_response_stats(self, final_request_response, final_response_datetime):\n        self.response_str = final_request_response.to_str\n        self.response_status_code = final_request_response.status_code\n        self.response_status_text = final_request_response.status_text\n        self.response_headers = final_request_response.headers\n        self.response_body = final_request_response.body\n        self.response_received_timestamp = final_response_datetime"}
{"Repository": "anomaly-detection-in-industry-manufacturing", "input": "Abstract class for the inference. className Inferencer(ABC) Method load_model Method pre_process Method forward Method _superimpose_segmentation_mask Method __call__ Method _load_meta_data", "label": "class Inferencer(ABC):\n    @abstractmethod\n    def load_model(self, path: Union[str, Path]):\n        raise NotImplementedError\n\n    @abstractmethod\n    def pre_process(self, image: np.ndarray) -> Union[np.ndarray, Tensor]:\n        raise NotImplementedError\n\n    @abstractmethod\n    def forward(self, image: Union[np.ndarray, Tensor]) -> Union[np.ndarray, Tensor]:\n        raise NotImplementedError\n\n    @abstractmethod\n    def post_process(\n        self, predictions: Union[np.ndarray, Tensor], meta_data: Optional[Dict[str, Any]]\n    ) -> Dict[str, Any]:\n        raise NotImplementedError\n\n    def predict(\n        self,\n        image: Union[str, np.ndarray, Path],\n        meta_data: Optional[Dict[str, Any]] = None,\n    ) -> ImageResult:\n        if meta_data is None:\n            if hasattr(self, \"meta_data\"):\n                meta_data = getattr(self, \"meta_data\")\n            else:\n                meta_data = {}\n        if isinstance(image, (str, Path)):\n            image_arr: np.ndarray = read_image(image)\n        else:  # image is already a numpy array. Kept for mypy compatibility.\n            image_arr = image\n        meta_data[\"image_shape\"] = image_arr.shape[:2]\n\n        processed_image = self.pre_process(image_arr)\n        predictions = self.forward(processed_image)\n        output = self.post_process(predictions, meta_data=meta_data)\n\n        return ImageResult(\n            image=image_arr,\n            pred_score=output[\"pred_score\"],\n            pred_label=output[\"pred_label\"],\n            anomaly_map=output[\"anomaly_map\"],\n            pred_mask=output[\"pred_mask\"],\n        )\n\n    def _superimpose_segmentation_mask(self, meta_data: dict, anomaly_map: np.ndarray, image: np.ndarray):\n        pred_mask = compute_mask(anomaly_map, 0.5)  # assumes predictions are normalized.\n        image_height = meta_data[\"image_shape\"][0]\n        image_width = meta_data[\"image_shape\"][1]\n        pred_mask = cv2.resize(pred_mask, (image_width, image_height))\n        boundaries = find_boundaries(pred_mask)\n        outlines = dilation(boundaries, np.ones((7, 7)))\n        image[outlines] = [255, 0, 0]\n        return image\n\n    def __call__(self, image: np.ndarray) -> ImageResult:\n        return self.predict(image)\n\n    def _normalize(\n        self,\n        anomaly_maps: Union[Tensor, np.ndarray],\n        pred_scores: Union[Tensor, np.float32],\n        meta_data: Union[Dict, DictConfig],\n    ) -> Tuple[Union[np.ndarray, Tensor], float]:\n        # min max normalization\n        if \"min\" in meta_data and \"max\" in meta_data:\n            anomaly_maps = normalize_min_max(\n                anomaly_maps, meta_data[\"pixel_threshold\"], meta_data[\"min\"], meta_data[\"max\"]\n            )\n            pred_scores = normalize_min_max(\n                pred_scores, meta_data[\"image_threshold\"], meta_data[\"min\"], meta_data[\"max\"]\n            )\n\n        # standardize pixel scores\n        if \"pixel_mean\" in meta_data.keys() and \"pixel_std\" in meta_data.keys():\n            anomaly_maps = standardize(\n                anomaly_maps, meta_data[\"pixel_mean\"], meta_data[\"pixel_std\"], center_at=meta_data[\"image_mean\"]\n            )\n            anomaly_maps = normalize_cdf(anomaly_maps, meta_data[\"pixel_threshold\"])\n\n        # standardize image scores\n        if \"image_mean\" in meta_data.keys() and \"image_std\" in meta_data.keys():\n            pred_scores = standardize(pred_scores, meta_data[\"image_mean\"], meta_data[\"image_std\"])\n            pred_scores = normalize_cdf(pred_scores, meta_data[\"image_threshold\"])\n\n        return anomaly_maps, float(pred_scores)\n\n    def _load_meta_data(self, path: Optional[Union[str, Path]] = None) -> Union[DictConfig, Dict]:\n        meta_data: Union[DictConfig, Dict[str, Union[float, np.ndarray, Tensor]]] = {}\n        if path is not None:\n            config = OmegaConf.load(path)\n            meta_data = cast(DictConfig, config)\n        return meta_data"}
{"Repository": "colorcore", "input": "Provides conversion helpers. className Convert(object) Method __init__ Method to_coin Method base58_to_asset_id Method asset_id_to_base58 Method script_to_address Method script_to_display_string Attribute asset_byte", "label": "class Convert(object):\n    def __init__(self, asset_byte):\n        self.asset_byte = asset_byte\n\n    @staticmethod\n    def to_coin(satoshis):\n        return '{0:.8f}'.format(decimal.Decimal(satoshis) / decimal.Decimal(bitcoin.core.COIN))\n\n    def base58_to_asset_id(self, base58_asset_id):\n        try:\n            asset_id = bitcoin.base58.CBase58Data(base58_asset_id)\n        except bitcoin.base58.Base58ChecksumError:\n            raise colorcore.routing.ControllerError(\"Invalid asset ID.\")\n\n        if asset_id.nVersion != self.asset_byte or len(asset_id) != 20:\n            raise colorcore.routing.ControllerError(\"Invalid asset ID.\")\n\n        return bytes(asset_id)\n\n    def asset_id_to_base58(self, asset_id):\n        return str(bitcoin.base58.CBase58Data.from_bytes(asset_id, self.asset_byte))\n\n    @staticmethod\n    def script_to_address(script):\n        try:\n            return bitcoin.wallet.CBitcoinAddress.from_scriptPubKey(bitcoin.core.CScript(script))\n        except bitcoin.wallet.CBitcoinAddressError:\n            return None\n\n    @classmethod\n    def script_to_display_string(cls, script):\n        address = cls.script_to_address(script)\n        return str(address) if address is not None else \"Unknown script\""}
{"Repository": "opencve", "input": "Add custom properties in default Flask-User objects. className CustomUserManager(UserManager) Method customize Method _unique_email_validator", "label": "class CustomUserManager(UserManager):\n    def customize(self, app):\n        def _unique_email_validator(form, field):\n            if field.data.lower() == current_user.email.lower():\n                return\n            unique_email_validator(form, field)\n\n        # Add the email field and make first and last names as not required\n        class CustomUserProfileForm(EditUserProfileForm):\n            first_name = StringField(\"First name\")\n            last_name = StringField(\"Last name\")\n            email = StringField(\n                \"Email\",\n                validators=[\n                    validators.DataRequired(),\n                    validators.Email(),\n                    _unique_email_validator,\n                ],\n            )\n\n        self.EditUserProfileFormClass = CustomUserProfileForm\n\n        # Add the reCaptcha\n        if app.config.get(\"DISPLAY_RECAPTCHA\"):\n\n            class CustomRegisterForm(RegisterForm):\n                recaptcha = RecaptchaField()\n\n            self.RegisterFormClass = CustomRegisterForm\n\n        # Allow emails to be send using sendmail\n        if app.config.get(\"EMAIL_ADAPTER\") == \"sendmail\":\n            from flask_user.email_adapters import SendmailEmailAdapter\n\n            self.email_adapter = SendmailEmailAdapter(app)"}
{"Repository": "Spawning", "input": "Class to make finding out system information all in one place. className System(dict) Method __init__ Method __getattr__ Method __setattr__ Method __repr__", "label": "class System(dict):\n    def __init__(self):\n        dict.__init__(self, {\n            'architecture': platform.architecture(),\n            'max_int': sys.maxint,\n            'max_size': sys.maxsize,\n            'max_unicode': sys.maxunicode,\n            'name': platform.node(),\n            'path_seperator': os.path.sep,\n            'processor': platform.processor(),\n            'python_version': platform.python_version(),\n            'python_branch': platform.python_branch(),\n            'python_build': platform.python_build(),\n            'python_compiler': platform.python_compiler(),\n            'python_implementation': platform.python_implementation(),\n            'python_revision': platform.python_revision(),\n            'python_version_tuple': platform.python_version_tuple(),\n            'python_path': sys.path,\n            'login': os.getlogin(),\n            'system': platform.system(),\n            'temp_directory': tempfile.gettempdir(),\n            'uname': platform.uname(),\n    })\n\n    def __getattr__(self, name):\n        data = dict(self).get(name)\n        if data == None:\n            raise AttributeError(\"'%s' has no attribute '%s'\" % (\n                self.__class__.__name__, name))\n        return data\n\n    def __setattr__(self, key, value):\n        raise AttributeError(\"can't set attribute\")\n\n    def __repr__(self):\n        return unicode(\n            \"<Platform: system='%s', name='%s', arch=%s, processor='%s'>\" % (\n            self.system, self.name, self.architecture, self.processor))\n\n    # Method aliases\n    __str__ = __repr__\n    __unicode__ = __repr__\n    __setitem__ = __setattr__"}
{"Repository": "MONet-pytorch", "input": "This dataset class can load a set of natural images in RGB, and convert RGB format into (L, ab) pairs in Lab color space. className ColorizationDataset(BaseDataset) Method modify_commandline_options Method __init__ Method __getitem__ Method __len__ Attribute dir Attribute AB_paths Attribute transform", "label": "class ColorizationDataset(BaseDataset):\n    @staticmethod\n    def modify_commandline_options(parser, is_train):\n        parser.set_defaults(input_nc=1, output_nc=2, direction='AtoB')\n        return parser\n\n    def __init__(self, opt):\n        BaseDataset.__init__(self, opt)\n        self.dir = os.path.join(opt.dataroot)\n        self.AB_paths = sorted(make_dataset(self.dir, opt.max_dataset_size))\n        assert(opt.input_nc == 1 and opt.output_nc == 2 and opt.direction == 'AtoB')\n        self.transform = get_transform(self.opt, convert=False)\n\n    def __getitem__(self, index):\n        path = self.AB_paths[index]\n        im = Image.open(path).convert('RGB')\n        im = self.transform(im)\n        im = np.array(im)\n        lab = color.rgb2lab(im).astype(np.float32)\n        lab_t = transforms.ToTensor()(lab)\n        A = lab_t[[0], ...] / 50.0 - 1.0\n        B = lab_t[[1, 2], ...] / 110.0\n        return {'A': A, 'B': B, 'A_paths': path, 'B_paths': path}\n\n    def __len__(self):\n        return len(self.AB_paths)"}
{"Repository": "ghfeat", "input": "Defines the video writer. className VideoWriter(object) Method __init__ Method __del__ Method write Attribute path Attribute frame_height Attribute frame_width Attribute fps Attribute codec Attribute video", "label": "class VideoWriter(object):\n  def __init__(self, path, frame_height, frame_width, fps=24, codec='DIVX'):\n    self.path = path\n    self.frame_height = frame_height\n    self.frame_width = frame_width\n    self.fps = fps\n    self.codec = codec\n\n    self.video = cv2.VideoWriter(filename=path,\n                                 fourcc=cv2.VideoWriter_fourcc(*codec),\n                                 fps=fps,\n                                 frameSize=(frame_width, frame_height))\n\n  def __del__(self):\n    self.video.release()\n\n  def write(self, frame):\n    self.video.write(frame[:, :, ::-1])"}
{"Repository": "cotopaxi", "input": "Wrapper for all sniffer variables. className ReflectorSniffer(object) Method __init__ Method count_packet Method update_record_amplify Method __init__ Method filter_action Method __str__ Attribute packets_in_nr Attribute packets_out_nr Attribute packets_in_size Attribute packets_out_size Attribute packet_record_amplify Attribute packet_record_desc", "label": "class ReflectorSniffer(object):\n    class Statistics(object):\n        def __init__(self):\n            self.packets_in_nr = 0\n            self.packets_out_nr = 0\n            self.packets_in_size = 0\n            self.packets_out_size = 0\n            self.packet_record_amplify = 0\n            self.packet_record_desc = \"[.] No interesting amplification cases!\"\n\n        def count_packet(self, packet, direction_in):\n            if direction_in:\n                self.packets_in_nr += 1\n                self.packets_in_size += len(packet)\n            else:\n                self.packets_out_nr += 1\n                self.packets_out_size += len(packet)\n\n        def update_record_amplify(self, from_target, to_target, ampl_factor):\n            self.packet_record_amplify = ampl_factor\n            desc = [\n                \"[+] Highest amplify packet factor: {:0.2f}%\".format(ampl_factor),\n                \"TO TARGET\",\n                scrap_packet(to_target),\n                \"FROM TARGET\",\n                scrap_packet(from_target),\n                80 * \"-\",\n            ]\n            self.packet_record_desc = \"\\n\".join(desc)\n            return self.packet_record_desc\n\n    def __init__(self, options):\n        self.input_options = options\n        self.stats = self.Statistics()\n        self.time_displayed = time.time()\n        self.last_packet_in = None\n\n    def filter_action(self, packet):\n        if self.input_options:\n            observed_ip_addr = self.input_options.dest_ip\n            observed_port = parse_port(self.input_options.port)\n        else:\n            observed_ip_addr = \"1.1.1.1\"\n            observed_port = None\n\n        if IP in packet and UDP in packet:\n            if (\n                packet[IP].src == observed_ip_addr\n                and packet[IP].dst not in UNICAST_ADDRESSES\n            ):\n                self.stats.count_packet(packet, False)\n                if (\n                    self.last_packet_in\n                    and self.last_packet_in[IP].src == packet[IP].dst\n                    and self.last_packet_in[UDP].sport == packet[UDP].dport\n                ):\n                    ampl_factor = amplification_factor(\n                        len(self.last_packet_in), len(packet)\n                    )\n                    if self.input_options.verbose:\n                        print(\n                            \"Amplification factor of current packet: \"\n                            \"{:0.2f}%\".format(ampl_factor)\n                        )\n                    if ampl_factor > self.stats.packet_record_amplify:\n                        print(\n                            self.stats.update_record_amplify(\n                                packet, self.last_packet_in, ampl_factor\n                            )\n                        )\n\n            elif packet[IP].dst == observed_ip_addr:\n                self.stats.count_packet(packet, True)\n                self.last_packet_in = packet\n        if time.time() - self.time_displayed > self.input_options.interval:\n            self.time_displayed = time.time()\n            if observed_port:\n                target = \"{}:{}\".format(observed_ip_addr, observed_port)\n            else:\n                target = observed_ip_addr\n            return (\n                \"TARGET: {} | TO TARGET packets: {}, bytes: {} | FROM TARGET \"\n                \"packets: {}, bytes: {} | AMPLIF FACTOR: {:0.2f}%\"\n            ).format(\n                target,\n                self.stats.packets_in_nr,\n                self.stats.packets_in_size,\n                self.stats.packets_out_nr,\n                self.stats.packets_out_size,\n                amplification_factor(\n                    self.stats.packets_in_size, self.stats.packets_out_size\n                ),\n            )\n        return None\n\n    def __str__(self):\n        if self.stats.packet_record_desc:\n            return self.stats.packet_record_desc\n        return \"\""}
{"Repository": "PyDP", "input": "Customized setuptools build command - builds protos on build. className Build(build_ext) Method run", "label": "class Build(build_ext):\n    def run(self):\n        if platform.system() != \"Linux\":\n            # For Windows and Mac setup.py is not used for wheel build but the\n            # build is performed from GitHub actions files. The special\n            # treatment for Linux is required because wheel build is performed\n            # inside manylinux Docker images with the help of cibuildwheel\n            # (https://github.com/pypa/cibuildwheel). Building with manylinux\n            # ensures that PyDP can work on old Linux versions.\n            return\n\n        # Build _pydp.so (wrappers for C++).\n        os.system(\"./build_PyDP_linux.sh\")\n\n        # Copy _pydp.so to cibuildwheel directory.\n        pydp_lib = \"src/pydp/_pydp.so\"\n        version_str = f\"{sys.version_info.major}{sys.version_info.minor}\"\n        destination_dir = f\"build/lib.linux-x86_64-cpython-{version_str}/pydp\"\n        os.system(f\"cp {pydp_lib} {destination_dir}\")\n\n        build_ext.run(self)"}
{"Repository": "fondant", "input": "Component that transforms the incoming dataset partition per partition as a pandas DataFrame. className PandasTransformComponent(DaskComponent) Method transform", "label": "class PandasTransformComponent(DaskComponent):\n    @abstractmethod\n    def transform(self, dataframe: pd.DataFrame) -> pd.DataFrame:"}
{"Repository": "djangorestframework-camel-case", "input": "Parser for multipart form data, which may include file data. className CamelCaseMultiPartParser(MultiPartParser) Method parse", "label": "class CamelCaseMultiPartParser(MultiPartParser):\n    media_type = \"multipart/form-data\"\n\n    def parse(self, stream, media_type=None, parser_context=None):\n        parser_context = parser_context or {}\n        request = parser_context[\"request\"]\n        encoding = parser_context.get(\"encoding\", settings.DEFAULT_CHARSET)\n        meta = request.META.copy()\n        meta[\"CONTENT_TYPE\"] = media_type\n        upload_handlers = request.upload_handlers\n\n        try:\n            parser = DjangoMultiPartParser(meta, stream, upload_handlers, encoding)\n            data, files = parser.parse()\n            return DataAndFiles(\n                underscoreize(data, **api_settings.JSON_UNDERSCOREIZE),\n                underscoreize(files, **api_settings.JSON_UNDERSCOREIZE),\n            )\n        except MultiPartParserError as exc:\n            raise ParseError(\"Multipart form parse error - %s\" % str(exc))"}
{"Repository": "aws-cli", "input": "CLI arg parser based on an argument table. className ArgTableArgParser(CLIArgParser) Method __init__ Method _build Method parse_known_args", "label": "class ArgTableArgParser(CLIArgParser):\n    def __init__(self, argument_table, command_table=None):\n        # command_table is an optional subcommand_table.  If it's passed\n        # in, then we'll update the argparse to parse a 'subcommand' argument\n        # and populate the choices field with the command table keys.\n        super(ArgTableArgParser, self).__init__(\n            formatter_class=self.Formatter,\n            add_help=False,\n            usage=USAGE,\n            conflict_handler='resolve')\n        if command_table is None:\n            command_table = {}\n        self._build(argument_table, command_table)\n\n    def _build(self, argument_table, command_table):\n        for arg_name in argument_table:\n            argument = argument_table[arg_name]\n            argument.add_to_parser(self)\n        if command_table:\n            self.add_argument('subcommand', action=CommandAction,\n                              command_table=command_table, nargs='?')\n\n    def parse_known_args(self, args, namespace=None):\n        if len(args) == 1 and args[0] == 'help':\n            namespace = argparse.Namespace()\n            namespace.help = 'help'\n            return namespace, []\n        else:\n            return super(ArgTableArgParser, self).parse_known_args(\n                args, namespace)"}
{"Repository": "tornado.ext", "input": "Example:: db = SQLAlchemy(\"mysql://user:pass@host:port/db\", pool_recycle=3600) from sqlalchemy import Column, String class User(db. className SQLAlchemy(object) Method __init__ Method Model Method _ping_db Method create_db Attribute engine Attribute session Attribute slaves", "label": "class SQLAlchemy(object):\n    def __init__(self, master, slaves=[], **kwargs):\n        self.engine = create_engine(master, **kwargs)\n        self.session = create_session(self.engine)\n        self.slaves = []\n        for slave in  slaves:\n            slave = create_engine(slave, **kwargs)\n            self.slaves.append(create_session(slave))\n\n        if 'pool_recycle' in kwargs:\n            # ping db, so that mysql won't goaway\n            PeriodicCallback(self._ping_db,\n                             kwargs['pool_recycle'] * 1000).start()\n\n    @property\n    def Model(self):\n        if hasattr(self, '_base'):\n            base = self._base\n        else:\n            base = declarative_base(cls=Model, name='Model')\n            self._base = base\n        if self.slaves:\n            slave = random.choice(self.slaves)\n            base.query = slave.query_property()\n        else:\n            base.query = self.session.query_property()\n        return base\n\n    def _ping_db(self):\n        self.session.execute('show variables')\n        for slave in self.slaves:\n            slave.execute('show variables')\n\n    def create_db(self):\n        self.Model.metadata.create_all(self.engine)"}
{"Repository": "movingpandas", "input": "Generalizes based on distance. className MinDistanceGeneralizer(TrajectoryGeneralizer) Method _generalize_traj", "label": "class MinDistanceGeneralizer(TrajectoryGeneralizer):\n    def _generalize_traj(self, traj, tolerance):\n        temp_df = traj.df.copy()\n        prev_pt = temp_df.iloc[0][traj.get_geom_col()]\n        keep_rows = [0]\n        for i, pt in enumerate(temp_df[traj.get_geom_col()]):\n            dist = measure_distance(pt, prev_pt, traj.is_latlon)\n            if dist >= tolerance:\n                keep_rows.append(i)\n                prev_pt = pt\n\n        keep_rows.append(len(traj.df) - 1)\n        new_df = traj.df.iloc[keep_rows]\n        new_traj = Trajectory(new_df, traj.id, traj_id_col=traj.get_traj_id_col())\n        return new_traj"}
{"Repository": "related-website-sets", "input": "A validator was asked to validate an instance against an unknown type. className UnknownType(Exception) Method __init__ Method __str__ Attribute type Attribute instance Attribute schema", "label": "class UnknownType(Exception):\n    def __init__(self, type, instance, schema):\n        self.type = type\n        self.instance = instance\n        self.schema = schema\n\n    def __str__(self):\n        prefix = 16 * \" \"\n\n        return dedent(\n            f\"\"\"\\\n            Unknown type {self.type!r} for validator with schema:\n                {indent(pformat(self.schema, width=72), prefix).lstrip()}\n\n            While checking instance:\n                {indent(pformat(self.instance, width=72), prefix).lstrip()}\n            \"\"\".rstrip(),\n        )"}
{"Repository": "pandas_flavor", "input": "Support setup.py upload. className UploadCommand(Command) Method status Method initialize_options Method finalize_options Method run", "label": "class UploadCommand(Command):\n    description = \"Build and publish the package.\"\n    user_options = []\n\n    @staticmethod\n    def status(s: str):\n        print(\"\\033[1m{0}\\033[0m\".format(s))\n\n    def initialize_options(self):\n        pass\n\n    def finalize_options(self):\n        pass\n\n    def run(self):\n        try:\n            self.status(\"Removing previous builds\")\n            rmtree(os.path.join(here, \"dist\"))\n        except OSError:\n            pass\n\n        self.status(\"Building Source and Wheel (universal) distribution\")\n        os.system(\n            \"{0} setup.py sdist bdist_wheel --universal\".format(sys.executable)\n        )\n\n        self.status(\"Uploading the package to PyPi via Twine\")\n        os.system(\"twine upload dist/*\")\n\n        sys.exit()"}
{"Repository": "ToCo", "input": "from mmseg className PhotoMetricDistortion(object) Method convert Method brightness Method contrast Method saturation Method hue Method __call__ Method __repr__", "label": "class PhotoMetricDistortion(object):\n    def __init__(self,\n                 brightness_delta=32,\n                 contrast_range=(0.5, 1.5),\n                 saturation_range=(0.5, 1.5),\n                 hue_delta=18):\n        \n        self.brightness_delta = brightness_delta\n        self.contrast_lower, self.contrast_upper = contrast_range\n        self.saturation_lower, self.saturation_upper = saturation_range\n        self.hue_delta = hue_delta\n\n    def convert(self, img, alpha=1, beta=0):\n        img = img.astype(np.float32) * alpha + beta\n        img = np.clip(img, 0, 255)\n        return img.astype(np.uint8)\n\n    def brightness(self, img):\n        if np.random.randint(2):\n            return self.convert(\n                img,\n                beta=random.uniform(-self.brightness_delta,\n                                    self.brightness_delta))\n        return img\n\n    def contrast(self, img):\n        if np.random.randint(2):\n            return self.convert(\n                img,\n                alpha=random.uniform(self.contrast_lower, self.contrast_upper))\n        return img\n\n    def saturation(self, img):\n        if np.random.randint(2):\n            img = mmcv.bgr2hsv(img)\n            img[:, :, 1] = self.convert(\n                img[:, :, 1],\n                alpha=random.uniform(self.saturation_lower,\n                                     self.saturation_upper))\n            img = mmcv.hsv2bgr(img)\n        return img\n\n    def hue(self, img):\n        if np.random.randint(2):\n            img = mmcv.bgr2hsv(img)\n            img[:, :,\n                0] = (img[:, :, 0].astype(int) +\n                      np.random.randint(-self.hue_delta, self.hue_delta)) % 180\n            img = mmcv.hsv2bgr(img)\n        return img\n\n    def __call__(self, img):\n        #img = results['img']\n        # random brightness\n        img = self.brightness(img)\n\n        # mode == 0 --> do random contrast first\n        # mode == 1 --> do random contrast last\n        mode = np.random.randint(2)\n        if mode == 1:\n            img = self.contrast(img)\n\n        # random saturation\n        img = self.saturation(img)\n\n        # random hue\n        img = self.hue(img)\n\n        # random contrast\n        if mode == 0:\n            img = self.contrast(img)\n\n        #results['img'] = img\n        return img\n\n    def __repr__(self):\n        repr_str = self.__class__.__name__\n        repr_str += (f'(brightness_delta={self.brightness_delta}, '\n                     f'contrast_range=({self.contrast_lower}, '\n                     f'{self.contrast_upper}), '\n                     f'saturation_range=({self.saturation_lower}, '\n                     f'{self.saturation_upper}), '\n                     f'hue_delta={self.hue_delta})')\n        return repr_str"}
{"Repository": "RedditHumorDetection", "input": "Processor for the CoLA data set (GLUE version). className ColaProcessor(DataProcessor) Method get_train_examples Method get_dev_examples Method get_labels Method _create_examples", "label": "class ColaProcessor(DataProcessor):\n    def get_train_examples(self, data_dir):\n        return self._create_examples(\n            self._read_tsv(os.path.join(data_dir, \"train.tsv\")), \"train\")\n\n    def get_dev_examples(self, data_dir):\n        return self._create_examples(\n            self._read_tsv(os.path.join(data_dir, \"dev.tsv\")), \"dev\")\n\n    def get_labels(self):\n        return [\"0\", \"1\"]\n\n    def _create_examples(self, lines, set_type):\n        examples = []\n        for (i, line) in enumerate(lines):\n            guid = \"%s-%s\" % (set_type, i)\n            text_a = line[3]\n            label = line[1]\n            examples.append(\n                InputExample(guid=guid, text_a=text_a, text_b=None, label=label))\n        return examples"}
{"Repository": "kaggle-galaxies", "input": "Stochastic pooling implemented in Theano using reshapes, since the Pylearn2 class for it is way too slow. className StochasticPoolingC01BLayer(object) Method __init__ Method get_output_shape Method output Attribute pool_size Attribute epsilon Attribute input_layer Attribute input_shape Attribute mb_size Attribute params Attribute bias_params", "label": "class StochasticPoolingC01BLayer(object):\n    def __init__(self, input_layer, pool_size, epsilon=1e-12):\n        self.pool_size = pool_size\n        self.epsilon = epsilon\n        self.input_layer = input_layer\n        self.input_shape = self.input_layer.get_output_shape()\n        self.mb_size = self.input_layer.mb_size\n\n        self.params = []\n        self.bias_params = []\n\n    def get_output_shape(self):\n        output_shape = list(self.input_shape) # make a mutable copy\n        output_shape[1] = output_shape[1] // self.pool_size\n        output_shape[2] = output_shape[2] // self.pool_size\n        return tuple(output_shape)\n\n    def output(self, dropout_active=True, *args, **kwargs):\n        input = self.input_layer.output(*args, **kwargs)\n\n        output_shape = self.get_output_shape()\n        pool_shape = (output_shape[0], output_shape[1], self.pool_size, output_shape[2], self.pool_size, output_shape[3])\n        merged_shape = (output_shape[0], output_shape[1], output_shape[2], output_shape[3], self.pool_size**2)\n        flat_shape = (output_shape[0] * output_shape[1] * output_shape[2] * output_shape[3], self.pool_size**2)\n        input_reshaped = input.reshape(pool_shape).transpose(0, 1, 3, 5, 2, 4).reshape(flat_shape) #pools are now in axis 4\n\n        input_reshaped += self.epsilon # add a small constant to prevent division by 0 in what follows.\n\n        if dropout_active:\n            probabilities = input_reshaped / input_reshaped.sum(axis=1, keepdims=True)\n            samples = layers.srng.multinomial(pvals=probabilities, dtype=theano.config.floatX)\n            output_flat = T.sum(input_reshaped * samples, axis=1)\n            output = output_flat.reshape(output_shape)\n        else:\n            # no dropout, so compute the weighted average instead.\n            # this amounts to the sum of squares normalised by the sum of the values.\n            numerator = T.sum(input_reshaped**2, axis=1)\n            denominator = T.sum(input_reshaped, axis=1)\n            output_flat = numerator / denominator\n            output = output_flat.reshape(output_shape)\n            \n        return output"}
{"Repository": "Pixelization", "input": "This class includes test options. className TestOptions(BaseOptions) Method initialize", "label": "class TestOptions(BaseOptions):\n    def initialize(self, parser):\n        parser = BaseOptions.initialize(self, parser)  # define shared options\n        parser.add_argument('--results_dir', type=str, default='./results/', help='saves results here.')\n        parser.add_argument('--aspect_ratio', type=float, default=1.0, help='aspect ratio of result images')\n        parser.add_argument('--phase', type=str, default='test', help='train, val, test, etc')\n        # Dropout and Batchnorm has different behavioir during training and test.\n        parser.add_argument('--eval', action='store_true', help='use eval mode during test time.')\n        parser.add_argument('--num_test', type=int, default=50, help='how many test images to run')\n        # rewrite devalue values\n        parser.set_defaults(model='pixelization')\n        # To avoid cropping, the load_size should be the same as crop_size\n        parser.set_defaults(load_size=parser.get_default('crop_size'))\n        self.isTrain = False\n        return parser"}
{"Repository": "django-anymail", "input": "(useful only for these tests) className SettingsTestBackend(TestBackend) Method __init__ Attribute sample_setting Attribute username Attribute password", "label": "class SettingsTestBackend(TestBackend):\n    def __init__(self, *args, **kwargs):\n        esp_name = self.esp_name\n        self.sample_setting = get_anymail_setting(\n            \"sample_setting\", esp_name=esp_name, kwargs=kwargs, allow_bare=True\n        )\n        self.username = get_anymail_setting(\n            \"username\", esp_name=esp_name, kwargs=kwargs, default=None, allow_bare=True\n        )\n        self.password = get_anymail_setting(\n            \"password\", esp_name=esp_name, kwargs=kwargs, default=None, allow_bare=True\n        )\n        super().__init__(*args, **kwargs)"}
{"Repository": "back2future.pytorch", "input": "Computes and stores the average and current value className AverageMeter(object) Method __init__ Method reset Method update Method __repr__ Attribute meters Attribute precision", "label": "class AverageMeter(object):\n    def __init__(self, i=1, precision=3):\n        self.meters = i\n        self.precision = precision\n        self.reset(self.meters)\n\n    def reset(self, i):\n        self.val = [0]*i\n        self.avg = [0]*i\n        self.sum = [0]*i\n        self.count = 0\n\n    def update(self, val, n=1):\n        if not isinstance(val, list):\n            val = [val]\n        assert(len(val) == self.meters)\n        self.count += n\n        for i,v in enumerate(val):\n            self.val[i] = v\n            self.sum[i] += v * n\n            self.avg[i] = self.sum[i] / self.count\n\n    def __repr__(self):\n        val = ' '.join(['{:.{}f}'.format(v, self.precision) for v in self.val])\n        avg = ' '.join(['{:.{}f}'.format(a, self.precision) for a in self.avg])\n        return '{} ({})'.format(val, avg)"}
{"Repository": "mechanize", "input": "HTTP server w/ a few modifications that make it useful for loopback testing purposes. className LoopbackHttpServer(HTTPServer) Method __init__ Method get_request", "label": "class LoopbackHttpServer(HTTPServer):\n    def __init__(self, server_address, RequestHandlerClass):\n        HTTPServer.__init__(self, server_address, RequestHandlerClass)\n\n        # Set the timeout of our listening socket really low so\n        # that we can stop the server easily.\n        self.socket.settimeout(1.0)\n\n    def get_request(self):\n        request, client_address = self.socket.accept()\n\n        # It's a loopback connection, so setting the timeout\n        # really low shouldn't affect anything, but should make\n        # deadlocks less likely to occur.\n        request.settimeout(10.0)\n\n        return (request, client_address)"}
{"Repository": "neuralbody", "input": "Sampler certain frames for test className FrameSampler(Sampler) Method __init__ Method __iter__ Method __len__ Attribute inds", "label": "class FrameSampler(Sampler):\n    def __init__(self, dataset):\n        inds = np.arange(0, len(dataset.ims))\n        ni = len(dataset.ims) // dataset.num_cams\n        inds = inds.reshape(ni, -1)[::cfg.test.frame_sampler_interval]\n        self.inds = inds.ravel()\n\n    def __iter__(self):\n        return iter(self.inds)\n\n    def __len__(self):\n        return len(self.inds)"}
{"Repository": "pyarmor", "input": "Help message formatter which adds default values to argument help. className ArgumentDefaultsHelpFormatter(HelpFormatter) Method _get_help_string", "label": "class ArgumentDefaultsHelpFormatter(HelpFormatter):\n    def _get_help_string(self, action):\n        help = action.help\n        if '%(default)' not in action.help:\n            if action.default is not SUPPRESS:\n                defaulting_nargs = [OPTIONAL, ZERO_OR_MORE]\n                if action.option_strings or action.nargs in defaulting_nargs:\n                    help += ' (default: %(default)s)'\n        return help"}
{"Repository": "scouter", "input": "A context manager that replaces :func:`torch. className ReLUContext(object) Method __init__ Method __enter__ Method __exit__ Attribute relu_func Attribute patches", "label": "class ReLUContext(object):\n    def __init__(self, relu_func):\n        assert isinstance(relu_func, torch.autograd.function.FunctionMeta)\n        self.relu_func = relu_func\n        self.patches = []\n\n    def __enter__(self):\n        relu = self.relu_func().apply\n        self.patches = [\n            Patch('torch.relu', relu),\n            Patch('torch.relu_', relu),\n        ]\n        return self\n\n    def __exit__(self, type, value, traceback):\n        for p in self.patches:\n            p.remove()\n        return False  # re-raise any exception"}
{"Repository": "3D-RetinaNet", "input": "Normalize a tensor image with mean and standard deviation. className Normalize(object) Method __init__ Method __call__ Attribute mean Attribute std", "label": "class Normalize(object):\n    def __init__(self, mean, std):\n        self.mean = mean\n        self.std = std\n\n    def __call__(self, clip):\n        for i in range(len(self.mean)):\n            clip[i] = (clip[i] - self.mean[i])/ self.std[i]\n        # print('after norm ', clip.shape)\n        return clip"}
{"Repository": "picopt", "input": "Container for reported stats from optimization operations. className ReportStats(ReportStatBase) Method _new_percent_saved Method _get_full_path Method _report_saved Method _report_error Method report", "label": "class ReportStats(ReportStatBase):\n    _TAB = \" \" * 4\n\n    def __init__(\n        self,\n        *args,\n        config: AttrDict | None = None,\n        path_info: PathInfo | None = None,\n        **kwargs,\n    ) -> None:\n        # Don't store these large data structs, just tidbits.\n        self.bigger: bool = config.bigger if config else False\n        self.test: bool = config.test if config else False\n        self.convert: bool = path_info.convert if path_info else False\n        self.container_paths: tuple[str, ...] = (\n            tuple(path_info.container_paths) if path_info else ()\n        )\n        super().__init__(*args, **kwargs)\n        self.saved = self.bytes_in - self.bytes_out\n\n    def _new_percent_saved(self) -> str:\n        ratio = 1.0 if self.bytes_in <= 0 else self.bytes_out / self.bytes_in\n        saved = naturalsize(self.saved)\n        percent_saved = (1 - ratio) * 100\n\n        return f\"{percent_saved:.2f}% ({saved})\"\n\n    def _get_full_path(self) -> str:\n        cps = self.container_paths\n        return CONTAINER_PATH_DELIMETER.join((*cps, str(self.path)))\n\n    def _report_saved(self) -> str:\n        report = f\"{self._get_full_path()}: \"\n        report += self._new_percent_saved()\n        if self.test:\n            report += \" would be\"\n        if self.saved > 0:\n            report += \" saved\"\n        elif self.saved < 0:\n            report += \" lost\"\n        if self.saved <= 0 and not self.bigger:\n            report += \", kept original\"\n\n        return report\n\n    def _report_error(self) -> str:\n        report = f\"ERROR: {self._get_full_path()}\"\n        if isinstance(self.exc, CalledProcessError):\n            report += f\"\\n{self._TAB}retcode: {self.exc.returncode}\"\n            if self.exc.cmd:\n                cmd = \" \".join(self.exc.cmd)\n                report += f\"\\n{self._TAB}command: {cmd}\"\n            if self.exc.stdout:\n                report += f\"\\n{self._TAB}stdout: {self.exc.stdout}\"\n            if self.exc.stderr:\n                report += f\"\\n{self._TAB}stderr: {self.exc.stderr}\"\n        else:\n            report += f\"\\n{self._TAB}{self.exc!s}\"\n        return report\n\n    def report(self) -> None:\n        attrs = []\n        if self.exc:\n            report = self._report_error()\n            color: Color = \"red\"\n        else:\n            report = self._report_saved()\n            color: Color = \"cyan\" if self.convert else \"white\"\n\n            if self.saved <= 0:\n                color: Color = \"blue\"\n                attrs: list[Attribute] = [\"bold\"]\n\n        cprint(report, color, attrs=attrs)"}
{"Repository": "manim-slides", "input": "WARNING: this example does not seem to work with ManimGL. className ConvertExample(Slide) Method construct", "label": "class ConvertExample(Slide):\n    def construct(self):\n        self.wait_time_between_slides = 0.1\n\n        title = VGroup(\n            Text(\"From Manim animations\", t2c={\"From\": BLUE}),\n            Text(\"to slides presentation\", t2c={\"to\": BLUE}),\n            Text(\"with Manim Slides\", t2w={\"[-12:]\": BOLD}, t2c={\"[-13:]\": YELLOW}),\n        ).arrange(DOWN)\n\n        step_1 = Text(\"1. In your scenes file, import Manim Slides\")\n        step_2 = Text(\"2. Replace Scene with Slide\")\n        step_3 = Text(\"3. In construct, add pauses where you need\")\n        step_4 = Text(\"4. You can also create loops\")\n        step_5 = Text(\"5. Render you scene with Manim\")\n        step_6 = Text(\"6. Open your presentation with Manim Slides\")\n\n        for step in [step_1, step_2, step_3, step_4, step_5, step_6]:\n            step.scale(0.5).to_corner(UL)\n\n        step = step_1\n\n        self.play(FadeIn(title))\n\n        self.next_slide()\n\n        code = Code(\n            code=\"\"\"from manim import *"}
{"Repository": "two-scoops-of-django-3.x", "input": "Standard detail view className SprinkleDetail(DetailView) Method dispatch", "label": "class SprinkleDetail(DetailView):\n    model = Sprinkle\n\n    def dispatch(self, request, *args, **kwargs):\n        request = check_sprinkles(request)\n        return super().dispatch(request, *args, **kwargs)"}
{"Repository": "CMUA-Watermark", "input": "Average Smoothing 2D. className AverageSmoothing2D(ConvSmoothing2D) Method __init__", "label": "class AverageSmoothing2D(ConvSmoothing2D):\n    def __init__(self, channels, kernel_size):\n        kernel = torch.ones((channels, 1, kernel_size, kernel_size)) / (\n            kernel_size * kernel_size)\n        super(AverageSmoothing2D, self).__init__(kernel)"}
{"Repository": "homeassistant", "input": "Xiaomi Sensor info className SensorInfo(object) Method __init__ Attribute name Attribute data_key Attribute units Attribute max Attribute min Attribute value_modifier", "label": "class SensorInfo(object):\n    def __init__(self, name, data_key, units, max_value, min_value, value_modifier):\n\n        self.name = name\n\n        self.data_key = data_key\n\n        self.units = units\n\n        self.max = max_value\n\n        self.min = min_value\n\n        self.value_modifier = value_modifier"}
{"Repository": "fglib", "input": "Input-output variable node. className IOVNode(VNode) Method __init__ Method set_callback", "label": "class IOVNode(VNode):\n    def __init__(self, label, init=None, observed=False, callback=None):\n        super().__init__(label, init, observed)\n        if callback is not None:\n            self.set_callback(callback)\n\n    def set_callback(self, callback):\n        self.spa = MethodType(callback, self)\n        self.mpa = MethodType(callback, self)\n        self.msa = MethodType(callback, self)\n        self.mf = MethodType(callback, self)"}
{"Repository": "HarmonyView", "input": "returns a pair of (image, caption) className CocoImagesAndCaptionsTrain2014(CocoBase) Method __init__ Method get_split Method year", "label": "class CocoImagesAndCaptionsTrain2014(CocoBase):\n    def __init__(self, size, onehot_segmentation=False, use_stuffthing=False, crop_size=None, force_no_crop=False,crop_type='random'):\n        super().__init__(size=size,\n                         dataroot=\"data/coco/train2014\",\n                         datajson=\"data/coco/annotations2014/annotations/captions_train2014.json\",\n                         onehot_segmentation=onehot_segmentation,\n                         use_stuffthing=use_stuffthing, crop_size=crop_size, force_no_crop=force_no_crop,\n                         use_segmentation=False,\n                         crop_type=crop_type)\n\n    def get_split(self):\n        return \"train\"\n\n    def year(self):\n        return '2014'"}
{"Repository": "bump-pydantic", "input": "This codemod adds the default value `None` to all fields of a pydantic model that are either type `Optional[T]`, `Union[T, None]` or `Any`. className AddDefaultNoneCommand(VisitorBasedCodemodCommand) Method __init__ Method visit_ClassDef Method leave_ClassDef Method visit_AnnAssign Method leave_AnnAssign", "label": "class AddDefaultNoneCommand(VisitorBasedCodemodCommand):\n    METADATA_DEPENDENCIES = (FullyQualifiedNameProvider,)\n\n    def __init__(self, context: CodemodContext) -> None:\n        super().__init__(context)\n\n        self.inside_base_model = False\n        self.should_add_none = False\n\n    def visit_ClassDef(self, node: cst.ClassDef) -> None:\n        fqn_set = self.get_metadata(FullyQualifiedNameProvider, node)\n\n        if not fqn_set:\n            return None\n\n        fqn: QualifiedName = next(iter(fqn_set))  # type: ignore\n        if fqn.name in self.context.scratch[ClassDefVisitor.BASE_MODEL_CONTEXT_KEY]:\n            self.inside_base_model = True\n\n    def leave_ClassDef(self, original_node: cst.ClassDef, updated_node: cst.ClassDef) -> cst.ClassDef:\n        self.inside_base_model = False\n        return updated_node\n\n    def visit_AnnAssign(self, node: cst.AnnAssign) -> None:\n        if m.matches(\n            node.annotation.annotation,\n            m.Subscript(m.Name(\"Optional\") | m.Attribute(m.Name(\"typing\"), m.Name(\"Optional\")))\n            | m.Subscript(\n                m.Name(\"Union\") | m.Attribute(m.Name(\"typing\"), m.Name(\"Union\")),\n                slice=[\n                    m.ZeroOrMore(),\n                    m.SubscriptElement(slice=m.Index(m.Name(\"None\"))),\n                    m.ZeroOrMore(),\n                ],\n            )\n            | m.Name(\"Any\")\n            | m.Attribute(m.Name(\"typing\"), m.Name(\"Any\"))\n            # TODO: This can be recursive. Can it?\n            | m.BinaryOperation(operator=m.BitOr(), left=m.Name(\"None\"))\n            | m.BinaryOperation(operator=m.BitOr(), right=m.Name(\"None\")),\n        ):\n            self.should_add_none = True\n        return None\n\n    def leave_AnnAssign(self, original_node: cst.AnnAssign, updated_node: cst.AnnAssign) -> cst.AnnAssign:\n        if self.inside_base_model and self.should_add_none:\n            if updated_node.value is None:\n                updated_node = updated_node.with_changes(value=cst.Name(\"None\"))\n            elif m.matches(updated_node.value, m.Call(func=m.Name(\"Field\"))):\n                assert isinstance(updated_node.value, cst.Call)\n                args = updated_node.value.args\n                if args:\n                    # NOTE: It has a \"default\" value as positional argument. Nothing to do.\n                    if args[0].keyword is None:\n                        ...\n                    # NOTE: It has a \"default\" or \"default_factory\" keyword argument. Nothing to do.\n                    elif any(arg.keyword and arg.keyword.value in (\"default\", \"default_factory\") for arg in args):\n                        ...\n                    else:\n                        updated_node = updated_node.with_changes(\n                            value=updated_node.value.with_changes(args=[cst.Arg(value=cst.Name(\"None\")), *args])\n                        )\n\n                # NOTE: This is the case where `Field` is called without any arguments e.g. `Field()`.\n                else:\n                    updated_node = updated_node.with_changes(\n                        value=updated_node.value.with_changes(args=[cst.Arg(value=cst.Name(\"None\"))])  # type: ignore\n                    )\n\n        self.inside_an_assign = False\n        self.should_add_none = False\n        return updated_node"}
{"Repository": "name_matching", "input": "Bag distance. className Bag(_TokenDistance) Method dist_abs Method dist", "label": "class Bag(_TokenDistance):\n    def __init__(\n        self,\n        tokenizer: Optional[_Tokenizer] = None,\n        intersection_type: str = 'crisp',\n        **kwargs: Any\n    ) -> None:\n        if tokenizer is None:\n            tokenizer = CharacterTokenizer()\n        super(Bag, self).__init__(\n            tokenizer=tokenizer, intersection_type=intersection_type, **kwargs\n        )\n\n    def dist_abs(self, src: str, tar: str, normalized: bool = False) -> float:\n        if tar == src:\n            return 0\n        elif not src:\n            return len(tar)\n        elif not tar:\n            return len(src)\n\n        self._tokenize(src, tar)\n\n        dist = max(self._src_only_card(), self._tar_only_card())\n\n        if normalized:\n            dist /= max(self._src_card(), self._tar_card())\n\n        return dist\n\n    def dist(self, src: str, tar: str) -> float:\n        if tar == src:\n            return 0.0\n        if not src or not tar:\n            return 1.0\n\n        return self.dist_abs(src, tar, normalized=True)"}
{"Repository": "AS-GCN", "input": "Base layer class. className Layer(object) Method __init__ Method _call Method __call__ Method _log_vars Attribute name Attribute vars Attribute logging Attribute sparse_inputs", "label": "class Layer(object):\n    def __init__(self, **kwargs):\n        allowed_kwargs = {'name', 'logging'}\n        for kwarg in kwargs.keys():\n            assert kwarg in allowed_kwargs, 'Invalid keyword argument: ' + kwarg\n        name = kwargs.get('name')\n        if not name:\n            layer = self.__class__.__name__.lower()\n            name = layer + '_' + str(get_layer_uid(layer))\n        self.name = name\n        self.vars = {}\n        logging = kwargs.get('logging', False)\n        self.logging = logging\n        self.sparse_inputs = False\n\n    def _call(self, inputs):\n        return inputs\n\n    def __call__(self, inputs):\n        with tf.name_scope(self.name):\n            if self.logging and not self.sparse_inputs:\n                tf.summary.histogram(self.name + '/inputs', inputs)\n            outputs = self._call(inputs)\n            if self.logging:\n                for output in outputs:\n                    tf.summary.histogram(self.name + '/outputs', output)\n            return outputs\n\n    def _log_vars(self):\n        for var in self.vars:\n            tf.summary.histogram(self.name + '/vars/' + var, self.vars[var])"}
{"Repository": "pyemu", "input": "Convenience section handling class. className SectionStructure(Structure) Method get_data Method get_offset_from_rva Method contains", "label": "class SectionStructure(Structure):\n    def get_data(self, start, length=None):\n        end = None\n        offset = start - self.VirtualAddress\n\n        if length:\n            end = offset+length\n        return self.data[offset:end]\n\n    def get_offset_from_rva(self, rva):\n        return (rva - self.VirtualAddress) + self.PointerToRawData\n\n    def contains(self, address):\n        return address>=self.VirtualAddress and address<self.VirtualAddress+len(self.data)"}
{"Repository": "torndsession", "input": "save session data in process memory className MemorySession(SessionDriver) Method __init__ Method get Method save Method clear Method remove_expires Attribute _data_handler", "label": "class MemorySession(SessionDriver):\n    MAX_SESSION_OBJECTS = 1024\n    def __init__(self, **settings):\n        # check settings\n        super(MemorySession, self).__init__(**settings)\n        host = settings.get(\"host\")\n        if not host:\n            raise SessionConfigurationError(\n                'memory session driver can not found persistence position')\n        if not hasattr(host, \"session_container\"):\n            setattr(host, \"session_container\", {})\n        self._data_handler = host.session_container\n\n    def get(self, session_id):\n        if session_id not in self._data_handler:\n            return {}\n\n        session_obj = self._data_handler[session_id]\n        now = datetime.utcnow()\n        expires = session_obj.get('__expires__', now)\n        if expires > now:\n            return session_obj\n        return {}\n\n    def save(self, session_id, session_data, expires=None):\n        session_data = session_data or {}\n        if expires:\n            session_data.update(__expires__=expires)\n        if len(self._data_handler) >= self.MAX_SESSION_OBJECTS:\n            self.remove_expires()\n        if len(self._data_handler) >= self.MAX_SESSION_OBJECTS:\n            print(\"system session pool is full. need more memory to save session object.\")\n        self._data_handler[session_id] = session_data\n\n    def clear(self, session_id):\n        if self._data_handler.haskey(session_id):\n            del self._data_handler[session_id]\n\n    def remove_expires(self):\n        keys = []\n        for key, val in iteritems(self._data_handler):\n            now = datetime.utcnow()\n            expires = val.get(\"__expires__\", now)\n            if now >= expires:\n                keys.append(key)\n        for key in keys:\n            del self._data_handler[key]"}
{"Repository": "machine-learning-diff-private-federated-learning", "input": "class that defines a conv layer. className ConvParameters(object) Method __init__ Attribute patch_size Attribute stride Attribute in_channels Attribute out_channels Attribute with_bias Attribute relu Attribute max_pool Attribute max_pool_size Attribute max_pool_stride Attribute trainable Attribute in_size Attribute name Attribute num_outputs Attribute bias_stddev", "label": "class ConvParameters(object):\n  def __init__(self):\n    self.patch_size = 5\n    self.stride = 1\n    self.in_channels = 1\n    self.out_channels = 0\n    self.with_bias = True\n    self.relu = True\n    self.max_pool = True\n    self.max_pool_size = 2\n    self.max_pool_stride = 2\n    self.trainable = False\n    self.in_size = 28\n    self.name = \"\"\n    self.num_outputs = 0\n    self.bias_stddev = 0.1"}
{"Repository": "sds1004x_bode", "input": "Parses the commands sent by the oscilloscope and sends them to the AWG. className CommandParser(object) Method __init__ Method parse_scpi_command Method parse_bswv Method parse_outp Attribute awg", "label": "class CommandParser(object):\n    def __init__(self, awg):\n        self.awg = awg\n\n    def parse_scpi_command(self, line):\n        if line.endswith(\"?\"):\n            return\n\n        channel = int(line[1])\n        \n        commands = line[3:].split(';')\n        \n        for command in commands:\n            token = command[0:4]\n            args = command[5:].split(',')\n            \n            if token == \"BSWV\":\n                self.parse_bswv(args, channel)\n            \n            elif token == \"OUTP\":\n                self.parse_outp(args, channel)\n    \n    def parse_bswv(self, args, channel):\n        n = 0\n        while n < len(args):\n            if args[n] == \"WVTP\":\n                self.awg.set_wave_type(channel, constants.SINE)\n                n += 2\n\n            elif args[n] == \"FRQ\":\n                freq = float(args[n+1])\n                self.awg.set_frequency(channel, freq)\n                n += 2\n\n            elif args[n] == \"AMP\":\n                ampl = float(args[n+1])\n                self.awg.set_amplitue(channel, ampl)\n                n += 2\n            \n            elif args[n] == \"OFST\":\n                offset = float(args[n+1])\n                self.awg.set_offset(channel, offset)\n                n += 2\n            \n            elif args[n] == \"PHSE\":\n                phase = float(args[n+1])\n                self.awg.set_phase(phase)\n                n += 2\n            \n            else:\n                n += 1\n    \n    def parse_outp(self, args, channel):\n        n = 0\n        while n < len(args):\n            if args[n] == \"ON\":\n                self.awg.enable_output(channel, True)\n                n += 1\n\n            elif args[n] == \"LOAD\":\n                if args[n+1] == \"HZ\":\n                    z = constants.HI_Z\n                else:\n                    z = int(args[n+1]) \n                self.awg.set_load_impedance(channel, z)\n                n += 2\n            \n            elif args[n] == \"OFF\":\n                self.awg.enable_output(channel, False)\n                n += 1\n            \n            else:\n                n += 1"}
{"Repository": "ghizmo", "input": "Assembles user-supplied arguments plus environment vars for convenient access. className UserArgs(object) Method __init__ Method add_explicit Method add_implicit Method get Method get_bool Method get_int Method get_float Method __getattr__ Method __str__ Attribute _explicit_keys Attribute dict", "label": "class UserArgs(object):\n  def __init__(self):\n    self._explicit_keys = []\n    self.dict = {}\n\n  def add_explicit(self, d):\n    self.dict.update(d)\n    self._explicit_keys.extend(list(d.keys()))\n\n  def add_implicit(self, d):\n    self.dict.update(d)\n\n  def get(self, item, default=None):\n    return self.dict[item] if item in self.dict else default\n\n  def get_bool(self, item, default=False):\n    return to_bool(self.get(item, default))\n\n  def get_int(self, item, default):\n    val = self.get(item, default)\n    return None if val is None else int(val)\n\n  def get_float(self, item, default):\n    val = self.get(item, default)\n    return None if val is None else float(val)\n\n  def __getattr__(self, item):\n    try:\n      return self.dict[item]\n    except KeyError:\n      raise AttributeError(\"Missing user-supplied argument '%s' (set with: --arg %s=VALUE)\" % (item, item))\n\n  def __str__(self):\n    return \", \".join([\"%s=%s\" % (key, self.dict[key]) for key in self._explicit_keys])"}
{"Repository": "P2B", "input": "Computes and stores the Precision className Precision(object) Method __init__ Method reset Method add_accuracy Method count Method value Method average Attribute max_accuracy Attribute Xaxis", "label": "class Precision(object):\n    def __init__(self, n=21, max_accuracy=2):\n        self.max_accuracy = max_accuracy\n        self.Xaxis = np.linspace(0, self.max_accuracy, n)\n        self.reset()\n\n    def reset(self):\n        self.accuracies = []\n\n    def add_accuracy(self, val):\n        self.accuracies.append(val)\n\n    @property\n    def count(self):\n        return len(self.accuracies)\n\n    @property\n    def value(self):\n        prec = [\n            np.sum(i <= thres\n                   for i in self.accuracies).astype(float) / self.count\n            for thres in self.Xaxis\n        ]\n        return np.array(prec)\n\n    @property\n    def average(self):\n        if len(self.accuracies) == 0:\n            return 0\n        return np.trapz(self.value, x=self.Xaxis) * 100 / self.max_accuracy"}
{"Repository": "solax", "input": "X1-Smart with Pocket WiFi v2. className X1Smart(Inverter) Method response_decoder Method inverter_serial_number_getter Method build_all_variants", "label": "class X1Smart(Inverter):\n    # pylint: disable=duplicate-code\n    _schema = vol.Schema(\n        {\n            vol.Required(\"type\", \"type\"): vol.All(int, 8),\n            vol.Required(\n                \"sn\",\n            ): str,\n            vol.Required(\"ver\"): str,\n            vol.Required(\"data\"): vol.Schema(\n                vol.All(\n                    [vol.Coerce(float)],\n                    vol.Length(min=200, max=200),\n                )\n            ),\n            vol.Required(\"information\"): vol.Schema(vol.All(vol.Length(min=8, max=8))),\n        },\n        extra=vol.REMOVE_EXTRA,\n    )\n\n    @classmethod\n    def response_decoder(cls):\n        return {\n            \"Network Voltage\": (0, Units.V, div10),\n            \"Output Current\": (1, Units.A, div10),\n            \"AC Power\": (2, Units.W),\n            \"PV1 Voltage\": (3, Units.V, div10),\n            \"PV2 Voltage\": (4, Units.V, div10),\n            \"PV1 Current\": (5, Units.A, div10),\n            \"PV2 Current\": (6, Units.A, div10),\n            \"PV1 Power\": (7, Units.W),\n            \"PV2 Power\": (8, Units.W),\n            \"Grid Frequency\": (9, Units.HZ, div100),\n            \"Total Energy\": (11, Total(Units.KWH), div10),\n            \"Today's Energy\": (13, DailyTotal(Units.KWH), div10),\n            \"Inverter Temperature\": (39, Units.C),\n            \"Exported Power\": (48, Units.W, to_signed),\n            \"Total Feed-in Energy\": (50, Total(Units.KWH), div100),\n            \"Total Consumption\": (52, Total(Units.KWH), div100),\n        }\n\n    @classmethod\n    def inverter_serial_number_getter(cls, response: Dict[str, Any]) -> Optional[str]:\n        return response[\"information\"][2]\n\n    @classmethod\n    def build_all_variants(cls, host, port, pwd=\"\"):\n        versions = [\n            cls._build(host, port, pwd, True),\n            cls._build(host, port, pwd, False),\n        ]\n        for inverter in versions:\n            inverter.http_client = inverter.http_client.with_headers(\n                {\"X-Forwarded-For\": \"5.8.8.8\"}\n            )\n        return versions"}
{"Repository": "mcts", "input": "The central MCTS class, which performs the tree search. className MCTS(object) Method __init__ Method __call__ Attribute tree_policy Attribute default_policy Attribute backup", "label": "class MCTS(object):\n    def __init__(self, tree_policy, default_policy, backup):\n        self.tree_policy = tree_policy\n        self.default_policy = default_policy\n        self.backup = backup\n\n    def __call__(self, root, n=1500):\n        if root.parent is not None:\n            raise ValueError(\"Root's parent must be None.\")\n\n        for _ in range(n):\n            node = _get_next_node(root, self.tree_policy)\n            node.reward = self.default_policy(node)\n            self.backup(node)\n\n        return utils.rand_max(root.children.values(), key=lambda x: x.q).action"}
{"Repository": "hyppo", "input": "A source of data allowing resampling. className DataSource(ABC) Method sample", "label": "class DataSource(ABC):\n    @abstractmethod\n    def sample(self, n, seed):\n        raise NotImplementedError()"}
{"Repository": "deepAutoController", "input": "Do not use this class as a learning objective. className MeanRelativeSquaredReconstructionError(GSNFriendlyCost) Method cost", "label": "class MeanRelativeSquaredReconstructionError(GSNFriendlyCost):\n    @staticmethod\n    def cost(a, b):\n        val1 = ((a - b) ** 2).sum(axis=1)\n        val2 = ((a - a.mean(axis=0)) ** 2).sum(axis=1)\n        return (val1/val2).mean()"}
{"Repository": "checkmate", "input": "Maintains a directory containing only the best n checkpoints Inside the directory is a best_checkpoints JSON file containing a dictionary mapping of the best checkpoint filepaths to the values by which the checkpoints are compared. className BestCheckpointSaver(object) Method __init__ Method handle Method _save_best_checkpoints_file Method _remove_outdated_checkpoint_files Method _update_internal_saver_state Method _load_best_checkpoints_file Method _sort Attribute _num_to_keep Attribute _save_dir Attribute _save_path Attribute _maximize Attribute _saver Attribute best_checkpoints_file", "label": "class BestCheckpointSaver(object):\n    def __init__(self, save_dir, num_to_keep=1, maximize=True, saver=None):\n        self._num_to_keep = num_to_keep\n        self._save_dir = save_dir\n        self._save_path = os.path.join(save_dir, 'best.ckpt')\n        self._maximize = maximize\n        self._saver = saver if saver else tf.train.Saver(\n            max_to_keep=None,\n            save_relative_paths=True\n        )\n\n        if not os.path.exists(save_dir):\n            os.makedirs(save_dir)\n        self.best_checkpoints_file = os.path.join(save_dir, 'best_checkpoints')\n\n    def handle(self, value, sess, global_step_tensor):\n        global_step = sess.run(global_step_tensor)\n        current_ckpt = 'best.ckpt-{}'.format(global_step)\n        value = float(value)\n        if not os.path.exists(self.best_checkpoints_file):\n            self._save_best_checkpoints_file({current_ckpt: value})\n            self._saver.save(sess, self._save_path, global_step_tensor)\n            return\n\n        best_checkpoints = self._load_best_checkpoints_file()\n\n        if len(best_checkpoints) < self._num_to_keep:\n            best_checkpoints[current_ckpt] = value\n            self._save_best_checkpoints_file(best_checkpoints)\n            self._saver.save(sess, self._save_path, global_step_tensor)\n            return\n\n        if self._maximize:\n            should_save = not all(current_best >= value\n                                  for current_best in best_checkpoints.values())\n        else:\n            should_save = not all(current_best <= value\n                                  for current_best in best_checkpoints.values())\n        if should_save:\n            best_checkpoint_list = self._sort(best_checkpoints)\n\n            worst_checkpoint = os.path.join(self._save_dir,\n                                            best_checkpoint_list.pop(-1)[0])\n            self._remove_outdated_checkpoint_files(worst_checkpoint)\n            self._update_internal_saver_state(best_checkpoint_list)\n\n            best_checkpoints = dict(best_checkpoint_list)\n            best_checkpoints[current_ckpt] = value\n            self._save_best_checkpoints_file(best_checkpoints)\n\n            self._saver.save(sess, self._save_path, global_step_tensor)\n\n    def _save_best_checkpoints_file(self, updated_best_checkpoints):\n        with open(self.best_checkpoints_file, 'w') as f:\n            json.dump(updated_best_checkpoints, f, indent=3)\n\n    def _remove_outdated_checkpoint_files(self, worst_checkpoint):\n        os.remove(os.path.join(self._save_dir, 'checkpoint'))\n        for ckpt_file in glob.glob(worst_checkpoint + '.*'):\n            os.remove(ckpt_file)\n\n    def _update_internal_saver_state(self, best_checkpoint_list):\n        best_checkpoint_files = [\n            (ckpt[0], np.inf)  # TODO: Try to use actual file timestamp\n            for ckpt in best_checkpoint_list\n        ]\n        self._saver.set_last_checkpoints_with_time(best_checkpoint_files)\n\n    def _load_best_checkpoints_file(self):\n        with open(self.best_checkpoints_file, 'r') as f:\n            best_checkpoints = json.load(f)\n        return best_checkpoints\n\n    def _sort(self, best_checkpoints):\n        best_checkpoints = [\n            (ckpt, best_checkpoints[ckpt])\n            for ckpt in sorted(best_checkpoints,\n                               key=best_checkpoints.get,\n                               reverse=self._maximize)\n        ]\n        return best_checkpoints"}
{"Repository": "sentence2vec", "input": "Used in the tutorial on distributed computing and likely not useful anywhere else. className RepeatCorpus(SaveLoad) Method __init__ Method __iter__ Attribute corpus Attribute reps", "label": "class RepeatCorpus(SaveLoad):\n    def __init__(self, corpus, reps):\n        self.corpus = corpus\n        self.reps = reps\n\n    def __iter__(self):\n        return itertools.islice(itertools.cycle(self.corpus), self.reps)"}
{"Repository": "hitherdither", "input": "Support setup.py upload. className UploadCommand(Command) Method status Method initialize_options Method finalize_options Method run", "label": "class UploadCommand(Command):\n    description = 'Build and publish the package.'\n    user_options = []\n\n    @staticmethod\n    def status(s):\n        print('\\033[1m{0}\\033[0m'.format(s))\n\n    def initialize_options(self):\n        pass\n\n    def finalize_options(self):\n        pass\n\n    def run(self):\n        try:\n            self.status('Removing previous builds')\n            rmtree(os.path.join(here, 'dist'))\n        except OSError:\n            pass\n\n        self.status('Building Source and Wheel (universal) distribution')\n        os.system('{0} setup.py sdist bdist_wheel --universal'.format(sys.executable))\n\n        self.status('Uploading the package to PyPi via Twine')\n        os.system('twine upload dist/*')\n\n        sys.exit()"}
{"Repository": "IDAFuzzy", "input": "A simple chooser to be used as an embedded chooser className EmbeddedChooserClass(Choose) Method __init__ Method OnGetIcon Method OnSelectionChange Method OnClose Method OnGetLine Method OnGetSize Attribute n Attribute items Attribute icon", "label": "class EmbeddedChooserClass(Choose):\n    def __init__(self, title, nb=5, flags=0):\n        Choose.__init__(self,\n                        title,\n                        [[\"Action\", 30 | Choose.CHCOL_PLAIN]],\n                        embedded=True, height=10, flags=flags)\n        # embedded=True, width=30, height=20, flags=flags)\n\n        self.n = 0\n        self.items = []\n        self.icon = 0\n\n    def OnGetIcon(self, n):\n        # print(\"get icon %d\" % n)\n        return choices[self.items[n][0]].get_icon()\n\n    def OnSelectionChange(self, n):\n        pass\n        # print(\"selection change %d\" % n)\n\n    def OnClose(self):\n        pass\n\n    def OnGetLine(self, n):\n        # print(\"getline %d\" % n)\n        return self.items[n]\n\n    def OnGetSize(self):\n        n = len(self.items)\n        # print(\"getsize -> %d\" % n)\n        return n"}
{"Repository": "matrixcli", "input": "Handles one-time keys accounting for an OlmDevice. className OneTimeKeysManager(object) Method __init__ Method server_counts Method server_counts Method update_keys_to_upload Method should_upload Method curve25519_to_upload Method signed_curve25519_to_upload Attribute target_counts Attribute _server_counts Attribute to_upload Attribute keys_threshold", "label": "class OneTimeKeysManager(object):\n    def __init__(self, target_keys_number, signed_keys_proportion, keys_threshold):\n        self.target_counts = {\n            'signed_curve25519': int(round(signed_keys_proportion * target_keys_number)),\n            'curve25519': int(round((1 - signed_keys_proportion) * target_keys_number)),\n        }\n        self._server_counts = {}\n        self.to_upload = {}\n        self.keys_threshold = keys_threshold\n\n    @property\n    def server_counts(self):\n        return self._server_counts\n\n    @server_counts.setter\n    def server_counts(self, server_counts):\n        self._server_counts = server_counts\n        self.update_keys_to_upload()\n\n    def update_keys_to_upload(self):\n        for key_type, target_number in self.target_counts.items():\n            num_keys = self._server_counts.get(key_type, 0)\n            num_to_create = max(target_number - num_keys, 0)\n            self.to_upload[key_type] = num_to_create\n\n    def should_upload(self):\n        if not self._server_counts:\n            return True\n        for key_type, target_number in self.target_counts.items():\n            if self._server_counts.get(key_type, 0) < target_number * self.keys_threshold:\n                return True\n        return False\n\n    @property\n    def curve25519_to_upload(self):\n        return self.to_upload.get('curve25519', 0)\n\n    @property\n    def signed_curve25519_to_upload(self):\n        return self.to_upload.get('signed_curve25519', 0)"}
{"Repository": "invest", "input": "Command to compile translation message catalogs before building. className build_py(_build_py) Method run", "label": "class build_py(_build_py):\n    def run(self):\n        # internationalization: compile human-readable PO message catalogs\n        # into machine-readable MO message catalogs used by gettext\n        # the MO files are included as package data\n        locale_dir = os.path.abspath(os.path.join(\n            os.path.dirname(__file__),\n            'src/natcap/invest/internationalization/locales'))\n        for locale in os.listdir(locale_dir):\n            subprocess.run([\n                'pybabel',\n                'compile',\n                '--input-file', f'{locale_dir}/{locale}/LC_MESSAGES/messages.po',\n                '--output-file', f'{locale_dir}/{locale}/LC_MESSAGES/messages.mo'])\n        # then execute the original run method\n        _build_py.run(self)"}
{"Repository": "nops", "input": "A property declaration. className Declaration(object) Method __init__ Method __repr__ Attribute name Attribute value Attribute priority Attribute line Attribute column", "label": "class Declaration(object):\n    def __init__(self, name, value, priority, line, column):\n        self.name = name\n        self.value = TokenList(value)\n        self.priority = priority\n        self.line = line\n        self.column = column\n\n    def __repr__(self):\n        priority = ' !' + self.priority if self.priority else ''\n        return ('<{0.__class__.__name__} {0.line}:{0.column}'\n                ' {0.name}: {1}{2}>'.format(\n                    self, self.value.as_css(), priority))"}
{"Repository": "calfem-python", "input": "Main window class of our UI className MainWindow(QMainWindow) Method __init__ Method update_view Method update_problem Method on_updateButton_clicked Method on_tabWidget_currentChanged Attribute fig_geometry Attribute fig_mesh Attribute fig_element_values Attribute fig_displacements Attribute problem", "label": "class MainWindow(QMainWindow):\n    def __init__(self):\n        super(MainWindow, self).__init__()\n\n        # Load user interface from UI-file\n\n        loadUi('exm_qt_app.ui', self)\n\n        # Query for figure class name\n\n        figure = cfv.figureClass()\n\n        # Create figure widgets to insert in UI\n\n        self.fig_geometry = figure(self)\n        self.fig_mesh = figure(self)\n        self.fig_element_values = figure(self)\n        self.fig_displacements = figure(self)\n\n        # Insert widgets in gridLayout\n\n        self.middleLayout.addWidget(self.fig_geometry._widget, 20)\n        self.middleLayout.addWidget(self.fig_mesh._widget, 20)\n        self.resultLayout.addWidget(self.fig_element_values._widget, 20)\n        self.resultLayout.addWidget(self.fig_displacements._widget, 20)\n\n        # Create our problem instance\n\n        self.problem = PlaneStress2DProblem()\n        self.update_view()\n        \n    def update_view(self):\n        self.lengthEdit.setText(str(self.problem.w))\n        self.heightEdit.setText(str(self.problem.h))\n        self.thicknessEdit.setText(str(self.problem.t))\n        self.elasticModulusEdit.setText(str(self.problem.E))\n        self.youngEdit.setText(str(self.problem.v))\n        self.maxAreaEdit.setText(str(self.problem.maxArea))\n        \n    def update_problem(self):\n        self.problem.w = float(self.lengthEdit.text())\n        self.problem.h = float(self.heightEdit.text())\n        self.problem.t = float(self.thicknessEdit.text())\n        self.problem.E = float(self.elasticModulusEdit.text())\n        self.problem.v = float(self.youngEdit.text())\n        self.problem.maxArea = float(self.maxAreaEdit.text())\n        \n    @Slot()\n    def on_updateButton_clicked(self):\n        self.update_problem()        \n        self.problem.update_mesh()\n\n        # Draw geometry\n\n        self.problem.draw_geometry(self.fig_geometry)\n        \n        # Draw mesh\n        \n        self.problem.draw_mesh(self.fig_mesh)\n        \n    @Slot(int)\n    def on_tabWidget_currentChanged(self, tabIndex):\n        if tabIndex == 1:        \n            \n            # Only calculate when on tab 1 (results)\n       \n            self.update_problem()        \n            self.problem.update_mesh()\n            self.problem.solve()\n            \n            # Draw geometry\n    \n            self.problem.draw_geometry(self.fig_geometry)\n            \n            # Draw mesh\n            \n            self.problem.draw_mesh(self.fig_mesh)\n            \n            # Draw results\n            \n            self.problem.draw_elementValues(self.fig_element_values)\n            self.problem.draw_displacements(self.fig_displacements)"}
{"Repository": "organize", "input": "Matches files / folders by created date Attributes: years (int): specify number of years months (int): specify number of months weeks (float): specify number of weeks days (float): specify number of days hours (float): specify number of hours minutes (float): specify number of minutes seconds (float): specify number of seconds mode (str): either 'older' or 'newer'. className Created(TimeFilter) Method get_datetime", "label": "class Created(TimeFilter):\n    filter_config: ClassVar[FilterConfig] = FilterConfig(\n        name=\"created\",\n        files=True,\n        dirs=True,\n    )\n\n    def get_datetime(self, path: Path) -> datetime:\n        return read_created(path)"}
{"Repository": "riscv-dv", "input": "RISC-V instruction trace entry className RiscvInstructionTraceEntry(object) Method __init__ Method get_trace_string Attribute gpr Attribute csr Attribute instr Attribute operand Attribute pc Attribute binary Attribute instr_str Attribute mode", "label": "class RiscvInstructionTraceEntry(object):\n    def __init__(self):\n        self.gpr = []\n        self.csr = []\n        self.instr = \"\"\n        self.operand = \"\"\n        self.pc = \"\"\n        self.binary = \"\"\n        self.instr_str = \"\"\n        self.mode = \"\"\n\n    def get_trace_string(self):\n        return (\"pc[{}] {}: {} {}\".format(\n            self.pc, self.instr_str, \" \".join(self.gpr), \" \".join(self.csr)))"}
{"Repository": "Modmail", "input": "View that is used for pagination. className PaginatorView(View) Method __init__ Method fill_items Attribute handler", "label": "class PaginatorView(View):\n    def __init__(self, handler: PaginatorSession, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.handler = handler\n        self.clear_items()  # clear first so we can control the order\n        self.fill_items()\n\n    @discord.ui.button(label=\"Stop\", style=ButtonStyle.danger)\n    async def stop_button(self, interaction: Interaction, button: Button):\n        await self.handler.close(interaction=interaction)\n\n    def fill_items(self):\n        if self.handler.select_menu is not None:\n            self.add_item(self.handler.select_menu)\n\n        for label, callback in self.handler.callback_map.items():\n            if len(self.handler.pages) == 2 and label in (\"<<\", \">>\"):\n                continue\n\n            if label in (\"<<\", \">>\"):\n                style = ButtonStyle.secondary\n            else:\n                style = ButtonStyle.primary\n\n            button = PageButton(self.handler, callback, label=label, style=style)\n\n            self.handler._buttons_map[label] = button\n            self.add_item(button)\n        self.add_item(self.stop_button)\n\n    async def interaction_check(self, interaction: Interaction):\n        if interaction.user != self.handler.ctx.author:\n            await interaction.response.send_message(\n                \"Only the original author can control this!\", ephemeral=True\n            )\n            return False\n        return True"}
{"Repository": "python-openstackclient", "input": "Fake one or more project. className FakeProject(object) Method create_one_project Method create_projects", "label": "class FakeProject(object):\n    @staticmethod\n    def create_one_project(attrs=None):\n        attrs = attrs or {}\n\n        # set default attributes.\n        project_info = {\n            'id': 'project-id-' + uuid.uuid4().hex,\n            'name': 'project-name-' + uuid.uuid4().hex,\n            'description': 'project_description',\n            'enabled': True,\n        }\n        project_info.update(attrs)\n\n        project = fakes.FakeResource(\n            info=copy.deepcopy(project_info), loaded=True\n        )\n        return project\n\n    @staticmethod\n    def create_projects(attrs=None, count=2):\n        projects = []\n        for i in range(0, count):\n            projects.append(FakeProject.create_one_project(attrs))\n\n        return projects"}
{"Repository": "dingdang-robot", "input": "Generic parent class for all speakers className AbstractTTSEngine(object) Method get_config Method get_instance Method is_available Method __init__ Method say Method play Attribute _logger", "label": "class AbstractTTSEngine(object):\n    __metaclass__ = ABCMeta\n\n    @classmethod\n    def get_config(cls):\n        return {}\n\n    @classmethod\n    def get_instance(cls):\n        config = cls.get_config()\n        instance = cls(**config)\n        return instance\n\n    @classmethod\n    @abstractmethod\n    def is_available(cls):\n        return diagnose.check_executable('aplay')\n\n    def __init__(self, **kwargs):\n        self._logger = logging.getLogger(__name__)\n\n    @abstractmethod\n    def say(self, phrase, *args):\n        pass\n\n    def play(self, filename):\n        cmd = ['aplay', str(filename)]\n        self._logger.debug('Executing %s', ' '.join([pipes.quote(arg)\n                                                     for arg in cmd]))\n        with tempfile.TemporaryFile() as f:\n            subprocess.call(cmd, stdout=f, stderr=f)\n            f.seek(0)\n            output = f.read()\n            if output:\n                self._logger.debug(\"Output was: '%s'\", output)"}
{"Repository": "gamification-engine", "input": "Rewards are given when reaching :class:`Achievement`s. className Reward(ABase) Method __unicode__", "label": "class Reward(ABase):\n    def __unicode__(self, *args, **kwargs):\n        return self.name + \" (ID: %s)\" % (self.id,)"}
{"Repository": "mp3chaps", "input": "Support setup.py upload. className UploadCommand(Command) Method status Method initialize_options Method finalize_options Method run", "label": "class UploadCommand(Command):\n  description = 'Build and publish the package.'\n  user_options = []\n\n  @staticmethod\n  def status(s):\n    print('\\033[1m{0}\\033[0m'.format(s))\n\n  def initialize_options(self):\n    pass\n\n  def finalize_options(self):\n    pass\n\n  def run(self):\n    try:\n      self.status('Removing previous builds')\n      rmtree(os.path.join(here, 'dist'))\n    except OSError:\n      pass\n\n    self.status('Building Source and Wheel (universal) distribution')\n    os.system('{0} setup.py sdist bdist_wheel --universal'.format(sys.executable))\n\n    self.status('Uploading the package to PyPi via Twine')\n    os.system('twine upload dist/*')\n\n    sys.exit()"}
{"Repository": "SC-SfMLearner-Release", "input": "Converts a list of numpy.ndarray (H x W x C) along with a intrinsics matrix to a list of torch.FloatTensor of shape (C x H x W) with a intrinsics tensor. className ArrayToTensor(object) Method __call__", "label": "class ArrayToTensor(object):\n    def __call__(self, images, intrinsics):\n        tensors = []\n        for im in images:\n            # put it from HWC to CHW format\n            im = np.transpose(im, (2, 0, 1))\n            # handle numpy array\n            tensors.append(torch.from_numpy(im).float()/255)\n        return tensors, intrinsics"}
{"Repository": "pke", "input": "Reader for raw text. className RawTextReader(Reader) Method __init__ Method read Attribute language Attribute language", "label": "class RawTextReader(Reader):\n    def __init__(self, language=None):\n        self.language = language\n\n        if language is None:\n            self.language = 'en'\n\n        if len(self.language) != 2:\n            raise ValueError('`language` is \\'{}\\', but should be an iso2 language code (\\'en\\' instead of \\'english\\')'.format(self.language))\n\n    def read(self, text, spacy_model=None):\n        nlp = spacy_model\n\n        if nlp is None:\n\n            # list installed models\n            installed_models = [m for m in spacy.util.get_installed_models() if m[:2] == self.language]\n\n            # select first model for the language\n            if len(installed_models):\n                nlp = spacy.load(installed_models[0], disable=['ner', 'textcat', 'parser'])\n\n            # stop execution is no model is available\n            else:\n                excp_msg = 'No downloaded spacy model for \\'{}\\' language.'.format(self.language)\n                excp_msg += '\\nA list of downloadable spacy models is available at https://spacy.io/models.'\n                excp_msg += '\\nAlternatively, preprocess your document as a list of sentence tuple (word, pos), such as:'\n                excp_msg += \"\\n\\t[[('The', 'DET'), ('brown', 'ADJ'), ('fox', 'NOUN'), ('.', 'PUNCT')]]\"\n                raise Exception(excp_msg)\n\n            # add the sentence splitter\n            nlp.add_pipe('sentencizer')\n\n        # Fix for non splitting words with hyphens with spacy taken from\n        # https://spacy.io/usage/linguistic-features#native-tokenizer-additions\n        nlp.tokenizer.infix_finditer = infix_re.finditer\n\n        # process the document\n        spacy_doc = nlp(text)\n\n        sentences = []\n        for sentence_id, sentence in enumerate(spacy_doc.sents):\n            sentences.append(Sentence(\n                words=[token.text for token in sentence],\n                pos=[token.pos_ or token.tag_ for token in sentence],\n                meta={\n                    \"lemmas\": [token.lemma_ for token in sentence],\n                    \"char_offsets\": [(token.idx, token.idx + len(token.text))\n                                     for token in sentence]\n                }\n            ))\n        return sentences"}
{"Repository": "normal-depth-diffusion", "input": "Wraps an arbitrary object with __len__ and __getitem__ into a pytorch dataset className WrappedDataset(Dataset) Method __init__ Method __len__ Method __getitem__ Attribute data", "label": "class WrappedDataset(Dataset):\n    def __init__(self, dataset):\n        self.data = dataset\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        return self.data[idx]"}
{"Repository": "dfirwizard", "input": "Encapsulates a single USB device className UsbDevice() Method __init__ Attribute vendor Attribute product Attribute version Attribute serial_number Attribute vid Attribute pid Attribute parent_prefix_id Attribute drive_letter Attribute volume_name Attribute guid Attribute disk_signature Attribute mountpoint Attribute device_classes_datetime_53f56307b6bf11d094f200a0c91efb8b Attribute device_classes_datetime_10497b1bba5144e58318a65c837b6661 Attribute vid_pid_datetime Attribute usb_stor_datetime Attribute install_datetime Attribute usbstor_datetime64 Attribute usbstor_datetime65 Attribute usbstor_datetime66 Attribute usbstor_datetime67 Attribute mountpoint2 Attribute emdmgmt", "label": "class UsbDevice():\n    def __init__(self):\n        self.vendor = ''\n        self.product = ''\n        self.version = ''\n        self.serial_number = ''\n        self.vid = ''\n        self.pid = ''\n        self.parent_prefix_id = ''\n        self.drive_letter = ''\n        self.volume_name = ''\n        self.guid = ''\n        self.disk_signature = ''\n        self.mountpoint = ''\n        self.device_classes_datetime_53f56307b6bf11d094f200a0c91efb8b = datetime.min\n        self.device_classes_datetime_10497b1bba5144e58318a65c837b6661 = datetime.min\n        self.vid_pid_datetime = datetime.min\n        self.usb_stor_datetime = datetime.min\n        self.install_datetime = datetime.min\n        self.usbstor_datetime64 = datetime.min\n        self.usbstor_datetime65 = datetime.min\n        self.usbstor_datetime66 = datetime.min\n        self.usbstor_datetime67 = datetime.min\n        self.mountpoint2 = []\n        self.emdmgmt = []"}
{"Repository": "tnt", "input": "A callback that performs periodic synchronous garbage collection. className GarbageCollector(Callback) Method __init__ Method on_train_start Method on_train_step_end Method on_train_end Method on_eval_start Method on_eval_step_end Method on_eval_end Method on_predict_start Method on_predict_step_end Method on_predict_end", "label": "class GarbageCollector(Callback):\n    def __init__(self, step_interval: int) -> None:\n        self._step_interval = step_interval\n\n    def on_train_start(self, state: State, unit: TTrainUnit) -> None:\n        gc.disable()\n\n    def on_train_step_end(self, state: State, unit: TTrainUnit) -> None:\n        gc.collect(generation=1)\n\n        total_num_steps_completed = unit.train_progress.num_steps_completed\n        if state.entry_point == EntryPoint.FIT:\n            # if fitting, include the num eval steps completed in the total steps completed\n            none_throws(state.eval_state)\n            # if fitting, unit should also subclass EvalUnit\n            unit_as_eval_unit = cast(TEvalUnit, unit)\n            total_num_steps_completed += (\n                unit_as_eval_unit.eval_progress.num_steps_completed\n            )\n\n        if total_num_steps_completed % self._step_interval == 0:\n            gc.collect()\n\n        # Ensure that GC is disabled, in case GC was reenabled elsewhere\n        gc.disable()\n\n    def on_train_end(self, state: State, unit: TTrainUnit) -> None:\n        gc.enable()\n\n    def on_eval_start(self, state: State, unit: TEvalUnit) -> None:\n        if state.entry_point == EntryPoint.FIT:\n            # if fitting, this is already handled in on_train_start\n            return\n        gc.disable()\n\n    def on_eval_step_end(self, state: State, unit: TEvalUnit) -> None:\n        gc.collect(generation=1)\n        total_num_steps_completed = unit.eval_progress.num_steps_completed\n        if state.entry_point == EntryPoint.FIT:\n            train_progress = cast(Progress, unit.train_progress)\n            # if fitting, include the num train steps completed in the total steps completed\n            total_num_steps_completed += train_progress.num_steps_completed\n\n        if total_num_steps_completed % self._step_interval == 0:\n            gc.collect()\n\n    def on_eval_end(self, state: State, unit: TEvalUnit) -> None:\n        if state.entry_point == EntryPoint.FIT:\n            # if fitting, this will be handled in on_train_end\n            return\n        gc.enable()\n\n    def on_predict_start(self, state: State, unit: TPredictUnit) -> None:\n        gc.disable()\n\n    def on_predict_step_end(self, state: State, unit: TPredictUnit) -> None:\n        gc.collect(generation=1)\n        if unit.predict_progress.num_steps_completed % self._step_interval == 0:\n            gc.collect()\n\n    def on_predict_end(self, state: State, unit: TPredictUnit) -> None:\n        gc.enable()"}
{"Repository": "transactions", "input": "Raised when the bitcoin daemon returns an error when trying to push a transaction. className TransactionError(Exception) Method __init__ Method __str__ Attribute message", "label": "class TransactionError(Exception):\n    def __init__(self, message):\n        self.message = message\n\n    def __str__(self):\n        return self.message"}
{"Repository": "asyncssh", "input": "An integer value className _Integer(DERType) Method encode Method decode", "label": "class _Integer(DERType):\n    @staticmethod\n    def encode(value: object) -> bytes:\n        i = cast(int, value)\n        l = i.bit_length()\n        l = l // 8 + 1 if l % 8 == 0 else (l + 7) // 8\n        result = i.to_bytes(l, 'big', signed=True)\n        return result[1:] if result.startswith(b'\\xff\\x80') else result\n\n    @classmethod\n    def decode(cls, constructed: bool, content: bytes) -> int:\n        if constructed:\n            raise ASN1DecodeError('INTEGER should not be constructed')\n\n        return int.from_bytes(content, 'big', signed=True)"}
{"Repository": "python-basics", "input": "Node in a tree. className Node(object) Method __init__ Method count_employees Attribute name Attribute children", "label": "class Node(object):\n    def __init__(self, name, children=None):\n        self.name = name\n        self.children = children or []\n\n\n    def count_employees(self):\n        if not self.children:\n            return 0\n\n        count = 0\n\n        for child in self.children:\n            count += 1 + child.count_employees()\n\n        return count"}
{"Repository": "Free-Auto-GPT", "input": "Agent that uses and inference endpoint to generate code. className HfAgent(Agent) Method generate_one", "label": "class HfAgent(Agent):\n    def __init__(\n        self, url_endpoint, token=None, chat_prompt_template=None, run_prompt_template=None, additional_tools=None\n    ):\n        self.url_endpoint = url_endpoint\n        if token is None:\n            self.token = f\"Bearer {HfFolder().get_token()}\"\n        elif token.startswith(\"Bearer\") or token.startswith(\"Basic\"):\n            self.token = token\n        else:\n            self.token = f\"Bearer {token}\"\n        super().__init__(\n            chat_prompt_template=chat_prompt_template,\n            run_prompt_template=run_prompt_template,\n            additional_tools=additional_tools,\n        )\n\n    def generate_one(self, prompt, stop):\n        headers = {\"Authorization\": self.token}\n        inputs = {\n            \"inputs\": prompt,\n            \"parameters\": {\"max_new_tokens\": 200, \"return_full_text\": False, \"stop\": stop},\n        }\n\n        response = requests.post(self.url_endpoint, json=inputs, headers=headers)\n        if response.status_code == 429:\n            print(\"Getting rate-limited, waiting a tiny bit before trying again.\")\n            time.sleep(1)\n            return self._generate_one(prompt)\n        elif response.status_code != 200:\n            raise ValueError(f\"Error {response.status_code}: {response.json()}\")\n\n        result = response.json()[0][\"generated_text\"]\n        # Inference API returns the stop sequence\n        for stop_seq in stop:\n            if result.endswith(stop_seq):\n                result = result[: -len(stop_seq)]\n        return result"}
{"Repository": "sevabot", "input": "Handle post-commit hook from Github. className GitHubPullRequest(SendMessage) Method compose", "label": "class GitHubPullRequest(SendMessage):\n    def compose(self):\n\n        payload = json.loads(request.form[\"payload\"])\n        \n        if payload[\"action\"] == \"opened\":\n            msg = u\"(*) %s new pull request %s from %s - %s\\n\" % (payload[\"repository\"][\"name\"], payload[\"number\"], payload[\"pull_request\"][\"user\"][\"login\"], payload[\"pull_request\"][\"html_url\"])\n        elif payload[\"action\"] == \"closed\":\n            msg = u\"(y) %s pull request %s merged by %s - %s\\n\" % (payload[\"repository\"][\"name\"], payload[\"number\"], payload[\"pull_request\"][\"merged_by\"][\"login\"], payload[\"pull_request\"][\"html_url\"])\n        else:\n            msg = u\"\"\n        return msg"}
{"Repository": "transitfeed", "input": "A DataSetMerger for routes. className RouteMerger(DataSetMerger) Method _GetIter Method _GetById Method _MergeEntities Method _Migrate Method _Add Method _GetId Method MergeDataSets", "label": "class RouteMerger(DataSetMerger):\n  ENTITY_TYPE_NAME = 'route'\n  FILE_NAME = 'routes.txt'\n  DATASET_NAME = 'Routes'\n\n  def _GetIter(self, schedule):\n    return schedule.GetRouteList()\n\n  def _GetById(self, schedule, route_id):\n    return schedule.GetRoute(route_id)\n\n  def _MergeEntities(self, a, b):\n    scheme = {'route_short_name': self._MergeIdentical,\n              'route_long_name': self._MergeIdentical,\n              'agency_id': self._MergeSameAgency,\n              'route_type': self._MergeIdentical,\n              'route_id': self._MergeIdentical,\n              'route_url': self._MergeOptional,\n              'route_color': self._MergeOptional,\n              'route_text_color': self._MergeOptional}\n    return self._SchemedMerge(scheme, a, b)\n\n  def _Migrate(self, entity, schedule, newid):\n    migrated_route = transitfeed.Route(field_dict=entity)\n    if newid:\n      migrated_route.route_id = self.feed_merger.GenerateId(entity.route_id)\n    if entity.agency_id:\n      original_agency = schedule.GetAgency(entity.agency_id)\n    else:\n      original_agency = schedule.GetDefaultAgency()\n\n    migrated_route.agency_id = original_agency._migrated_entity.agency_id\n    return migrated_route\n\n  def _Add(self, a, b, migrated_route):\n    self.feed_merger.Register(a, b, migrated_route)\n    self.feed_merger.merged_schedule.AddRouteObject(migrated_route)\n\n  def _GetId(self, entity):\n    return entity.route_id\n\n  def MergeDataSets(self):\n    self._MergeSameId()\n    return True"}
{"Repository": "alibabacloud-quantization-networks", "input": "Computes and stores the average and current value. className AverageMeter(object) Method __init__ Method reset Method update", "label": "class AverageMeter(object):\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0.0\n        self.avg = 0.0\n        self.sum = 0.0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val*n\n        self.count += n\n        self.avg = self.sum / self.count"}
{"Repository": "python-scripts", "input": "Selector using round-robin. className RoundRobinSelector(ConnectionSelector) Method __init__ Method select Attribute rr", "label": "class RoundRobinSelector(ConnectionSelector):\n    def __init__(self, opts):\n        super(RoundRobinSelector, self).__init__(opts)\n        self.rr = -1\n\n    def select(self, connections):\n        self.rr += 1\n        self.rr %= len(connections)\n        return connections[self.rr]"}
{"Repository": "pytai", "input": "Information specific to Finder. className FinderInfo(KaitaiStruct) Method __init__ Method _read Method __init__ Method _read Attribute _io Attribute _parent Attribute _root Attribute _debug", "label": "class FinderInfo(KaitaiStruct):\n        SEQ_FIELDS = [\"file_type\", \"file_creator\", \"flags\", \"location\", \"folder_id\"]\n        def __init__(self, _io, _parent=None, _root=None):\n            self._io = _io\n            self._parent = _parent\n            self._root = _root if _root else self\n            self._debug = collections.defaultdict(dict)\n\n        def _read(self):\n            self._debug['file_type']['start'] = self._io.pos()\n            self.file_type = self._io.read_bytes(4)\n            self._debug['file_type']['end'] = self._io.pos()\n            self._debug['file_creator']['start'] = self._io.pos()\n            self.file_creator = self._io.read_bytes(4)\n            self._debug['file_creator']['end'] = self._io.pos()\n            self._debug['flags']['start'] = self._io.pos()\n            self.flags = self._io.read_u2be()\n            self._debug['flags']['end'] = self._io.pos()\n            self._debug['location']['start'] = self._io.pos()\n            self.location = AppleSingleDouble.Point(self._io, self, self._root)\n            self.location._read()\n            self._debug['location']['end'] = self._io.pos()\n            self._debug['folder_id']['start'] = self._io.pos()\n            self.folder_id = self._io.read_u2be()\n            self._debug['folder_id']['end'] = self._io.pos()\n\n\n    class Point(KaitaiStruct):\n        SEQ_FIELDS = [\"x\", \"y\"]\n        def __init__(self, _io, _parent=None, _root=None):\n            self._io = _io\n            self._parent = _parent\n            self._root = _root if _root else self\n            self._debug = collections.defaultdict(dict)\n\n        def _read(self):\n            self._debug['x']['start'] = self._io.pos()\n            self.x = self._io.read_u2be()\n            self._debug['x']['end'] = self._io.pos()\n            self._debug['y']['start'] = self._io.pos()\n            self.y = self._io.read_u2be()\n            self._debug['y']['end'] = self._io.pos()"}
{"Repository": "shadowproxy", "input": "Case Insensitive dict where all keys are converted to lowercase className CIDict(dict) Method get Method __getitem__ Method __setitem__ Method __contains__", "label": "class CIDict(dict):\n    def get(self, key, default=None):\n        return super().get(key.lower(), default)\n\n    def __getitem__(self, key):\n        return super().__getitem__(key.lower())\n\n    def __setitem__(self, key, value):\n        return super().__setitem__(key.lower(), value)\n\n    def __contains__(self, key):\n        return super().__contains__(key.lower())"}
{"Repository": "ydcmd", "input": "    className ydItem(object) Method __init__ Method isdir Method isfile Method __str__ Method __repr__ Attribute type Attribute type", "label": "class ydItem(object):\n    def __init__(self, info = None):\n        common_attr = [\"name\", \"created\", \"modified\", \"path\", \"type\"]\n        file_attr   = [\"mime_type\", \"md5\", \"sha256\"]\n\n        for attr in common_attr:\n            if attr not in info:\n                raise ValueError(\"{0} not exists (incomplete response?)\".format(attr))\n\n        if info != None:\n            for key, value in iteritems(info):\n                self.__dict__[key] = value\n\n        if self.type == \"file\":\n            for attr in file_attr:\n                if attr not in info:\n                    raise ValueError(\"{0} not exists (incomplete response?)\".format(attr))\n            if \"size\" not in info:\n                self.__dict__[\"size\"] = 0\n        elif self.type == \"dir\":\n            pass\n        else:\n            raise ValueError(\"Unknown item type: {0}\".format(self.type))\n\n\n    def isdir(self):\n        return self.type == \"dir\"\n\n\n    def isfile(self):\n        return self.type == \"file\"\n\n\n    def __str__(self):\n        result = \"\"\n        for key, value in iteritems(self.__dict__):\n            result += \"{0:>12}: {1}\\n\".format(key if key != \"custom_properties\" else \"custom\", value)\n        return result\n\n\n    def __repr__(self):\n        return \"{0!s}({1!r})\".format(self.__class__, self.__dict__)"}
{"Repository": "caliban", "input": "a caliban base image spec className ImageSpec(NamedTuple) Method image_type Method tag", "label": "class ImageSpec(NamedTuple):\n  dockerfile: str = _CPU_DOCKERFILE\n  base_image: Optional[str] = _CPU_BASE_IMAGE\n  cpu_version: Optional[str] = None\n  gpu_version: Optional[str] = None\n  build_args: Dict[str, str] = {}\n\n  @property\n  def image_type(self) -> ImageType:\n    if self.gpu_version is None:\n      return ImageType.CPU\n    else:\n      if self.cpu_version is None:\n        return ImageType.GPU_BASE\n      else:\n        return ImageType.GPU\n\n  @property\n  def tag(self) -> str:\n    parts = []\n    if self.image_type == ImageType.CPU:\n      parts = [_CPU_TAG, self.base_image, self.cpu_version]\n    else:\n      parts.append(_GPU_TAG)\n      if self.image_type == ImageType.GPU_BASE:\n        parts += [\"base\", self.base_image, self.gpu_version]\n      else:\n        parts += [self.base_image, self.cpu_version, self.gpu_version]\n\n    return f\"{FLAGS.base_url}:\" + \"-\".join(parts)"}
{"Repository": "amsn2", "input": "aMSNAccount : a Class to represent an aMSN account className aMSNAccount(object) Method __init__ Method signOut Method lock Method unlock Method load Method save Method set_dp Attribute view Attribute personalinfoview Attribute do_save Attribute backend_manager Attribute client", "label": "class aMSNAccount(object):\n    #TODO: use the personnal info stuff instead of the view\n    def __init__(self, core, accountview):\n        self.view = accountview\n        self.personalinfoview = core._personalinfo_manager._personalinfoview\n        self.do_save = accountview.save\n        self.backend_manager = core._backend_manager\n        self.client = None\n        self.lock()\n        self.load()\n\n    def signOut(self):\n        if self.do_save:\n            self.save()\n        self.backend_manager.clean()\n        self.unlock()\n\n    def lock(self):\n        #TODO\n        pass\n\n    def unlock(self):\n        #TODO\n        pass\n\n    def load(self):\n        #TODO:\n        self.config = self.backend_manager.loadConfig(self)\n\n    def save(self):\n        self.view.nick = self.personalinfoview.nick\n        self.view.psm = self.personalinfoview.psm\n        self.view.dp = self.personalinfoview.dp\n        self.backend_manager.saveAccount(self)\n\n    def set_dp(self, path):\n        if path:\n            try:\n                im = Image.open(path)\n                im.resize((96, 96), Image.BILINEAR)\n\n                # Write the file and rename it instead of creating a tmpfile\n                profile = self.client.profile\n                dp_path_tmp = self.backend_manager.getFileLocationDP(self.view.email, profile.id, 'tmp')\n                im.save(dp_path_tmp, \"PNG\")\n                f = open(dp_path_tmp)\n                dp_object = papyon.p2p.MSNObject(self.client.profile,\n                                                 os.path.getsize(dp_path_tmp),\n                                                 papyon.p2p.MSNObjectType.DISPLAY_PICTURE,\n                                                 os.path.basename(path),\n                                                 os.path.basename(path),\n                                                 data=f)\n                f.close()\n\n                dp_path = self.backend_manager.getFileLocationDP(self.view.email, profile.id, dp_object._data_sha)\n                os.rename(dp_path_tmp, dp_path)\n\n            except OSError, e:\n                # FIXME: on Windows, it's raised if dp_path already exists\n                # http://docs.python.org/library/os.html#os.rename\n                logger.error('Trying to overwrite a saved dp')\n                return\n\n            except IOError, e:\n                logger.error(e)\n                return\n\n            else:\n                self.client.msn_object_store.publish(dp_object)\n                self.personalinfoview.dp = dp_object"}
{"Repository": "FedEM", "input": "Implements `Agnostic Federated Learning`__(https://arxiv. className AgnosticAggregator(CentralizedAggregator) Method mix", "label": "class AgnosticAggregator(CentralizedAggregator):\n    def __init__(\n            self,\n            clients,\n            global_learners_ensemble,\n            log_freq,\n            global_train_logger,\n            global_test_logger,\n            lr_lambda,\n            sampling_rate=1.,\n            sample_with_replacement=False,\n            test_clients=None,\n            verbose=0,\n            seed=None\n    ):\n        super(AgnosticAggregator, self).__init__(\n            clients=clients,\n            global_learners_ensemble=global_learners_ensemble,\n            log_freq=log_freq,\n            global_train_logger=global_train_logger,\n            global_test_logger=global_test_logger,\n            sampling_rate=sampling_rate,\n            sample_with_replacement=sample_with_replacement,\n            test_clients=test_clients,\n            verbose=verbose,\n            seed=seed\n        )\n\n        self.lr_lambda = lr_lambda\n\n    def mix(self):\n        self.sample_clients()\n\n        clients_losses = []\n        for client in self.sampled_clients:\n            client_losses = client.step()\n            clients_losses.append(client_losses)\n\n        clients_losses = torch.tensor(clients_losses)\n\n        for learner_id, learner in enumerate(self.global_learners_ensemble):\n            learners = [client.learners_ensemble[learner_id] for client in self.clients]\n\n            average_learners(\n                learners=learners,\n                target_learner=learner,\n                weights=self.clients_weights,\n                average_gradients=True\n            )\n\n        # update parameters\n        self.global_learners_ensemble.optimizer_step()\n\n        # update clients weights\n        self.clients_weights += self.lr_lambda * clients_losses.mean(dim=1)\n        self.clients_weights = simplex_projection(self.clients_weights)\n\n        # assign the updated model to all clients\n        self.update_clients()\n\n        self.c_round += 1\n\n        if self.c_round % self.log_freq == 0:\n            self.write_logs()"}
{"Repository": "zihaowordcloud", "input": "Create a color function object which assigns DIFFERENT SHADES of specified colors to certain words based on the color to words mapping. className GroupedColorFunc(object) Method __init__ Method get_color_func Method __call__ Attribute color_func_to_words Attribute default_color_func", "label": "class GroupedColorFunc(object):\n    def __init__(self, color_to_words, default_color):\n        self.color_func_to_words = [\n            (get_single_color_func(color), set(words))\n            for (color, words) in color_to_words.items()]\n\n        self.default_color_func = get_single_color_func(default_color)\n\n    def get_color_func(self, word):\n        try:\n            color_func = next(\n                color_func for (color_func, words) in self.color_func_to_words\n                if word in words)\n        except StopIteration:\n            color_func = self.default_color_func\n\n        return color_func\n\n    def __call__(self, word, **kwargs):\n        return self.get_color_func(word)(word, **kwargs)"}
{"Repository": "WaDIQaM", "input": "Evaluation of IQA methods using SROCC, KROCC, PLCC, RMSE, MAE. className IQAPerformance(Metric) Method reset Method update Method compute", "label": "class IQAPerformance(Metric):\n    def reset(self):\n        self._y_pred = []\n        self._y      = []\n        self._y_std  = []\n\n    def update(self, output):\n        y_pred, y = output\n\n        self._y.append(y[0].item())\n        self._y_std.append(y[1].item())\n        n = int(y_pred.size(0) / y[0].size(0))  # n=1 if images; n>1 if patches\n        y_pred_im = y_pred.reshape((y[0].size(0), n)).mean(dim=1, keepdim=True)\n        self._y_pred.append(y_pred_im.item())\n\n    def compute(self):\n        sq = np.reshape(np.asarray(self._y), (-1,))\n        sq_std = np.reshape(np.asarray(self._y_std), (-1,))\n        q = np.reshape(np.asarray(self._y_pred), (-1,))\n\n        srocc = stats.spearmanr(sq, q)[0]\n        krocc = stats.stats.kendalltau(sq, q)[0]\n        plcc = stats.pearsonr(sq, q)[0]\n        rmse = np.sqrt(((sq - q) ** 2).mean())\n        mae = np.abs((sq - q)).mean()\n        outlier_ratio = (np.abs(sq - q) > 2 * sq_std).mean()\n\n        return srocc, krocc, plcc, rmse, mae, outlier_ratio"}
{"Repository": "tauthon", "input": "By default, all components in the Python installer can run from source. className PyDirectory(Directory) Method __init__", "label": "class PyDirectory(Directory):\n    def __init__(self, *args, **kw):\n        if not kw.has_key(\"componentflags\"):\n            kw['componentflags'] = 2 #msidbComponentAttributesOptional\n        Directory.__init__(self, *args, **kw)"}
{"Repository": "mmpretrain", "input": "Implements an example backbone. className ExampleNet(ResNet) Method __init__ Method forward", "label": "class ExampleNet(ResNet):\n    def __init__(self, **kwargs) -> None:\n        print('#############################\\n'\n              '#     Hello MMPretrain!     #\\n'\n              '#############################')\n        super().__init__(**kwargs)\n\n    def forward(self, x):\n        return super().forward(x)"}
{"Repository": "asyncmongo", "input": "Client connection to represent a remote database. className Client(object) Method __init__ Method __getattr__ Method __getitem__ Method connection Method collection_names Method _collection_names_result Attribute _pool", "label": "class Client(object):\n    def __init__(self, pool_id=None, **kwargs):\n        self._pool = ConnectionPools.get_connection_pool(pool_id, **kwargs)\n    \n    def __getattr__(self, name):\n        return self.connection(name)\n\n    def __getitem__(self, name):\n        return self.connection(name)\n    \n    def connection(self, collectionname, dbname=None):\n        if not collectionname or \"..\" in collectionname:\n            raise DataError(\"collection names cannot be empty\")\n        if \"$\" in collectionname and not (collectionname.startswith(\"oplog.$main\") or\n                                collectionname.startswith(\"$cmd\")):\n            raise DataError(\"collection names must not \"\n                              \"contain '$': %r\" % collectionname)\n        if collectionname.startswith(\".\") or collectionname.endswith(\".\"):\n            raise DataError(\"collecion names must not start \"\n                            \"or end with '.': %r\" % collectionname)\n        if \"\\x00\" in collectionname:\n            raise DataError(\"collection names must not contain the \"\n                              \"null character\")\n        return Cursor(dbname or self._pool._dbname, collectionname, self._pool)\n\n    def collection_names(self, callback):\n        callback = partial(self._collection_names_result, callback)\n        self[\"system.namespaces\"].find(_must_use_master=True, callback=callback)\n\n    def _collection_names_result(self, callback, results, error=None):\n        names = [r['name'] for r in results if r['name'].count('.') == 1]\n        assert error == None, repr(error)\n        strip = len(self._pool._dbname) + 1\n        callback([name[strip:] for name in names])\n\n    def command(self, command, value=1, callback=None,\n                check=True, allowable_errors=[], **kwargs):\n        if isinstance(command, basestring):\n            command = SON([(command, value)])\n\n        command.update(kwargs)\n\n        self.connection(\"$cmd\").find_one(command,callback=callback,\n                                       _must_use_master=True,\n                                       _is_command=True)"}
{"Repository": "Pytorch-STN", "input": "A class to keep track of a quantity className AverageMeter(object) Method __init__ Method update Method __call__ Attribute _val Attribute _step", "label": "class AverageMeter(object):\n    def __init__(self):\n        self._val = 0 \n        self._step = 0 \n    \n    def update(self, val):\n        self._val += val \n        self._step += 1 \n    \n    def __call__(self):\n        return self._val/float(self._step)"}
{"Repository": "boar", "input": "This class will work as a file object when created, but with the additional functionality that it will not allow the file to exceed the given size. className StrictFileWriter(object) Method __init__ Method write Method close Method __close Method is_closed Method __enter__ Method __exit__ Attribute filename Attribute expected_md5 Attribute expected_size Attribute f Attribute md5summer Attribute written_bytes", "label": "class StrictFileWriter(object):\n    def __init__(self, filename, md5, size, overwrite = False):\n        assert is_md5sum(md5)\n        assert type(size) == int or type(size) == int\n        assert size >= 0\n        self.filename = filename\n        self.expected_md5 = md5\n        self.expected_size = size\n        if not overwrite and os.path.exists(filename):\n            raise ConstraintViolation(\"Violation of file contract (file already exists): \"+str(filename))\n        self.f = open(self.filename, \"wb\")\n        self.f.seek(0)\n        self.f.truncate() # Erase any existing file content\n        self.f.seek(size)\n        self.f.truncate() # Create a sparse file to reduce file fragmentation on NTFS\n        self.f.seek(0)\n        self.md5summer = hashlib.md5()\n        self.written_bytes = 0\n\n    def write(self, buf):\n        assert type(buf) == bytes\n        if self.written_bytes + len(buf) > self.expected_size:\n            self.__close()\n            raise SizeViolation(\"Violation of file contract (too big) detected: \"+str(self.filename))\n        self.md5summer.update(buf)\n        if self.written_bytes + len(buf) == self.expected_size:\n            if self.md5summer.hexdigest() != self.expected_md5:\n                self.__close()\n                raise ContentViolation(\"Violation of file contract (checksum) detected: \"+str(self.filename))\n        self.f.write(buf)\n        self.written_bytes += len(buf)\n\n    def close(self):\n        if self.is_closed():\n            return\n        self.__close()\n        if self.written_bytes != self.expected_size:\n            raise SizeViolation(\"Violation of file contract (too small, %s < %s) detected: %s\" %\n                                   (self.written_bytes, self.expected_size, self.filename))\n\n    def __close(self):\n        if not self.f:\n            return\n        self.f.close()\n        self.f = None\n\n    def is_closed(self):\n        return self.f == None\n\n    def __enter__(self):\n        return self\n\n    def __exit__(self, type, value, traceback):\n        if type:\n            # An exception has occured within the \"with\" clause. Let's\n            # not hide it.\n            self.__close()\n        else:\n            self.close()"}
{"Repository": "MolVS", "input": "A class for filtering out fragments using SMARTS patterns. className FragmentRemover(object) Method __init__ Method __call__ Method remove Attribute fragments Attribute leave_last", "label": "class FragmentRemover(object):\n    def __init__(self, fragments=REMOVE_FRAGMENTS, leave_last=LEAVE_LAST):\n        log.debug('Initializing FragmentRemover')\n        self.fragments = fragments\n        self.leave_last = leave_last\n\n    def __call__(self, mol):\n        return self.remove(mol)\n\n    def remove(self, mol):\n        log.debug('Running FragmentRemover')\n        # Iterate FragmentPatterns and remove matching fragments\n        for frag in self.fragments:\n            # If nothing is left or leave_last and only one fragment, end here\n            if mol.GetNumAtoms() == 0 or (self.leave_last and len(Chem.GetMolFrags(mol)) <= 1):\n                break\n            # Apply removal for this FragmentPattern\n            removed = Chem.DeleteSubstructs(mol, frag.smarts, onlyFrags=True)\n            if not mol.GetNumAtoms() == removed.GetNumAtoms():\n                log.info('Removed fragment: %s', frag.name)\n            if self.leave_last and removed.GetNumAtoms() == 0:\n                # All the remaining fragments match this pattern - leave them all\n                break\n            mol = removed\n        return mol"}
{"Repository": "osx-config-check", "input": "Thrown when trying to change constant className ConstError(TypeError) Method __setattr__", "label": "class ConstError(TypeError):\n        pass\n    def __setattr__(self, name, value):\n        if self.__dict__.has_key(name):\n            raise self.ConstError, \"Can't rebind const(%s)\" % name\n        self.__dict__[name] = value"}
{"Repository": "py-radius", "input": "Dictionary-style interface. className Attributes(UserDict) Method __init__ Method __getkeys Method __contains__ Method __getitem__ Method __setitem__ Method update Method nameditems Method pack Method unpack", "label": "class Attributes(UserDict):\n    def __init__(self, initialdata={}):\n        UserDict.__init__(self, {})\n        # Set keys via update() to invoke validation.\n        self.update(initialdata)\n\n    def __getkeys(self, value):\n        if isinstance(value, int):\n            return value, ATTRS.get(value, None)\n\n        else:\n            id = ATTR_NAMES[value.lower()]\n            return id, ATTRS[id]\n\n    def __contains__(self, key):\n        code = self.__getkeys(key)[0]\n        return UserDict.__contains__(self, code)\n\n    def __getitem__(self, key):\n        for k in self.__getkeys(key):\n            try:\n                return UserDict.__getitem__(self, k)\n            except KeyError:\n                continue\n        raise KeyError(key)\n\n    def __setitem__(self, key, value):\n        try:\n            code, name = self.__getkeys(key)\n\n        except KeyError:\n            raise ValueError('Unknown radius attribute: %s' % key)\n\n        if name is None:\n            LOGGER.warning('Unknown radius attribute code %s' % code)\n\n        values = self.get(code, [])\n        values.append(value)\n        UserDict.__setitem__(self, code, values)\n\n    def update(self, data):\n        for k, v in data.items():\n            self[k] = v\n\n    def nameditems(self):\n        for k, v in self.items():\n            yield self.__getkeys(k)[1], v\n\n    def pack(self):\n        data = []\n        for key, values in self.items():\n            for value in values:\n                data.append(struct.pack('BB%ds' % len(value), key,\n                                        len(value) + 2, bytes_safe(value)))\n        return join(data)\n\n    @staticmethod\n    def unpack(data):\n        pos, attrs = 0, {}\n        while pos < len(data):\n            code, length = struct.unpack('BB', data[pos:pos + 2])\n            attrs[code] = data[pos + 2:pos + length]\n            pos += length\n        return Attributes(attrs)"}
{"Repository": "Parakeet", "input": "Normalize the input text sequence and convert into pronunciation id sequence. className English(Phonetics) Method __init__ Method phoneticize Method numericalize Method reverse Method __call__ Method vocab_size Attribute backend Attribute phonemes Attribute punctuations Attribute vocab", "label": "class English(Phonetics):\n    def __init__(self):\n        self.backend = G2p()\n        self.phonemes = list(self.backend.phonemes)\n        self.punctuations = get_punctuations(\"en\")\n        self.vocab = Vocab(self.phonemes + self.punctuations)\n\n    def phoneticize(self, sentence):\n        start = self.vocab.start_symbol\n        end = self.vocab.end_symbol\n        phonemes = ([] if start is None else [start]) \\\n                   + self.backend(sentence) \\\n                   + ([] if end is None else [end])\n        phonemes = [item for item in phonemes if item in self.vocab.stoi]\n        return phonemes\n\n    def numericalize(self, phonemes):\n        ids = [\n            self.vocab.lookup(item) for item in phonemes\n            if item in self.vocab.stoi\n        ]\n        return ids\n\n    def reverse(self, ids):\n        return [self.vocab.reverse(i) for i in ids]\n\n    def __call__(self, sentence):\n        return self.numericalize(self.phoneticize(sentence))\n\n    @property\n    def vocab_size(self):\n        return len(self.vocab)"}
{"Repository": "friture", "input": "Raised when attempting to get an event which is undefined. className UndefinedEvent(Exception) Method __init__ Method __str__ Attribute event", "label": "class UndefinedEvent(Exception):\n    def __init__(self, event):\n        Exception.__init__(self)\n        self.event = event\n\n    def __str__(self):\n        return 'unspecified event %s' % self.event.name"}
{"Repository": "maildown", "input": "Verifies your ownership of an email address. className VerifyCommand(Command) Method handle", "label": "class VerifyCommand(Command):\n    def handle(self):\n        email = self.argument(\"email-address\")\n        __backend = available_backends.get(self.option(\"backend\"))\n        if not __backend:\n            return self.line(\n                f'No backend called {self.option(\"backend\")} exists', \"error\"\n            )\n        backend = __backend()\n\n        verified = backend.verify_address(email)\n\n        if verified:\n            self.info(\"This email address has already been verified\")\n\n        else:\n            self.info(\n                f\"Email sent to {email}. You must click the link in this email to verify ownership before \"\n                f\"you can send any emails\"\n            )"}
{"Repository": "speechbox", "input": "A custom distutils command that updates the dependency table. className DepsTableUpdateCommand(Command) Method initialize_options Method finalize_options Method run", "label": "class DepsTableUpdateCommand(Command):\n    description = \"build runtime dependency table\"\n    user_options = [\n        # format: (long option, short option, description).\n        (\"dep-table-update\", None, \"updates src/diffusers/dependency_versions_table.py\"),\n    ]\n\n    def initialize_options(self):\n        pass\n\n    def finalize_options(self):\n        pass\n\n    def run(self):\n        entries = \"\\n\".join([f'    \"{k}\": \"{v}\",' for k, v in deps.items()])\n        content = [\n            \"# THIS FILE HAS BEEN AUTOGENERATED. To update:\",\n            \"# 1. modify the `_deps` dict in setup.py\",\n            \"# 2. run `make deps_table_update``\",\n            \"deps = {\",\n            entries,\n            \"}\",\n            \"\",\n        ]\n        target = \"src/speechbox/dependency_versions_table.py\"\n        print(f\"updating {target}\")\n        with open(target, \"w\", encoding=\"utf-8\", newline=\"\\n\") as f:\n            f.write(\"\\n\".join(content))"}
{"Repository": "GridMask", "input": "Sampler that restricts data loading to a subset of the dataset. className DistributedSampler(Sampler) Method __init__ Method __iter__ Method __len__ Method set_epoch Attribute dataset Attribute num_replicas Attribute rank Attribute epoch Attribute num_samples Attribute total_size Attribute shuffle", "label": "class DistributedSampler(Sampler):\n    def __init__(self, dataset, num_replicas=None, rank=None, shuffle=True):\n        if num_replicas is None:\n            if not dist.is_available():\n                raise RuntimeError(\"Requires distributed package to be available\")\n            num_replicas = dist.get_world_size()\n        if rank is None:\n            if not dist.is_available():\n                raise RuntimeError(\"Requires distributed package to be available\")\n            rank = dist.get_rank()\n        self.dataset = dataset\n        self.num_replicas = num_replicas\n        self.rank = rank\n        self.epoch = 0\n        self.num_samples = int(math.ceil(len(self.dataset) * 1.0 / self.num_replicas))\n        self.total_size = self.num_samples * self.num_replicas\n        self.shuffle = shuffle\n\n    def __iter__(self):\n        if self.shuffle:\n            # deterministically shuffle based on epoch\n            g = torch.Generator()\n            g.manual_seed(self.epoch)\n            indices = torch.randperm(len(self.dataset), generator=g).tolist()\n        else:\n            indices = torch.arange(len(self.dataset)).tolist()\n\n        # add extra samples to make it evenly divisible\n        indices += indices[: (self.total_size - len(indices))]\n        assert len(indices) == self.total_size\n\n        # subsample\n        offset = self.num_samples * self.rank\n        indices = indices[offset : offset + self.num_samples]\n        assert len(indices) == self.num_samples\n\n        return iter(indices)\n\n    def __len__(self):\n        return self.num_samples\n\n    def set_epoch(self, epoch):\n        self.epoch = epoch"}
{"Repository": "SMART-Single_Emotional_TTS", "input": "KORSpeech dataset. className PostKORDatasets(Dataset) Method __init__ Method __len__ Method __getitem__ Attribute landmarks_frame Attribute root_dir", "label": "class PostKORDatasets(Dataset):\n    def __init__(self, csv_file, root_dir):\n        self.landmarks_frame = pd.read_csv(csv_file, sep='|', header=None)\n        self.root_dir = root_dir\n\n    def __len__(self):\n        return len(self.landmarks_frame)\n\n    def __getitem__(self, idx):"}
{"Repository": "nodeenv", "input": "Configuration namespace. className Config(object) Method _load Method _dump", "label": "class Config(object):\n    # Defaults\n    node = 'latest'\n    npm = 'latest'\n    with_npm = False\n    jobs = '2'\n    without_ssl = False\n    debug = False\n    profile = False\n    make = 'make'\n    prebuilt = True\n    ignore_ssl_certs = False\n    mirror = None\n\n    @classmethod\n    def _load(cls, configfiles, verbose=False):\n        for configfile in reversed(configfiles):\n            configfile = os.path.expanduser(configfile)\n            if not os.path.exists(configfile):\n                continue\n\n            ini_file = ConfigParser()\n            ini_file.read(configfile)\n            section = \"nodeenv\"\n            if not ini_file.has_section(section):\n                continue\n\n            for attr, val in iteritems(vars(cls)):\n                if attr.startswith('_') or not \\\n                   ini_file.has_option(section, attr):\n                    continue\n\n                if isinstance(val, bool):\n                    val = ini_file.getboolean(section, attr)\n                else:\n                    val = ini_file.get(section, attr)\n\n                if verbose:\n                    print('CONFIG {0}: {1} = {2}'.format(\n                        os.path.basename(configfile), attr, val))\n                setattr(cls, attr, val)\n\n        if os.path.exists(\".node-version\"):\n            with open(\".node-version\", \"r\") as v_file:\n                setattr(cls, \"node\", v_file.readlines(1)[0].strip())\n\n    @classmethod\n    def _dump(cls):\n        print(\"    [nodeenv]\")\n        print(\"    \" + \"\\n    \".join(\n            \"%s = %s\" % (k, v) for k, v in sorted(iteritems(vars(cls)))\n            if not k.startswith('_')))"}
{"Repository": "CompRess", "input": "Computes and stores the average and current value className AverageMeter(object) Method __init__ Method reset Method update Attribute val Attribute avg Attribute sum Attribute count", "label": "class AverageMeter(object):\n    def __init__(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count"}
{"Repository": "mongothon", "input": "A wrapper for the standard pymongo Cursor object which ensures all objects returned by the cursor's query are wrapped in an instance of the given Model class. className CursorWrapper(object) Method __init__ Method __getitem__ Method __iter__ Method __getattr__ Method attr_wrapper Attribute _wrapped Attribute _model_class", "label": "class CursorWrapper(object):\n    RETURNS_CURSOR = ['rewind', 'clone', 'add_option', 'remove_option',\n                      'limit', 'batch_size', 'skip', 'max_scan', 'sort',\n                      'hint', 'where']\n\n    def __init__(self, wrapped_cursor, model_class):\n        self._wrapped = wrapped_cursor\n        self._model_class = model_class\n\n    def __getitem__(self, index):\n        return self._model_class(self._wrapped[index], initial_state=Model.PERSISTED)\n\n    def __iter__(self):\n        return IteratorWrapper(self._wrapped.__iter__(), self._model_class)\n\n    def __getattr__(self, name):\n        attr = getattr(self._wrapped, name)\n        if name in self.RETURNS_CURSOR:\n            def attr_wrapper(*args, **kwargs):\n                return CursorWrapper(attr(*args, **kwargs), self._model_class)\n\n            return attr_wrapper\n        return attr"}
{"Repository": "giotto-time", "input": "LIME explainer for the predictions of a scikit-learn regressor. className _LimeExplainer(_RegressorExplainer) Method predict Method plot_explanation Method _compute_lime_explanations", "label": "class _LimeExplainer(_RegressorExplainer):\n    def fit(\n        self, model: RegressorMixin, X: np.ndarray, feature_names: List[str] = None\n    ):\n        check_is_fitted(model)\n        if feature_names is None:\n            feature_names = self._define_feature_names(X)\n        else:\n            feature_names = list(feature_names)\n\n        self.model_ = model\n        self.explainer_ = lime_tabular.LimeTabularExplainer(\n            X, feature_names=feature_names, mode=\"regression\"\n        )\n        self.feature_names_ = feature_names\n        return self\n\n    def predict(self, X: np.ndarray) -> np.ndarray:\n        check_is_fitted(self)\n\n        self._explanations_ = self._compute_lime_explanations(X)\n        predictions = self._extract_predictions_from_explanations(self._explanations_)\n        self.explanations_ = self._reformat_explanations(self._explanations_)\n\n        return predictions\n\n    def plot_explanation(self, i: int):\n        raise NotImplementedError\n        # self._explanations_[i].show_in_notebook(show_tale=True)\n\n    def _compute_lime_explanations(self, X: np.ndarray) -> List[Explanation]:\n        return [\n            self.explainer_.explain_instance(item, self.model_.predict) for item in X\n        ]\n\n    def _extract_predictions_from_explanations(\n        self, explanations: List[Explanation]\n    ) -> np.ndarray:\n        return np.asarray([exp.predicted_value for exp in explanations])\n\n    def _reformat_explanations(\n        self, explanations: List[Explanation]\n    ) -> List[Dict[str, float]]:\n        explanations_lists = [\n            exp.as_map()[0] for exp in explanations\n        ]  # not clear why 0\n        return [\n            {self.feature_names_[name]: value for name, value in exp}\n            for exp in explanations_lists\n        ]"}
{"Repository": "pyribs", "input": "An archive that divides each dimension into uniformly-sized cells. className GridArchive(ArchiveBase) Method dims Method lower_bounds Method upper_bounds Method interval_size Method epsilon Method boundaries Method index_of Method grid_to_int_index Method int_to_grid_index", "label": "class GridArchive(ArchiveBase):\n    def __init__(self,\n                 *,\n                 solution_dim,\n                 dims,\n                 ranges,\n                 learning_rate=None,\n                 threshold_min=-np.inf,\n                 epsilon=1e-6,\n                 qd_score_offset=0.0,\n                 seed=None,\n                 dtype=np.float64,\n                 extra_fields=None):\n        self._dims = np.array(dims, dtype=np.int32)\n        if len(self._dims) != len(ranges):\n            raise ValueError(f\"dims (length {len(self._dims)}) and ranges \"\n                             f\"(length {len(ranges)}) must be the same length\")\n\n        ArchiveBase.__init__(\n            self,\n            solution_dim=solution_dim,\n            cells=np.prod(self._dims),\n            measure_dim=len(self._dims),\n            learning_rate=learning_rate,\n            threshold_min=threshold_min,\n            qd_score_offset=qd_score_offset,\n            seed=seed,\n            dtype=dtype,\n            extra_fields=extra_fields,\n        )\n\n        ranges = list(zip(*ranges))\n        self._lower_bounds = np.array(ranges[0], dtype=self.dtype)\n        self._upper_bounds = np.array(ranges[1], dtype=self.dtype)\n        self._interval_size = self._upper_bounds - self._lower_bounds\n        self._epsilon = self.dtype(epsilon)\n\n        self._boundaries = []\n        for dim, lower_bound, upper_bound in zip(self._dims, self._lower_bounds,\n                                                 self._upper_bounds):\n            self._boundaries.append(\n                np.linspace(lower_bound, upper_bound, dim + 1))\n\n    @property\n    def dims(self):\n        return self._dims\n\n    @property\n    def lower_bounds(self):\n        return self._lower_bounds\n\n    @property\n    def upper_bounds(self):\n        return self._upper_bounds\n\n    @property\n    def interval_size(self):\n        return self._interval_size\n\n    @property\n    def epsilon(self):\n        return self._epsilon\n\n    @property\n    def boundaries(self):\n        return self._boundaries\n\n    def index_of(self, measures):\n        measures = np.asarray(measures)\n        check_batch_shape(measures, \"measures\", self.measure_dim, \"measure_dim\")\n        check_finite(measures, \"measures\")\n\n        # Adding epsilon accounts for floating point precision errors from\n        # transforming measures. We then cast to int32 to obtain integer\n        # indices.\n        grid_indices = ((self._dims *\n                         (measures - self._lower_bounds) + self._epsilon) /\n                        self._interval_size).astype(np.int32)\n\n        # Clip indices to the archive dimensions (for example, for 20 cells, we\n        # want indices to run from 0 to 19).\n        grid_indices = np.clip(grid_indices, 0, self._dims - 1)\n\n        return self.grid_to_int_index(grid_indices)\n\n    def grid_to_int_index(self, grid_indices):\n        grid_indices = np.asarray(grid_indices)\n        check_batch_shape(grid_indices, \"grid_indices\", self.measure_dim,\n                          \"measure_dim\")\n\n        return np.ravel_multi_index(grid_indices.T, self._dims).astype(np.int32)\n\n    def int_to_grid_index(self, int_indices):\n        int_indices = np.asarray(int_indices)\n        check_is_1d(int_indices, \"int_indices\")\n\n        return np.asarray(np.unravel_index(\n            int_indices,\n            self._dims,\n        )).T.astype(np.int32)"}
{"Repository": "tfdeploy", "input": "Building block of a model. className Tensor(object) Method __init__ Method get Method _get Method eval Method __call__ Attribute name Attribute value_index Attribute op Attribute value Attribute last_uuid Attribute value Attribute value Attribute op", "label": "class Tensor(object):\n    def __init__(self, tf_tensor, tf_sess, tf_feed_dict=None):\n        super(Tensor, self).__init__()\n\n        if not tf_sess:\n            raise ValueError(\"bad tensorflow session: %s\" % tf_sess)\n\n        self.name = tf_tensor.name\n        self.value_index = tf_tensor.value_index\n        self.op = None\n        self.value = None\n        self.last_uuid = None\n\n        # guess the value\n        # explicitly evaluate variables and constants, use feed_dict for placeholders\n        if tf_tensor.op.type in (\"Variable\", \"VariableV2\", \"Const\"):\n            self.value = tf_tensor.eval(session=tf_sess, feed_dict=tf_feed_dict)\n        elif tf_tensor.op.type == \"Placeholder\":\n            if tf_feed_dict is not None and tf_tensor in tf_feed_dict:\n                self.value = tf_feed_dict[tf_tensor]\n\n        # create the op\n        # no op for variables, placeholders and constants\n        if tf_tensor.op.type not in (\"Variable\", \"VariableV2\", \"Const\", \"Placeholder\"):\n            self.op = Operation.new(tf_tensor.op, tf_sess, tf_feed_dict=tf_feed_dict)\n\n    def get(self, *names):\n        tensors = tuple(self._get(name) for name in names)\n        return tensors[0] if len(names) == 1 else tensors\n\n    def _get(self, name):\n        if self.name == name:\n            return self\n        elif self.op is None:\n            return None\n        else:\n            return self.op.get(name)\n\n    def eval(self, feed_dict=None, _uuid=None):\n        # set a cache uuid for this eval call\n        if _uuid is None:\n            _uuid = uuid4()\n\n        # already cached? this is important for tensors that are used multiple time within the graph\n        if _uuid == self.last_uuid:\n            return self.value\n        else:\n            self.last_uuid = _uuid\n\n        if feed_dict is None:\n            feed_dict = {}\n\n        # when _this_ tensor is in the feed_dict, return the fed value\n        # otherwise, eval the op\n        if self in feed_dict:\n            self.value = feed_dict[self]\n        elif self.op is not None:\n            self.value = self.op.eval(feed_dict=feed_dict, _uuid=_uuid)[self.value_index]\n\n        return self.value\n\n    def __call__(self, *args, **kwargs):\n        return self.eval(*args, **kwargs)"}
{"Repository": "xo", "input": "Name Complete Editor Testing stage at this point className NameCompleteEditor(DequeEditor) Method run Method text_at", "label": "class NameCompleteEditor(DequeEditor):\n    def run(self, main_display):\n        r = self.get_edit_text()\n        main_display.insert_name_completion(r)\n\n    def text_at(self, i):\n        return self.deq[i]"}
{"Repository": "guardrails", "input": "Element tag: `<float>` className Float(ScalarType) Method get_example Method from_str", "label": "class Float(ScalarType):\n    tag = \"float\"\n\n    def get_example(self):\n        return 1.5\n\n    def from_str(self, s: str) -> Optional[float]:\n        return to_float(s)"}
{"Repository": "vincent", "input": "Test Scatter Chart className TestScatter(object) Method test_init", "label": "class TestScatter(object):\n    def test_init(self):\n\n        scatter = Scatter([1, 2, 3])\n\n        scales = [{'domain': {'data': 'table', 'field': 'data.idx'},\n                   'name': 'x',\n                   'range': 'width',\n                   'type': 'linear'},\n                  {'domain': {'data': 'table', 'field': 'data.val'},\n                   'name': 'y',\n                   'range': 'height',\n                   'nice': True},\n                  {'domain': {'data': 'table', 'field': 'data.col'},\n                   'name': 'color',\n                   'range': 'category20',\n                   'type': 'ordinal'}]\n\n        axes = [{'scale': 'x', 'type': 'x'},\n                {'scale': 'y', 'type': 'y'}]\n\n        marks = [{\n            'type': 'group',\n            'from': {\n                'data': 'table',\n                'transform': [\n                    {'keys': ['data.col'], 'type': 'facet'}\n                ]\n            },\n            'marks': [{\n                'type': 'symbol',\n                'properties': {\n                    'enter': {\n                        'fill': {'field': 'data.col', 'scale': 'color'},\n                        'size': {'value': 100},\n                        'x': {'field': 'data.idx', 'scale': 'x'},\n                        'y': {'field': 'data.val', 'scale': 'y'}}},\n            }]\n        }]\n\n        chart_runner(scatter, scales, axes, marks)"}
{"Repository": "DeepSpeech-examples", "input": "Streams raw audio from microphone. Data is received in a separate thread, and stored in a buffer, to be read from. className Audio(object) Method __init__ Method proxy_callback Method resample Method read_resampled Method read Method destroy Method write_wav", "label": "class Audio(object):\n    FORMAT = pyaudio.paInt16\n    # Network/VAD rate-space\n    RATE_PROCESS = 16000\n    CHANNELS = 1\n    BLOCKS_PER_SECOND = 50\n\n    def __init__(self, callback=None, device=None, input_rate=RATE_PROCESS, file=None):\n        def proxy_callback(in_data, frame_count, time_info, status):\n            #pylint: disable=unused-argument\n            if self.chunk is not None:\n                in_data = self.wf.readframes(self.chunk)\n            callback(in_data)\n            return (None, pyaudio.paContinue)\n        if callback is None: callback = lambda in_data: self.buffer_queue.put(in_data)\n        self.buffer_queue = queue.Queue()\n        self.device = device\n        self.input_rate = input_rate\n        self.sample_rate = self.RATE_PROCESS\n        self.block_size = int(self.RATE_PROCESS / float(self.BLOCKS_PER_SECOND))\n        self.block_size_input = int(self.input_rate / float(self.BLOCKS_PER_SECOND))\n        self.pa = pyaudio.PyAudio()\n\n        kwargs = {\n            'format': self.FORMAT,\n            'channels': self.CHANNELS,\n            'rate': self.input_rate,\n            'input': True,\n            'frames_per_buffer': self.block_size_input,\n            'stream_callback': proxy_callback,\n        }\n\n        self.chunk = None\n        # if not default device\n        if self.device:\n            kwargs['input_device_index'] = self.device\n        elif file is not None:\n            self.chunk = 320\n            self.wf = wave.open(file, 'rb')\n\n        self.stream = self.pa.open(**kwargs)\n        self.stream.start_stream()\n\n    def resample(self, data, input_rate):\n        data16 = np.fromstring(string=data, dtype=np.int16)\n        resample_size = int(len(data16) / self.input_rate * self.RATE_PROCESS)\n        resample = signal.resample(data16, resample_size)\n        resample16 = np.array(resample, dtype=np.int16)\n        return resample16.tostring()\n\n    def read_resampled(self):\n        return self.resample(data=self.buffer_queue.get(),\n                             input_rate=self.input_rate)\n\n    def read(self):\n        return self.buffer_queue.get()\n\n    def destroy(self):\n        self.stream.stop_stream()\n        self.stream.close()\n        self.pa.terminate()\n\n    frame_duration_ms = property(lambda self: 1000 * self.block_size // self.sample_rate)\n\n    def write_wav(self, filename, data):\n        logging.info(\"write wav %s\", filename)\n        wf = wave.open(filename, 'wb')\n        wf.setnchannels(self.CHANNELS)\n        # wf.setsampwidth(self.pa.get_sample_size(FORMAT))\n        assert self.FORMAT == pyaudio.paInt16\n        wf.setsampwidth(2)\n        wf.setframerate(self.sample_rate)\n        wf.writeframes(data)\n        wf.close()"}
{"Repository": "BEAL", "input": "Convert ndarrays in sample to Tensors. className ToTensor(object) Method __call__", "label": "class ToTensor(object):\n    def __call__(self, sample):\n        # swap color axis because\n        # numpy image: H x W x C\n        # torch image: C X H X W\n        img = np.array(sample['image']).astype(np.float32).transpose((2, 0, 1))\n        map = np.array(sample['map']).astype(np.uint8).transpose((2, 0, 1))\n        boundary = np.array(sample['boundary']).astype(np.float).transpose((2, 0, 1))\n        name = sample['img_name']\n        img = torch.from_numpy(img).float()\n        map = torch.from_numpy(map).float()\n        boundary = torch.from_numpy(boundary).float()\n\n        return {'image': img,\n                'map': map,\n                'boundary': boundary,\n                'img_name': name}"}
{"Repository": "ZenNAS", "input": "Computes and stores the average and current value className AverageMeter(object) Method __init__ Method reset Method update Method __str__ Attribute name Attribute fmt Attribute disp_avg", "label": "class AverageMeter(object):\n    def __init__(self, name, fmt=':4g', disp_avg=True):\n        self.name = name\n        self.fmt = fmt\n        self.disp_avg = disp_avg\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count\n\n    def __str__(self):\n        fmtstr = '{name}{val' + self.fmt + '}'\n        fmtstr = fmtstr.format(name=self.name, val=self.val)\n        if self.disp_avg:\n            fmtstr += '({avg' + self.fmt + '})'\n            fmtstr = fmtstr.format(avg=self.avg)\n        return fmtstr.format(**self.__dict__)"}
{"Repository": "cutmix", "input": "Computes and stores the average and current value className AverageMeter(object) Method __init__ Method reset Method update", "label": "class AverageMeter(object):\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count"}
{"Repository": "fly", "input": "Log the number of parameters of the model className ParamsLog(Callback) Method on_fit_start", "label": "class ParamsLog(Callback):\n    def __init__(self, total_params_log: bool = True, trainable_params_log: bool = True,\n                 non_trainable_params_log: bool = True):\n        super().__init__()\n        self._log_stats = AttributeDict(\n            {\n                'total_params_log': total_params_log,\n                'trainable_params_log': trainable_params_log,\n                'non_trainable_params_log': non_trainable_params_log,\n            }\n        )\n\n    @rank_zero_only\n    def on_fit_start(self, trainer: Trainer, pl_module: LightningModule) -> None:\n        logs = {}\n        if self._log_stats.total_params_log:\n            logs[\"model/params_total\"] = sum(p.numel() for p in pl_module.parameters())\n        if self._log_stats.trainable_params_log:\n            logs[\"model/params_trainable\"] = sum(p.numel() for p in pl_module.parameters()\n                                             if p.requires_grad)\n        if self._log_stats.non_trainable_params_log:\n            logs[\"model/params_not_trainable\"] = sum(p.numel() for p in pl_module.parameters()\n                                                     if not p.requires_grad)\n        if trainer.logger is not None:\n            trainer.logger.log_hyperparams(logs)"}
{"Repository": "darts", "input": "Base class of Detectors that need training. className FittableDetector(Detector) Method __init__ Method _fit_core Method fit", "label": "class FittableDetector(Detector):\n    def __init__(self, *args: Any, **kwargs: Any) -> None:\n        super().__init__(*args, **kwargs)\n        self._fit_called = False\n\n    def detect(\n        self,\n        series: Union[TimeSeries, Sequence[TimeSeries]],\n    ) -> Union[TimeSeries, Sequence[TimeSeries]]:\n        list_series = [series] if not isinstance(series, Sequence) else series\n\n        raise_if_not(\n            self._fit_called,\n            \"The Detector has not been fitted yet. Call `fit()` first.\",\n        )\n\n        raise_if_not(\n            all([self.width_trained_on == s.width for s in list_series]),\n            \"all series in `series` must have the same number of components as the data \"\n            + \"used for training the detector model, number of components in training: \"\n            + f\" {self.width_trained_on}.\",\n        )\n\n        return super().detect(series)\n\n    @abstractmethod\n    def _fit_core(self, input: Any) -> Any:\n        pass\n\n    def fit(self, series: Union[TimeSeries, Sequence[TimeSeries]]) -> None:\n        list_series = [series] if not isinstance(series, Sequence) else series\n\n        raise_if_not(\n            all([isinstance(s, TimeSeries) for s in list_series]),\n            \"all series in `series` must be of type TimeSeries.\",\n        )\n\n        raise_if_not(\n            all([s.is_deterministic for s in list_series]),\n            \"all series in `series` must be deterministic (number of samples equal to 1).\",\n        )\n\n        self.width_trained_on = list_series[0].width\n\n        raise_if_not(\n            all([s.width == self.width_trained_on for s in list_series]),\n            \"all series in `series` must have the same number of components.\",\n        )\n\n        self._fit_called = True\n        return self._fit_core(list_series)\n\n    def fit_detect(\n        self, series: Union[TimeSeries, Sequence[TimeSeries]]\n    ) -> Union[TimeSeries, Sequence[TimeSeries]]:\n        self.fit(series)\n        return self.detect(series)"}
{"Repository": "sagecell", "input": "Backend for SageMathCell EXAMPLES:: sage: from sage. className BackendCell(BackendIPython) Method _repr_ Method display_immediately Method supported_output Method threejs_offline_scripts", "label": "class BackendCell(BackendIPython):\n    def _repr_(self):\n        return 'SageMathCell'\n\n    def display_immediately(self, plain_text, rich_output):\n        if isinstance(rich_output, OutputPlainText):\n            return {u'text/plain': rich_output.text.get_str()}, {}\n        if isinstance(rich_output, OutputAsciiArt):\n            return {u'text/plain': rich_output.ascii_art.get_str()}, {}\n\n        if isinstance(rich_output, OutputLatex):\n            display_html(rich_output.latex.get_str())\n        elif isinstance(rich_output, OutputHtml):\n            display_html(rich_output.html.get_str())\n\n        elif isinstance(rich_output, OutputImageGif):\n            display_file(rich_output.gif.filename(), 'text/image-filename')\n        elif isinstance(rich_output, OutputImageJpg):\n            display_file(rich_output.jpg.filename(), 'text/image-filename')\n        elif isinstance(rich_output, OutputImagePdf):\n            display_file(rich_output.pdf.filename(), 'text/image-filename')\n        elif isinstance(rich_output, OutputImagePng):\n            display_file(rich_output.png.filename(), 'text/image-filename')\n        elif isinstance(rich_output, OutputImageSvg):\n            display_file(rich_output.svg.filename(), 'text/image-filename')\n            \n        elif isinstance(rich_output, OutputSceneJmol):\n            path = tempfile.mkdtemp(suffix=\".jmol\", dir=\".\")\n            os.chmod(path, stat.S_IRWXU + stat.S_IXGRP + stat.S_IXOTH)\n            rich_output.scene_zip.save_as(os.path.join(path, 'scene.zip'))\n            rich_output.preview_png.save_as(os.path.join(path, 'preview.png'))\n            display_message({'text/plain': 'application/x-jmol file',\n                             'application/x-jmol': path})\n        elif isinstance(rich_output, OutputSceneThreejs):\n            path = tempfile.mkstemp(suffix='.html', dir='.')[1]\n            path = os.path.relpath(path)\n            rich_output.html.save_as(path)\n            os.chmod(path, stat.S_IRUSR + stat.S_IRGRP + stat.S_IROTH)\n            display_html(\"\"\"\n                <iframe\n                    scrolling=\"no\"\n                    src=\"cell://{}\"\n                    style=\"\n                        border: 1px silver solid;\n                        height: 500px;\n                        min-width: 500px;\n                        width: 75%;\n                        \"\n                    >\n                </iframe>\n        Return the outputs that are supported by SageMathCell backend.\n\n        OUTPUT:\n\n        Iterable of output container classes, that is, subclass of\n        :class:`~sage.repl.rich_output.output_basic.OutputBase`).\n        The order is ignored.\n\n        EXAMPLES::\n\n            sage: from sage.repl.rich_output.backend_cell import BackendCell\n            sage: backend = BackendCell()\n            sage: supp = backend.supported_output();  supp     # random output\n            set([<class 'sage.repl.rich_output.output_graphics.OutputImageGif'>, \n                 ...,\n                 <class 'sage.repl.rich_output.output_graphics.OutputImagePng'>])\n            sage: from sage.repl.rich_output.output_basic import OutputLatex\n            sage: OutputLatex in supp\n            True\n        Return script tags for ``viewer=threejs`` with ``online=False``.\n\n        OUTPUT:\n\n        - a string\n\n        EXAMPLES::\n\n            sage: from sage.repl.rich_output.backend_cell import BackendCell\n            sage: backend = BackendCell()\n            sage: backend.threejs_offline_scripts()\n            '...<script ...</script>...'"}
{"Repository": "edx-platform", "input": "This isn't a checker, but setting django settings module when pylint command is ran. className SetDjangoSettingsChecker(BaseChecker) Method __init__ Method open", "label": "class SetDjangoSettingsChecker(BaseChecker):\n    __implements__ = IAstroidChecker\n\n    name = 'set-django-settings'\n\n    msgs = {'R0991': ('bogus', 'bogus', 'bogus')}\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n\n    def open(self):\n        name_checker = get_checker(self.linter, ForeignKeyStringsChecker)\n        # pylint command should not run with modules from both cms and (lms, common) at once\n        cms_module = False\n        lms_module = False\n        common_module = False\n        arguments = self.linter.cmdline_parser.parse_args()[1]\n        for arg in arguments:\n            if arg.startswith('cms'):\n                cms_module = True\n            elif arg.startswith('lms'):\n                lms_module = True\n            elif arg.startswith('common'):\n                common_module = True\n\n        if cms_module and (lms_module or common_module):\n            # when cms module is present in pylint command, it can't be parired with (lms, common)\n            # as common and lms gives error with cms test settings\n            raise ArgumentCompatibilityError(\n                \"Modules from both common and lms can't be paired with cms while running pylint\"\n            )\n        elif cms_module:\n            # If a module from cms is present in pylint command arguments\n            # and ony other module from (openedx, pavelib) is present\n            # than test setting of cms is used\n            name_checker.config.django_settings_module = 'cms.envs.test'\n        else:\n            # If any module form (lms, common, openedx, pavelib) is present in\n            # pylint command arguments than test setting of lms is used\n            name_checker.config.django_settings_module = 'lms.envs.test'"}
{"Repository": "pycalculix", "input": "Makes an object that stores an id number. className Idobj(object) Method __init__ Method set_id Method __hash__ Attribute id", "label": "class Idobj(object):\n    def __init__(self):\n        self.id = -1\n\n    def set_id(self, id):\n        self.id = id\n\n    def __hash__(self):\n        return self.id"}
{"Repository": "clusterman", "input": "Metrics client for simulations that substitutes some metrics with generated data. className ClustermanMetricsSimulationClient(ClustermanMetricsBotoClient) Method __init__ Method get_writer Method log_values Attribute generated_metrics", "label": "class ClustermanMetricsSimulationClient(ClustermanMetricsBotoClient):\n    def __init__(self, generated_metrics, *args, **kwargs):\n        super(ClustermanMetricsSimulationClient, self).__init__(*args, **kwargs)\n        _validate_metrics_object(generated_metrics)\n        self.generated_metrics = generated_metrics\n\n    @contextmanager\n    def get_writer(*args, **kwargs):\n        def log_values(*args, **kwargs):\n            while True:\n                values = yield\n                logger.warning(\"Client is read-only; not writing {values}\".format(values=values))\n\n        coroutine = log_values()\n        try:\n            next(coroutine)\n            yield coroutine\n        finally:\n            coroutine.close()\n\n    def _get_new_metric_values(\n        self,\n        key_prefix: str,\n        metric_query: str,\n        metric_type: str,\n        time_start: int,\n        time_end: int,\n        is_regex: bool = False,\n        extra_dimensions: Optional[Mapping[str, str]] = None,\n    ) -> MetricsValuesDict:\n        metrics: MetricsValuesDict = defaultdict(list)\n        if is_regex:\n            generated_keys = {\n                key_prefix + metric_key\n                for metric_key in self.generated_metrics.get(metric_type, {})\n                if re.search(metric_query, metric_key)\n            }\n        else:\n            generated_keys = {key_prefix + metric_query}\n\n        for metric_key in generated_keys:\n            full_query_key = generate_key_with_dimensions(metric_key, extra_dimensions)\n            try:\n                full_timeseries = self.generated_metrics.get(metric_type, {})[full_query_key]\n            except KeyError:\n                continue\n\n            start_index = bisect.bisect_left(full_timeseries, (time_start,))\n            end_index = bisect.bisect_right(full_timeseries, (time_end,))\n            # (time_end,) will always be less than (time_end, <val>), so if there is a value for\n            # (time_end, <val>) in the list, we need to increase the end index by one to include it.\n            # This assumes there is at most one value for each timestamp.\n            if len(full_timeseries) > end_index and full_timeseries[end_index][0] == time_end:\n                end_index += 1\n\n            metrics[metric_key] = full_timeseries[start_index:end_index]\n\n        if metrics:\n            return metrics\n        else:\n            return super(ClustermanMetricsSimulationClient, self)._get_new_metric_values(\n                key_prefix,\n                metric_query,\n                metric_type,\n                time_start,\n                time_end,\n                is_regex,\n                extra_dimensions,\n            )"}
{"Repository": "minio-py", "input": "Raised to indicate that S3 service returning HTTP server error. className ServerError(MinioException) Method __init__ Method status_code Attribute _status_code", "label": "class ServerError(MinioException):\n    def __init__(self, message: str, status_code: int):\n        self._status_code = status_code\n        super().__init__(message)\n\n    @property\n    def status_code(self) -> int:\n        return self._status_code"}
{"Repository": "django-queryable-properties", "input": "Distutils command to run tests via tox, which must be installed and be able to access all supported python versions. className ToxCommand(Command) Method initialize_options Method finalize_options Method run", "label": "class ToxCommand(Command):\n    description = ('Run tests using tox, which must already be installed and be able to access all supported python '\n                   'versions to create its environments.')\n    user_options = []\n\n    def initialize_options(self):\n        pass\n\n    def finalize_options(self):\n        pass\n\n    def run(self):\n        return subprocess.call(['tox'])"}
{"Repository": "scvi-tools", "input": "An AnnDataField for a collection of numerical . className NumericalJointField(BaseJointField) Method register_field Method get_summary_stats Method view_state_registry", "label": "class NumericalJointField(BaseJointField):\n    COLUMNS_KEY = \"columns\"\n\n    def __init__(\n        self,\n        registry_key: str,\n        attr_keys: Optional[list[str]],\n        field_type: Literal[\"obsm\", \"varm\"] = None,\n    ) -> None:\n        super().__init__(registry_key, attr_keys, field_type=field_type)\n\n        self.count_stat_key = f\"n_{self.registry_key}\"\n\n    def register_field(self, adata: AnnData) -> dict:\n        super().register_field(adata)\n        self._combine_fields(adata)\n        return {self.COLUMNS_KEY: getattr(adata, self.attr_name)[self.attr_key].columns.to_numpy()}\n\n    def transfer_field(\n        self,\n        state_registry: dict,\n        adata_target: AnnData,\n        **kwargs,\n    ) -> dict:\n        super().transfer_field(state_registry, adata_target, **kwargs)\n        return self.register_field(adata_target)\n\n    def get_summary_stats(self, _state_registry: dict) -> dict:\n        n_keys = len(self.attr_keys)\n        return {self.count_stat_key: n_keys}\n\n    def view_state_registry(self, state_registry: dict) -> Optional[rich.table.Table]:\n        if self.is_empty:\n            return None\n\n        t = rich.table.Table(title=f\"{self.registry_key} State Registry\")\n        t.add_column(\n            \"Source Location\",\n            justify=\"center\",\n            style=\"dodger_blue1\",\n            no_wrap=True,\n            overflow=\"fold\",\n        )\n        for key in state_registry[self.COLUMNS_KEY]:\n            t.add_row(f\"adata.{self.source_attr_name}['{key}']\")\n        return t"}
{"Repository": "python-websocket-server", "input": "Custom command to verify that the git tag matches our version className VerifyVersionCommand(install) Method run", "label": "class VerifyVersionCommand(install):\n    description = 'verify that the git tag matches our version'\n\n    def run(self):\n        tag_version = get_tag_version()\n        if tag_version and tag_version != VERSION:\n            sys.exit(f\"Git tag: {tag} does not match the version of this app: {VERSION}\")"}
{"Repository": "REINVENT", "input": "A class for handling encoding/decoding from SMILES to an array of indices className Vocabulary(object) Method __init__ Method encode Method decode Method tokenize Method add_characters Method init_from_file Method __len__ Method __str__ Attribute special_tokens Attribute additional_chars Attribute chars Attribute vocab_size Attribute vocab Attribute reversed_vocab Attribute max_length", "label": "class Vocabulary(object):\n    def __init__(self, init_from_file=None, max_length=140):\n        self.special_tokens = ['EOS', 'GO']\n        self.additional_chars = set()\n        self.chars = self.special_tokens\n        self.vocab_size = len(self.chars)\n        self.vocab = dict(zip(self.chars, range(len(self.chars))))\n        self.reversed_vocab = {v: k for k, v in self.vocab.items()}\n        self.max_length = max_length\n        if init_from_file: self.init_from_file(init_from_file)\n\n    def encode(self, char_list):\n        smiles_matrix = np.zeros(len(char_list), dtype=np.float32)\n        for i, char in enumerate(char_list):\n            smiles_matrix[i] = self.vocab[char]\n        return smiles_matrix\n\n    def decode(self, matrix):\n        chars = []\n        for i in matrix:\n            if i == self.vocab['EOS']: break\n            chars.append(self.reversed_vocab[i])\n        smiles = \"\".join(chars)\n        smiles = smiles.replace(\"L\", \"Cl\").replace(\"R\", \"Br\")\n        return smiles\n\n    def tokenize(self, smiles):\n        regex = '(\\[[^\\[\\]]{1,6}\\])'\n        smiles = replace_halogen(smiles)\n        char_list = re.split(regex, smiles)\n        tokenized = []\n        for char in char_list:\n            if char.startswith('['):\n                tokenized.append(char)\n            else:\n                chars = [unit for unit in char]\n                [tokenized.append(unit) for unit in chars]\n        tokenized.append('EOS')\n        return tokenized\n\n    def add_characters(self, chars):\n        for char in chars:\n            self.additional_chars.add(char)\n        char_list = list(self.additional_chars)\n        char_list.sort()\n        self.chars = char_list + self.special_tokens\n        self.vocab_size = len(self.chars)\n        self.vocab = dict(zip(self.chars, range(len(self.chars))))\n        self.reversed_vocab = {v: k for k, v in self.vocab.items()}\n\n    def init_from_file(self, file):\n        with open(file, 'r') as f:\n            chars = f.read().split()\n        self.add_characters(chars)\n\n    def __len__(self):\n        return len(self.chars)\n\n    def __str__(self):\n        return \"Vocabulary containing {} tokens: {}\".format(len(self), self.chars)"}
{"Repository": "video_feature_extractor", "input": "Pytorch video loader. className VideoLoader(Dataset) Method __len__ Method _get_video_dim Method _get_output_dim Method __getitem__", "label": "class VideoLoader(Dataset):\n    def __init__(\n            self,\n            csv,\n            framerate=1,\n            size=112,\n            centercrop=False,\n    ):\n        self.csv = pd.read_csv(csv)\n        self.centercrop = centercrop\n        self.size = size\n        self.framerate = framerate\n\n    def __len__(self):\n        return len(self.csv)\n\n    def _get_video_dim(self, video_path):\n        probe = ffmpeg.probe(video_path)\n        video_stream = next((stream for stream in probe['streams']\n                             if stream['codec_type'] == 'video'), None)\n        width = int(video_stream['width'])\n        height = int(video_stream['height'])\n        return height, width\n\n    def _get_output_dim(self, h, w):\n        if isinstance(self.size, tuple) and len(self.size) == 2:\n            return self.size\n        elif h >= w:\n            return int(h * self.size / w), self.size\n        else:\n            return self.size, int(w * self.size / h)\n\n    def __getitem__(self, idx):\n        video_path = self.csv['video_path'].values[idx]\n        output_file = self.csv['feature_path'].values[idx]\n\n        if not(os.path.isfile(output_file)) and os.path.isfile(video_path):\n            print('Decoding video: {}'.format(video_path))\n            try:\n                h, w = self._get_video_dim(video_path)\n            except:\n                print('ffprobe failed at: {}'.format(video_path))\n                return {'video': th.zeros(1), 'input': video_path,\n                        'output': output_file}\n            height, width = self._get_output_dim(h, w)\n            cmd = (\n                ffmpeg\n                .input(video_path)\n                .filter('fps', fps=self.framerate)\n                .filter('scale', width, height)\n            )\n            if self.centercrop:\n                x = int((width - self.size) / 2.0)\n                y = int((height - self.size) / 2.0)\n                cmd = cmd.crop(x, y, self.size, self.size)\n            out, _ = (\n                cmd.output('pipe:', format='rawvideo', pix_fmt='rgb24')\n                .run(capture_stdout=True, quiet=True)\n            )\n            if self.centercrop and isinstance(self.size, int):\n                height, width = self.size, self.size\n            video = np.frombuffer(out, np.uint8).reshape([-1, height, width, 3])\n            video = th.from_numpy(video.astype('float32'))\n            video = video.permute(0, 3, 1, 2)\n        else:\n            video = th.zeros(1)\n            \n        return {'video': video, 'input': video_path, 'output': output_file}"}
{"Repository": "vega", "input": "A Factory Class to manage all class need to register with config. className ClassFactory(object) Method register Method wrapper Method get_cls", "label": "class ClassFactory(object):\n    __registry__ = {}\n\n    @classmethod\n    def register(cls):\n        def wrapper(t_cls):\n            t_cls_name = t_cls.__name__\n            if t_cls_name not in cls.__registry__:\n                cls.__registry__[t_cls_name] = t_cls\n            else:\n                raise ValueError(\"Cannot register duplicate class ({})\".format(t_cls_name))\n\n            return t_cls\n\n        return wrapper\n\n    @classmethod\n    def get_cls(cls, cls_name=None):\n        t_cls = cls.__registry__.get(cls_name)\n        return t_cls"}
{"Repository": "flowattack", "input": "Converts a list of numpy.ndarray (H x W x C) along with a intrinsics matrix to a list of torch.FloatTensor of shape (C x H x W) with a intrinsics tensor. className ArrayToTensor(object) Method __call__", "label": "class ArrayToTensor(object):\n    def __call__(self, images):\n        tensors = []\n        for im in images:\n            # put it from HWC to CHW format\n            im = np.transpose(im, (2, 0, 1))\n            # handle numpy array\n            tensors.append(torch.from_numpy(im).float()/255)\n        return tensors"}
{"Repository": "epitran", "input": "Flite G2P using t2p. className FliteT2P(Flite) Method english_g2p", "label": "class FliteT2P(Flite):\n    def english_g2p(self, text):\n        text = self.normalize(text)\n        try:\n            arpa_text = subprocess.check_output(['t2p', '\"{}\"'.format(text)])\n            arpa_text = arpa_text.decode('utf-8')\n        except OSError:\n            logger.warning('t2p (from flite) is not installed.')\n            arpa_text = ''\n        except subprocess.CalledProcessError:\n            logger.warning('Non-zero exit status from t2p.')\n            arpa_text = ''\n        return self.arpa_to_ipa(arpa_text)"}
{"Repository": "SlicerMorph", "input": "This class should implement all the actual computation done by your module. className IDAVLMConverterLogic(ScriptedLoadableModuleLogic) Method run Method process", "label": "class IDAVLMConverterLogic(ScriptedLoadableModuleLogic):\n  def run(self, landmarkFilePath, outputDirectory, headerSize, loadFileOption):\n    # set landmark filename and length of header\n    headerSize = 2  #number of lines in the header\n    landmarkFileName = os.path.basename(landmarkFilePath)\n    (landmarkFileBase, ext) = os.path.splitext(landmarkFileName)\n\n\n    # Create a markups node for imported points\n    fiducialNode = slicer.vtkMRMLMarkupsFiducialNode()\n    slicer.mrmlScene.AddNode(fiducialNode)\n    fiducialNode.CreateDefaultDisplayNodes()\n    fiducialNode.SetName(landmarkFileBase)\n\n    # read landmarks\n    landmarkFile = open(landmarkFilePath)\n    lines = landmarkFile.readlines()\n    landmarkFile.close()\n\n    #iterate through list of and place in markups node\n    for i in range(headerSize,len(lines)-1):\n      # in this file format, lines contain [name, x-coordinate, y-coordinate, z-coordinate]\n      # by default, split command splits by whitespace\n      lineData = lines[i].split()\n      if len(lineData) == 4:\n        coordinates = [float(lineData[1]), float(lineData[2]), float(lineData[3])]\n        name = lineData[0]\n        fiducialNode.AddFiducialFromArray(coordinates, name)\n      else:\n        logging.debug(\"Error: not a supported landmark file format\")\n\n    outputPath = os.path.join(outputDirectory, landmarkFileBase + '.fcsv')\n    slicer.util.saveNode(fiducialNode, outputPath)\n    if not loadFileOption:\n      slicer.mrmlScene.RemoveNode(fiducialNode)  #remove node from scene\n\n    logging.info('Processing completed')\n\n    return True\n\n  def process(self, inputVolume, outputVolume, imageThreshold, invert=False, showResult=True):\n    if not inputVolume or not outputVolume:\n      raise ValueError(\"Input or output volume is invalid\")\n\n    import time\n    startTime = time.time()\n    logging.info('Processing started')\n\n    # Compute the thresholded output volume using the \"Threshold Scalar Volume\" CLI module\n    cliParams = {\n      'InputVolume': inputVolume.GetID(),\n      'OutputVolume': outputVolume.GetID(),\n      'ThresholdValue' : imageThreshold,\n      'ThresholdType' : 'Above' if invert else 'Below'\n      }\n    cliNode = slicer.cli.run(slicer.modules.thresholdscalarvolume, None, cliParams, wait_for_completion=True, update_display=showResult)\n    # We don't need the CLI module node anymore, remove it to not clutter the scene with it\n    slicer.mrmlScene.RemoveNode(cliNode)\n\n    stopTime = time.time()\n    logging.info(f'Processing completed in {stopTime-startTime:.2f} seconds')"}
{"Repository": "bidscoin", "input": "Just a stub at the Collector which would not do anything className InactiveDueCreditCollector(object) Method _donothing Method dcite Method nondecorating_decorator Method __repr__", "label": "class InactiveDueCreditCollector(object):\n    def _donothing(self, *args, **kwargs):\n        pass\n\n    def dcite(self, *args, **kwargs):\n        def nondecorating_decorator(func):\n            return func\n        return nondecorating_decorator\n\n    active = False\n    activate = add = cite = dump = load = _donothing\n\n    def __repr__(self):\n        return self.__class__.__name__ + '()'"}
{"Repository": "mystic", "input": "AbstractEnsembleSolver base class for mystic optimizers that are called within a parallel map. className AbstractEnsembleSolver(AbstractMapSolver) Method __init__", "label": "class AbstractEnsembleSolver(AbstractMapSolver):\n    def __init__(self, dim, **kwds):\n        \"\"\""}
{"Repository": "spot-sdk", "input": "Build the specified wheels (defaults to all wheels). className BuildWheelsCommand(Command) Method __init__ Method run", "label": "class BuildWheelsCommand(Command):\n    NAME = 'build'\n\n    def __init__(self, subparsers, command_dict):\n        super(BuildWheelsCommand, self).__init__(subparsers, command_dict)\n        self._parser.add_argument(\n            '--latest-build-requirements', action='store_true', help=\n            'Install the latest supported versions of python dependencies for building bosdyn-api.')\n        self.parser.add_argument('--skip-git-check', action='store_true',\n                                 help=\"Do not check git status before building wheel\")\n        self.parser.add_argument(\n            '--install-deps', action='store_true', help=\n            'Install build dependencies.  If not set, the python environment should already be set '\n            'up for building.')\n        self._parser.add_argument('wheels', nargs='*', help=\"Names of wheels to build.\")\n\n    def run(self, options):\n        ret = True\n        for wheel in _wheels_to_build(options.wheels):\n            if wheel == 'bosdyn-api':\n                ret_ = build_proto_wheel(wheel_name=wheel, latest_requirements=options.latest_build_requirements,\n                                         dry_run=options.dry_run, verbose=options.verbose,\n                                         skip_git=options.skip_git_check, install_deps=options.install_deps)\n            elif wheel == 'bosdyn-choreography-protos':\n                ret_ = build_proto_wheel(wheel_name=wheel, proto_dir=CHOREOGRAPHY_PROTO_DIR,\n                                         latest_requirements=options.latest_build_requirements,\n                                         dry_run=options.dry_run, verbose=options.verbose,\n                                         skip_git=options.skip_git_check, install_deps=options.install_deps)\n            else:\n                ret_ = build_wheel(wheel, dry_run=options.dry_run, verbose=options.verbose,\n                                   skip_git=options.skip_git_check)\n            ret = ret and ret_\n        return ret"}
{"Repository": "gdsCAD", "input": "Collection of elements, both geometric objects and references to other cells. className Cell(object) Method __init__ Method elements Method objects Method references Method objects_by_laydat Method __str__ Method __getitem__ Method __setitem__ Method __iter__ Method __len__ Method unique_name Method to_gds Method copy Method add Method remove Attribute name Attribute _objects Attribute _references Attribute created Attribute created Attribute modified Attribute modified", "label": "class Cell(object):\n    show=_show\n     \n    def __init__(self, name, created=None, modified=None):\n        self.name = str(name)\n        self._objects = []\n        self._references = []\n\n        now = datetime.datetime.today()\n        if created:\n            self.created=created\n        else:\n            self.created=now\n        if modified:\n            self.modified=modified\n        else:\n            self.modified=now\n\n    @property\n    def elements(self):\n        return self.objects + self.references\n\n    @property\n    def objects(self):\n        return tuple(self._objects)\n\n    @property\n    def references(self):\n        return tuple(self._references)\n \n    def objects_by_laydat(self, laydat):\n        objects = []\n        for element in self.objects:\n            if (element.layer, element.datatype) == laydat:\n                objects.append(element)\n        return objects\n\n    def __str__(self):\n        return \"Cell (\\\"{}\\\", {} elements, {} references)\".format(self.name, len(self.objects),\n                                                                             len(self.references))\n\n    def __getitem__(self, index):\n        return self.elements[index]\n\n    def __setitem__(self, index, value):\n        self.elements[index]=value\n\n    def __iter__(self):\n        return iter(self.elements)\n\n    def __len__(self):\n        return len(self.elements)\n\n    @property\n    def unique_name(self):\n        return self.name + '_' + _compact_id(self)        \n\n    def to_gds(self, multiplier, duplicates=[]):\n        name = self.unique_name if self.name in duplicates else self.name\n\n        if len(name)%2 != 0:\n            name = name + '\\0'\n            \n        c = list(self.created.timetuple()[:6])\n        m = list(self.modified.timetuple()[:6])\n        data = struct.pack('>16h', 28, 0x0502,\n                            c[0], c[1], c[2], c[3], c[4], c[5],\n                            m[0], m[1], m[2], m[3], m[4], m[5],\n                           4 + len(name), 0x0606) + name.encode('ascii')\n        for element in self:\n            if isinstance(element, ReferenceBase):\n                data += element.to_gds(multiplier, duplicates)\n            else:\n                data += element.to_gds(multiplier)\n                \n        return data + struct.pack('>2h', 4, 0x0700)\n        \n    def copy(self, name=None, suffix=None):\n        new_cell=copy.deepcopy(self)\n        if name is None:            \n            if suffix is not None:\n                new_cell.name+=suffix\n        else:\n            new_cell.name = name\n        \n        if suffix is not None:\n            deps=new_cell.get_dependencies(include_elements=True)\n            for cell in [e for e in deps if isinstance(e, Cell)]:\n                cell.name += suffix\n\n        return new_cell\n\n\n    def add(self, element, *args, **kwargs):\n        if isinstance(element, Cell):\n            self._references.append(CellReference(element, *args, **kwargs))\n        elif isinstance(element, (ElementBase, Elements, ReferenceBase)):\n\n            if len(args)!=0 or len(kwargs)!=0:\n                raise TypeError('Cannot have extra arguments when adding elements')                        \n\n            if isinstance(element, ReferenceBase):\n                self._references.append(element)\n            else:\n                self._objects.append(element)\n\n        elif isinstance(element, (tuple, list)):\n            for e in element:\n                self.add(e, **kwargs)\n        \n        else:\n            raise TypeError('Cannot add type %s to cell.' % type(element))\n\n        self.bb_is_valid = False\n    \n    def remove(self, element):\n        if isinstance(element, (Elements)):\n            element = list(element)\n        elif not isinstance(element, (tuple, list)):\n            element = [element]\n\n        for e in element:\n            if isinstance(e, ReferenceBase):\n                self._references.remove(e)\n            else:\n                self._objects.remove(e)"}
{"Repository": "SRDC-CVPR2020", "input": "Computes and stores the average and current value className AverageMeter(object) Method __init__ Method reset Method update", "label": "class AverageMeter(object):\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count"}
{"Repository": "ghidra_scripts", "input": "docstring for Reference className Reference(object) Method __init__ Attribute callingAddr Attribute fromAddr Attribute toAddr", "label": "class Reference(object):\n\tdef __init__(self, callingAddr, fromAddr, toAddr):\n\t\tsuper(Reference, self).__init__()\n\t\tself.callingAddr = callingAddr\n\t\tself.fromAddr = fromAddr\n\t\tself.toAddr = toAddr"}
{"Repository": "enforce", "input": "A proxy object for safe addition of runtime type enforcement without mutating the original object className EnforceProxy(ObjectProxy) Method __init__ Method __enforcer__ Method __enforcer__ Method __call__ Attribute _self_enforcer", "label": "class EnforceProxy(ObjectProxy):\n    def __init__(self, wrapped, enforcer=None):\n        super().__init__(wrapped)\n        self._self_enforcer = enforcer\n\n    @property\n    def __enforcer__(self):\n        return self._self_enforcer\n\n    @__enforcer__.setter\n    def __enforcer__(self, value):\n        self._self_enforcer = value\n\n    def __call__(self, *args, **kwargs):\n        if type(self.__wrapped__) is type:\n            return EnforceProxy(self.__wrapped__(*args, **kwargs), self.__enforcer__)\n        return self.__wrapped__(*args, **kwargs)"}
{"Repository": "AR-Net", "input": "Random crop the given PIL. className GroupRandomSizedCrop(object) Method __init__ Method __call__ Attribute size Attribute interpolation", "label": "class GroupRandomSizedCrop(object):\n    def __init__(self, size, interpolation=Image.BILINEAR):\n        self.size = size\n        self.interpolation = interpolation\n\n    def __call__(self, img_group):\n        for attempt in range(10):\n            area = img_group[0].size[0] * img_group[0].size[1]\n            target_area = random.uniform(0.08, 1.0) * area\n            aspect_ratio = random.uniform(3. / 4, 4. / 3)\n\n            w = int(round(math.sqrt(target_area * aspect_ratio)))\n            h = int(round(math.sqrt(target_area / aspect_ratio)))\n\n            if random.random() < 0.5:\n                w, h = h, w\n\n            if w <= img_group[0].size[0] and h <= img_group[0].size[1]:\n                x1 = random.randint(0, img_group[0].size[0] - w)\n                y1 = random.randint(0, img_group[0].size[1] - h)\n                found = True\n                break\n        else:\n            found = False\n            x1 = 0\n            y1 = 0\n\n        if found:\n            out_group = list()\n            for img in img_group:\n                img = img.crop((x1, y1, x1 + w, y1 + h))\n                assert (img.size == (w, h))\n                out_group.append(img.resize((self.size, self.size), self.interpolation))\n            return out_group\n        else:\n            # Fallback\n            scale = GroupScale(self.size, interpolation=self.interpolation)\n            crop = GroupRandomCrop(self.size)\n            return crop(scale(img_group))"}
{"Repository": "glance_store", "input": "Connection Manager class responsible for initializing and managing swiftclient connections in store. className SwiftConnectionManager(object) Method get_connection Method client Method _init_connection Method _init_client Method _get_storage_url Method __enter__ Method __exit__", "label": "class SwiftConnectionManager(object):\n    AUTH_HEADER_NAME = 'X-Auth-Token'\n\n    def __init__(self, store, store_location, context=None,\n                 allow_reauth=False):\n        self._client = None\n        self.store = store\n        self.location = store_location\n        self.context = context\n        self.allow_reauth = allow_reauth\n        self.storage_url = self._get_storage_url()\n        self.connection = self._init_connection()\n\n    def get_connection(self):\n        if self.allow_reauth:\n            # we are refreshing token only and if only connection manager\n            # re-authentication is allowed. Token refreshing is setup by\n            # connection manager users. Also we disable re-authentication\n            # if there is not way to execute it (cannot initialize trusts for\n            # multi-tenant or auth_version is not 3)\n            auth_ref = self.client.session.auth.auth_ref\n            # if connection token is going to expire soon (keystone checks\n            # is token is going to expire or expired already)\n            if self.store.backend_group:\n                interval = getattr(\n                    self.store.conf, self.store.backend_group\n                ).swift_store_expire_soon_interval\n            else:\n                store_conf = self.store.conf.glance_store\n                interval = store_conf.swift_store_expire_soon_interval\n\n            if auth_ref.will_expire_soon(interval):\n                LOG.info(_LI(\"Requesting new token for swift connection.\"))\n                # request new token with session and client provided by store\n                auth_token = self.client.session.get_auth_headers().get(\n                    self.AUTH_HEADER_NAME)\n                LOG.info(_LI(\"Token has been successfully requested. \"\n                             \"Refreshing swift connection.\"))\n                # initialize new switclient connection with fresh token\n                self.connection = self.store.get_store_connection(\n                    auth_token, self.storage_url)\n        return self.connection\n\n    @property\n    def client(self):\n        if self._client is None:\n            self._client = self._init_client()\n        return self._client\n\n    def _init_connection(self):\n        auth_token = self.client.session.get_auth_headers().get(\n            self.AUTH_HEADER_NAME)\n        return self.store.get_store_connection(\n            auth_token, self.storage_url)\n\n    def _init_client(self):\n        return self.store.init_client(location=self.location,\n                                      context=self.context)\n\n    def _get_storage_url(self):\n        raise NotImplementedError()\n\n    def __enter__(self):\n        return self\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        pass"}
{"Repository": "nops", "input": "A parsed CSS stylesheet. className Stylesheet(object) Method __init__ Method __repr__ Attribute rules Attribute errors Attribute encoding", "label": "class Stylesheet(object):\n    def __init__(self, rules, errors, encoding):\n        self.rules = rules\n        self.errors = errors\n        self.encoding = encoding\n\n    def __repr__(self):\n        return '<{0.__class__.__name__} {1} rules {2} errors>'.format(\n            self, len(self.rules), len(self.errors))"}
{"Repository": "pytorch-deep-generative-replay", "input": "Mixin which defines a sampling iterface for a generative model. className GenerativeMixin(object) Method sample", "label": "class GenerativeMixin(object):\n    def sample(self, size):\n        raise NotImplementedError"}
{"Repository": "SMedBERT", "input": "Configuration class to store the configuration of a `GPT2Model`. className GPT2Config(PretrainedConfig) Method max_position_embeddings Method hidden_size Method num_attention_heads Method num_hidden_layers", "label": "class GPT2Config(PretrainedConfig):\n    pretrained_config_archive_map = GPT2_PRETRAINED_CONFIG_ARCHIVE_MAP\n\n\n\n\n\n    def __init__(\n\n\n        self,\n\n\n        vocab_size_or_config_json_file=50257,\n\n\n        n_positions=1024,\n\n\n        n_ctx=1024,\n\n\n        n_embd=768,\n\n\n        n_layer=12,\n\n\n        n_head=12,\n\n\n        resid_pdrop=0.1,\n\n\n        embd_pdrop=0.1,\n\n\n        attn_pdrop=0.1,\n\n\n        layer_norm_epsilon=1e-5,\n\n\n        initializer_range=0.02,\n\n\n\n\n\n        num_labels=1,\n\n\n        summary_type='cls_index',\n\n\n        summary_use_proj=True,\n\n\n        summary_activation=None,\n\n\n        summary_proj_to_labels=True,\n\n\n        summary_first_dropout=0.1,\n\n\n        **kwargs\n\n\n    ):\n        super(GPT2Config, self).__init__(**kwargs)\n\n\n\n\n\n        if isinstance(vocab_size_or_config_json_file, str) or (sys.version_info[0] == 2\n\n\n                        and isinstance(vocab_size_or_config_json_file, unicode)):\n\n\n            with open(vocab_size_or_config_json_file, \"r\", encoding=\"utf-8\") as reader:\n\n\n                json_config = json.loads(reader.read())\n\n\n            for key, value in json_config.items():\n\n\n                self.__dict__[key] = value\n\n\n        elif isinstance(vocab_size_or_config_json_file, int):\n\n\n            self.vocab_size = vocab_size_or_config_json_file\n\n\n            self.n_ctx = n_ctx\n\n\n            self.n_positions = n_positions\n\n\n            self.n_embd = n_embd\n\n\n            self.n_layer = n_layer\n\n\n            self.n_head = n_head\n\n\n            self.resid_pdrop = resid_pdrop\n\n\n            self.embd_pdrop = embd_pdrop\n\n\n            self.attn_pdrop = attn_pdrop\n\n\n            self.layer_norm_epsilon = layer_norm_epsilon\n\n\n            self.initializer_range = initializer_range\n\n\n\n\n\n            self.num_labels = num_labels\n\n\n            self.summary_type = summary_type\n\n\n            self.summary_use_proj = summary_use_proj\n\n\n            self.summary_activation = summary_activation\n\n\n            self.summary_first_dropout = summary_first_dropout\n\n\n            self.summary_proj_to_labels = summary_proj_to_labels\n\n\n        else:\n\n\n            raise ValueError(\n\n\n                \"First argument must be either a vocabulary size (int)\"\n\n\n                \"or the path to a pretrained model config file (str)\"\n\n\n            )\n\n\n\n\n\n    @property\n\n\n    def max_position_embeddings(self):\n\n\n        return self.n_positions\n\n\n\n\n\n    @property\n\n\n    def hidden_size(self):\n\n\n        return self.n_embd\n\n\n\n\n\n    @property\n\n\n    def num_attention_heads(self):\n\n\n        return self.n_head\n\n\n\n\n\n    @property\n\n\n    def num_hidden_layers(self):\n\n\n        return self.n_layer"}
{"Repository": "ViT-CIFAR", "input": "Randomly choose one of the best 24 Sub-policies on ImageNet. className ImageNetPolicy(object) Method __init__ Method __call__ Method __repr__ Attribute policies", "label": "class ImageNetPolicy(object):\n    def __init__(self, fillcolor=(128, 128, 128)):\n        self.policies = [\n            SubPolicy(0.4, \"posterize\", 8, 0.6, \"rotate\", 9, fillcolor),\n            SubPolicy(0.6, \"solarize\", 5, 0.6, \"autocontrast\", 5, fillcolor),\n            SubPolicy(0.8, \"equalize\", 8, 0.6, \"equalize\", 3, fillcolor),\n            SubPolicy(0.6, \"posterize\", 7, 0.6, \"posterize\", 6, fillcolor),\n            SubPolicy(0.4, \"equalize\", 7, 0.2, \"solarize\", 4, fillcolor),\n\n            SubPolicy(0.4, \"equalize\", 4, 0.8, \"rotate\", 8, fillcolor),\n            SubPolicy(0.6, \"solarize\", 3, 0.6, \"equalize\", 7, fillcolor),\n            SubPolicy(0.8, \"posterize\", 5, 1.0, \"equalize\", 2, fillcolor),\n            SubPolicy(0.2, \"rotate\", 3, 0.6, \"solarize\", 8, fillcolor),\n            SubPolicy(0.6, \"equalize\", 8, 0.4, \"posterize\", 6, fillcolor),\n\n            SubPolicy(0.8, \"rotate\", 8, 0.4, \"color\", 0, fillcolor),\n            SubPolicy(0.4, \"rotate\", 9, 0.6, \"equalize\", 2, fillcolor),\n            SubPolicy(0.0, \"equalize\", 7, 0.8, \"equalize\", 8, fillcolor),\n            SubPolicy(0.6, \"invert\", 4, 1.0, \"equalize\", 8, fillcolor),\n            SubPolicy(0.6, \"color\", 4, 1.0, \"contrast\", 8, fillcolor),\n\n            SubPolicy(0.8, \"rotate\", 8, 1.0, \"color\", 2, fillcolor),\n            SubPolicy(0.8, \"color\", 8, 0.8, \"solarize\", 7, fillcolor),\n            SubPolicy(0.4, \"sharpness\", 7, 0.6, \"invert\", 8, fillcolor),\n            SubPolicy(0.6, \"shearX\", 5, 1.0, \"equalize\", 9, fillcolor),\n            SubPolicy(0.4, \"color\", 0, 0.6, \"equalize\", 3, fillcolor),\n\n            SubPolicy(0.4, \"equalize\", 7, 0.2, \"solarize\", 4, fillcolor),\n            SubPolicy(0.6, \"solarize\", 5, 0.6, \"autocontrast\", 5, fillcolor),\n            SubPolicy(0.6, \"invert\", 4, 1.0, \"equalize\", 8, fillcolor),\n            SubPolicy(0.6, \"color\", 4, 1.0, \"contrast\", 8, fillcolor),\n            SubPolicy(0.8, \"equalize\", 8, 0.6, \"equalize\", 3, fillcolor)\n        ]\n\n    def __call__(self, img):\n        policy_idx = random.randint(0, len(self.policies) - 1)\n        return self.policies[policy_idx](img)\n\n    def __repr__(self):\n        return \"AutoAugment ImageNet Policy\""}
{"Repository": "MATRN", "input": "A simple class to map ids into strings. className CharsetMapper(object) Method _read_charset Method trim Method get_text Method get_labels Method pad_labels Method digits Method digit_labels Method alphabets Method alphabet_labels", "label": "class CharsetMapper(object):\n    def __init__(self,\n                 filename='',\n                 max_length=30,\n                 null_char=u'\\u2591'):\n        self.null_char = null_char\n        self.max_length = max_length\n\n        self.label_to_char = self._read_charset(filename)\n        self.char_to_label = dict(map(reversed, self.label_to_char.items()))\n        self.num_classes = len(self.label_to_char)\n \n    def _read_charset(self, filename):\n        import re\n        pattern = re.compile(r'(\\d+)\\t(.+)')\n        charset = {}\n        self.null_label = 0\n        charset[self.null_label] = self.null_char\n        with open(filename, 'r') as f:\n            for i, line in enumerate(f):\n                m = pattern.match(line)\n                assert m, f'Incorrect charset file. line #{i}: {line}'\n                label = int(m.group(1)) + 1\n                char = m.group(2)\n                charset[label] = char\n        return charset\n\n    def trim(self, text):\n        assert isinstance(text, str)\n        return text.replace(self.null_char, '')\n\n    def get_text(self, labels, length=None, padding=True, trim=False):\n        length = length if length else self.max_length\n        labels = [l.item() if isinstance(l, Tensor) else int(l) for l in labels]\n        if padding:\n            labels = labels + [self.null_label] * (length-len(labels))\n        text = ''.join([self.label_to_char[label] for label in labels])\n        if trim: text = self.trim(text)\n        return text\n\n    def get_labels(self, text, length=None, padding=True, case_sensitive=False):\n        length = length if length else self.max_length\n        if padding:\n            text = text + self.null_char * (length - len(text))\n        if not case_sensitive:\n            text = text.lower()\n        labels = [self.char_to_label[char] for char in text]\n        return labels\n\n    def pad_labels(self, labels, length=None):\n        length = length if length else self.max_length\n\n        return labels + [self.null_label] * (length - len(labels))\n\n    @property\n    def digits(self):\n        return '0123456789'\n\n    @property\n    def digit_labels(self):\n        return self.get_labels(self.digits, padding=False)\n\n    @property\n    def alphabets(self):\n        all_chars = list(self.char_to_label.keys())\n        valid_chars = []\n        for c in all_chars:\n            if c in 'abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ':\n                valid_chars.append(c)\n        return ''.join(valid_chars)\n\n    @property\n    def alphabet_labels(self):\n        return self.get_labels(self.alphabets, padding=False)"}
{"Repository": "A2J-Transformer", "input": "A simple timer. className Timer(object) Method __init__ Method tic Method toc Attribute total_time Attribute calls Attribute start_time Attribute diff Attribute average_time Attribute warm_up", "label": "class Timer(object):\n    def __init__(self):\n        self.total_time = 0.\n        self.calls = 0\n        self.start_time = 0.\n        self.diff = 0.\n        self.average_time = 0.\n        self.warm_up = 0\n\n    def tic(self):\n        # using time.time instead of time.clock because time time.clock\n        # does not normalize for multithreading\n        self.start_time = time.time()\n\n    def toc(self, average=True):\n        self.diff = time.time() - self.start_time\n        if self.warm_up < 10:\n            self.warm_up += 1\n            return self.diff\n        else:\n            self.total_time += self.diff\n            self.calls += 1\n            self.average_time = self.total_time / self.calls\n\n        if average:\n            return self.average_time\n        else:\n            return self.diff"}
{"Repository": "DECOLA", "input": "The basic residual block for ResNet-18 and ResNet-34, with two 3x3 conv layers and a projection shortcut if needed. className BasicBlock(CNNBlockBase) Method __init__ Method forward Attribute shortcut Attribute shortcut Attribute conv1 Attribute conv2", "label": "class BasicBlock(CNNBlockBase):\n    def __init__(self, in_channels, out_channels, *, stride=1, norm=\"BN\"):\n        super().__init__(in_channels, out_channels, stride)\n\n        if in_channels != out_channels:\n            self.shortcut = Conv2d(\n                in_channels,\n                out_channels,\n                kernel_size=1,\n                stride=stride,\n                bias=False,\n                norm=get_norm(norm, out_channels),\n            )\n        else:\n            self.shortcut = None\n\n        self.conv1 = Conv2d(\n            in_channels,\n            out_channels,\n            kernel_size=3,\n            stride=stride,\n            padding=1,\n            bias=False,\n            norm=get_norm(norm, out_channels),\n        )\n\n        self.conv2 = Conv2d(\n            out_channels,\n            out_channels,\n            kernel_size=3,\n            stride=1,\n            padding=1,\n            bias=False,\n            norm=get_norm(norm, out_channels),\n        )\n\n        for layer in [self.conv1, self.conv2, self.shortcut]:\n            if layer is not None:  # shortcut can be None\n                weight_init.c2_msra_fill(layer)\n\n    def forward(self, x):\n        out = self.conv1(x)\n        out = F.relu_(out)\n        out = self.conv2(out)\n\n        if self.shortcut is not None:\n            shortcut = self.shortcut(x)\n        else:\n            shortcut = x\n\n        out += shortcut\n        out = F.relu_(out)\n        return out"}
{"Repository": "bitmask_client", "input": "Update the _binaries. className cmd_binary_hash(Command) Method initialize_options Method finalize_options Method run Method exit", "label": "class cmd_binary_hash(Command):\n    user_options = []\n\n    def initialize_options(self):\n        pass\n\n    def finalize_options(self):\n        pass\n\n    def run(self, *args):\n        # TODO check gnupg binary too.\n\n        OPENVPN_BIN = os.environ.get('OPENVPN_BIN', None)\n        BITMASK_ROOT = os.environ.get('BITMASK_ROOT', None)\n\n        def exit():\n            print(\"Please set environment variables \"\n                  \"OPENVPN_BIN and BITMASK_ROOT pointing to the right path \"\n                  \"to use this command\")\n            sys.exit(1)\n\n        bin_paths = OPENVPN_BIN, BITMASK_ROOT\n        if not all(bin_paths):\n            exit()\n\n        if not all(map(os.path.isfile, bin_paths)):\n            exit()\n\n        openvpn_bin_hash, bitmask_root_hash = map(\n            lambda path: hashlib.sha256(open(path).read()).hexdigest(),\n            bin_paths)\n\n        template = r\"\"\""}
{"Repository": "pyflux", "input": "TruncatedNormal Distribution ---- This class contains methods relating to the truncated normal distribution for time series. className TruncatedNormal(Family) Method __init__ Method logpdf Method pdf Attribute mu0 Attribute sigma0 Attribute upper Attribute lower Attribute covariance_prior", "label": "class TruncatedNormal(Family):\n    def __init__(self, mu=0.0, sigma=1.0, lower=None, upper=None, transform=None, **kwargs):\n        super(TruncatedNormal, self).__init__(transform)\n        self.mu0 = mu\n        self.sigma0 = sigma\n        self.upper = upper\n        self.lower = lower\n        self.covariance_prior = False\n\n\n    def logpdf(self, mu):\n        if self.transform is not None:\n            mu = self.transform(mu)     \n        if mu < self.lower and self.lower is not None:\n            return -10.0**6\n        elif mu > self.upper and self.upper is not None:\n            return -10.0**6\n        else:\n            return -np.log(float(self.sigma0)) - (0.5*(mu-self.mu0)**2)/float(self.sigma0**2)\n\n    def pdf(self, mu):\n        if self.transform is not None:\n            mu = self.transform(mu)    \n        if mu < self.lower and self.lower is not None:\n            return 0.0\n        elif mu > self.upper and self.upper is not None:\n            return 0.0       \n        else:\n            return (1/float(self.sigma0))*np.exp(-(0.5*(mu-self.mu0)**2)/float(self.sigma0**2))"}
{"Repository": "deux", "input": "Exception for SMS that fails because phone number is not a valid number for receiving SMS's. className InvalidPhoneNumberError(FailedChallengeError) Method __init__", "label": "class InvalidPhoneNumberError(FailedChallengeError):\n    def __init__(self, message=strings.INVALID_PHONE_NUMBER_ERROR):\n        super(InvalidPhoneNumberError, self).__init__(message)"}
{"Repository": "exploration-by-disagreement", "input": "Uses cloudpickle to serialize contents (otherwise multiprocessing tries to use pickle) className CloudpickleWrapper(object) Method __init__ Method __getstate__ Method __setstate__ Attribute x", "label": "class CloudpickleWrapper(object):\n    def __init__(self, x):\n        self.x = x\n\n    def __getstate__(self):\n        import cloudpickle\n        return cloudpickle.dumps(self.x)\n\n    def __setstate__(self, ob):\n        import pickle\n        self.x = pickle.loads(ob)"}
{"Repository": "decuda", "input": "Error compiling className CompilationError(Exception) Method __init__ Method __str__ Attribute line", "label": "class CompilationError(Exception):\n    line = None\n    def __init__(self, line, message):\n        self.line = line\n        Exception.__init__(self, message)\n    def __str__(self):\n        return \"Error on line %i: %s\" % (self.line, self.message)"}
{"Repository": "bonsai", "input": "General LDAP error. className LDAPError(Exception) Method create Method hexcode Method __str__", "label": "class LDAPError(Exception):\n    code = 0\n\n    @classmethod\n    def create(cls, code: int) -> Type[\"LDAPError\"]:\n        cls.code = code\n        return cls\n\n    @property\n    def hexcode(self) -> int:\n        return (self.code + (1 << 16)) % (1 << 16)\n\n    def __str__(self) -> str:\n        return \"{} (0x{:04X} [{:d}])\".format(\n            self.args[0] if self.args else \"\", self.hexcode, self.code\n        )"}
{"Repository": "script.module.openscrapers", "input": "Abstract :class:`ParserElement` subclass, for defining atomic matching patterns. className Token(ParserElement) Method __init__", "label": "class Token(ParserElement):\n    def __init__(self):\n        super(Token, self).__init__(savelist=False)"}
{"Repository": "flowblade", "input": "GUI component displaying list of sequences in project className SequenceListView(ImageTextImageListView) Method __init__ Method fill_data_model Method _button_press_event Attribute sequence_popup_cb Attribute double_click_cb Attribute double_click_counter Attribute icon_path Attribute arrow_path", "label": "class SequenceListView(ImageTextImageListView):\n    def __init__(self, seq_name_edited_cb, sequence_popup_cb, double_click_cb):\n        ImageTextImageListView.__init__(self)\n        self.sequence_popup_cb = sequence_popup_cb\n        self.treeview.connect('button-press-event', self._button_press_event)\n\n        self.double_click_cb = double_click_cb\n        self.double_click_counter = 0 # We get 2 events for double click, we use this to only do one callback\n        \n        # Icon path\n        self.icon_path = respaths.IMAGE_PATH + \"sequence.png\"\n        self.arrow_path = respaths.IMAGE_PATH + \"filter_save.png\"\n        \n        # Set sequence name editable and connect 'edited' signal\n        self.text_rend_1.set_property(\"editable\", True)\n        self.text_rend_1.connect(\"edited\",\n                                 seq_name_edited_cb,\n                                 (self.storemodel, 1))\n\n        self.scroll.connect('button-press-event', self._button_press_event)\n        \n    def fill_data_model(self):\n        self.storemodel.clear()\n        for seq in PROJECT().sequences:\n            icon = GdkPixbuf.Pixbuf.new_from_file(self.icon_path)\n            arrow = GdkPixbuf.Pixbuf.new_from_file(self.arrow_path )\n            if seq != current_sequence():\n                arrow = None\n            row_data = [icon,\n                        seq.name,\n                        arrow]\n            self.storemodel.append(row_data)\n            self.scroll.queue_draw()\n\n    def _button_press_event(self, widget, event):\n        if event.button == 3:\n            self.sequence_popup_cb(widget, event)\n        # Double click handled separately\n        if event.button == 1 and event.type == Gdk.EventType._2BUTTON_PRESS:\n            self.double_click_counter += 1\n            if self.double_click_counter == 2:\n                self.double_click_counter = 0\n                self.double_click_cb()"}
{"Repository": "dqn-obstacle-avoidance", "input": "Ray sensor. className RaySensor(object) Method __init__ Method distances Method has_collided Method set_locator Method update Method _update_rays Method _cast_ray Method to_polydata Attribute _num_rays Attribute _radius Attribute _min_angle Attribute _max_angle Attribute _locator Attribute _state Attribute _hit Attribute _distances Attribute _intersections", "label": "class RaySensor(object):\n    def __init__(self, num_rays=16, radius=40, min_angle=-45, max_angle=45):\n        self._num_rays = num_rays\n        self._radius = radius\n        self._min_angle = math.radians(min_angle)\n        self._max_angle = math.radians(max_angle)\n\n        self._locator = None\n        self._state = [0., 0., 0.]  # x, y, theta\n\n        self._hit = np.zeros(num_rays)\n        self._distances = np.zeros(num_rays)\n        self._intersections = [[0, 0, 0] for i in range(num_rays)]\n\n        self._update_rays(self._state[2])\n\n    @property\n    def distances(self):\n        normalized_distances = [\n            self._distances[i] / self._radius if self._hit[i] else 1.0\n            for i in range(self._num_rays)\n        ]\n        return normalized_distances\n\n    def has_collided(self, max_distance=0.05):\n        for hit, distance in zip(self._hit, self._distances):\n            if hit and distance <= max_distance:\n                return True\n\n        return False\n\n    def set_locator(self, locator):\n        self._locator = locator\n\n    def update(self, x, y, theta):\n        self._update_rays(theta)\n        origin = np.array([x, y, 0])\n        self._state = [x, y, theta]\n\n        if self._locator is None:\n            return\n\n        for i in range(self._num_rays):\n            hit, dist, inter = self._cast_ray(origin, origin + self._rays[i])\n            self._hit[i] = hit\n            self._distances[i] = dist\n            self._intersections[i] = inter\n\n    def _update_rays(self, theta):\n        r = self._radius\n        angle_step = (self._max_angle - self._min_angle) / (self._num_rays - 1)\n        self._rays = [\n            np.array([\n                r * math.cos(theta + self._min_angle + i * angle_step),\n                r * math.sin(theta + self._min_angle + i * angle_step),\n                0\n            ])\n            for i in range(self._num_rays)\n        ]\n\n    def _cast_ray(self, start, end):\n        tolerance = 0.0                 # intersection tolerance\n        pt = [0.0, 0.0, 0.0]            # coordinate of intersection\n        distance = vtk.mutable(0.0)     # distance of intersection\n        pcoords = [0.0, 0.0, 0.0]       # location within intersected cell\n        subID = vtk.mutable(0)          # subID of intersected cell\n\n        hit = self._locator.IntersectWithLine(start, end, tolerance,\n                                              distance, pt, pcoords, subID)\n\n        return hit, distance, pt\n\n    def to_polydata(self):\n        d = DebugData()\n        origin = np.array([self._state[0], self._state[1], 0])\n        for hit, intersection, ray in zip(self._hit,\n                                          self._intersections,\n                                          self._rays):\n            if hit:\n                color = [1., 0.45882353, 0.51372549]\n                endpoint = intersection\n            else:\n                color = [0., 0.6, 0.58823529]\n                endpoint = origin + ray\n\n            d.addLine(origin, endpoint, color=color, radius=0.05)\n\n        return d.getPolyData()"}
{"Repository": "Relation-Extraction-Transformer", "input": "A wrapper class for the training and evaluation of models. className RelationModel(object) Method __init__ Method update Method predict Method update_lr Method save Method load Attribute opt Attribute model Attribute criterion Attribute parameters Attribute optimizer", "label": "class RelationModel(object):\n    def __init__(self, opt, emb_matrix=None):\n\n        self.opt = opt\n        self.model = PositionAwareRNN(opt, emb_matrix)\n\n        # pass weights per class, each class corresponds to its index\n        weights = [opt['weight_no_rel']]\n        rel_classes_weights = [opt[\"weight_rest\"]] * 41\n        weights.extend(rel_classes_weights)\n\n        print(\"Using weights\", weights)\n        assert len(weights) == 42\n\n        class_weights = torch.FloatTensor(weights).to(\"cuda\")\n\n        self.criterion = nn.CrossEntropyLoss(class_weights)  # weight=class_weights\n        self.parameters = [p for p in self.model.parameters() if p.requires_grad]\n        # print(self.parameters)\n        # print(len(self.parameters))\n\n        if opt['cuda']:\n            self.model.to(\"cuda\")\n            self.criterion.to(\"cuda\")\n\n        self.optimizer = torch_utils.get_optimizer(opt['optim'], self.parameters, opt['lr'])\n\n    def update(self, batch):\n        if self.opt['cuda']:\n            inputs = [b.to(\"cuda\") for b in batch[:9]]\n            labels = batch[9].to(\"cuda\")\n        else:\n            inputs = [b for b in batch[:9]]\n            labels = batch[9]\n\n        # step forward\n        self.model.train()\n        self.optimizer.zero_grad()\n        logits, _ = self.model(inputs)\n        # print(labels)\n        loss = self.criterion(logits, labels)\n\n        # backward step\n        loss.backward()\n\n        # do gradient clipping\n        torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.opt['max_grad_norm'])\n        self.optimizer.step()\n\n        loss_val = loss.item()\n\n        return loss_val\n\n    def predict(self, batch, unsort=True):\n        if self.opt['cuda']:\n            inputs = [b.to(\"cuda\") for b in batch[:9]]\n            labels = batch[9].to(\"cuda\")\n        else:\n            inputs = [b for b in batch[:9]]\n            labels = batch[9]\n\n        orig_idx = batch[10]\n\n        # forward\n        self.model.eval()\n        logits, _ = self.model(inputs)\n        loss = self.criterion(logits, labels)\n\n        probs = F.softmax(logits, dim=-1).data.cpu().numpy().tolist()\n        predictions = np.argmax(logits.data.cpu().numpy(), axis=1).tolist()\n        if unsort:\n            _, predictions, probs = [list(t) for t in zip(*sorted(zip(orig_idx, predictions, probs)))]\n\n        return predictions, probs, loss.item()\n\n    def update_lr(self, new_lr):\n        torch_utils.change_lr(self.optimizer, new_lr)\n\n    def save(self, filename, epoch):\n        params = {\n            'model': self.model.state_dict(),\n            'config': self.opt,\n            'epoch': epoch\n        }\n        try:\n            torch.save(params, filename)\n            print(\"model saved to {}\".format(filename))\n        except BaseException:\n            print(\"[Warning: Saving failed... continuing anyway.]\")\n\n    def load(self, filename):\n        try:\n            checkpoint = torch.load(filename)\n        except BaseException:\n            print(\"Cannot load model from {}\".format(filename))\n            exit()\n        self.model.load_state_dict(checkpoint['model'])\n        self.opt = checkpoint['config']"}
{"Repository": "doctr", "input": "Abstract class for all datasets className AbstractDataset(_AbstractDataset) Method _read_sample Method collate_fn", "label": "class AbstractDataset(_AbstractDataset):\n    def _read_sample(self, index: int) -> Tuple[torch.Tensor, Any]:\n        img_name, target = self.data[index]\n\n        # Check target\n        if isinstance(target, dict):\n            assert \"boxes\" in target, \"Target should contain 'boxes' key\"\n            assert \"labels\" in target, \"Target should contain 'labels' key\"\n        elif isinstance(target, tuple):\n            assert len(target) == 2\n            assert isinstance(target[0], str) or isinstance(\n                target[0], np.ndarray\n            ), \"first element of the tuple should be a string or a numpy array\"\n            assert isinstance(target[1], list), \"second element of the tuple should be a list\"\n        else:\n            assert isinstance(target, str) or isinstance(\n                target, np.ndarray\n            ), \"Target should be a string or a numpy array\"\n\n        # Read image\n        img = (\n            tensor_from_numpy(img_name, dtype=torch.float32)\n            if isinstance(img_name, np.ndarray)\n            else read_img_as_tensor(os.path.join(self.root, img_name), dtype=torch.float32)\n        )\n\n        return img, deepcopy(target)\n\n    @staticmethod\n    def collate_fn(samples: List[Tuple[torch.Tensor, Any]]) -> Tuple[torch.Tensor, List[Any]]:\n        images, targets = zip(*samples)\n        images = torch.stack(images, dim=0)  # type: ignore[assignment]\n\n        return images, list(targets)  # type: ignore[return-value]"}
{"Repository": "TradSimpChinese", "input": "This class takes in HTML files as a string. className HTML_TextProcessor(HTMLParser) Method __init__ Method setTextConvertor Method setLanguageAttribute Method replace_quotations Method multiple_replace Method processText Attribute recording Attribute result Attribute textConverter Attribute criteria Attribute converting Attribute language Attribute trad_to_simp_quotes Attribute trad_to_simp_re Attribute simp_to_trad_quotes Attribute simp_to_trad_re Attribute zh_re", "label": "class HTML_TextProcessor(HTMLParser):\n    def __init__(self, textConvertor = None):\n          super().__init__(convert_charrefs=False)\n          self.recording = 0\n          self.result = []\n          self.textConverter = textConvertor\n          self.criteria = None\n          self.converting = True\n          self.language = None\n\n          # Create regular expressions to modify quote styles\n          self.trad_to_simp_quotes = {'':'', '':'', '':'', '':''}\n          self.trad_to_simp_re = re.compile('|'.join(map(re.escape, self.trad_to_simp_quotes)))\n\n          self.simp_to_trad_quotes = {'':'', '':'', '':'', '':''}\n          self.simp_to_trad_re = re.compile('|'.join(map(re.escape, self.simp_to_trad_quotes)))\n\n          # Create regular expression to modify lang attribute\n          self.zh_re = re.compile(r'lang=\\\"zh-\\w+\\\"|lang=\\\"zh\\\"', re.IGNORECASE)\n\n\n    # Use this if one wants to reset the converter\n    def setTextConvertor(self, textConvertor):\n        self.textConverter = textConvertor\n\n    def setLanguageAttribute(self, language):\n        self.language = language\n\n    def replace_quotations(self, data):\n        # update quotes if desired\n        if self.criteria[QUOTATION_TYPE] == 1:\n            # traditional to simplified\n            htmlstr_corrected = self.trad_to_simp_re.sub(lambda match: self.trad_to_simp_quotes[match.group(0)], data)\n        elif self.criteria[QUOTATION_TYPE] == 2:\n            # simplified to traditional\n            htmlstr_corrected = self.simp_to_trad_re.sub(lambda match: self.simp_to_trad_quotes[match.group(0)], data)\n        else:\n            # no quote changes desired\n            htmlstr_corrected = data\n        return htmlstr_corrected\n\n\n    # multiple_replace copied from ActiveState http://code.activestate.com/recipes/81330-single-pass-multiple-replace/\n    # Copyright 2001 Xavier Defrang\n    # PSF (Python Software Foundation) license (GPL Compatible)\n    # https://docs.python.org/3/license.html\n    def multiple_replace(self, replace_regex, replace_dict, text):\n      # For each match, look-up corresponding value in dictionary\n      return replace_regex.sub(lambda mo: replace_dict[mo.string[mo.start():mo.end()]], text)\n\n    def processText(self, data, criteria):"}
{"Repository": "CellProfiler", "input": "Provides rendering of \"labels\" as outlines Needs self. className OutlinesMixin(ColorMixin) Method __init__ Method _flush_outlines Method _on_color_changed Method get_line_width Method set_line_width Method outlines Method points Attribute color Attribute _line_width", "label": "class OutlinesMixin(ColorMixin):\n    def __init__(self, outline_color, line_width):\n        super(OutlinesMixin, self).__init__()\n        self._flush_outlines()\n        self.color = outline_color\n        self._line_width = line_width\n\n    def _flush_outlines(self):\n        self._outlines = None\n        self._points = None\n\n    def _on_color_changed(self):\n        if self._points is not None:\n            self._points.set_color(self.color)\n\n    def get_line_width(self):\n        return self._line_width\n\n    def set_line_width(self, line_width):\n        self._line_width = line_width\n        self._outlines = None\n        if self._points is not None:\n            self._points.set_linewidth(line_width)\n\n    line_width = property(get_line_width, set_line_width)\n\n    @property\n    def outlines(self):\n        if self._outlines is None:\n            for i, labels in enumerate(self.labels):\n                if i == 0:\n                    self._outlines = centrosome.outline.outline(labels) != 0\n                else:\n                    self._outlines |= centrosome.outline.outline(labels) != 0\n            if self.line_width is not None and self.line_width > 1:\n                hw = float(self.line_width) / 2\n                d = scipy.ndimage.distance_transform_edt(~self._outlines)\n                dti, dtj = numpy.where((d < hw + 0.5) & ~self._outlines)\n                self._outlines = self._outlines.astype(numpy.float32)\n                self._outlines[dti, dtj] = numpy.minimum(1, hw + 0.5 - d[dti, dtj])\n\n        return self._outlines.astype(numpy.float32)\n\n    @property\n    def points(self):\n        if self._points is None:\n            self._points = CPOutlineArtist(\n                self.name, self.labels, linewidth=self.line_width, color=self.color\n            )\n        return self._points"}
{"Repository": "tensorflow-riemopt", "input": "Manifold of orthonormal p-frames in the n-dimensional space endowed with the Euclidean inner product. className StiefelEuclidean(_Stiefel) Method inner Method proju Method exp Method retr Method geodesic Method dist Method log Method transp", "label": "class StiefelEuclidean(_Stiefel):\n    name = \"Euclidean Stiefel\"\n\n    def inner(self, x, u, v, keepdims=False):\n        return tf.reduce_sum(u * v, axis=[-2, -1], keepdims=keepdims)\n\n    def proju(self, x, u):\n        xtu = utils.transposem(x) @ u\n        xtu_sym = (utils.transposem(xtu) + xtu) / 2.0\n        return u - x @ xtu_sym\n\n    def exp(self, x, u):\n        return self.geodesic(x, u, 1.0)\n\n    def retr(self, x, u):\n        q, r = tf.linalg.qr(x + u)\n        unflip = tf.cast(tf.sign(tf.linalg.diag_part(r)), r.dtype)\n        return q * unflip[..., tf.newaxis, :]\n\n    def geodesic(self, x, u, t):\n        xtu = utils.transposem(x) @ u\n        utu = utils.transposem(u) @ u\n        eye = tf.eye(\n            tf.shape(utu)[-1], batch_shape=tf.shape(utu)[:-2], dtype=x.dtype\n        )\n        logw = blockm(xtu, -utu, eye, xtu)\n        w = tf.linalg.expm(t * logw)\n        z = tf.concat([tf.linalg.expm(-xtu * t), tf.zeros_like(utu)], axis=-2)\n        y = tf.concat([x, u], axis=-1) @ w @ z\n        return y\n\n    def dist(self, x, y, keepdims=False):\n        raise NotImplementedError\n\n    def log(self, x, y):\n        return NotImplementedError\n\n    def transp(self, x, y, v):\n        return self.proju(y, v)"}
{"Repository": "quart", "input": "A HTTP Method (verb) specific view class. className MethodView(View) Method __init_subclass__", "label": "class MethodView(View):\n    def __init_subclass__(cls, **kwargs: Any) -> None:\n        super().__init_subclass__(**kwargs)\n\n        if \"methods\" not in cls.__dict__:\n            methods = set()\n\n            for base in cls.__bases__:\n                if getattr(base, \"methods\", None):\n                    methods.update(base.methods)  # type: ignore[attr-defined]\n\n            for key in http_method_funcs:\n                if hasattr(cls, key):\n                    methods.add(key.upper())\n\n            if methods:\n                cls.methods = methods\n\n    async def dispatch_request(self, **kwargs: Any) -> ResponseReturnValue:\n        handler = getattr(self, request.method.lower(), None)\n\n        if handler is None and request.method == \"HEAD\":\n            handler = getattr(self, \"get\", None)\n\n        return await current_app.ensure_async(handler)(**kwargs)"}
{"Repository": "become-yukarin", "input": "from https://github.com/r9y9/nnmnkwii/blob/4cade86b5c35b4e35615a2a8162ddc638018af0e/nnmnkwii/preprocessing/alignment.py#L14 className DTWAligner(object) Method __init__ Method align_x Method align_y Method align Method align_and_transform Method _interp_path", "label": "class DTWAligner(object):\n    def __init__(self, x, y, dist=lambda x, y: numpy.linalg.norm(x - y), radius=1) -> None:\n        assert x.ndim == 2 and y.ndim == 2\n\n        _, path = fastdtw.fastdtw(x, y, radius=radius, dist=dist)\n        path = numpy.array(path)\n        self.normed_path_x = path[:, 0] / len(x)\n        self.normed_path_y = path[:, 1] / len(y)\n\n    def align_x(self, x):\n        path = self._interp_path(self.normed_path_x, len(x))\n        return x[path]\n\n    def align_y(self, y):\n        path = self._interp_path(self.normed_path_y, len(y))\n        return y[path]\n\n    def align(self, x, y):\n        return self.align_x(x), self.align_y(y)\n\n    @staticmethod\n    def align_and_transform(x, y, *args, **kwargs):\n        aligner = DTWAligner(*args, x=x, y=y, **kwargs)\n        return aligner.align(x, y)\n\n    @staticmethod\n    def _interp_path(normed_path: numpy.ndarray, target_length: int):\n        path = numpy.floor(normed_path * target_length).astype(numpy.int)\n        return path"}
{"Repository": "recipe-scrapers", "input": "Scrape BettyBossi.ch recipes. className BettyBossi(AbstractScraper) Method host Method author Method title Method category Method total_time Method yields Method image Method ingredients Method instructions Method ratings Method cuisine Method description", "label": "class BettyBossi(AbstractScraper):\n    @classmethod\n    def host(cls):\n        return \"bettybossi.ch\"\n\n    def __init__(\n        self,\n        url: str,\n        proxies: Optional[\n            Dict[str, str]\n        ] = None,  # allows us to specify optional proxy server\n        timeout: Optional[\n            Union[float, Tuple[float, float], Tuple[float, None]]\n        ] = None,  # allows us to specify optional timeout for request\n        wild_mode: Optional[bool] = False,\n        html: Union[str, bytes, None] = None,\n    ) -> None:\n        if html is None:\n            with Session() as session:\n                session.proxies.update(proxies or {})\n                session.headers.update(HEADERS)\n\n                session.get(url, timeout=timeout)\n                html = session.get(url, timeout=timeout).content  # reload the page\n\n        # As the html content is provided, the parent will not query the page\n        super().__init__(url, proxies, timeout, wild_mode, html)\n\n    def author(self):\n        return self.schema.author()\n\n    def title(self):\n        return self.schema.title()\n\n    def category(self):\n        return self.schema.category()\n\n    def total_time(self):\n        return self.schema.total_time()\n\n    def yields(self):\n        return self.schema.yields()\n\n    def image(self):\n        return self.schema.image()\n\n    def ingredients(self):\n        return self.schema.ingredients()\n\n    def instructions(self):\n        return self.schema.instructions()\n\n    def ratings(self):\n        return self.schema.ratings()\n\n    def cuisine(self):\n        return self.schema.cuisine()\n\n    def description(self):\n        return self.schema.description()"}
{"Repository": "pyrelational", "input": "Class containing unit tests on benchmark regression datasets. className TestRegressionBenchmarkDatasets(TestCase) Method test_DiabetesDataset Method test_UCIConcrete Method test_UCIEnergy Method test_UCIPower Method test_UCIWine Method test_UCIYacht Method test_UCIAirfoil", "label": "class TestRegressionBenchmarkDatasets(TestCase):\n    @parameterized.expand(\n        [\n            (SynthReg1, 500, 5),\n            (SynthReg1, 1000, 3),\n            (SynthReg2, 500, 5),\n            (SynthReg2, 1000, 3),\n        ]\n    )\n    def test_synthetic_datasets(\n        self, dataset_class: Type[Union[SynthReg1, SynthReg2]], data_size: int, n_splits: int\n    ) -> None:\n        dataset = dataset_class(size=data_size, n_splits=n_splits)\n        self.assertEqual(len(dataset), data_size)\n        self.assertEqual(len(dataset.data_splits), n_splits)\n\n    def test_DiabetesDataset(self) -> None:\n        dataset = DiabetesDataset()\n        self.assertEqual(len(dataset), 442)\n        self.assertEqual(len(dataset.data_splits), 5)\n\n    def test_UCIConcrete(self) -> None:\n        dataset = UCIConcrete(data_dir=\"test_data/\")\n        self.assertEqual(len(dataset), 1030)\n        self.assertEqual(dataset.x.shape[1], 8)\n        self.assertEqual(len(dataset.data_splits), 5)\n\n    def test_UCIEnergy(self) -> None:\n        dataset = UCIEnergy(data_dir=\"test_data/\")\n        self.assertEqual(len(dataset), 768)\n        self.assertEqual(dataset.x.shape[1], 9)\n        self.assertEqual(len(dataset.data_splits), 5)\n\n    def test_UCIPower(self) -> None:\n        dataset = UCIPower(data_dir=\"test_data/\")\n        self.assertEqual(len(dataset), 9568)\n        self.assertEqual(dataset.x.shape[1], 4)\n        self.assertEqual(len(dataset.data_splits), 5)\n\n    def test_UCIWine(self) -> None:\n        dataset = UCIWine(data_dir=\"test_data/\")\n        self.assertEqual(len(dataset), 1598)\n        self.assertEqual(dataset.x.shape[1], 11)\n        self.assertEqual(len(dataset.data_splits), 5)\n\n    def test_UCIYacht(self) -> None:\n        dataset = UCIYacht(data_dir=\"test_data/\")\n        self.assertEqual(len(dataset), 306)\n        self.assertEqual(dataset.x.shape[1], 6)\n        self.assertEqual(len(dataset.data_splits), 5)\n\n    def test_UCIAirfoil(self) -> None:\n        dataset = UCIAirfoil(data_dir=\"test_data/\")\n        self.assertEqual(len(dataset), 1502)\n        self.assertEqual(dataset.x.shape[1], 5)\n        self.assertEqual(len(dataset.data_splits), 5)"}
{"Repository": "grpc-opentracing", "input": "The Beta API is deprecated for 0. className BetaCommandLineStub(object) Method Echo Method beta_create_CommandLine_server Method beta_create_CommandLine_stub", "label": "class BetaCommandLineStub(object):\n    def Echo(self, request, timeout, metadata=None, with_call=False, protocol_options=None):\n      raise NotImplementedError()\n    Echo.future = None\n\n\n  def beta_create_CommandLine_server(servicer, pool=None, pool_size=None, default_timeout=None, maximum_timeout=None):\n    request_deserializers = {\n      ('command_line.CommandLine', 'Echo'): CommandRequest.FromString,\n    }\n    response_serializers = {\n      ('command_line.CommandLine', 'Echo'): CommandResponse.SerializeToString,\n    }\n    method_implementations = {\n      ('command_line.CommandLine', 'Echo'): face_utilities.unary_unary_inline(servicer.Echo),\n    }\n    server_options = beta_implementations.server_options(request_deserializers=request_deserializers, response_serializers=response_serializers, thread_pool=pool, thread_pool_size=pool_size, default_timeout=default_timeout, maximum_timeout=maximum_timeout)\n    return beta_implementations.server(method_implementations, options=server_options)\n\n\n  def beta_create_CommandLine_stub(channel, host=None, metadata_transformer=None, pool=None, pool_size=None):\n    request_serializers = {\n      ('command_line.CommandLine', 'Echo'): CommandRequest.SerializeToString,\n    }\n    response_deserializers = {\n      ('command_line.CommandLine', 'Echo'): CommandResponse.FromString,\n    }\n    cardinalities = {\n      'Echo': cardinality.Cardinality.UNARY_UNARY,\n    }\n    stub_options = beta_implementations.stub_options(host=host, metadata_transformer=metadata_transformer, request_serializers=request_serializers, response_deserializers=response_deserializers, thread_pool=pool, thread_pool_size=pool_size)\n    return beta_implementations.dynamic_stub(channel, 'command_line.CommandLine', cardinalities, options=stub_options)"}
{"Repository": "MKGformer", "input": "Args: loss (:obj:`torch. className CLIPOutput(ModelOutput) Method to_tuple", "label": "class CLIPOutput(ModelOutput):\n    loss: Optional[torch.FloatTensor] = None\n    logits_per_image: torch.FloatTensor = None\n    logits_per_text: torch.FloatTensor = None\n    text_embeds: torch.FloatTensor = None\n    image_embeds: torch.FloatTensor = None\n    text_model_output: BaseModelOutputWithPooling = None\n    vision_model_output: BaseModelOutputWithPooling = None\n\n    def to_tuple(self) -> Tuple[Any]:\n        return tuple(\n            self[k] if k not in [\"text_model_output\", \"vision_model_output\"] else getattr(self, k).to_tuple()\n            for k in self.keys()\n        )"}
{"Repository": "python-taiga", "input": "CustomAttributeResource base class className CustomAttributeResource(InstanceResource) Method set_attribute Method _get_attributes Method get_attributes", "label": "class CustomAttributeResource(InstanceResource):\n    def set_attribute(self, id, value, version=1):  # noqa: A002\n        attributes = self._get_attributes(cache=True)\n        formatted_id = \"{}\".format(id)\n        attributes[\"attributes_values\"][formatted_id] = value\n        response = self.requester.patch(\n            \"/{endpoint}/custom-attributes-values/{id}\",\n            endpoint=self.endpoint,\n            id=self.id,\n            payload={\"attributes_values\": attributes[\"attributes_values\"], \"version\": version},\n        )\n        cache_key = self.requester.get_full_url(\n            \"/{endpoint}/custom-attributes-values/{id}\", endpoint=self.endpoint, id=self.id\n        )\n        self.requester.cache.put(cache_key, response)\n        return response.json()\n\n    def _get_attributes(self, cache=False):\n        response = self.requester.get(\n            \"/{endpoint}/custom-attributes-values/{id}\", endpoint=self.endpoint, id=self.id, cache=cache\n        )\n        return response.json()\n\n    def get_attributes(self):\n        return self._get_attributes()"}
{"Repository": "goxtool", "input": "This base class only exists because of the debug() method that is used in many of the goxtool objects to send debug output to the signal_debug. className BaseObject() Method __init__ Method debug Attribute signal_debug", "label": "class BaseObject():\n    def __init__(self):\n        self.signal_debug = Signal()\n\n    def debug(self, *args):\n        msg = \" \".join([str(x) for x in args])\n        if not self.signal_debug(self, (msg)):\n            logging.debug(msg)"}
{"Repository": "MPE_Util", "input": "The main class. className MPE_Util(ControlSurface) Method __init__ Method parse Attribute delayedCallbackHandler Attribute songModel", "label": "class MPE_Util(ControlSurface):\n    def __init__(self, c_instance):\n        versionStr = \"Live_{0}_{1}_{2}\".format(self.application.get_major_version(),\n                                               self.application.get_minor_version(),\n                                               self.application.get_bugfix_version())\n\n        #Call the ableton.v2.control_surface.control_surface init\n        super(MPE_Util, self).__init__(c_instance)\n\n        # Initialize the required instances to the InstanceContainer.\n        InstanceContainer.song = self.song\n        InstanceContainer.itemModelDataHandler = ItemModelDataHandler.ItemModelDataHandler(\"MPE_Util\")\n        self.delayedCallbackHandler = DelayedCallbackHandler()\n        InstanceContainer.add_delayed_callback = self.delayedCallbackHandler.add_delayed_callback\n        InstanceContainer.get_delayed_callback_exists = self.delayedCallbackHandler.get_delayed_callback_exists\n        \n        with self.component_guard():\n            self.songModel = SongModel(self.song)\n            self.parse()\n        log_message(\"MPE_Util running on\",versionStr)\n        self.show_message(\"MPEUtil loaded!\")\n        \n\n\n    def parse(self):\n        self.delayedCallbackHandler.run_cycles_delayed_callbacks()\n        self.schedule_message(1, self.parse)"}
{"Repository": "mutagen", "input": "Abstract stream information object. className StreamInfo(object) Method pprint", "label": "class StreamInfo(object):\n    __module__ = \"mutagen\"\n\n    def pprint(self) -> str:\n        raise NotImplementedError"}
{"Repository": "website", "input": "Form for game platforms className PlatformForm(AutoSlugForm) Method __init__", "label": "class PlatformForm(AutoSlugForm):\n    class Meta:  # pylint: disable=too-few-public-methods\n        model = models.Platform\n        fields = '__all__'\n\n    def __init__(self, *args, **kwargs):\n        super(PlatformForm, self).__init__(*args, **kwargs)\n        self.fields['default_installer'].required = False"}
{"Repository": "pointcloud2gazebo", "input": "Configuration of Cropping className Crop(Configurable) Method __init__", "label": "class Crop(Configurable):\n    # Minimum and maximum height of points to make Gazebo world.\n    min_z = Float(0.0).tag(config=True)\n    max_z = Float(2.0).tag(config=True)\n\n    def __init__(self, **kwargs):\n        super(Crop, self).__init__(**kwargs)"}
{"Repository": "ucupaint", "input": "Fake, bare minimum fields and functions for the updater object. className SingletonUpdaterNone(object) Method __init__ Method clear_state Method run_update Method check_for_update Attribute invalid_updater Attribute addon Attribute verbose Attribute use_print_traces Attribute error Attribute error_msg Attribute async_checking", "label": "class SingletonUpdaterNone(object):\n        def __init__(self):\n            self.invalid_updater = True  # Used to distinguish bad install.\n\n            self.addon = None\n            self.verbose = False\n            self.use_print_traces = True\n            self.error = None\n            self.error_msg = None\n            self.async_checking = None\n\n        def clear_state(self):\n            self.addon = None\n            self.verbose = False\n            self.invalid_updater = True\n            self.error = None\n            self.error_msg = None\n            self.async_checking = None\n\n        def run_update(self, force, callback, clean):\n            pass\n\n        def check_for_update(self, now):\n            pass\n\n    updater = SingletonUpdaterNone()\n    updater.error = \"Error initializing updater module\"\n    updater.error_msg = str(e)"}
{"Repository": "Efficient-PyTorch", "input": "Computes and stores the average and current value className AverageMeter(object) Method __init__ Method reset Method update", "label": "class AverageMeter(object):\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count"}
{"Repository": "rdpy", "input": "@summary: for KO tests className LIC_FAIL(Exception) Method test_valid_client_licensing_error_message Method test_new_license Method __init__ Method sendFlagged Method getGCCServerSettings Method __init__ Method __init__ Method __init__ Attribute _state", "label": "class LIC_FAIL(Exception):\n        pass\n    \n    def test_valid_client_licensing_error_message(self):\n        l = lic.LicenseManager(None)\n        s = type.Stream()\n        s.writeType(lic.createValidClientLicensingErrorMessage())\n        #reinit position\n        s.pos = 0\n        \n        self.assertTrue(l.recv(s), \"Manager can retrieve valid case\")\n        \n    def test_new_license(self):\n        class Transport(object):\n            def __init__(self):\n                self._state = False\n            def sendFlagged(self, flag, message):\n                if flag != sec.SecurityFlag.SEC_LICENSE_PKT:\n                    return\n                s = type.Stream()\n                s.writeType(message)\n                s.pos = 0\n                s.readType(lic.LicPacket(lic.ClientNewLicenseRequest()))\n                self._state = True\n            def getGCCServerSettings(self):\n                class A:\n                    def __init__(self):\n                        self._is_readed = False\n                class B:\n                    def __init__(self):\n                        self.serverCertificate = A()\n                class C:\n                    def __init__(self):\n                        self.SC_SECURITY = B()\n                return C()\n        \n        t = Transport()\n        l = lic.LicenseManager(t)\n        \n        s = type.Stream(SERVERREQUEST.decode(\"base64\"))\n        \n        self.assertFalse(l.recv(s) and t._state, \"Bad message after license request\")"}
{"Repository": "videonotes", "input": "Web handler for the main page. className HomePage(BaseHandler) Method get", "label": "class HomePage(BaseHandler):\n    TEMPLATE = 'index.html'\n\n    def get(self, *args):\n        # Generate a state instance for the request, this includes the action, and\n        # the file id(s) that have been sent from the Drive user interface.\n\n        return self.RenderTemplate(HomePage.TEMPLATE)"}
{"Repository": "pytorch-metric-learning", "input": "modified from https://github. className MultiSimilarityLoss(GenericPairLoss) Method __init__ Method _compute_loss Method get_default_distance Attribute alpha Attribute beta Attribute base", "label": "class MultiSimilarityLoss(GenericPairLoss):\n    def __init__(self, alpha=2, beta=50, base=0.5, **kwargs):\n        super().__init__(mat_based_loss=True, **kwargs)\n        self.alpha = alpha\n        self.beta = beta\n        self.base = base\n        self.add_to_recordable_attributes(\n            list_of_names=[\"alpha\", \"beta\", \"base\"], is_stat=False\n        )\n\n    def _compute_loss(self, mat, pos_mask, neg_mask):\n        pos_exp = self.distance.margin(mat, self.base)\n        neg_exp = self.distance.margin(self.base, mat)\n        pos_loss = (1.0 / self.alpha) * lmu.logsumexp(\n            self.alpha * pos_exp, keep_mask=pos_mask.bool(), add_one=True\n        )\n        neg_loss = (1.0 / self.beta) * lmu.logsumexp(\n            self.beta * neg_exp, keep_mask=neg_mask.bool(), add_one=True\n        )\n        return {\n            \"loss\": {\n                \"losses\": pos_loss + neg_loss,\n                \"indices\": c_f.torch_arange_from_size(mat),\n                \"reduction_type\": \"element\",\n            }\n        }\n\n    def get_default_distance(self):\n        return CosineSimilarity()"}
{"Repository": "qiling", "input": "Callback Functions for Hook Operation className Callback_Functions() Method read_mem Method read_reg Method write_mem Method write_reg Method emu_start Method emu_stop Method save", "label": "class Callback_Functions():\n    @staticmethod\n    def read_mem(ql: Qiling, *args):\n        user_data = args[-1]\n        buff = ql.mem.read(user_data[\"address\"], user_data[\"bytes_size\"])\n        ql.log.info(f\"Hook was triggered at -> {user_data['address']}\")\n        ql.log.info(buff)\n\n    @staticmethod\n    def read_reg(ql: Qiling, *args):\n        user_data = args[-1]\n        buff = ql.reg.read(user_data[\"register_name\"])\n        ql.log.info(f\"Hook was triggered at -> {user_data['register_name']}\")\n        ql.log.info(buff)\n\n    @staticmethod\n    def write_mem(ql: Qiling, *args):\n        user_data = args[-1]\n        buff = ql.mem.write(user_data[\"address\"], user_data[\"value\"])\n        ql.log.info(f\"Hook was triggered at -> {user_data['address']}\")\n        ql.log.info(buff)\n\n    @staticmethod\n    def write_reg(ql: Qiling, *args):\n        user_data = args[-1]\n        buff = ql.reg.write(user_data[\"register_name\"], user_data[\"value\"])\n        ql.log.info(f\"Hook was triggered at -> {user_data['register_name']}\")\n        ql.log.info(buff)\n\n    @staticmethod\n    def emu_start(ql: Qiling, *args):\n        user_data = args[-1]\n        ql.emu_start(begin=user_data[\"start\"], end=user_data[\"end\"])\n\n    @staticmethod\n    def emu_stop(ql: Qiling, *args):\n        ql.log.info('killer switch found, stopping')\n        ql.emu_stop()\n\n    @staticmethod\n    def save(ql: Qiling, *args):\n        ql.save()"}
{"Repository": "mendeley-python-sdk", "input": "Model class that also keeps track of a session. className SessionResponseObject(ResponseObject) Method __init__ Attribute session", "label": "class SessionResponseObject(ResponseObject):\n    def __init__(self, session, json):\n        super(SessionResponseObject, self).__init__(json)\n\n        self.session = session"}
{"Repository": "transfuser", "input": "Main class of the metrics module. className MetricsManager(object) Method __init__ Method _get_recorder Method _get_criteria Method _get_metric_class Method _get_recorder_map Attribute _args", "label": "class MetricsManager(object):\n    def __init__(self, args):\n        self._args = args\n\n        # Parse the arguments\n        recorder_str = self._get_recorder(self._args.log)\n        criteria_dict = self._get_criteria(self._args.criteria)\n\n        # Get the correct world and load it\n        map_name = self._get_recorder_map(recorder_str)\n        world = self._client.load_world(map_name)\n        town_map = world.get_map()\n\n        # Instanciate the MetricsLog, used to querry the needed information\n        log = MetricsLog(recorder_str)\n\n        # Read and run the metric class\n        metric_class = self._get_metric_class(self._args.metric)\n        metric_class(town_map, log, criteria_dict)\n\n    def _get_recorder(self, log):\n        # Get the log information.\n        self._client = carla.Client(self._args.host, self._args.port)\n        recorder_file = \"{}/{}\".format(os.getenv('SCENARIO_RUNNER_ROOT', \"./\"), log)\n\n        # Check that the file is correct\n        if recorder_file[-4:] != '.log':\n            print(\"ERROR: The log argument has to point to a .log file\")\n            sys.exit(-1)\n        if not os.path.exists(recorder_file):\n            print(\"ERROR: The specified log file does not exist\")\n            sys.exit(-1)\n\n        recorder_str = self._client.show_recorder_file_info(recorder_file, True)\n\n        return recorder_str\n\n    def _get_criteria(self, criteria_file):\n        if criteria_file:\n            with open(criteria_file) as fd:\n                criteria_dict = json.load(fd)\n        else:\n            criteria_dict = None\n\n        return criteria_dict\n\n    def _get_metric_class(self, metric_file):\n        # Get their module\n        module_name = os.path.basename(metric_file).split('.')[0]\n        sys.path.insert(0, os.path.dirname(metric_file))\n        metric_module = importlib.import_module(module_name)\n\n        # And their members of type class\n        for member in inspect.getmembers(metric_module, inspect.isclass):\n            # Get the first one with parent BasicMetrics\n            member_parent = member[1].__bases__[0]\n            if 'BasicMetric' in str(member_parent):\n                return member[1]\n\n        print(\"No child class of BasicMetric was found ... Exiting\")\n        sys.exit(-1)\n\n    def _get_recorder_map(self, recorder_str):\n        header = recorder_str.split(\"\\n\")\n        sim_map = header[1][5:]\n\n        return sim_map"}
{"Repository": "pylearn2", "input": "An intermediate representation between initial YAML parse and object instantiation. className Proxy(BaseProxy) Method __hash__", "label": "class Proxy(BaseProxy):\n    __slots__ = []\n\n    def __hash__(self):\n        return hash(id(self))"}
{"Repository": "codethesaur.us", "input": "TestCase for the urls className TestUrls(SimpleTestCase) Method test_index_url Method test_about_url Method test_compare_url Method test_reference_url Method test_api_reference_url Method test_api_compare_url", "label": "class TestUrls(SimpleTestCase):\n    def test_index_url(self):\n        url = reverse('index')\n        self.assertEqual(resolve(url).func, index)\n\n    def test_about_url(self):\n        url = reverse('about')\n        self.assertEqual(resolve(url).func, about)\n\n    def test_compare_url(self):\n        url = reverse('compare')\n        self.assertEqual(resolve(url).func, concepts)\n\n    def test_reference_url(self):\n        url = reverse('reference')\n        self.assertEqual(resolve(url).func, concepts)\n\n    def test_api_reference_url(self):\n        url = reverse(api_reference, args=['classes', 'javascript', 'ECMAScript 2023'])\n        self.assertEqual(resolve(url).func, api_reference)\n\n    def test_api_compare_url(self):\n        url = reverse(api_compare, args=['classes', 'javascript', 'ECMAScript 2023', 'java', 'java17'])\n        self.assertEqual(resolve(url).func, api_compare)"}
{"Repository": "Fatigue-Driven-Detection-Based-on-CNN", "input": "Transforms a VOC annotation into a Tensor of bbox coords and label index className VOCAnnotationTransform(object) Method __init__ Method __call__ Attribute class_to_ind Attribute keep_difficult", "label": "class VOCAnnotationTransform(object):\n    def __init__(self, class_to_ind=None, keep_difficult=False):\n        self.class_to_ind = class_to_ind or dict(\n            zip(VOC_CLASSES, range(len(VOC_CLASSES))))\n        self.keep_difficult = keep_difficult\n\n    def __call__(self, target, width, height):\n        res = []\n        for obj in target.iter('object'):\n            difficult = int(obj.find('difficult').text) == 1\n            if not self.keep_difficult and difficult:\n                continue\n            name = obj.find('name').text.lower().strip()\n            bbox = obj.find('bndbox')\n\n            pts = ['xmin', 'ymin', 'xmax', 'ymax']\n            bndbox = []\n            for i, pt in enumerate(pts):\n                cur_pt = int(bbox.find(pt).text) - 1\n                # scale height or width\n                cur_pt = cur_pt / width if i % 2 == 0 else cur_pt / height\n                bndbox.append(cur_pt)\n            label_idx = self.class_to_ind[name]\n            bndbox.append(label_idx)\n            res += [bndbox]  # [xmin, ymin, xmax, ymax, label_ind]\n            # img_id = target.find('filename').text[:-4]\n\n        return res  # [[xmin, ymin, xmax, ymax, label_ind], ... ]"}
{"Repository": "breathe", "input": "Base class handle the main work when given the appropriate file and project info to work from. className _BaseFileDirective(BaseDirective) Method handle_contents", "label": "class _BaseFileDirective(BaseDirective):\n    # We use inheritance here rather than a separate object and composition, because so much\n    # information is present in the Directive class from the docutils framework that we'd have to\n    # pass way too much stuff to a helper object to be reasonable.\n\n    def handle_contents(self, file_, project_info):\n        finder = self.finder_factory.create_finder(project_info)\n        finder_filter = self.filter_factory.create_file_finder_filter(file_)\n\n        matches = []\n        finder.filter_(finder_filter, matches)\n\n        if len(matches) > 1:\n            warning = self.create_warning(None, file=file_, directivename=self.directive_name)\n            return warning.warn('{directivename}: Found multiple matches for file \"{file} {tail}')\n        elif not matches:\n            warning = self.create_warning(None, file=file_, directivename=self.directive_name)\n            return warning.warn('{directivename}: Cannot find file \"{file} {tail}')\n\n        target_handler = create_target_handler(self.options, project_info, self.state.document)\n        filter_ = self.filter_factory.create_file_filter(file_, self.options)\n\n        node_list = []\n        for node_stack in matches:\n            object_renderer = SphinxRenderer(\n                self.parser_factory.app,\n                project_info,\n                node_stack,\n                self.state,\n                self.state.document,\n                target_handler,\n                self.parser_factory.create_compound_parser(project_info),\n                filter_,\n            )\n\n            mask_factory = NullMaskFactory()\n            context = RenderContext(node_stack, mask_factory, self.directive_args)\n            node_list.extend(object_renderer.render(node_stack[0], context))\n\n        return node_list"}
{"Repository": "grid-cells", "input": "Calculates the dist over place cells given an absolute position. className PlaceCellEnsemble(CellEnsemble) Method unnor_logpdf", "label": "class PlaceCellEnsemble(CellEnsemble):\n  def __init__(self, n_cells, stdev=0.35, pos_min=-5, pos_max=5, seed=None,\n               soft_targets=None, soft_init=None):\n    super(PlaceCellEnsemble, self).__init__(n_cells, soft_targets, soft_init)\n    # Create a random MoG with fixed cov over the position (Nx2)\n    rs = np.random.RandomState(seed)\n    self.means = rs.uniform(pos_min, pos_max, size=(self.n_cells, 2))\n    self.variances = np.ones_like(self.means) * stdev**2\n\n  def unnor_logpdf(self, trajs):\n    # Output the probability of each component at each point (BxTxN)\n    diff = trajs[:, :, tf.newaxis, :] - self.means[np.newaxis, np.newaxis, ...]\n    unnor_logp = -0.5 * tf.reduce_sum((diff**2)/ self.variances, axis=-1)\n    return unnor_logp"}
{"Repository": "pyobjc", "input": "return NO for both className PyObjC_TestClassAndInstanceInstanceOverride(PyObjC_TestClassAndInstance) Method isInstance", "label": "class PyObjC_TestClassAndInstanceInstanceOverride(PyObjC_TestClassAndInstance):\n    def isInstance(self):\n        return objc.NO"}
{"Repository": "dash-vtk", "input": "An Algorithm component. className Algorithm(Component) Method __init__ Attribute _prop_names Attribute _type Attribute _namespace Attribute _valid_wildcard_attributes Attribute available_properties Attribute available_wildcard_properties", "label": "class Algorithm(Component):\n    @_explicitize_args\n    def __init__(self, children=None, id=Component.UNDEFINED, port=Component.UNDEFINED, vtkClass=Component.UNDEFINED, state=Component.UNDEFINED, **kwargs):\n        self._prop_names = ['children', 'id', 'port', 'state', 'vtkClass']\n        self._type = 'Algorithm'\n        self._namespace = 'dash_vtk'\n        self._valid_wildcard_attributes =            []\n        self.available_properties = ['children', 'id', 'port', 'state', 'vtkClass']\n        self.available_wildcard_properties =            []\n        _explicit_args = kwargs.pop('_explicit_args')\n        _locals = locals()\n        _locals.update(kwargs)  # For wildcard attrs\n        args = {k: _locals[k] for k in _explicit_args if k != 'children'}\n        for k in []:\n            if k not in args:\n                raise TypeError(\n                    'Required argument `' + k + '` was not specified.')\n        super(Algorithm, self).__init__(children=children, **args)"}
{"Repository": "datasketch", "input": "A graph layer in the HNSW index. className _Layer(object) Method __init__ Method __contains__ Method __getitem__ Method __setitem__ Method __delitem__ Method __eq__ Method __len__ Method __iter__ Method copy Method get_reverse_edges", "label": "class _Layer(object):\n    def __init__(self, key: Hashable) -> None:\n        # self._graph[key] contains a {j: dist} dictionary,\n        # where j is a neighbor of key and dist is distance.\n        self._graph: Dict[Hashable, Dict[Hashable, float]] = {key: {}}\n\n    def __contains__(self, key: Hashable) -> bool:\n        return key in self._graph\n\n    def __getitem__(self, key: Hashable) -> Dict[Hashable, float]:\n        return self._graph[key]\n\n    def __setitem__(self, key: Hashable, value: Dict[Hashable, float]) -> None:\n        self._graph[key] = value\n\n    def __delitem__(self, key: Hashable) -> None:\n        del self._graph[key]\n\n    def __eq__(self, __value: object) -> bool:\n        if not isinstance(__value, _Layer):\n            return False\n        return self._graph == __value._graph\n\n    def __len__(self) -> int:\n        return len(self._graph)\n\n    def __iter__(self) -> Iterable[Hashable]:\n        return iter(self._graph)\n\n    def copy(self) -> _Layer:\n        new_layer = _Layer(None)\n        new_layer._graph = {k: dict(v) for k, v in self._graph.items()}\n        return new_layer\n\n    def get_reverse_edges(self, key: Hashable) -> Set[Hashable]:\n        reverse_edges = set()\n        for neighbor, neighbors in self._graph.items():\n            if key in neighbors:\n                reverse_edges.add(neighbor)\n        return reverse_edges"}
{"Repository": "babel", "input": "Exception thrown when a locale is requested for which no locale data is available. className UnknownLocaleError(Exception) Method __init__", "label": "class UnknownLocaleError(Exception):\n    def __init__(self, identifier: str) -> None:\n        Exception.__init__(self, f\"unknown locale {identifier!r}\")\n\n        #: The identifier of the locale that could not be found.\n        self.identifier = identifier"}
{"Repository": "skweak", "input": "Annotator of entities in documents, combining several sub-annotators className CombinedAnnotator(AbstractAnnotator) Method __init__ Method __call__ Method pipe Method add_annotator Method add_annotators Method get_annotator Attribute annotators", "label": "class CombinedAnnotator(AbstractAnnotator):\n    def __init__(self):\n        super(CombinedAnnotator, self).__init__(\"\")\n        self.annotators = []\n\n    def __call__(self, doc: Doc) -> Doc:\n        for annotator in self.annotators:\n            doc = annotator(doc)\n        return doc\n\n    def pipe(self, docs: Iterable[Doc]) -> Iterable[Doc]:\n        # We duplicate the streams of documents\n        streams = itertools.tee(docs, len(self.annotators)+1)\n\n        # We create one pipe per annotator\n        pipes = [annotator.pipe(stream) for annotator, stream in\n                 zip(self.annotators, streams[1:])]\n\n        for doc in streams[0]:\n            for pipe in pipes:\n                try:\n                    next(pipe)\n                except BaseException as e:\n                    print(\"ignoring document:\", doc)\n                    raise e\n\n            yield doc\n\n    def add_annotator(self, annotator: AbstractAnnotator):\n        self.annotators.append(annotator)\n        return self\n\n    def add_annotators(self, *annotators: AbstractAnnotator):\n        for annotator in annotators:\n            self.add_annotator(annotator)\n        return self\n\n    def get_annotator(self, annotator_name: str):\n        for annotator in self.annotators:\n            if annotator.name == annotator_name:\n                return annotator\n\n        raise RuntimeError(\"Could not find annotator %s\" % annotator_name)"}
{"Repository": "RESim", "input": "Simple Form to test multilinetext and combo box controls className okTextForm(Form) Method __init__ Attribute prompt", "label": "class okTextForm(Form):\n    def __init__(self, lines, prompt):\n        self.prompt = prompt        \n        print(\"in okTextForm init\")\n        if prompt:\n            Form.__init__(self, r\"\"\"STARTITEM 0"}
{"Repository": "python-blockchain-tutorial", "input": "*Electronic Code Book (ECB)*. className EcbMode(object) Method __init__ Method encrypt Method decrypt Attribute _state Attribute _state", "label": "class EcbMode(object):\n    def __init__(self, block_cipher):\n        self._state = VoidPointer()\n        result = raw_ecb_lib.ECB_start_operation(block_cipher.get(),\n                                                 self._state.address_of())\n        if result:\n            raise ValueError(\"Error %d while instantiating the ECB mode\"\n                             % result)\n\n        # Ensure that object disposal of this Python object will (eventually)\n        # free the memory allocated by the raw library for the cipher\n        # mode\n        self._state = SmartPointer(self._state.get(),\n                                   raw_ecb_lib.ECB_stop_operation)\n\n        # Memory allocated for the underlying block cipher is now owned\n        # by the cipher mode\n        block_cipher.release()\n\n    def encrypt(self, plaintext, output=None):\n        if output is None:\n            ciphertext = create_string_buffer(len(plaintext))\n        else:\n            ciphertext = output\n            \n            if not is_writeable_buffer(output):\n                raise TypeError(\"output must be a bytearray or a writeable memoryview\")\n        \n            if len(plaintext) != len(output):\n                raise ValueError(\"output must have the same length as the input\"\n                                 \"  (%d bytes)\" % len(plaintext))\n\n        result = raw_ecb_lib.ECB_encrypt(self._state.get(),\n                                         c_uint8_ptr(plaintext),\n                                         c_uint8_ptr(ciphertext),\n                                         c_size_t(len(plaintext)))\n        if result:\n            if result == 3:\n                raise ValueError(\"Data must be aligned to block boundary in ECB mode\")\n            raise ValueError(\"Error %d while encrypting in ECB mode\" % result)\n        \n        if output is None:\n            return get_raw_buffer(ciphertext)\n        else:\n            return None\n\n    def decrypt(self, ciphertext, output=None):\n        if output is None:\n            plaintext = create_string_buffer(len(ciphertext))\n        else:\n            plaintext = output\n\n            if not is_writeable_buffer(output):\n                raise TypeError(\"output must be a bytearray or a writeable memoryview\")\n            \n            if len(ciphertext) != len(output):\n                raise ValueError(\"output must have the same length as the input\"\n                                 \"  (%d bytes)\" % len(plaintext))\n\n        result = raw_ecb_lib.ECB_decrypt(self._state.get(),\n                                         c_uint8_ptr(ciphertext),\n                                         c_uint8_ptr(plaintext),\n                                         c_size_t(len(ciphertext)))\n        if result:\n            if result == 3:\n                raise ValueError(\"Data must be aligned to block boundary in ECB mode\")\n            raise ValueError(\"Error %d while decrypting in ECB mode\" % result)\n\n        if output is None:\n            return get_raw_buffer(plaintext)\n        else:\n            return None"}
{"Repository": "biolink-model", "input": "A collection of exclusion objects className ExcludeListContainer(YAMLRoot) Method __post_init__", "label": "class ExcludeListContainer(YAMLRoot):\n    _inherited_slots: ClassVar[List[str]] = []\n\n    class_class_uri: ClassVar[URIRef] = BIOLINK.ExcludeListContainer\n    class_class_curie: ClassVar[str] = \"biolink:ExcludeListContainer\"\n    class_name: ClassVar[str] = \"ExcludeListContainer\"\n    class_model_uri: ClassVar[URIRef] = SEL.ExcludeListContainer\n\n    excluded_semmedb_records: Optional[Union[Union[dict, \"ExcludedSemmedbRecord\"], List[Union[dict, \"ExcludedSemmedbRecord\"]]]] = empty_list()\n\n    def __post_init__(self, *_: List[str], **kwargs: Dict[str, Any]):\n        if not isinstance(self.excluded_semmedb_records, list):\n            self.excluded_semmedb_records = [self.excluded_semmedb_records] if self.excluded_semmedb_records is not None else []\n        self.excluded_semmedb_records = [v if isinstance(v, ExcludedSemmedbRecord) else ExcludedSemmedbRecord(**as_dict(v)) for v in self.excluded_semmedb_records]\n\n        super().__post_init__(**kwargs)"}
{"Repository": "organize", "input": "Matches files by the time the file was added to a folder. className DateAdded(TimeFilter) Method __post_init__ Method get_datetime", "label": "class DateAdded(TimeFilter):\n    filter_config: ClassVar[FilterConfig] = FilterConfig(\n        name=\"date_added\",\n        files=True,\n        dirs=True,\n    )\n\n    def __post_init__(self):\n        if sys.platform != \"darwin\":\n            raise EnvironmentError(\"date_added is only available on macOS\")\n        return super().__post_init__()\n\n    def get_datetime(self, path: Path) -> datetime:\n        return read_date_added(path)"}
{"Repository": "Machine_Learning_Study_Path", "input": "A mutable set is a finite, iterable container. className MutableSet(Set) Method add Method discard Method remove Method pop Method clear Method __ior__ Method __iand__ Method __ixor__ Method __isub__", "label": "class MutableSet(Set):\n    __slots__ = ()\n\n    @abstractmethod\n    def add(self, value):\n        raise NotImplementedError\n\n    @abstractmethod\n    def discard(self, value):\n        raise NotImplementedError\n\n    def remove(self, value):\n        if value not in self:\n            raise KeyError(value)\n        self.discard(value)\n\n    def pop(self):\n        it = iter(self)\n        try:\n            value = next(it)\n        except StopIteration:\n            raise KeyError\n        self.discard(value)\n        return value\n\n    def clear(self):\n        try:\n            while True:\n                self.pop()\n        except KeyError:\n            pass\n\n    def __ior__(self, it):\n        for value in it:\n            self.add(value)\n        return self\n\n    def __iand__(self, it):\n        for value in (self - it):\n            self.discard(value)\n        return self\n\n    def __ixor__(self, it):\n        if it is self:\n            self.clear()\n        else:\n            if not isinstance(it, Set):\n                it = self._from_iterable(it)\n            for value in it:\n                if value in self:\n                    self.discard(value)\n                else:\n                    self.add(value)\n        return self\n\n    def __isub__(self, it):\n        if it is self:\n            self.clear()\n        else:\n            for value in it:\n                self.discard(value)\n        return self"}
{"Repository": "bert-qa", "input": "A single set of features of data. className InputFeatures(object) Method __init__ Attribute unique_id Attribute tokens Attribute input_ids Attribute input_mask Attribute input_type_ids", "label": "class InputFeatures(object):\n  def __init__(self, unique_id, tokens, input_ids, input_mask, input_type_ids):\n    self.unique_id = unique_id\n    self.tokens = tokens\n    self.input_ids = input_ids\n    self.input_mask = input_mask\n    self.input_type_ids = input_type_ids"}
{"Repository": "cloudpathlib", "input": "Polymorphic virtual superclass for CloudPath and pathlib. className AnyPath(ABC) Method __new__ Method __get_pydantic_core_schema__ Method validate Method __get_validators__ Method _validate", "label": "class AnyPath(ABC):\n    def __new__(cls, *args, **kwargs) -> Union[CloudPath, Path]:  # type: ignore\n        try:\n            return CloudPath(*args, **kwargs)  # type: ignore\n        except InvalidPrefixError as cloudpath_exception:\n            try:\n                if isinstance(args[0], str) and args[0].lower().startswith(\"file:\"):\n                    path = path_from_fileurl(args[0], **kwargs)\n                    for part in args[1:]:\n                        path /= part\n                    return path\n\n                return Path(*args, **kwargs)\n            except TypeError as path_exception:\n                raise AnyPathTypeError(\n                    \"Invalid input for both CloudPath and Path. \"\n                    f\"CloudPath exception: {repr(cloudpath_exception)} \"\n                    f\"Path exception: {repr(path_exception)}\"\n                )\n\n    # ===========  pydantic integration special methods ===============\n    @classmethod\n    def __get_pydantic_core_schema__(cls, _source_type: Any, _handler):\n        try:\n            from pydantic_core import core_schema\n\n            return core_schema.no_info_after_validator_function(\n                cls.validate,\n                core_schema.any_schema(),\n            )\n        except ImportError:\n            return None\n\n    @classmethod\n    def validate(cls, v: str) -> Union[CloudPath, Path]:\n        try:\n            return cls.__new__(cls, v)\n        except AnyPathTypeError as e:\n            # type errors no longer converted to validation errors\n            #  https://docs.pydantic.dev/2.0/migration/#typeerror-is-no-longer-converted-to-validationerror-in-validators\n            raise ValueError(e)\n\n    @classmethod\n    def __get_validators__(cls):\n        yield cls._validate\n\n    @classmethod\n    def _validate(cls, value) -> Union[CloudPath, Path]:\n        # Note __new__ is static method and not a class method\n        return cls.__new__(cls, value)"}
{"Repository": "off-policy", "input": "R_MATD3 Critic class. Identical to R_MADDPG Critic, but with 2 Q outputs. className R_MATD3_Critic(R_MADDPG_Critic) Method __init__", "label": "class R_MATD3_Critic(R_MADDPG_Critic):\n    def __init__(self, args, central_obs_dim, central_act_dim, device):\n        super(R_MATD3_Critic, self).__init__(args, central_obs_dim, central_act_dim, device, num_q_outs=2)"}
{"Repository": "fglib", "input": "Abstract base class for all random variables. className RandomVariable(ABC) Method unity Method dim Method __str__ Method __add__ Method __sub__ Method __mul__ Method __iadd__ Method __isub__ Method __imul__ Method __eq__ Method normalize Method marginalize Method maximize Method argmax Method log", "label": "class RandomVariable(ABC):\n    @abstractclassmethod\n    def unity(cls, *args):\n    @abstractproperty\n    def dim(self):\n    @abstractmethod\n    def __str__(self):\n    @abstractmethod\n    def __add__(self):\n    @abstractmethod\n    def __sub__(self):\n    @abstractmethod\n    def __mul__(self):\n    @abstractmethod\n    def __iadd__(self):\n    @abstractmethod\n    def __isub__(self):\n    @abstractmethod\n    def __imul__(self):\n    @abstractmethod\n    def __eq__(self):\n    @abstractmethod\n    def normalize(self):\n    @abstractmethod\n    def marginalize(self):\n    @abstractmethod\n    def maximize(self):\n    @abstractmethod\n    def argmax(self):\n    @abstractmethod\n    def log(self):"}
{"Repository": "awesome-self-supervised-gnn", "input": "Factory for creating BeautifulSoup instances. className SoupKitchen(object) Method make_soup", "label": "class SoupKitchen(object):\n    @staticmethod\n    def make_soup(markup, parser=None):\n        if 'bs4' in sys.modules:\n            # We support parser specification. If the caller didn't\n            # specify one, leave it to BeautifulSoup to pick the most\n            # suitable one, but suppress the user warning that asks to\n            # select the most suitable parser ... which BS then\n            # selects anyway.\n            if parser is None:\n                warnings.filterwarnings('ignore', 'No parser was explicitly specified')\n            return BeautifulSoup(markup, parser)\n\n        return BeautifulSoup(markup)"}
{"Repository": "disnake", "input": "The base class for application commands. className ApplicationCommand(ABC) Method default_member_permissions Method __repr__ Method __str__ Method __eq__ Method to_dict Method localize", "label": "class ApplicationCommand(ABC):\n    __repr_info__: ClassVar[Tuple[str, ...]] = (\n        \"type\",\n        \"name\",\n        \"dm_permission\",\n        \"default_member_permisions\",\n        \"nsfw\",\n    )\n\n    def __init__(\n        self,\n        type: ApplicationCommandType,\n        name: LocalizedRequired,\n        dm_permission: Optional[bool] = None,\n        default_member_permissions: Optional[Union[Permissions, int]] = None,\n        nsfw: Optional[bool] = None,\n    ) -> None:\n        self.type: ApplicationCommandType = enum_if_int(ApplicationCommandType, type)\n\n        name_loc = Localized._cast(name, True)\n        self.name: str = name_loc.string\n        self.name_localizations: LocalizationValue = name_loc.localizations\n        self.nsfw: bool = False if nsfw is None else nsfw\n\n        self.dm_permission: bool = True if dm_permission is None else dm_permission\n\n        self._default_member_permissions: Optional[int]\n        if default_member_permissions is None:\n            # allow everyone to use the command if its not supplied\n            self._default_member_permissions = None\n        elif isinstance(default_member_permissions, bool):\n            raise TypeError(\"`default_member_permissions` cannot be a bool\")\n        elif isinstance(default_member_permissions, int):\n            self._default_member_permissions = default_member_permissions\n        else:\n            self._default_member_permissions = default_member_permissions.value\n\n        self._always_synced: bool = False\n\n        # reset `default_permission` if set before\n        self._default_permission: bool = True\n\n    @property\n    def default_member_permissions(self) -> Optional[Permissions]:\n        if self._default_member_permissions is None:\n            return None\n        return Permissions(self._default_member_permissions)\n\n    def __repr__(self) -> str:\n        attrs = \" \".join(f\"{key}={getattr(self, key)!r}\" for key in self.__repr_info__)\n        return f\"<{type(self).__name__} {attrs}>\"\n\n    def __str__(self) -> str:\n        return self.name\n\n    def __eq__(self, other) -> bool:\n        return (\n            self.type == other.type\n            and self.name == other.name\n            and self.name_localizations == other.name_localizations\n            and self.nsfw == other.nsfw\n            and self._default_member_permissions == other._default_member_permissions\n            # ignore `dm_permission` if comparing guild commands\n            and (\n                any(\n                    (isinstance(obj, _APIApplicationCommandMixin) and obj.guild_id)\n                    for obj in (self, other)\n                )\n                or self.dm_permission == other.dm_permission\n            )\n            and self._default_permission == other._default_permission\n        )\n\n    def to_dict(self) -> EditApplicationCommandPayload:\n        data: EditApplicationCommandPayload = {\n            \"type\": try_enum_to_int(self.type),\n            \"name\": self.name,\n            \"dm_permission\": self.dm_permission,\n            \"default_permission\": True,\n            \"nsfw\": self.nsfw,\n        }\n\n        if self._default_member_permissions is None:\n            data[\"default_member_permissions\"] = None\n        else:\n            data[\"default_member_permissions\"] = str(self._default_member_permissions)\n        if (loc := self.name_localizations.data) is not None:\n            data[\"name_localizations\"] = loc\n\n        return data\n\n    def localize(self, store: LocalizationProtocol) -> None:\n        self.name_localizations._link(store)"}
{"Repository": "imylu", "input": "Gradient Boosting Classifier className GradientBoostingClassifier(GradientBoostingBase) Method _get_init_val Method _get_score Method predict_one_prob Method predict_prob Method predict", "label": "class GradientBoostingClassifier(GradientBoostingBase):\n    def _get_init_val(self, label: ndarray):\n        n_rows = len(label)\n        tot = label.sum()\n\n        return np.log(tot / (n_rows - tot))\n\n    @staticmethod\n    def _get_score(idxs: List[int], prediction: ndarray, residuals: ndarray) -> float:\n        numerator = residuals[idxs].sum()\n        denominator = (prediction[idxs] * (1 - prediction[idxs])).sum()\n\n        return numerator / denominator\n\n    def _update_score(self, tree: RegressionTree, data: ndarray,\n                      prediction: ndarray, residuals: ndarray):\n        nodes = self._get_leaves(tree)\n\n        regions = self._divide_regions(tree, nodes, data)\n        for node, idxs in regions.items():\n            node.avg = self._get_score(idxs, prediction, residuals)\n        tree.get_rules()\n\n    def predict_one_prob(self, row: ndarray) -> float:\n        return sigmoid(self.predict_one(row))\n\n    def predict_prob(self, data: ndarray) -> ndarray:\n        return np.apply_along_axis(self.predict_one_prob, axis=1, arr=data)\n\n    def predict(self, data: ndarray, threshold=0.5) -> ndarray:\n        prob = self.predict_prob(data)\n        return (prob >= threshold).astype(int)"}
{"Repository": "moonshot", "input": "A basic test strategy that buys below 10 and shorts above 10. className BuyBelow1000ShortAbove1000(Moonshot) Method prices_to_signals Method mock_get_prices Method mock_download_master_file", "label": "class BuyBelow1000ShortAbove1000(Moonshot):\n            COMMISSION_CLASS = TestFuturesCommission\n\n            def prices_to_signals(self, prices):\n                long_signals = prices.loc[\"Close\"] <= 1000\n                short_signals = prices.loc[\"Close\"] > 1000\n                signals = long_signals.astype(int).where(long_signals, -short_signals.astype(int))\n                return signals\n\n        def mock_get_prices(*args, **kwargs):\n\n            dt_idx = pd.DatetimeIndex([\"2018-05-01\",\"2018-05-02\",\"2018-05-03\", \"2018-05-04\"])\n            fields = [\"Close\"]\n            idx = pd.MultiIndex.from_product([fields, dt_idx], names=[\"Field\", \"Date\"])\n\n            prices = pd.DataFrame(\n                {\n                    \"FI12345\": [\n                        # Close\n                        900,\n                        1100,\n                        1050,\n                        999,\n                    ],\n                    \"FI23456\": [\n                        # Close\n                        900,\n                        1100,\n                        1050,\n                        999,\n                    ],\n                 },\n                index=idx\n            )\n\n            return prices\n\n        def mock_download_master_file(f, *args, **kwargs):\n\n            master_fields = [\"Timezone\", \"Symbol\", \"Currency\", \"SecType\", \"PriceMagnifier\", \"Multiplier\"]\n            securities = pd.DataFrame(\n                {\n                    \"FI12345\": [\n                        \"America/Chicago\",\n                        \"HE\",\n                        \"USD\",\n                        \"FUT\",\n                        1,\n                        10\n                    ],\n                    \"FI23456\": [\n                        \"America/Chicago\",\n                        \"HE\",\n                        \"USD\",\n                        \"FUT\",\n                        100,\n                        10\n                    ]\n                },\n                index=master_fields\n            )\n            securities.columns.name = \"Sid\"\n            securities.T.to_csv(f, index=True, header=True)\n            f.seek(0)\n\n        with patch(\"moonshot.strategies.base.get_prices\", new=mock_get_prices):\n            with patch(\"moonshot.strategies.base.download_master_file\", new=mock_download_master_file):\n                results = BuyBelow1000ShortAbove1000().backtest()\n\n        self.assertSetEqual(\n            set(results.index.get_level_values(\"Field\")),\n            {'Commission',\n             'AbsExposure',\n             'Signal',\n             'Return',\n             'Slippage',\n             'NetExposure',\n             'TotalHoldings',\n             'Turnover',\n             'AbsWeight',\n             'Weight'}\n        )\n\n        results = results.round(7)\n        results = results.where(results.notnull(), \"nan\")\n\n        signals = results.loc[\"Signal\"].reset_index()\n        signals[\"Date\"] = signals.Date.dt.strftime(\"%Y-%m-%dT%H:%M:%S%z\")\n        self.assertDictEqual(\n            signals.to_dict(orient=\"list\"),\n            {'Date': [\n                '2018-05-01T00:00:00',\n                '2018-05-02T00:00:00',\n                '2018-05-03T00:00:00',\n                '2018-05-04T00:00:00'],\n             \"FI12345\": [1,\n                     -1,\n                     -1,\n                     1],\n             \"FI23456\": [1,\n                     -1,\n                     -1,\n                     1]}\n        )\n\n        weights = results.loc[\"Weight\"].reset_index()\n        weights[\"Date\"] = weights.Date.dt.strftime(\"%Y-%m-%dT%H:%M:%S%z\")\n        self.assertDictEqual(\n            weights.to_dict(orient=\"list\"),\n            {'Date': [\n                '2018-05-01T00:00:00',\n                '2018-05-02T00:00:00',\n                '2018-05-03T00:00:00',\n                '2018-05-04T00:00:00'],\n             \"FI12345\": [0.5,\n                     -0.5,\n                     -0.5,\n                     0.5],\n             \"FI23456\": [0.5,\n                     -0.5,\n                     -0.5,\n                     0.5]}\n        )\n\n        net_positions = results.loc[\"NetExposure\"].reset_index()\n        net_positions[\"Date\"] = net_positions.Date.dt.strftime(\"%Y-%m-%dT%H:%M:%S%z\")\n        self.assertDictEqual(\n            net_positions.to_dict(orient=\"list\"),\n            {'Date': [\n                '2018-05-01T00:00:00',\n                '2018-05-02T00:00:00',\n                '2018-05-03T00:00:00',\n                '2018-05-04T00:00:00'],\n             \"FI12345\": [\"nan\",\n                     0.5,\n                     -0.5,\n                     -0.5],\n             \"FI23456\": [\"nan\",\n                     0.5,\n                     -0.5,\n                     -0.5]}\n        )\n\n        turnover = results.loc[\"Turnover\"].reset_index()\n        turnover[\"Date\"] = turnover.Date.dt.strftime(\"%Y-%m-%dT%H:%M:%S%z\")\n        self.assertDictEqual(\n            turnover.to_dict(orient=\"list\"),\n            {'Date': [\n                '2018-05-01T00:00:00',\n                '2018-05-02T00:00:00',\n                '2018-05-03T00:00:00',\n                '2018-05-04T00:00:00'],\n             \"FI12345\": [\"nan\",\n                     0.5,\n                     1.0,\n                     0.0],\n             \"FI23456\": [\"nan\",\n                     0.5,\n                     1.0,\n                     0.0]}\n        )\n\n        commissions = results.loc[\"Commission\"].reset_index()\n        commissions[\"Date\"] = commissions.Date.dt.strftime(\"%Y-%m-%dT%H:%M:%S%z\")\n        self.assertDictEqual(\n            commissions.to_dict(orient=\"list\"),\n            {'Date': [\n                '2018-05-01T00:00:00',\n                '2018-05-02T00:00:00',\n                '2018-05-03T00:00:00',\n                '2018-05-04T00:00:00'],\n             \"FI12345\": [\"nan\",\n                     0.0000909,\n                     0.0001905,\n                     0.0],\n             \"FI23456\": [\"nan\",\n                     0.0090909,\n                     0.0190476,\n                     0.0]}\n        )\n\n        # The FUT with 100x the price magnifier (\"FI23456\") requires 100x the\n        # contracts and therefore incurs 100x the commission\n        self.assertAlmostEqual(\n            commissions[\"FI12345\"].iloc[1] * 100, commissions[\"FI23456\"].iloc[1],\n            places=5\n        )"}
{"Repository": "oio-sds", "input": "Http client for a given KMS domain className HttpClient(object) Method request", "label": "class HttpClient(object):\n    def __init__(\n        self,\n        domain,\n        endpoint,\n        key_id,\n        cert_file,\n        key_file,\n        connect_timeout,\n        read_timeout,\n        logger,\n        statsd,\n        kmsapi_mock_server=False,\n    ):\n        self.logger = logger\n        self.domain = domain\n        self.endpoint = endpoint\n        self.key_id = key_id\n        self.statsd = statsd\n        if kmsapi_mock_server:\n            self.http = urllib3.PoolManager()\n        else:\n            self.http = urllib3.PoolManager(\n                cert_reqs=\"CERT_REQUIRED\",\n                cert_file=cert_file,\n                key_file=key_file,\n                timeout=urllib3.Timeout(\n                    connect=connect_timeout,\n                    read=read_timeout,\n                ),\n            )\n\n    def request(self, action, body, key_id=None):\n        if key_id is None:\n            key_id = self.key_id\n        url = f\"{self.endpoint}/v1/servicekey/{key_id}/{action}\"\n\n        start_time = time.monotonic()\n        try:\n            resp = self.http.request(\n                \"POST\",\n                url,\n                body=body,\n                headers={\"Content-Type\": \"application/json\"},\n            )\n            status = resp.status\n        except Exception as exc:\n            self.logger.exception(exc)\n            status = type(exc).__name__\n            raise exc\n        finally:\n            duration = time.monotonic() - start_time\n            self.statsd.timing(\n                f\"openio.account.kmsapi.{self.domain}.{action}.{status}.timing\",\n                duration * 1000,  # in milliseconds\n            )\n        if status != 200:\n            raise from_response(resp, resp.data)\n\n        # Inject the key_id associated to the request into the response\n        json_data = json.loads(resp.data)\n        json_data[\"key_id\"] = key_id\n\n        return json_data"}
{"Repository": "noiseprotocol", "input": "TODO document className Pattern(object) Method __init__ Method has_pre_messages Method get_initiator_pre_messages Method get_responder_pre_messages Method apply_pattern_modifiers Method get_required_keypairs Attribute pre_messages Attribute tokens Attribute name Attribute one_way Attribute psk_count", "label": "class Pattern(object):\n    def __init__(self):\n        # As per specification, if both parties have pre-messages, the initiator is listed first. To reduce complexity,\n        # pre_messages shall be a list of two lists:\n        # the first for the initiator's pre-messages, the second for the responder\n        self.pre_messages = [\n            [],\n            []\n        ]\n\n        # List of lists of valid tokens, alternating between tokens for initiator and responder\n        self.tokens = []\n\n        self.name = ''\n        self.one_way = False\n        self.psk_count = 0\n\n    def has_pre_messages(self):\n        return any(map(lambda x: len(x) > 0, self.pre_messages))\n\n    def get_initiator_pre_messages(self) -> list:\n        return self.pre_messages[0].copy()\n\n    def get_responder_pre_messages(self) -> list:\n        return self.pre_messages[1].copy()\n\n    def apply_pattern_modifiers(self, modifiers: List[str]) -> None:\n        # Applies given pattern modifiers to self.tokens of the Pattern instance.\n        for modifier in modifiers:\n            if modifier.startswith('psk'):\n                try:\n                    index = int(modifier.replace('psk', '', 1))\n                except ValueError:\n                    raise ValueError('Improper psk modifier {}'.format(modifier))\n\n                if index // 2 > len(self.tokens):\n                    raise ValueError('Modifier {} cannot be applied - pattern has not enough messages'.format(modifier))\n\n                # Add TOKEN_PSK in the correct place in the correct message\n                if index == 0:  # if 0, insert at the beginning of first message\n                    self.tokens[0].insert(0, TOKEN_PSK)\n                else:  # if bigger than zero, append at the end of first, second etc.\n                    self.tokens[index - 1].append(TOKEN_PSK)\n                self.psk_count += 1\n\n            elif modifier == 'fallback':\n                raise NotImplementedError  # TODO implement\n\n            else:\n                raise ValueError('Unknown pattern modifier {}'.format(modifier))\n\n    def get_required_keypairs(self, initiator: bool) -> list:\n        required = []\n        if initiator:\n            if self.name[0] in ('K', 'X', 'I'):\n                required.append('s')\n            if self.one_way or self.name[1] == 'K':\n                required.append('rs')\n        else:\n            if self.name[0] == 'K':\n                required.append('rs')\n            if self.one_way or self.name[1] in ['K', 'X']:\n                required.append('s')\n        return required"}
{"Repository": "GeneticAlgorithmPython", "input": "Implementing the average pooling layer. className AveragePooling2D(CustomLogger) Method average_pooling", "label": "class AveragePooling2D(CustomLogger):\n    def __init__(self, \n\n                 pool_size, \n\n                 previous_layer, \n\n                 stride=2,\n\n                 logger=None):\n        super().__init__()\n\n\n\n        # If logger is None, then the CustomLogger.logger is created.\n\n        if logger is None:\n\n            pass\n\n        else:\n\n            self.logger = logger\n\n\n\n        if not (type(pool_size) is int):\n\n            msg = \"The expected type of the pool_size is int but {pool_size_type} found.\".format(pool_size_type=type(pool_size))\n\n            self.logger.error(msg)\n\n            raise ValueError(msg)\n\n\n\n        if pool_size <= 0:\n\n            msg = \"The passed value to the pool_size parameter cannot be <= 0.\"\n\n            self.logger.error(msg)\n\n            raise ValueError(msg)\n\n        self.pool_size = pool_size\n\n\n\n        if stride <= 0:\n\n            msg = \"The passed value to the stride parameter cannot be <= 0.\"\n\n            self.logger.error(msg)\n\n            raise ValueError(msg)\n\n        self.stride = stride\n\n\n\n        if previous_layer is None:\n\n            msg = \"The previous layer cannot be of Type 'None'. Please pass a valid layer to the 'previous_layer' parameter.\"\n\n            self.logger.error(msg)\n\n            raise TypeError(msg)\n\n        # A reference to the layer that preceeds the current layer in the network architecture.\n\n        self.previous_layer = previous_layer\n\n\n\n        # Size of the input to the layer.\n\n        self.layer_input_size = self.previous_layer.layer_output_size\n\n\n\n        # Size of the output from the layer.\n\n        self.layer_output_size = (numpy.uint16((self.previous_layer.layer_output_size[0] - self.pool_size + 1)/stride + 1), \n\n                                  numpy.uint16((self.previous_layer.layer_output_size[1] - self.pool_size + 1)/stride + 1), \n\n                                  self.previous_layer.layer_output_size[-1])\n\n\n\n        # The layer_output attribute holds the latest output from the layer.\n\n        self.layer_output = None\n\n\n\n    def average_pooling(self, input2D):\n        # Preparing the output of the pooling operation.\n\n        pool_out = numpy.zeros((numpy.uint16((input2D.shape[0]-self.pool_size+1)/self.stride+1),\n\n                                numpy.uint16((input2D.shape[1]-self.pool_size+1)/self.stride+1),\n\n                                input2D.shape[-1]))\n\n        for map_num in range(input2D.shape[-1]):\n\n            r2 = 0\n\n            for r in numpy.arange(0,input2D.shape[0]-self.pool_size+1, self.stride):\n\n                c2 = 0\n\n                for c in numpy.arange(0, input2D.shape[1]-self.pool_size+1, self.stride):\n\n                    pool_out[r2, c2, map_num] = numpy.mean([input2D[r:r+self.pool_size,  c:c+self.pool_size, map_num]])\n\n                    c2 = c2 + 1\n\n                r2 = r2 +1\n\n\n\n        self.layer_output = pool_out"}
{"Repository": "pystache", "input": "An exception raised when a key is not found in a context stack. className KeyNotFoundError(PystacheError) Method __init__ Method __str__ Attribute key Attribute details", "label": "class KeyNotFoundError(PystacheError):\n    def __init__(self, key, details):\n        self.key = key\n        self.details = details\n\n    def __str__(self):\n        return \"Key %s not found: %s\" % (repr(self.key), self.details)"}
{"Repository": "AI-Challenger-Retinal-Edema-Segmentation", "input": "Given mean: (R, G, B) and std: (R, G, B), will normalize each channel of the torch. className Normalize(object) Method __init__ Method __call__ Attribute mean Attribute std", "label": "class Normalize(object):\n    def __init__(self, mean, std):\n        self.mean = torch.FloatTensor(mean)\n        self.std = torch.FloatTensor(std)\n\n    def __call__(self, image, label=None):\n        for t, m, s in zip(image, self.mean, self.std):\n            t.sub_(m).div_(s)\n        if label is None:\n            return image,\n        else:\n            return image, label"}
{"Repository": "chameleon", "input": "Expression object value. className Value(Node) Method __repr__", "label": "class Value(Node):\n    _fields: ClassVar[tuple[str, ...]]\n    _fields = \"value\", \"default\", \"default_marker\"\n\n    default = None\n\n    default_marker = None\n\n    def __repr__(self):\n        try:\n            line, column = self.value.location\n        except AttributeError:\n            line, column = 0, 0\n\n        return \"<%s %r (%d:%d)>\" % (\n            type(self).__name__,\n            self.value,\n            line,\n            column,\n        )"}
{"Repository": "optscale", "input": "Used to wrap sync controller methods to return futures className BaseAsyncControllerWrapper(object) Method __init__ Method controller Method model_type Method _get_controller_class Method check_token_valid Method get_awaitable Method __getattr__ Method _missing Attribute session Attribute _config Attribute _db Attribute _controller Attribute executor Attribute io_loop", "label": "class BaseAsyncControllerWrapper(object):\n    def __init__(self, db_session, config=None):\n        self.session = db_session\n        self._config = config\n        self._db = None\n        self._controller = None\n        self.executor = tp_executor\n        self.io_loop = IOLoop.current()\n\n    @property\n    def controller(self):\n        if not self._controller:\n            self._controller = self._get_controller_class()(\n                self.session, self._config)\n        return self._controller\n\n    @property\n    def model_type(self):\n        return self.controller._get_model_type()\n\n    def _get_controller_class(self):\n        raise NotImplementedError\n\n    def check_token_valid(self, key):\n        return self.io_loop.run_in_executor(\n            None, self.controller.access_token_store.check_token_valid, key)\n\n    def get_awaitable(self, meth_name, *args, **kwargs):\n        method = getattr(self.controller, meth_name)\n        return self.io_loop.run_in_executor(\n            self.executor, functools.partial(method, *args, **kwargs))\n\n    def __getattr__(self, name):\n\n        def _missing(*args, **kwargs):\n            return self.get_awaitable(name, *args, **kwargs)\n        return _missing"}
{"Repository": "Tianchi-2019-Guangdong-Intelligent-identification-of-cloth-defects-rank5", "input": "Sampler that restricts data loading to a subset of the dataset. className DistributedGroupSampler(Sampler) Method __iter__ Method __len__ Method set_epoch", "label": "class DistributedGroupSampler(Sampler):\n    def __init__(self,\n                 dataset,\n                 samples_per_gpu=1,\n                 num_replicas=None,\n                 rank=None):\n        _rank, _num_replicas = get_dist_info()\n        if num_replicas is None:\n            num_replicas = _num_replicas\n        if rank is None:\n            rank = _rank\n        self.dataset = dataset\n        self.samples_per_gpu = samples_per_gpu\n        self.num_replicas = num_replicas\n        self.rank = rank\n        self.epoch = 0\n\n        assert hasattr(self.dataset, 'flag')\n        self.flag = self.dataset.flag\n        self.group_sizes = np.bincount(self.flag)\n\n        self.num_samples = 0\n        for i, j in enumerate(self.group_sizes):\n            self.num_samples += int(\n                math.ceil(self.group_sizes[i] * 1.0 / self.samples_per_gpu /\n                          self.num_replicas)) * self.samples_per_gpu\n        self.total_size = self.num_samples * self.num_replicas\n\n    def __iter__(self):\n        # deterministically shuffle based on epoch\n        g = torch.Generator()\n        g.manual_seed(self.epoch)\n\n        indices = []\n        for i, size in enumerate(self.group_sizes):\n            if size > 0:\n                indice = np.where(self.flag == i)[0]\n                assert len(indice) == size\n                indice = indice[list(torch.randperm(int(size),\n                                                    generator=g))].tolist()\n                extra = int(\n                    math.ceil(\n                        size * 1.0 / self.samples_per_gpu / self.num_replicas)\n                ) * self.samples_per_gpu * self.num_replicas - len(indice)\n                # pad indice\n                tmp = indice.copy()\n                for _ in range(extra // size):\n                    indice.extend(tmp)\n                indice.extend(tmp[:extra % size])\n                indices.extend(indice)\n\n        assert len(indices) == self.total_size\n\n        indices = [\n            indices[j] for i in list(\n                torch.randperm(\n                    len(indices) // self.samples_per_gpu, generator=g))\n            for j in range(i * self.samples_per_gpu, (i + 1) *\n                           self.samples_per_gpu)\n        ]\n\n        # subsample\n        offset = self.num_samples * self.rank\n        indices = indices[offset:offset + self.num_samples]\n        assert len(indices) == self.num_samples\n\n        return iter(indices)\n\n    def __len__(self):\n        return self.num_samples\n\n    def set_epoch(self, epoch):\n        self.epoch = epoch"}
{"Repository": "robomasterpy", "input": "Support setup.py upload. className UploadCommand(Command) Method status Method initialize_options Method finalize_options Method run", "label": "class UploadCommand(Command):\n    description = 'Build and publish the package.'\n    user_options = []\n\n    @staticmethod\n    def status(s):\n        print('\\033[1m{0}\\033[0m'.format(s))\n\n    def initialize_options(self):\n        pass\n\n    def finalize_options(self):\n        pass\n\n    def run(self):\n        try:\n            self.status('Removing previous builds')\n            rmtree(os.path.join(here, 'dist'))\n        except OSError:\n            pass\n\n        self.status('Preparing {0} v{1}...'.format(about['__title__'], about['__version__']))\n\n        self.status('Building Source and Wheel (universal) distribution')\n        os.system('{0} setup.py sdist bdist_wheel --universal'.format(sys.executable))\n\n        self.status('Uploading the package to PyPI via Twine')\n        os.system('twine upload dist/*')\n\n        sys.exit()"}
{"Repository": "construct", "input": "Adapter for tunneling (as in protocol tunneling). className TunnelAdapter(Adapter) Method __init__ Method _decode Method _encode Attribute inner_subcon", "label": "class TunnelAdapter(Adapter):\n    __slots__ = [\"inner_subcon\"]\n    def __init__(self, subcon, inner_subcon):\n        Adapter.__init__(self, subcon)\n        self.inner_subcon = inner_subcon\n    def _decode(self, obj, context):\n        return self.inner_subcon._parse(StringIO(obj), context)\n    def _encode(self, obj, context):\n        stream = StringIO()\n        self.inner_subcon._build(obj, stream, context)\n        return stream.getvalue()"}
{"Repository": "SparK", "input": "We use the \"DefaultTrainer\" which contains pre-defined default logic for standard training workflow. className Trainer(DefaultTrainer) Method build_optimizer Method build_evaluator Method test_with_TTA", "label": "class Trainer(DefaultTrainer):\n    # [modification] override the `build_optimizer` for using Adam and layer-wise lr decay\n    lr_decay_ratio: float = 1.0\n    @classmethod\n    def build_optimizer(cls, cfg, model):\n        is_resnet50 = int(cfg.MODEL.RESNETS.DEPTH) == 50\n        if comm.is_main_process():\n            dbg = defaultdict(list)\n            for module_name, module in model.named_modules():\n                for module_param_name, value in module.named_parameters(recurse=False):\n                    if not value.requires_grad:\n                        continue\n                    lrf = lr_factor_func(f\"{module_name}.{module_param_name}\", is_resnet50=is_resnet50, dec=cls.lr_decay_ratio, debug=True)\n                    dbg[lrf].append(f\"{module_name}.{module_param_name}\")\n            for k in sorted(dbg.keys()):\n                print(f'[{k}] {sorted(dbg[k])}')\n            print()\n        \n        params = get_default_optimizer_params(\n            model,\n            base_lr=cfg.SOLVER.BASE_LR,\n            weight_decay_norm=cfg.SOLVER.WEIGHT_DECAY_NORM,\n            bias_lr_factor=cfg.SOLVER.BIAS_LR_FACTOR,\n            weight_decay_bias=cfg.SOLVER.WEIGHT_DECAY_BIAS,\n            lr_factor_func=partial(lr_factor_func, is_resnet50=is_resnet50, dec=cls.lr_decay_ratio, debug=False)\n        )\n        \n        opt_clz = {\n            'sgd': partial(torch.optim.SGD, momentum=cfg.SOLVER.MOMENTUM, nesterov=cfg.SOLVER.NESTEROV),\n            'adamw': torch.optim.AdamW,\n            'adam': torch.optim.AdamW,\n        }[cfg.SOLVER.OPTIMIZER.lower()]\n        return maybe_add_gradient_clipping(cfg, opt_clz)(params, lr=cfg.SOLVER.BASE_LR, weight_decay=cfg.SOLVER.WEIGHT_DECAY)\n    \n    @classmethod\n    def build_evaluator(cls, cfg, dataset_name, output_folder=None):\n        return build_evaluator(cfg, dataset_name, output_folder)\n    \n    @classmethod\n    def test_with_TTA(cls, cfg, model):\n        logger = logging.getLogger(\"detectron2.trainer\")\n        # In the end of training, run an evaluation with TTA\n        # Only support some R-CNN models.\n        logger.info(\"Running inference with test-time augmentation ...\")\n        model = GeneralizedRCNNWithTTA(cfg, model)\n        evaluators = [\n            cls.build_evaluator(\n                cfg, name, output_folder=os.path.join(cfg.OUTPUT_DIR, \"inference_TTA\")\n            )\n            for name in cfg.DATASETS.TEST\n        ]\n        res = cls.test(cfg, model, evaluators)\n        res = OrderedDict({k + \"_TTA\": v for k, v in res.items()})\n        return res"}
{"Repository": "GlobalTrack", "input": "ResNeXt backbone. className ResNeXt(ResNet) Method __init__ Attribute groups Attribute base_width Attribute inplanes Attribute res_layers Attribute inplanes", "label": "class ResNeXt(ResNet):\n    arch_settings = {\n        50: (Bottleneck, (3, 4, 6, 3)),\n        101: (Bottleneck, (3, 4, 23, 3)),\n        152: (Bottleneck, (3, 8, 36, 3))\n    }\n\n    def __init__(self, groups=1, base_width=4, **kwargs):\n        super(ResNeXt, self).__init__(**kwargs)\n        self.groups = groups\n        self.base_width = base_width\n\n        self.inplanes = 64\n        self.res_layers = []\n        for i, num_blocks in enumerate(self.stage_blocks):\n            stride = self.strides[i]\n            dilation = self.dilations[i]\n            dcn = self.dcn if self.stage_with_dcn[i] else None\n            gcb = self.gcb if self.stage_with_gcb[i] else None\n            planes = 64 * 2**i\n            res_layer = make_res_layer(\n                self.block,\n                self.inplanes,\n                planes,\n                num_blocks,\n                stride=stride,\n                dilation=dilation,\n                groups=self.groups,\n                base_width=self.base_width,\n                style=self.style,\n                with_cp=self.with_cp,\n                conv_cfg=self.conv_cfg,\n                norm_cfg=self.norm_cfg,\n                dcn=dcn,\n                gcb=gcb)\n            self.inplanes = planes * self.block.expansion\n            layer_name = 'layer{}'.format(i + 1)\n            self.add_module(layer_name, res_layer)\n            self.res_layers.append(layer_name)\n\n        self._freeze_stages()"}
{"Repository": "pygame-samples", "input": "This class will represent our user controlled character. className Player(object) Method __init__ Method make_image Method update Method draw Attribute rect Attribute speed Attribute image", "label": "class Player(object):\n    SIZE = (100, 100)\n    \n    def __init__(self, pos, speed):\n        self.rect = pg.Rect((0,0), Player.SIZE)\n        self.rect.center = pos\n        self.speed = speed\n        self.image = self.make_image()\n\n    def make_image(self):\n        image = pg.Surface(self.rect.size).convert_alpha()\n        image.fill(TRANSPARENT)\n        image_rect = image.get_rect()\n        pg.draw.ellipse(image, pg.Color(\"black\"), image_rect)\n        pg.draw.ellipse(image, pg.Color(\"red\"), image_rect.inflate(-12, -12))\n        return image\n\n    def update(self, keys, screen_rect):\n        for key in DIRECT_DICT:\n            if keys[key]:\n                self.rect.x += DIRECT_DICT[key][0]*self.speed\n                self.rect.y += DIRECT_DICT[key][1]*self.speed\n        self.rect.clamp_ip(screen_rect) # Keep player on screen.\n\n    def draw(self, surface):\n        surface.blit(self.image, self.rect)"}
{"Repository": "backintime", "input": "encode path with encfsctl. className Encode(object) Method __init__ Method __del__ Method startProcess Method path Method exclude Method include Method remote Method close Attribute encfs Attribute password Attribute chroot Attribute remote_path Attribute re_asterisk Attribute re_separate_asterisk", "label": "class Encode(object):\n    def __init__(self, encfs):\n        self.encfs = encfs\n        self.password = self.encfs.password\n        self.chroot = self.encfs.rev_root.currentMountpoint\n        if not self.chroot[-1] == os.sep:\n            self.chroot += os.sep\n        self.remote_path = self.encfs.ssh.path\n        if not self.remote_path[-1] == os.sep:\n            self.remote_path += os.sep\n\n        #precompile some regular expressions\n        self.re_asterisk = re.compile(r'\\*')\n        self.re_separate_asterisk = re.compile(r'(.*?)(\\*+)(.*)')\n\n    def __del__(self):\n        self.close()\n\n    def startProcess(self):\n        thread = password_ipc.TempPasswordThread(self.password)\n        env = self.encfs.env()\n        env['ASKPASS_TEMP'] = thread.temp_file\n        with thread.starter():\n            logger.debug('start \\'encfsctl encode\\' process', self)\n            encfsctl = ['encfsctl', 'encode', '--extpass=backintime-askpass', '/']\n            logger.debug('Call command: %s'\n                         %' '.join(encfsctl),\n                         self)\n            self.p = subprocess.Popen(encfsctl, env = env, bufsize = 0,\n                                    stdin=subprocess.PIPE,\n                                    stdout=subprocess.PIPE,\n                                    universal_newlines = True)\n\n    def path(self, path):\n        if not 'p' in vars(self):\n            self.startProcess()\n        if not self.p.returncode is None:\n            logger.warning('\\'encfsctl encode\\' process terminated. Restarting.', self)\n            del self.p\n            self.startProcess()\n        self.p.stdin.write(path + '\\n')\n        ret = self.p.stdout.readline().strip('\\n')\n        if not len(ret) and len(path):\n            logger.debug('Failed to encode %s. Got empty string'\n                         %path, self)\n            raise EncodeValueError()\n        return ret\n\n    def exclude(self, path):\n        if tools.patternHasNotEncryptableWildcard(path):\n            return None\n\n        enc = ''\n        m = self.re_asterisk.search(path)\n        if not m is None:\n            path_ = path[:]\n            while True:\n                #search for foo/*, foo/*/bar, */bar or **/bar\n                #but not foo* or foo/*bar\n                m = self.re_separate_asterisk.search(path_)\n                if m is None:\n                    return None\n                if m.group(1):\n                    if not m.group(1).endswith(os.sep):\n                        return None\n                    enc = os.path.join(enc, self.path(m.group(1)))\n                enc = os.path.join(enc, m.group(2))\n                if m.group(3):\n                    if not m.group(3).startswith(os.sep):\n                        return None\n                    m1 = self.re_asterisk.search(m.group(3))\n                    if m1 is None:\n                        enc = os.path.join(enc, self.path(m.group(3)))\n                        break\n                    else:\n                        path_ = m.group(3)\n                        continue\n                else:\n                    break\n        else:\n            enc = self.path(path)\n        if os.path.isabs(path):\n            return os.path.join(os.sep, enc)\n        return enc\n\n    def include(self, path):\n        return os.path.join(os.sep, self.path(path))\n\n    def remote(self, path):\n        enc_path = self.path(path[len(self.remote_path):])\n        return os.path.join(self.remote_path, enc_path)\n\n    def close(self):\n        if 'p' in vars(self) and self.p.returncode is None:\n            logger.debug('stop \\'encfsctl encode\\' process', self)\n            self.p.communicate()"}
{"Repository": "auto-pts", "input": "A Bluez test case class className BTestCase(TestCaseLT1) Method __init__ Attribute stack Attribute bluezctrl", "label": "class BTestCase(TestCaseLT1):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, ptsproject_name=\"bluez\", **kwargs)\n\n        self.stack = get_stack()\n        self.bluezctrl = get_iut()\n\n        # first command is to start bluez btpclient\n        self.cmds.insert(0, TestFunc(self.bluezctrl.start))\n        self.cmds.insert(1, TestFunc(self.bluezctrl.wait_iut_ready_event))\n\n        self.cmds.append(TestFuncCleanUp(self.stack.cleanup))\n        # last command is to stop bluez btpclient\n        self.cmds.append(TestFuncCleanUp(self.bluezctrl.stop))"}
{"Repository": "Funnel-Transformer", "input": "Yelp-5. className Yelp5Processor(DataProcessor) Method get_train_examples Method get_dev_examples Method _create_examples Method get_labels", "label": "class Yelp5Processor(DataProcessor):\n  def get_train_examples(self, data_dir):\n    examples = self._create_examples(\n        self._read_tsv(os.path.join(data_dir, \"train.csv\"),\n                       quotechar=\"\\\"\",\n                       delimiter=\",\"), \"train\")\n    return examples\n\n  def get_dev_examples(self, data_dir):\n    examples = self._create_examples(\n        self._read_tsv(os.path.join(data_dir, \"test.csv\"),\n                       quotechar=\"\\\"\",\n                       delimiter=\",\"), \"test\")\n    return examples\n\n  def _create_examples(self, lines, set_type):\n    examples = []\n    for (i, line) in enumerate(lines):\n      guid = \"%s-%d\" % (set_type, i)\n      label = line[0]\n      text_a = classifier_utils.clean_web_text(line[1])\n      examples.append(\n          InputExample(guid=guid, text_a=text_a, text_b=None, label=label))\n    return examples\n\n  def get_labels(self):\n    return [\"1\", \"2\", \"3\", \"4\", \"5\"]"}
{"Repository": "Context-Transformer", "input": "In training, we only care about the \"infinite stream\" of training data. className TrainingSampler(Sampler) Method __init__ Method __iter__ Method _infinite_indices Attribute _size Attribute _shuffle Attribute _seed", "label": "class TrainingSampler(Sampler):\n    def __init__(self, size: int, shuffle: bool = True, seed: Optional[int] = None):\n        self._size = size\n        assert size > 0\n        self._shuffle = shuffle\n        if seed is None:\n            seed = np.random.randint(2 ** 31)\n        self._seed = int(seed)\n\n    def __iter__(self):\n        yield from self._infinite_indices()\n\n    def _infinite_indices(self):\n        g = torch.Generator()\n        g.manual_seed(self._seed)\n        while True:\n            if self._shuffle:\n                yield from torch.randperm(self._size)\n            else:\n                yield from torch.arange(self._size)"}
{"Repository": "webapp-improved", "input": "Base HTTP request handler. className RequestHandler(object) Method __init__ Method initialize Method dispatch Method error Method abort Method uri_for Method handle_exception", "label": "class RequestHandler(object):\n    #: A :class:`Request` instance.\n    request = None\n    #: A :class:`Response` instance.\n    response = None\n    #: A :class:`WSGIApplication` instance.\n    app = None\n\n    def __init__(self, request=None, response=None):\n        self.initialize(request, response)\n\n    def initialize(self, request, response):\n        self.request = request\n        self.response = response\n        self.app = WSGIApplication.active_instance\n\n    def dispatch(self):\n        request = self.request\n        method_name = request.route.handler_method\n        if not method_name:\n            method_name = _normalize_handler_method(request.method)\n\n        method = getattr(self, method_name, None)\n        if method is None:\n            # 405 Method Not Allowed.\n            # The response MUST include an Allow header containing a\n            # list of valid methods for the requested resource.\n            # http://www.w3.org/Protocols/rfc2616/rfc2616-sec10.html#sec10.4.6\n            valid = ', '.join(_get_handler_methods(self))\n            self.abort(405, headers=[('Allow', valid)])\n\n        # The handler only receives *args if no named variables are set.\n        args, kwargs = request.route_args, request.route_kwargs\n        if kwargs:\n            args = ()\n\n        try:\n            return method(*args, **kwargs)\n        except Exception, e:\n            return self.handle_exception(e, self.app.debug)\n\n    def error(self, code):\n        self.response.status = code\n        self.response.clear()\n\n    def abort(self, code, *args, **kwargs):\n        abort(code, *args, **kwargs)\n\n    def redirect(self, uri, permanent=False, abort=False, code=None,\n                 body=None):\n        return redirect(uri, permanent=permanent, abort=abort, code=code,\n                        body=body, request=self.request,\n                        response=self.response)\n\n    def redirect_to(self, _name, _permanent=False, _abort=False, _code=None,\n                    _body=None, *args, **kwargs):\n        uri = self.uri_for(_name, *args, **kwargs)\n        return self.redirect(uri, permanent=_permanent, abort=_abort,\n                             code=_code, body=_body)\n\n    def uri_for(self, _name, *args, **kwargs):\n        return self.app.router.build(self.request, _name, args, kwargs)\n    # Alias.\n    url_for = uri_for\n\n    def handle_exception(self, exception, debug):\n        raise"}
{"Repository": "SSD-keras", "input": "Composes several augmentations together. className Compose(object) Method __init__ Method __call__ Attribute transforms", "label": "class Compose(object):\n    def __init__(self, transforms):\n        self.transforms = transforms\n\n    def __call__(self, img, boxes=None, labels=None):\n        for t in self.transforms:\n            img, boxes, labels = t(img, boxes, labels)\n        return img, boxes, labels"}
{"Repository": "aiodnsbrute", "input": "A quick and dirty metasploit style console output logger that doesn't mess up tqdm output. className ConsoleLogger(object) Method __init__ Method __getattr__ Attribute verbosity Attribute msg_type", "label": "class ConsoleLogger(object):\n    def __init__(self, verbosity):\n        self.verbosity = verbosity\n        self.msg_type = {\n            \"info\": (\"[*]\", \"blue\", 1),\n            \"success\": (\"[+]\", \"green\", 1),\n            \"error\": (\"[-]\", \"red\", 1),\n            \"warn\": (\"[!]\", \"yellow\", 1),\n            \"debug\": (\"[D]\", \"cyan\", 3),\n        }\n\n    def __getattr__(self, attr):\n        try:\n            decorator = style(\n                f\"{self.msg_type[attr][0]} \", fg=self.msg_type[attr][1], bold=True\n            )\n            msg_verbosity = self.msg_type[attr][2]\n        except KeyError:\n            decorator = \"\"\n            msg_verbosity = 1\n        finally:\n            if self.verbosity >= msg_verbosity:\n                return lambda msg: tqdm.write(f\"{decorator}{msg}\")\n            else:\n                return lambda msg: None"}
{"Repository": "GraphMVP", "input": "Data loader which merges data objects from a :class:`torch_geometric. className DataLoaderSubstructContext3D(DataLoader) Method __init__", "label": "class DataLoaderSubstructContext3D(DataLoader):\n    def __init__(self, dataset, batch_size=1, shuffle=True, **kwargs):\n        super(DataLoaderSubstructContext3D, self).__init__(\n            dataset,\n            batch_size,\n            shuffle,\n            collate_fn=lambda data_list: BatchSubstructContext3D.from_data_list(data_list),\n            **kwargs)"}
{"Repository": "diffstack", "input": "python's Unpickler extended to interpreter sessions and more types className CustomUnpickler(StockUnpickler) Method find_class Method __init__ Method load Attribute _main Attribute _ignore Attribute _replace_dict", "label": "class CustomUnpickler(StockUnpickler):\n    from dill.settings import settings\n    _session = False\n\n    def find_class(self, module, name):\n        if (module, name) == ('__builtin__', '__main__'):\n            return self._main.__dict__ #XXX: above set w/save_module_dict\n        elif (module, name) == ('__builtin__', 'NoneType'):\n            return type(None) #XXX: special case: NoneType missing\n        if module == 'dill.dill': module = 'dill._dill'\n        old_module = module        \n        for name1, name2 in self._replace_dict.items():\n            if module.startswith(name1):\n                module = name2 + module[len(name1):]\n        print (f\"{old_module} --> {module} ({name})\")\n        return StockUnpickler.find_class(self, module, name)\n\n    def __init__(self, *args, replace_dict={}, **kwds):\n        settings = Pickler.settings\n        _ignore = kwds.pop('ignore', None)\n        StockUnpickler.__init__(self, *args, **kwds)\n        self._main = _main_module\n        self._ignore = settings['ignore'] if _ignore is None else _ignore\n        self._replace_dict = replace_dict\n\n    def load(self): #NOTE: if settings change, need to update attributes\n        obj = StockUnpickler.load(self)\n        if type(obj).__module__ == getattr(_main_module, '__name__', '__main__'):\n            if not self._ignore:\n                # point obj class to main\n                try: obj.__class__ = getattr(self._main, type(obj).__name__)\n                except (AttributeError,TypeError): pass # defined in a file\n       #_main_module.__dict__.update(obj.__dict__) #XXX: should update globals ?\n        return obj\n    load.__doc__ = StockUnpickler.load.__doc__\n    pass"}
{"Repository": "K-wav2vec", "input": "Wrapper around an iterable that maintains the iteration count. className CountingIterator(object) Method __init__ Method __len__ Method __iter__ Method __next__ Method has_next Method skip Method take Attribute iterable Attribute itr Attribute n Attribute n Attribute total Attribute total", "label": "class CountingIterator(object):\n    def __init__(self, iterable, start=None, total=None):\n        self.iterable = iterable\n        self.itr = iter(self)\n\n        if start is None:\n            self.n = getattr(iterable, \"n\", 0)\n        else:\n            self.n = start\n\n        if total is None:\n            self.total = self.n + len(iterable)\n        else:\n            self.total = total\n\n    def __len__(self):\n        return self.total\n\n    def __iter__(self):\n        for x in self.iterable:\n            if self.n >= self.total:\n                raise RuntimeError(\n                    \"Mismatch between actual and expected iterable length. \"\n                    \"This may be caused by resuming training from a checkpoint using \"\n                    \"a different number of GPUs, in which case you can try the \"\n                    \"--reset-dataloader option. Alternatively you may have a train or \"\n                    \"validation set that is smaller than the number of GPUs. If none \"\n                    \"of these apply, please report this to the fairseq developers.\"\n                )\n            self.n += 1\n            yield x\n\n    def __next__(self):\n        return next(self.itr)\n\n    def has_next(self):\n        return self.n < len(self)\n\n    def skip(self, num_to_skip):\n        next(itertools.islice(self.itr, num_to_skip, num_to_skip), None)\n        return self\n\n    def take(self, n):\n        self.total = min(self.total, n)\n\n        # Propagate this change to the underlying iterator\n        # Only take after what we have already consumed (i.e. after restarting\n        # from checkpoint mid epoch, we have to subtract self.n which is the\n        # starting point)\n        #\n        # This to maintain the invariant self.total = self.n + len(iterable),\n        # before calling __next__ or __iter__\n        propagated_take = max(n - self.n, 0)\n        if hasattr(self.iterable, \"take\"):\n            self.iterable.take(propagated_take)\n        else:\n            self.iterable = itertools.islice(self.iterable, propagated_take)"}
{"Repository": "elasticintel", "input": "Attaches HTTP Basic Authentication to the given Request object. className HTTPBasicAuth(AuthBase) Method __init__ Method __call__ Attribute username Attribute password", "label": "class HTTPBasicAuth(AuthBase):\n    def __init__(self, username, password):\n        self.username = username\n        self.password = password\n\n    def __call__(self, r):\n        r.headers['Authorization'] = _basic_auth_str(self.username, self.password)\n        return r"}
{"Repository": "gomill", "input": "Schedule a single sequence of games. className Simple_scheduler(object) Method __init__ Method _check_consistent Method __getstate__ Method __setstate__ Method issue Method fix Method rollback Attribute next_new Attribute outstanding Attribute to_reissue Attribute issued Attribute fixed", "label": "class Simple_scheduler(object):\n    def __init__(self):\n        self.next_new = 0\n        self.outstanding = set()\n        self.to_reissue = set()\n        self.issued = 0\n        self.fixed = 0\n        #self._check_consistent()\n\n    def _check_consistent(self):\n        assert self.issued == \\\n            self.next_new - len(self.to_reissue)\n        assert self.fixed == \\\n            self.next_new - len(self.outstanding) - len(self.to_reissue)\n\n    def __getstate__(self):\n        return (self.next_new, self.outstanding, self.to_reissue)\n\n    def __setstate__(self, state):\n        (self.next_new, self.outstanding, self.to_reissue) = state\n        self.issued = self.next_new - len(self.to_reissue)\n        self.fixed = self.issued - len(self.outstanding)\n        #self._check_consistent()\n\n    def issue(self):\n        if self.to_reissue:\n            result = min(self.to_reissue)\n            self.to_reissue.discard(result)\n        else:\n            result = self.next_new\n            self.next_new += 1\n        self.outstanding.add(result)\n        self.issued += 1\n        #self._check_consistent()\n        return result\n\n    def fix(self, token):\n        self.outstanding.remove(token)\n        self.fixed += 1\n        #self._check_consistent()\n\n    def rollback(self):\n        self.issued -= len(self.outstanding)\n        self.to_reissue.update(self.outstanding)\n        self.outstanding = set()\n        #self._check_consistent()"}
{"Repository": "django-ledger", "input": "A custom defined QuerySet for the BankAccountModel. className BankAccountModelQuerySet(QuerySet) Method active Method hidden", "label": "class BankAccountModelQuerySet(QuerySet):\n    def active(self) -> QuerySet:\n        return self.filter(active=True)\n\n    def hidden(self) -> QuerySet:\n        return self.filter(hidden=True)"}
{"Repository": "certbot-debian", "input": "Test cli.read_file className TestReadFile(TempDirTestCase) Method test_read_file", "label": "class TestReadFile(TempDirTestCase):\n    def test_read_file(self):\n        curr_dir = os.getcwd()\n        try:\n            # On Windows current directory may be on a different drive than self.tempdir.\n            # However a relative path between two different drives is invalid. So we move to\n            # self.tempdir to ensure that we stay on the same drive.\n            os.chdir(self.tempdir)\n            # The read-only filesystem introduced with macOS Catalina can break\n            # code using relative paths below. See\n            # https://bugs.python.org/issue38295 for another example of this.\n            # Eliminating any possible symlinks in self.tempdir before passing\n            # it to os.path.relpath solves the problem. This is done by calling\n            # filesystem.realpath which removes any symlinks in the path on\n            # POSIX systems.\n            real_path = filesystem.realpath(os.path.join(self.tempdir, 'foo'))\n            relative_path = os.path.relpath(real_path)\n            self.assertRaises(\n                argparse.ArgumentTypeError, cli.read_file, relative_path)\n\n            test_contents = b'bar\\n'\n            with open(relative_path, 'wb') as f:\n                f.write(test_contents)\n\n            path, contents = cli.read_file(relative_path)\n            self.assertEqual(path, os.path.abspath(path))\n            self.assertEqual(contents, test_contents)\n        finally:\n            os.chdir(curr_dir)"}
{"Repository": "gdsfactory", "input": "CrossSection to extrude a path with a waveguide. className Section(BaseModel) Method serialize_functions", "label": "class Section(BaseModel):\n    width: float\n    offset: float = 0\n    insets: tuple[float, float] | None = None\n    layer: LayerSpec | None = None\n    port_names: tuple[str | None, str | None] = (None, None)\n    port_types: tuple[str, str] = (\"optical\", \"optical\")\n    name: str | None = None\n    hidden: bool = False\n    simplify: float | None = None\n\n    width_function: Callable | None = Field(default=None)\n    offset_function: Callable | None = Field(default=None)\n\n    model_config = ConfigDict(extra=\"forbid\", frozen=True)\n\n    @field_serializer(\"width_function\", \"offset_function\")\n    def serialize_functions(self, func: Callable | None) -> str | None:\n        if func is None:\n            return None\n        t_values = np.linspace(0, 1, 11)\n        return \",\".join([str(round(width, 3)) for width in func(t_values)])"}
{"Repository": "pytorch-onn", "input": "Custom clean command to tidy up the project root. className CleanCommand(Command) Method initialize_options Method finalize_options Method run", "label": "class CleanCommand(Command):\n    user_options = []\n\n    def initialize_options(self):\n        self.build_base = None\n        self.build_lib = None\n        self.build_temp = None\n        self.build_scripts = None\n        self.bdist_base = None\n        self.all = None\n\n    def finalize_options(self):\n        self.set_undefined_options(\n            \"build\",\n            (\"build_base\", \"build_base\"),\n            (\"build_lib\", \"build_lib\"),\n            (\"build_scripts\", \"build_scripts\"),\n            (\"build_temp\", \"build_temp\"),\n        )\n        self.set_undefined_options(\"bdist\", (\"bdist_base\", \"bdist_base\"))\n\n    def run(self):\n        os.system(\"rm -vrf ./build ./dist ./*.pyc ./*.tgz ./*.egg-info ./*.so ./torchonn/*.egg-info\")\n        os.system(\"rm -vrf ./build ./dist ./*.pyc ./*.tgz ./*.egg-info ./*.so ./torchonn/*.egg-info\")"}
{"Repository": "PolyFuzz", "input": "Embed words into vectors and use cosine similarity to find the best matches between two lists of strings Arguments: embedding_model: The Spacy model to use, this can be either a string or the model directly min_similarity: The minimum similarity between strings, otherwise return 0 similarity top_n: The number of best matches you want returned cosine_method: The method/package for calculating the cosine similarity. className SpacyEmbeddings(BaseMatcher) Method _embed", "label": "class SpacyEmbeddings(BaseMatcher):\n    def __init__(self,\n                 embedding_model = \"en_core_web_md\",\n                 min_similarity: float = 0.75,\n                 top_n: int = 1,\n                 cosine_method: str = \"sparse\",\n                 model_id: str = None):\n        super().__init__(model_id)\n        self.type = \"Embeddings\"\n\n        if isinstance(embedding_model, str):\n            self.embedding_model = spacy.load(embedding_model, exclude=['tagger', 'parser', 'ner', 'attribute_ruler', 'lemmatizer'])\n        elif \"spacy\" in str(type(embedding_model)):\n            self.embedding_model = embedding_model\n        else:\n            raise ValueError(\"Please select a correct Spacy model by either using a string such as 'en_core_web_md' \"\n                             \"or create a nlp model using: `nlp = spacy.load('en_core_web_md')\")\n\n        self.min_similarity = min_similarity\n        self.top_n = top_n\n        self.cosine_method = cosine_method\n\n        self.embeddings_to = None\n\n    def match(self,\n              from_list: List[str],\n              to_list: List[str] = None,\n              embeddings_from: np.ndarray = None,\n              embeddings_to: np.ndarray = None,\n              re_train: bool = True) -> pd.DataFrame:\n        # Extract embeddings from the `from_list`\n        if not isinstance(embeddings_from, np.ndarray):\n            embeddings_from = self._embed(from_list)\n\n        # Extract embeddings from the `to_list` if it exists\n        if not isinstance(embeddings_to, np.ndarray):\n            if not re_train:\n                embeddings_to = self.embeddings_to\n            elif to_list is None:\n                embeddings_to = self._embed(from_list)\n            else:\n                embeddings_to = self._embed(to_list)\n\n        matches = cosine_similarity(embeddings_from, embeddings_to,\n                                    from_list, to_list,\n                                    self.min_similarity,\n                                    top_n=self.top_n,\n                                    method=self.cosine_method)\n\n        self.embeddings_to = embeddings_to\n\n        return matches\n\n    def _embed(self, strings: List[str]) -> np.ndarray:\n        # Extract embeddings from a transformer model\n        if \"transformer\" in self.embedding_model.component_names:\n            embeddings = []\n            for doc in strings:\n                try:\n                    embedding = self.embedding_model(doc)._.trf_data.tensors[-1][0].tolist()\n                except:\n                    embedding = self.embedding_model(\"An empty document\")._.trf_data.tensors[-1][0].tolist()\n                embeddings.append(embedding)\n            embeddings = np.array(embeddings)\n\n        # Extract embeddings from a general spacy model\n        else:\n            embeddings = []\n            for doc in strings:\n                try:\n                    vector = self.embedding_model(doc).vector\n                except ValueError:\n                    vector = self.embedding_model(\"An empty document\").vector\n                embeddings.append(vector)\n            embeddings = np.array(embeddings)\n\n        return embeddings"}
{"Repository": "tornado", "input": "The Fast Hoeffding Drift Detection Method (FHDDM) class. className FHDDM(SuperDetector) Method __init__ Method run Method reset Method get_settings Attribute __DELTA Attribute __N Attribute __E Attribute __WIN Attribute __MU_M", "label": "class FHDDM(SuperDetector):\n    DETECTOR_NAME = TornadoDic.FHDDM\n\n\n\n    def __init__(self, n=100, delta=0.000001):\n\n\n\n        super().__init__()\n\n\n\n        self.__DELTA = delta\n\n        self.__N = n\n\n        self.__E = math.sqrt(math.log((1 / self.__DELTA), math.e) / (2 * self.__N))\n\n\n\n        self.__WIN = []\n\n        self.__MU_M = 0\n\n\n\n    def run(self, pr):\n\n\n\n        drift_status = False\n\n\n\n        if len(self.__WIN) >= self.__N:\n\n            self.__WIN.pop(0)\n\n        self.__WIN.append(pr)\n\n\n\n        if len(self.__WIN) >= self.__N:\n\n            mu_t = self.__WIN.count(True) / self.__N\n\n            if self.__MU_M < mu_t:\n\n                self.__MU_M = mu_t\n\n            drift_status = (self.__MU_M - mu_t) > self.__E\n\n\n\n        return False, drift_status\n\n\n\n    def reset(self):\n\n        super().reset()\n\n        self.__WIN.clear()\n\n        self.__MU_M = 0\n\n\n\n    def get_settings(self):\n\n        settings = [str(self.__N) + \".\" + str(self.__DELTA),\n\n                    \"$n$:\" + str(self.__N) + \", \" +\n\n                    \"$\\delta$:\" + str(self.__DELTA).upper()]\n\n        return settings"}
{"Repository": "python-weather", "input": "Represents the list of supported locales/languages by this library. className Locale(Enum) Method __repr__ Method __str__", "label": "class Locale(Enum):\n  __slots__ = ()\n\n  AFRIKAANS = 'af'\n  AMHARIC = 'am'\n  ARABIC = 'ar'\n  ARMENIAN = 'hy'\n  AZERBAIJANI = 'az'\n  BANGLA = 'bn'\n  BASQUE = 'eu'\n  BELARUSIAN = 'be'\n  BOSNIAN = 'bs'\n  BULGARIAN = 'bg'\n  CATALAN = 'ca'\n  CHINESE_SIMPLIFIED = 'zh'\n  CHINESE_SIMPLIFIED_CHINA = 'zh-cn'\n  CHINESE_TRADITIONAL_TAIWAN = 'zh-tw'\n  CROATIAN = 'hr'\n  CZECH = 'cs'\n  DANISH = 'da'\n  DUTCH = 'nl'\n  ENGLISH = 'en'\n  ESPERANTO = 'eo'\n  ESTONIAN = 'et'\n  FINNISH = 'fi'\n  FRENCH = 'fr'\n  FRISIAN = 'fy'\n  GALICIAN = 'gl'\n  GEORGIAN = 'ka'\n  GERMAN = 'de'\n  GREEK = 'el'\n  HINDI = 'hi'\n  HIRI_MOTU = 'ho'\n  HUNGARIAN = 'hu'\n  ICELANDIC = 'is'\n  INDONESIAN = 'id'\n  INTERLINGUA = 'ia'\n  IRISH = 'ga'\n  ITALIAN = 'it'\n  JAPANESE = 'ja'\n  JAVANESE = 'jv'\n  KAZAKH = 'kk'\n  KISWAHILI = 'sw'\n  KOREAN = 'ko'\n  KYRGYZ = 'ky'\n  LATVIAN = 'lv'\n  LITHUANIAN = 'lt'\n  MACEDONIAN = 'mk'\n  MALAGASY = 'mg'\n  MALAYALAM = 'ml'\n  MARATHI = 'mr'\n  NORWEGIAN_BOKMAL = 'nb'\n  NORWEGIAN_NYNORSK = 'nn'\n  OCCITAN = 'oc'\n  PERSIAN = 'fa'\n  POLISH = 'pl'\n  PORTUGUESE = 'pt'\n  PORTUGUESE_BRAZIL = 'pt-br'\n  ROMANIAN = 'ro'\n  RUSSIAN = 'ru'\n  SERBIAN = 'sr'\n  SERBIAN_LATIN = 'sr-lat'\n  SLOVAK = 'sk'\n  SLOVENIAN = 'sl'\n  SPANISH = 'es'\n  SWEDISH = 'sv'\n  TAMIL = 'ta'\n  TELUGU = 'te'\n  THAI = 'th'\n  TURKISH = 'tr'\n  UKRAINIAN = 'uk'\n  UZBEK = 'uz'\n  VIETNAMESE = 'vi'\n  WELSH = 'cy'\n  ZULU = 'zu'\n\n  def __repr__(self) -> str:\n    return f'{self.__class__.__name__}.{self.name}'\n\n  def __str__(self) -> str:\n    arr = self.name.title().split('_')\n    return f'{arr[:-1].join(\" \")} ({arr[-1]})' if len(arr) != 1 else arr[0]"}
{"Repository": "supervisor", "input": "Manage available architectures. className CpuArch(CoreSysAttributes) Method __init__ Method default Method supervisor Method supported Method is_supported Method match Method detect_cpu", "label": "class CpuArch(CoreSysAttributes):\n    def __init__(self, coresys: CoreSys) -> None:\n        self.coresys = coresys\n        self._supported_arch: list[str] = []\n        self._supported_set: set[str] = set()\n        self._default_arch: str\n\n    @property\n    def default(self) -> str:\n        return self._default_arch\n\n    @property\n    def supervisor(self) -> str:\n        return self.sys_supervisor.arch\n\n    @property\n    def supported(self) -> list[str]:\n        return self._supported_arch\n\n    async def load(self) -> None:\n        try:\n            arch_data = read_json_file(ARCH_JSON)\n        except ConfigurationFileError:\n            _LOGGER.warning(\"Can't read arch json file from %s\", ARCH_JSON)\n            return\n\n        native_support = self.detect_cpu()\n\n        # Evaluate current CPU/Platform\n        if not self.sys_machine or self.sys_machine not in arch_data:\n            _LOGGER.warning(\"Can't detect the machine type!\")\n            self._default_arch = native_support\n            self._supported_arch.append(self.default)\n            return\n\n        # Use configs from arch.json\n        self._supported_arch.extend(arch_data[self.sys_machine])\n        self._default_arch = self.supported[0]\n\n        # Make sure native support is in supported list\n        if native_support not in self._supported_arch:\n            self._supported_arch.append(native_support)\n\n        self._supported_set = set(self._supported_arch)\n\n    def is_supported(self, arch_list: list[str]) -> bool:\n        return not self._supported_set.isdisjoint(arch_list)\n\n    def match(self, arch_list: list[str]) -> str:\n        for self_arch in self.supported:\n            if self_arch in arch_list:\n                return self_arch\n        raise HassioArchNotFound()\n\n    def detect_cpu(self) -> str:\n        cpu = platform.machine()\n        for check, value in MAP_CPU.items():\n            if cpu.startswith(check):\n                return value\n        return self.sys_supervisor.arch"}
{"Repository": "gaeutilities", "input": "Event is a simple publish/subscribe based event dispatcher. className Event(object) Method __init__ Method subscribe Method unsubscribe Method fire_event Attribute events", "label": "class Event(object):\n    def __init__(self):\n        self.events = []\n\n    def subscribe(self, event, callback, args = None):\n        if not {\"event\": event, \"callback\": callback, \"args\": args, } \\\n            in self.events:\n            self.events.append({\"event\": event, \"callback\": callback, \\\n                \"args\": args, })\n        return True\n\n    def unsubscribe(self, event, callback, args = None):\n        if {\"event\": event, \"callback\": callback, \"args\": args, }\\\n            in self.events:\n            self.events.remove({\"event\": event, \"callback\": callback,\\\n                \"args\": args, })\n\n        return True\n\n    def fire_event(self, event = None):\n        for e in self.events:\n            if e[\"event\"] == event:\n                if type(e[\"args\"]) == type([]):\n                    e[\"callback\"](*e[\"args\"])\n                elif type(e[\"args\"]) == type({}):\n                    e[\"callback\"](**e[\"args\"])\n                elif e[\"args\"] == None:\n                    e[\"callback\"]()\n                else:\n                    e[\"callback\"](e[\"args\"])\n        return True"}
{"Repository": "OpenCDN", "input": "Create job json description className OcdnJSON(object) Method __init__ Method create_job_json Attribute json_template", "label": "class OcdnJSON(object):\n\tdef __init__(self):\n\t\tself.json_template = {\n\t\t\t'JobName' : None,\n\t\t\t'Description' : 'OpenCDN',\n\t\t\t'TaskList' : [], \n\t\t\t'CurrentTask': None, \n\t\t\t'TimeOut' : 10,\n\t\t\t'RunTimesLimit': \n\t\t\t{\n\t\t\t\t'AlreadyRunTimes': 0,\n\t\t\t\t'MaxRunTimes' : 10\n\t\t\t},\n\t\t\t'Parameters':{\n\n\t\t\t}\n\t\t}\n\n\tdef create_job_json(self, JobName, TaskList=[], Parameters=[]):\n\t\tself.json_template['JobName'] = JobName\n\t\tself.json_template['TaskList'] = TaskList\n\t\tself.json_template['Parameters'] = Parameters\n\t\ttry:\n\t\t\tself.json_template['CurrentTask'] = TaskList[0]\n\t\texcept Exception, e:\n\t\t\treturn None\n\t\t\n\t\treturn self.json_template"}
{"Repository": "TencentPretrain", "input": "refer to the paper: FGM(Fast Gradient Method) className FGM(object) Method __init__ Method attack Method restore Attribute model Attribute backup", "label": "class FGM(object):\n    def __init__(self, model):\n        self.model = model\n        self.backup = {}\n\n    def attack(self, epsilon=1e-6, emd_name=\"embedding\"):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad and emd_name in name:\n                self.backup[name] = param.data.clone()\n                norm = torch.norm(param.grad)\n                if norm != 0 and not torch.isnan(norm):\n                    r_at = epsilon * param.grad / norm\n                    param.data.add_(r_at)\n\n    def restore(self, emd_name=\"embedding\"):\n        for name, param in self.model.named_parameters():\n            if param.requires_grad and emd_name in name:\n                assert name in self.backup\n                param.data = self.backup[name]\n        self.backup = {}"}
{"Repository": "dumbo", "input": "A Generic test driver that passes input stream through a callable and checks output stream matches specified one. className BaseDriver(object) Method __init__ Method with_params Method with_input Method with_output Method run Method _instrument_class Attribute _callable Attribute _callable Attribute _input_source Attribute _output_source", "label": "class BaseDriver(object):\n    def __init__(self, kallable):\n        # Check if given callable is a function or a class \n        # type that needs instantiation\n        if inspect.isclass(kallable):\n            # Re-derive class using dumbo's common MapRedBase object.\n            kallable = self._instrument_class(kallable)\n            self._callable = kallable()\n        else:\n            self._callable = kallable        \n            \t\n        self._input_source = None\n        self._output_source = None\n    \n    def with_params(self, params):\n        for k, v in params:\n            os.environ[k] = v\n        return self\n        \n    def with_input(self, input_source):\n        if not hasattr(input_source, \"next\"):\n            # Not an iterator\n            self._input_source = iter(input_source)\n        else:\n            self._input_source = input_source\n        return self\n    \n    def with_output(self, output_source):\n        if not hasattr(output_source, 'next'):\n            # Not an iterator\n            self._output_source = iter(output_source)\n        else:\n            self._output_source = output_source\n        return self\n        \n    def run(self):\n        assert_iters_equal(self._output_source, imap(self._func, self._input_source))\n      \n    def _instrument_class(self, cls):\n        newcls = type('InstrumentedClass', (cls, MapRedBase), {})\n        return newcls"}
{"Repository": "django-excel", "input": "Support setup.py upload. className PublishCommand(Command) Method status Method initialize_options Method finalize_options Method run", "label": "class PublishCommand(Command):\n    description = \"Build and publish the package on github and pypi\"\n    user_options = []\n\n    @staticmethod\n    def status(s):\n        print(\"\\033[1m{0}\\033[0m\".format(s))\n\n    def initialize_options(self):\n        pass\n\n    def finalize_options(self):\n        pass\n\n    def run(self):\n        try:\n            self.status(\"Removing previous builds...\")\n            rmtree(os.path.join(HERE, \"dist\"))\n            rmtree(os.path.join(HERE, \"build\"))\n            rmtree(os.path.join(HERE, \"django_excel.egg-info\"))\n        except OSError:\n            pass\n\n        self.status(\"Building Source and Wheel (universal) distribution...\")\n        run_status = True\n        if has_gease():\n            run_status = os.system(GS_COMMAND) == 0\n        else:\n            self.status(NO_GS_MESSAGE)\n        if run_status:\n            if os.system(PUBLISH_COMMAND) != 0:\n                self.status(UPLOAD_FAILED_MSG)\n\n        sys.exit()"}
{"Repository": "disrupting-deepfakes", "input": "A template dataset class for you to implement custom datasets. className TemplateDataset(BaseDataset) Method modify_commandline_options Method __init__ Method __getitem__ Method __len__ Attribute image_paths Attribute transform", "label": "class TemplateDataset(BaseDataset):\n    @staticmethod\n    def modify_commandline_options(parser, is_train):\n        parser.add_argument('--new_dataset_option', type=float, default=1.0, help='new dataset option')\n        parser.set_defaults(max_dataset_size=10, new_dataset_option=2.0)  # specify dataset-specific default values\n        return parser\n\n    def __init__(self, opt):\n        # save the option and dataset root\n        BaseDataset.__init__(self, opt)\n        # get the image paths of your dataset;\n        self.image_paths = []  # You can call sorted(make_dataset(self.root, opt.max_dataset_size)) to get all the image paths under the directory self.root\n        # define the default transform function. You can use <base_dataset.get_transform>; You can also define your custom transform function\n        self.transform = get_transform(opt)\n\n    def __getitem__(self, index):\n        path = 'temp'    # needs to be a string\n        data_A = None    # needs to be a tensor\n        data_B = None    # needs to be a tensor\n        return {'data_A': data_A, 'data_B': data_B, 'path': path}\n\n    def __len__(self):\n        return len(self.image_paths)"}
{"Repository": "SMP2019-ECDT-NLU", "input": "BERT model with the masked language modeling head. className BertForMaskedLM(BertPreTrainedModel) Method __init__ Method forward Attribute output_attentions Attribute bert Attribute cls", "label": "class BertForMaskedLM(BertPreTrainedModel):\n    def __init__(self, config, output_attentions=False, keep_multihead_output=False):\n        super(BertForMaskedLM, self).__init__(config)\n        self.output_attentions = output_attentions\n        self.bert = BertModel(config, output_attentions=output_attentions,\n                                      keep_multihead_output=keep_multihead_output)\n        self.cls = BertOnlyMLMHead(config, self.bert.embeddings.word_embeddings.weight)\n        self.apply(self.init_bert_weights)\n\n    def forward(self, input_ids, token_type_ids=None, attention_mask=None, masked_lm_labels=None, head_mask=None):\n        outputs = self.bert(input_ids, token_type_ids, attention_mask,\n                                       output_all_encoded_layers=False,\n                                       head_mask=head_mask)\n        if self.output_attentions:\n            all_attentions, sequence_output, _ = outputs\n        else:\n            sequence_output, _ = outputs\n        prediction_scores = self.cls(sequence_output)\n\n        if masked_lm_labels is not None:\n            loss_fct = CrossEntropyLoss(ignore_index=-1)\n            masked_lm_loss = loss_fct(prediction_scores.view(-1, self.config.vocab_size), masked_lm_labels.view(-1))\n            return masked_lm_loss\n        elif self.output_attentions:\n            return all_attentions, prediction_scores\n        return prediction_scores"}
{"Repository": "tracklite", "input": "Definition of a parser for DarkNet-based YOLOv3-608 (only tested for this topology). className DarkNetParser(object) Method __init__ Method parse_cfg_file Method _next_layer Method _parse_params Attribute layer_configs Attribute supported_layers Attribute layer_counter", "label": "class DarkNetParser(object):\n    def __init__(self, supported_layers):\n        # A list of YOLOv3 layers containing dictionaries with all layer\n        # parameters:\n        self.layer_configs = OrderedDict()\n        self.supported_layers = supported_layers\n        self.layer_counter = 0\n\n    def parse_cfg_file(self, cfg_file_path):\n        with open(cfg_file_path) as cfg_file:\n            remainder = cfg_file.read()\n            while remainder is not None:\n                layer_dict, layer_name, remainder = self._next_layer(remainder)\n                if layer_dict is not None:\n                    self.layer_configs[layer_name] = layer_dict\n        return self.layer_configs\n\n    def _next_layer(self, remainder):\n        remainder = remainder.split('[', 1)\n        if len(remainder) == 2:\n            remainder = remainder[1]\n        else:\n            return None, None, None\n        remainder = remainder.split(']', 1)\n        if len(remainder) == 2:\n            layer_type, remainder = remainder\n        else:\n            return None, None, None\n        if remainder.replace(' ', '')[0] == '#':\n            remainder = remainder.split('\\n', 1)[1]\n\n        layer_param_block, remainder = remainder.split('\\n\\n', 1)\n        layer_param_lines = layer_param_block.split('\\n')[1:]\n        layer_name = str(self.layer_counter).zfill(3) + '_' + layer_type\n        layer_dict = dict(type=layer_type)\n        if layer_type in self.supported_layers:\n            for param_line in layer_param_lines:\n                if param_line[0] == '#':\n                    continue\n                param_type, param_value = self._parse_params(param_line)\n                layer_dict[param_type] = param_value\n        self.layer_counter += 1\n        return layer_dict, layer_name, remainder\n\n    def _parse_params(self, param_line):\n        param_line = param_line.replace(' ', '')\n        param_type, param_value_raw = param_line.split('=')\n        param_value = None\n        if param_type == 'layers':\n            layer_indexes = list()\n            for index in param_value_raw.split(','):\n                layer_indexes.append(int(index))\n            param_value = layer_indexes\n        elif isinstance(param_value_raw, str) and not param_value_raw.isalpha():\n            condition_param_value_positive = param_value_raw.isdigit()\n            condition_param_value_negative = param_value_raw[0] == '-' and \\\n                param_value_raw[1:].isdigit()\n            if condition_param_value_positive or condition_param_value_negative:\n                param_value = int(param_value_raw)\n            else:\n                param_value = float(param_value_raw)\n        else:\n            param_value = str(param_value_raw)\n        return param_type, param_value"}
{"Repository": "PSMNet", "input": "Lighting noise(AlexNet - style PCA - based noise) className Lighting(object) Method __init__ Method __call__ Attribute alphastd Attribute eigval Attribute eigvec", "label": "class Lighting(object):\n    def __init__(self, alphastd, eigval, eigvec):\n        self.alphastd = alphastd\n        self.eigval = eigval\n        self.eigvec = eigvec\n\n    def __call__(self, img):\n        if self.alphastd == 0:\n            return img\n\n        alpha = img.new().resize_(3).normal_(0, self.alphastd)\n        rgb = self.eigvec.type_as(img).clone()\\\n            .mul(alpha.view(1, 3).expand(3, 3))\\\n            .mul(self.eigval.view(1, 3).expand(3, 3))\\\n            .sum(1).squeeze()\n\n        return img.add(rgb.view(3, 1, 1).expand_as(img))"}
{"Repository": "texar", "input": "Processor for the MultiNLI data set (GLUE version). className MnliProcessor(DataProcessor) Method get_train_examples Method get_dev_examples Method get_test_examples Method get_labels Method _create_examples", "label": "class MnliProcessor(DataProcessor):\n    def get_train_examples(self, data_dir):\n        return self._create_examples(\n            self._read_tsv(os.path.join(data_dir, \"train.tsv\")), \"train\")\n\n    def get_dev_examples(self, data_dir):\n        return self._create_examples(\n            self._read_tsv(os.path.join(data_dir, \"dev_matched.tsv\")),\n            \"dev_matched\")\n\n    def get_test_examples(self, data_dir):\n        return self._create_examples(\n            self._read_tsv(os.path.join(data_dir, \"test_matched.tsv\")),\n            \"test\")\n\n    def get_labels(self):\n        return [\"contradiction\", \"entailment\", \"neutral\"]\n\n    @staticmethod\n    def _create_examples(lines, set_type):\n        examples = []\n        for (i, line) in enumerate(lines):\n            if i == 0:\n                continue\n            guid = \"%s-%s\" % (set_type,\n                              tx.utils.compat_as_text(line[0]))\n            text_a = tx.utils.compat_as_text(line[8])\n            text_b = tx.utils.compat_as_text(line[9])\n            if set_type == \"test\":\n                label = \"contradiction\"\n            else:\n                label = tx.utils.compat_as_text(line[-1])\n            examples.append(InputExample(guid=guid, text_a=text_a,\n                                         text_b=text_b, label=label))\n        return examples"}
{"Repository": "coinbin.org", "input": "A Coin, unlike Mario's. className Coin() Method __init__ Method update Method usd Method btc Method value Method __repr__ Attribute ticker Attribute name Attribute rank Attribute _value", "label": "class Coin():\n    def __init__(self, ticker):\n        self.ticker = ticker\n        self.name = None\n        self.rank = None\n        self._value = None\n\n        self.update()\n\n    def update(self):\n        coins = get_coins()\n        print(f'Fetching data on {crayons.cyan(self.ticker)}...')\n\n        self.name = coins[self.ticker]['name']\n        self.rank = coins[self.ticker]['rank']\n        self._usd = coins[self.ticker]['usd']\n\n    @property\n    def usd(self):\n        return self._usd\n\n    @property\n    def btc(self):\n        coins = get_coins()\n        rate = coins['btc']['usd']\n        return convert_to_decimal(self.usd / rate)\n\n    def value(self, coin):\n        return convert_to_decimal(self.btc / Coin(coin).btc)\n\n    def __repr__(self):\n        return f'<Coin ticker={self.ticker!r}>'"}
{"Repository": "HCMoCo", "input": "Randomly selects a rectangle region in an image and erases its pixels. className RandomErasing(object) Method __init__ Method __call__ Attribute probability Attribute mean Attribute sl Attribute sh Attribute r1 Attribute scale", "label": "class RandomErasing(object):\n    def __init__(self, probability = 0.5, sl = 0.02, sh = 0.4, r1 = 0.3, mean=[3.44405131],scale=1):\n        self.probability = probability\n        self.mean = mean\n        self.sl = sl\n        self.sh = sh\n        self.r1 = r1\n        self.scale = scale\n    def __call__(self, img):\n\n        if random.uniform(0, 1) > self.probability:\n            return img\n\n        for attempt in range(100):\n            area = img.size()[1] * img.size()[2]\n       \n            target_area = random.uniform(self.sl, self.sh) * area\n            aspect_ratio = random.uniform(self.r1, 1/self.r1)\n\n            h = int(round(math.sqrt(target_area * aspect_ratio)))\n            w = int(round(math.sqrt(target_area / aspect_ratio)))\n\n            if w < img.size()[2] and h < img.size()[1]:\n                rand_patch = self.scale*torch.randn(h,w)\n                x1 = random.randint(0, img.size()[1] - h)\n                y1 = random.randint(0, img.size()[2] - w)\n                if img.size()[0] == 3:\n                    img[0, x1:x1+h, y1:y1+w] += rand_patch#self.mean[0]\n                    img[1, x1:x1+h, y1:y1+w] += rand_patch#self.mean[1]\n                    img[2, x1:x1+h, y1:y1+w] += rand_patch#self.mean[2]\n                else:\n                    img[0, x1:x1+h, y1:y1+w] += rand_patch#self.mean[0]\n                return img\n\n        return img"}
{"Repository": "django-garnett", "input": "This middleware catches the requested \"garnett language\" and: className TranslationContextNotFoundMiddleware(TranslationContextMiddleware) Method validate", "label": "class TranslationContextNotFoundMiddleware(TranslationContextMiddleware):\n    def validate(self, language):\n        if not is_valid_language(language):\n            lang_obj = language\n            lang_name = lang_obj.display_name(language)\n            lang_en_name = lang_obj.display_name()\n            raise Http404(\n                _(\"This server does not support %(lang_name)s\" \" [%(lang_en_name)s].\")\n                % {\n                    \"lang_name\": lang_name,\n                    \"lang_en_name\": lang_en_name,\n                }\n            )"}
{"Repository": "SublimePTY", "input": "A screen subclass, which keeps track of screen history and allows pagination. className HistoryScreen(DiffScreen) Method __init__ Method __before__ Method __after__ Method reset Method index Method reverse_index Method prev_page Method next_page Attribute history", "label": "class HistoryScreen(DiffScreen):\n    def __init__(self, columns, lines, history=100, ratio=.5):\n        self.history = History(deque(maxlen=history // 2),\n                               deque(maxlen=history),\n                               float(ratio),\n                               history,\n                               history)\n\n        super(HistoryScreen, self).__init__(columns, lines)\n\n    def __before__(self, command):\n        if command not in [\"prev_page\", \"next_page\"]:\n            while self.history.position < self.history.size:\n                self.next_page()\n\n        super(HistoryScreen, self).__before__(command)\n\n    def __after__(self, command):\n        if command in [\"prev_page\", \"next_page\"]:\n            for idx, line in enumerate(self):\n                if len(line) > self.columns:\n                    self[idx] = line[:self.columns]\n                elif len(line) < self.columns:\n                    self[idx] = line + take(self.columns - len(line),\n                                            self.default_line)\n\n        # If we're at the bottom of the history buffer and `DECTCEM`\n        # mode is set -- show the cursor.\n        self.cursor.hidden = not (\n            abs(self.history.position - self.history.size) < self.lines and\n            mo.DECTCEM in self.mode\n        )\n\n        super(HistoryScreen, self).__after__(command)\n\n    def reset(self):\n        super(HistoryScreen, self).reset()\n\n        self.history.top.clear()\n        self.history.bottom.clear()\n        self.history = self.history._replace(position=self.history.size)\n\n    def index(self):\n        top, bottom = self.margins\n\n        if self.cursor.y == bottom:\n            self.history.top.append(self[top])\n\n        super(HistoryScreen, self).index()\n\n    def reverse_index(self):\n        top, bottom = self.margins\n\n        if self.cursor.y == top:\n            self.history.bottom.append(self[bottom])\n\n        super(HistoryScreen, self).reverse_index()\n\n    def prev_page(self):\n        if self.history.position > self.lines and self.history.top:\n            mid = min(len(self.history.top),\n                      int(math.ceil(self.lines * self.history.ratio)))\n\n            self.history.bottom.extendleft(reversed(self[-mid:]))\n            self.history = self.history \\\n                ._replace(position=self.history.position - self.lines)\n\n            self[:] = list(reversed([\n                self.history.top.pop() for _ in range(mid)\n            ])) + self[:-mid]\n\n            self.dirty = set(range(self.lines))\n\n    def next_page(self):\n        if self.history.position < self.history.size and self.history.bottom:\n            mid = min(len(self.history.bottom),\n                      int(math.ceil(self.lines * self.history.ratio)))\n\n            self.history.top.extend(self[:mid])\n            self.history = self.history \\\n                ._replace(position=self.history.position + self.lines)\n\n            self[:] = self[mid:] + [\n                self.history.bottom.popleft() for _ in range(mid)\n            ]\n\n            self.dirty = set(range(self.lines))"}
{"Repository": "collectd-rabbitmq", "input": "Class that contains configuration data. className Config(object) Method is_ignored", "label": "class Config(object):\n    def __init__(self, auth, connection, data_to_ignore=None,\n                 vhost_prefix=None):\n        self.auth = auth\n        self.connection = connection\n        self.data_to_ignore = dict()\n        self.vhost_prefix = vhost_prefix\n\n        if data_to_ignore:\n            for key, values in data_to_ignore.items():\n                self.data_to_ignore[key] = list()\n                for value in values:\n                    self.data_to_ignore[key].append(re.compile(value))\n\n    def is_ignored(self, stat_type, name):\n        if stat_type in self.data_to_ignore:\n            for regex in self.data_to_ignore[stat_type]:\n                match = regex.match(name)\n                if match:\n                    return True\n        return False"}
{"Repository": "enaml-web", "input": "A block for rendering Markdown source. className Markdown(Raw) Method _update_proxy Method _notify_modified", "label": "class Markdown(Raw):\n    #: Extensions to use when rendering\n    extensions = d_(\n        List(\n            default=[\n                \"markdown.extensions.codehilite\",\n                \"markdown.extensions.fenced_code\",\n                \"markdown.extensions.tables\",\n            ]\n        )\n    ).tag(attr=False)\n\n    #: Configuration for them\n    extension_configs = d_(\n        Dict(\n            default={\n                \"markdown.extensions.codehilite\": {\"css_class\": \"highlight\"},\n            }\n        )\n    ).tag(attr=False)\n\n    #: Disallow raw HTMl\n    safe_mode = d_(Bool()).tag(attr=False)\n\n    #: Output format\n    output_format = d_(Enum(\"xhtml\", \"html5\")).tag(attr=False)\n\n    #: Tab size\n    tab_length = d_(Int(4)).tag(attr=False)\n\n    #: Reference to the proxy\n    proxy = Typed(ProxyMarkdown)\n\n    @observe(\n        \"extensions\", \"extension_configs\", \"safe_mode\", \"output_format\", \"tab_length\"\n    )\n    def _update_proxy(self, change: ChangeDict):\n        super()._update_proxy(change)\n\n    def _notify_modified(self, root: Optional[Tag], change: ChangeDict):\n        if change[\"type\"] == \"update\" and change[\"name\"] == \"source\":\n            change[\"value\"] = self.render()\n        super()._notify_modified(root, change)"}
{"Repository": "few-shot-meta-baseline", "input": "A Learner. className Learner(object) Method update_ema Method compute_loss Method forward_pass", "label": "class Learner(object):\n  def __init__(self, is_training, transductive_batch_norm,\n               backprop_through_moments, ema_object, embedding_fn, data):\n    self.is_training = is_training\n    self.transductive_batch_norm = transductive_batch_norm\n    self.backprop_through_moments = backprop_through_moments\n    self.ema_object = ema_object\n    self.embedding_fn = embedding_fn\n    self.data = data\n\n  def update_ema(self):\n  def compute_loss(self):\n  def forward_pass(self):"}
{"Repository": "gspread", "input": "The class holds the returned values. className ValueRange(list) Method from_json Method range Method major_dimension Method first", "label": "class ValueRange(list):\n    _json: MutableMapping[str, str] = {}\n\n    @classmethod\n    def from_json(cls: Type[ValueRangeType], json: Mapping[str, Any]) -> ValueRangeType:\n        values = json.get(\"values\", [])\n        new_obj = cls(values)\n        new_obj._json = {\n            \"range\": json[\"range\"],\n            \"majorDimension\": json[\"majorDimension\"],\n        }\n\n        return new_obj\n\n    @property\n    def range(self) -> str:\n        return self._json[\"range\"]\n\n    @property\n    def major_dimension(self) -> str:\n        return self._json[\"majorDimension\"]\n\n    def first(self, default: Optional[str] = None) -> Optional[str]:\n        try:\n            return self[0][0]\n        except IndexError:\n            return default"}
{"Repository": "FreqShow", "input": "Scrolling waterfall plot of spectrogram data. className WaterfallSpectrogram(SpectrogramBase) Method __init__ Method clear_waterfall Method render_spectrogram Attribute color_func Attribute waterfall", "label": "class WaterfallSpectrogram(SpectrogramBase):\n\tdef __init__(self, model, controller):\n\t\tsuper(WaterfallSpectrogram, self).__init__(model, controller)\n\t\tself.color_func = gradient_func(freqshow.WATERFALL_GRAD)\n\t\tself.waterfall = pygame.Surface((model.width, model.height))\n\n\tdef clear_waterfall(self):\n\t\tself.waterfall.fill(freqshow.MAIN_BG)\n\n\tdef render_spectrogram(self, screen):\n\t\t# Grab spectrogram data.\n\t\tfreqs = self.model.get_data()\n\t\t# Scroll up the waterfall display.\n\t\tself.waterfall.scroll(0, -1)\n\t\t# Scale the FFT values to the range 0 to 1.\n\t\tfreqs = (freqs-self.model.min_intensity)/self.model.range\n\t\t# Convert scaled values to pixels drawn at the bottom of the display.\n\t\tx, y, width, height = screen.get_rect()\n\t\twx, wy, wwidth, wheight = self.waterfall.get_rect()\n\t\toffset = wheight - height\n\t\t# Draw FFT values mapped through the gradient function to a color.\n\t\tself.waterfall.lock()\n\t\tfor i in range(width):\n\t\t\tpower = clamp(freqs[i], 0.0, 1.0)\n\t\t\tself.waterfall.set_at((i, wheight-1), self.color_func(power))\n\t\tself.waterfall.unlock()\n\t\tscreen.blit(self.waterfall, (0, 0), area=(0, offset, width, height))"}
{"Repository": "mysql_utils", "input": "Represents a MySQL server (as we can monitor more than 1 MySQL). className DB(object) Method __init__ Method __str__ Method __repr__ Method query Method close Method _reconnect Attribute sockfile Attribute dbname Attribute db Attribute replica_set Attribute cursor Attribute version Attribute port Attribute db_type Attribute major Attribute medium Attribute major Attribute medium", "label": "class DB(object):\n    def __init__(self, sockfile, db, cursor, version, port, db_type):\n        host = socket.gethostname()\n        self.sockfile = sockfile\n        self.dbname = ':'.join([host, port])\n        self.db = db\n        self.replica_set = host[0:host.rindex('-')]\n        self.cursor = cursor\n        self.version = version\n        self.port = port\n        self.db_type = db_type\n        version = version.split(\".\")\n        try:\n            self.major = int(version[0])\n            self.medium = int(version[1])\n        except (ValueError, IndexError):\n            self.major = self.medium = 0\n\n    def __str__(self):\n        return \"DB(%r, %r, version=%r)\" % (self.sockfile, self.dbname,\n                                           self.version)\n\n    def __repr__(self):\n        return self.__str__()\n\n    def query(self, sql):\n        assert self.cursor, \"%s already closed?\" % (self,)\n        try:\n            self.cursor.execute(sql)\n        except MySQLdb.OperationalError, (errcode, msg):\n            if errcode != 2006:  # \"MySQL server has gone away\"\n                raise\n            self._reconnect()\n        return self.cursor.fetchall()\n\n    def close(self):\n        if self.cursor:\n            self.cursor.close()\n            self.cursor = None\n        if self.db:\n            self.db.close()\n            self.db = None\n\n    def _reconnect(self):\n        self.close()\n        self.db = mysql_connect(self.sockfile)\n        self.cursor = self.db.cursor()"}
{"Repository": "spopt", "input": "Ensures that the spatially extensive attribute adds up to a given threshold in each region. className AllowMoveAZPMaxPRegions(AllowMoveStrategy) Method __init__ Method start_new_component Method __call__ Method __getattr__ Attribute _decorated_strategy Attribute spatially_extensive_attr_all Attribute spatially_extensive_attr Attribute threshold", "label": "class AllowMoveAZPMaxPRegions(AllowMoveStrategy):\n    def __init__(self, spatially_extensive_attr, threshold, decorated_strategy):\n        self._decorated_strategy = decorated_strategy\n        self.spatially_extensive_attr_all = spatially_extensive_attr\n        self.spatially_extensive_attr = None\n        self.threshold = threshold\n\n    def start_new_component(self, initial_labels, attr, objective_func, comp_idx):\n        self.spatially_extensive_attr = self.spatially_extensive_attr_all[comp_idx]\n        super().start_new_component(initial_labels, attr, objective_func, comp_idx)\n        self._decorated_strategy.start_new_component(\n            initial_labels, attr, objective_func, comp_idx\n        )\n\n    def __call__(self, moving_area, new_region, labels):\n        sp_ext = self.spatially_extensive_attr\n\n        if (sp_ext[moving_area]).any() > 0:\n            donor_region = labels[moving_area]\n            donor_idx = np.where(labels == donor_region)[0]\n            donor_sum = sum(sp_ext[donor_idx]) - sp_ext[moving_area]\n            threshold_reached_donor = (donor_sum >= self.threshold).all()\n            if not threshold_reached_donor:\n                return False\n\n        elif (sp_ext[moving_area]).any() < 0:\n            recipient_idx = np.where(labels == new_region)[0]\n            recipient_sum = sum(sp_ext[recipient_idx]) + sp_ext[moving_area]\n            threshold_reached_recipient = (recipient_sum >= self.threshold).all()\n            if not threshold_reached_recipient:\n                return False\n\n        return self._decorated_strategy(moving_area, new_region, labels)\n\n    def __getattr__(self, name):\n        return getattr(self._decorated_strategy, name)"}
{"Repository": "iSubRip", "input": "A NamedTuple representing a config setting. className ConfigSetting(NamedTuple) Method __eq__", "label": "class ConfigSetting(NamedTuple):\n    key: str\n    # TODO: Use `types.UnionType` instead of `typing._UnionGenericAlias`, once minimum Python version >= 3.10.\n    # TODO: Update 'InvalidConfigType' exception as well.\n    value_type: type | typing._UnionGenericAlias  # type: ignore[name-defined]\n    category: str | tuple[str, ...] | None = None\n    required: bool = False\n    enum_type: Type[Enum] | None = None\n    special_type: SpecialConfigType | list[SpecialConfigType] | None = None\n\n    def __eq__(self, other: Any) -> bool:\n        if isinstance(other, ConfigSetting):\n            return self.key == other.key and self.category == other.category\n        return False"}
{"Repository": "ibm_zos_core", "input": "Jinja2 filters for use with WTOR response objects returned by zos_operator_action_query module. className FilterModule(object) Method filters", "label": "class FilterModule(object):\n    def filters(self):\n        filters = {\n            \"filter_wtor_messages\": filter_wtor_messages,\n        }\n        return filters"}
{"Repository": "pyethsaletool", "input": "Custom error Class used in the Keccak implementation className KeccakError(Exception) Method __init__ Method __str__ Attribute value", "label": "class KeccakError(Exception):\n  def __init__(self, value):\n    self.value = value\n\n  def __str__(self):\n    return repr(self.value)"}
{"Repository": "texthero", "input": "A Pandas DataFrame representing a matrix (e. className DataFrame(HeroTypes) Method check_type", "label": "class DataFrame(HeroTypes):\n    @staticmethod\n    def check_type(df: pd.DataFrame, input_output=\"\") -> Tuple[bool, str]:\n        error_string = (\n            \"should be DataFrame: The input should be a Pandas DataFrame\"\n            \" representing a matrix, where every cell is one entry of the matrix.\"\n            \" See help(hero.HeroTypes) for more information.\"\n        )\n\n        if not isinstance(df, pd.DataFrame):\n            return False, error_string\n        else:\n            return True, \"\""}
{"Repository": "minREV", "input": "Custom Backpropagation function to allow (A) flusing memory in foward and (B) activation recomputation reversibly in backward for gradient calculation. className RevBackProp(Function) Method backward", "label": "class RevBackProp(Function):\n    @staticmethod\n    def forward(\n        ctx,\n        x,\n        layers,\n    ):\n        # obtaining X_1 and X_2 from the concatenated input\n        X_1, X_2 = torch.chunk(x, 2, dim=-1)\n\n        for layer in layers:\n            X_1, X_2 = layer(X_1, X_2)\n            all_tensors = [X_1.detach(), X_2.detach()]\n\n        # saving only the final activations of the last reversible block\n        # for backward pass, no intermediate activations are needed.\n        ctx.save_for_backward(*all_tensors)\n        ctx.layers = layers\n\n        return torch.cat([X_1, X_2], dim=-1)\n\n    @staticmethod\n    def backward(ctx, dx):\n        # obtaining gradients dX_1 and dX_2 from the concatenated input\n        dX_1, dX_2 = torch.chunk(dx, 2, dim=-1)\n\n        # retrieve the last saved activations, to start rev recomputation\n        X_1, X_2 = ctx.saved_tensors\n        # layer weights\n        layers = ctx.layers\n\n        for _, layer in enumerate(layers[::-1]):\n            # this is recomputing both the activations and the gradients wrt\n            # those activations.\n            X_1, X_2, dX_1, dX_2 = layer.backward_pass(\n                Y_1=X_1,\n                Y_2=X_2,\n                dY_1=dX_1,\n                dY_2=dX_2,\n            )\n        # final input gradient to be passed backward to the patchification layer\n        dx = torch.cat([dX_1, dX_2], dim=-1)\n\n        del dX_1, dX_2, X_1, X_2\n\n        return dx, None, None"}
{"Repository": "inputs", "input": "Loosely emulate Evdev mouse behaviour on the Macs. className MacMouseListener(AppKitMouseBaseListener) Method install_handle_input Method __del__", "label": "class MacMouseListener(AppKitMouseBaseListener):\n        def install_handle_input(self):\n            self.app = NSApplication.sharedApplication()\n            # pylint: disable=no-member\n            delegate = MacMouseSetup.alloc().init_with_handler(\n                self.handle_input)\n            NSApp().setDelegate_(delegate)\n            AppHelper.runEventLoop()\n\n        def __del__(self):\n            AppHelper.stopEventLoop()\n\n    # pylint: disable=unused-variable\n    mouse = MacMouseListener(pipe, events=[])"}
{"Repository": "privacyidea", "input": "Custom Response class overwriting the flask. className PiResponseClass(Response) Method json", "label": "class PiResponseClass(Response):\n    @property\n    def json(self):\n        return self.get_json(cache=False)\n\n    default_mimetype = 'application/json'"}
{"Repository": "pygcode", "input": "GCode block (effectively any gcode file line that defines any <word><value>) className Block(object) Method __init__ Method text Method _assert_gcodes Method __getattr__ Method __len__ Method __bool__ Method __str__ Attribute _raw_text Attribute _text Attribute words Attribute gcodes Attribute modal_params Attribute dialect Attribute _word_map Attribute _raw_text Attribute _text Attribute words", "label": "class Block(object):\n    def __init__(self, text=None, dialect=None, verify=True):\n        self._raw_text = None\n        self._text = None\n        self.words = []\n        self.gcodes = []\n        self.modal_params = []\n\n        if dialect is None:\n            dialect = dialects.get_default()\n        self.dialect = dialect\n\n        self._word_map = getattr(getattr(dialects, dialect), 'WORD_MAP')\n\n        # clean up block string\n        if text:\n            self._raw_text = text  # unaltered block content (before alteration)\n            text = re.sub(r'(^\\s+|\\s+$)', '', text) # remove whitespace padding\n            text = re.sub(r'\\s+', ' ', text) # remove duplicate whitespace with ' '\n            self._text = text  # cleaned up block content\n\n            # Get words from text, and group into gcodes\n            self.words = list(text2words(self._text))\n            (self.gcodes, self.modal_params) = words2gcodes(self.words)\n\n            # Verification\n            if verify:\n                self._assert_gcodes()\n\n    @property\n    def text(self):\n        if self._text:\n            return self._text\n        return str(self)\n\n    def _assert_gcodes(self):\n        modal_groups = set()\n        code_words = set()\n\n        for gc in self.gcodes:\n\n            # Assert all gcodes are not repeated in the same block\n            if gc.word in code_words:\n                raise AssertionError(\"%s cannot be in the same block\" % ([\n                    x for x in self.gcodes\n                    if x.modal_group == gc.modal_group\n                ]))\n            code_words.add(gc.word)\n\n            # Assert all gcodes are from different modal groups\n            if gc.modal_group is not None:\n                if gc.modal_group in modal_groups:\n                    raise AssertionError(\"%s cannot be in the same block\" % ([\n                        x for x in self.gcodes\n                        if x.modal_group == gc.modal_group\n                    ]))\n                modal_groups.add(gc.modal_group)\n\n    def __getattr__(self, k):\n        if k in self._word_map:\n            for w in self.words:\n                if w.letter == k:\n                    return w\n            # if word is not in this block:\n            return None\n\n        else:\n            raise AttributeError(\"'{cls}' object has no attribute '{key}'\".format(\n                cls=self.__class__.__name__,\n                key=k\n            ))\n\n    def __len__(self):\n        length = len(self.gcodes)\n        if self.modal_params:\n            length += 1\n        return length\n\n    def __bool__(self):\n        return bool(self.words)\n\n    __nonzero__ = __bool__  # python < 3 compatability\n\n    def __str__(self):\n        return ' '.join(str(x) for x in (self.gcodes + self.modal_params))"}
{"Repository": "nucypher", "input": "Raised when attempting to access a contract that is not deployed on the current network. className ContractNotDeployed(Exception) Method __repr__ Method __eq__ Method contract Method contract_address", "label": "class ContractNotDeployed(Exception):\n    class RequirementError(Exception):\n    def __init__(\n        self,\n        blockchain_endpoint: str,\n        registry: ContractRegistry,\n        contract: Optional[Contract] = None,\n        transaction_gas: Optional[Wei] = None,\n    ):\n\n        self.log = Logger(self.__class__.__name__)\n        self.registry = registry\n\n        self.blockchain = BlockchainInterfaceFactory.get_or_create_interface(\n            endpoint=blockchain_endpoint\n        )\n\n        if not contract:  # Fetch the contract\n            contract = self.blockchain.get_contract_by_name(\n                registry=registry,\n                contract_name=self.contract_name,\n            )\n\n        self.__contract = contract\n        self.events = events.ContractEvents(contract)\n        if not transaction_gas:\n            transaction_gas = EthereumContractAgent.DEFAULT_TRANSACTION_GAS_LIMITS['default']\n        self.transaction_gas = transaction_gas\n\n        self.log.info(\n            \"Initialized new {} for {} with {} and {}\".format(\n                self.__class__.__name__,\n                self.contract.address,\n                self.blockchain.endpoint,\n                str(self.registry),\n            )\n        )\n\n    def __repr__(self) -> str:\n        class_name = self.__class__.__name__\n        r = \"{}(registry={}, contract={})\"\n        return r.format(class_name, str(self.registry), self.contract_name)\n\n    def __eq__(self, other: Any) -> bool:\n        return bool(self.contract.address == other.contract.address)\n\n    @property  # type: ignore\n    def contract(self) -> Contract:\n        return self.__contract\n\n    @property  # type: ignore\n    def contract_address(self) -> ChecksumAddress:\n        return self.__contract.address"}
{"Repository": "KindleEar", "input": "A mixed list of :class:`~. className TokenList(list) Method line Method column Method as_css", "label": "class TokenList(list):\n    @property\n    def line(self):\n        return self[0].line\n\n    @property\n    def column(self):\n        return self[0].column\n\n    def as_css(self):\n        return ''.join(token.as_css() for token in self)"}
{"Repository": "python-ibmdb", "input": "This is the base class of all other exception thrown by this module. className Error(exception) Method __init__ Method __str__ Attribute _message", "label": "class Error(exception):\n    def __init__(self, message):\n        self._message = message\n        super(Error, self).__init__(message)\n    def __str__(self):\n        return 'ibm_db_dbi::'+str(self.__class__.__name__)+': '+str(self._message)"}
{"Repository": "secretary", "input": "Test encoding of line feed and tab variables valuess className EscapingVariablesValues(TestCase) Method test_encode_linefeed_char Method test_encode_linefeed_char Method test_escape_xml_reserved_chars", "label": "class EscapingVariablesValues(TestCase):\n    def test_encode_linefeed_char(self):\n        xml = 'This\\nLF'\n        expected = 'This<text:line-break/>LF'\n        assert (Renderer.get_escaped_var_value(xml) == expected)\n\n    def test_encode_linefeed_char(self):\n        xml = 'This\\tTab char'\n        expected = 'This<text:tab/>Tab char'\n        assert (Renderer.get_escaped_var_value(xml) == expected)\n\n    def test_escape_xml_reserved_chars(self):\n        ''' Should also escape minor and mayor signs '''\n        xml = '1 is > than 0 & -1 is <'\n        expected = '1 is &gt; than 0 &amp; -1 is &lt;'\n        assert (Renderer.get_escaped_var_value(xml) == expected)"}
{"Repository": "GSM", "input": "Rotate entire clip randomly by a random angle within given bounds Args: degrees (sequence or int): Range of degrees to select from If degrees is a number instead of sequence like (min, max), the range of degrees, will be (-degrees, +degrees). className RandomRotation(object) Method __init__ Method __call__ Attribute degrees", "label": "class RandomRotation(object):\n    def __init__(self, degrees):\n        if isinstance(degrees, numbers.Number):\n            if degrees < 0:\n                raise ValueError('If degrees is a single number,'\n                                 'must be positive')\n            degrees = (-degrees, degrees)\n        else:\n            if len(degrees) != 2:\n                raise ValueError('If degrees is a sequence,'\n                                 'it must be of len 2.')\n\n        self.degrees = degrees\n\n    def __call__(self, clip):\n        angle = random.uniform(self.degrees[0], self.degrees[1])\n        if isinstance(clip[0], np.ndarray):\n            rotated = [rotate(image=img, angle=angle, preserve_range=True) for img in clip]\n        elif isinstance(clip[0], Image.Image):\n            rotated = [img.rotate(angle) for img in clip]\n        else:\n            raise TypeError('Expected numpy.ndarray or PIL.Image' +\n                            'but got list of {0}'.format(type(clip[0])))\n\n        return rotated"}
{"Repository": "faceswap", "input": "Parent class for package installers. className Installer() Method __call__ Method call Method _print_conda Method _print_pip Method _non_gui_print Method _seen_line_log", "label": "class Installer():\n    def __init__(self,\n                 environment: Environment,\n                 package: str,\n                 command: list[str],\n                 is_gui: bool) -> None:\n        logger.info(\"Installing %s\", package)\n        logger.debug(\"argv: %s\", command)\n        self._env = environment\n        self._package = package\n        self._command = command\n        self._is_conda = \"conda\" in command\n        self._is_gui = is_gui\n\n        self._progess_bar = ProgressBar()\n        self._re_conda = re.compile(\n            rb\"(?P<lib>^\\S+)\\s+\\|\\s+(?P<tot>\\d+\\.?\\d*\\s\\w+).*\\|\\s+(?P<prg>\\d+%)\")\n        self._re_pip_pkg = re.compile(rb\"^\\s*Downloading\\s(?P<lib>\\w+-.+?)-\")\n        self._re_pip = re.compile(rb\"(?P<done>\\d+\\.?\\d*)/(?P<tot>\\d+\\.?\\d*\\s\\w+)\")\n        self._pip_pkg = \"\"\n        self._seen_lines: set[str] = set()\n\n    def __call__(self) -> int:\n        try:\n            returncode = self.call()\n        except Exception as err:  # pylint:disable=broad-except\n            logger.debug(\"Failed to install with %s. Falling back to subprocess. Error: %s\",\n                         self.__class__.__name__, str(err))\n            self._progess_bar.close()\n            returncode = SubProcInstaller(self._env, self._package, self._command, self._is_gui)()\n\n        logger.debug(\"Package: %s, returncode: %s\", self._package, returncode)\n        self._progess_bar.close()\n        return returncode\n\n    def call(self) -> int:\n        raise NotImplementedError()\n\n    def _print_conda(self, text: bytes) -> None:\n        data = self._re_conda.match(text)\n        if not data:\n            return\n        lib = data.groupdict()[\"lib\"].decode(\"utf-8\", errors=\"replace\")\n        size = data.groupdict()[\"tot\"].decode(\"utf-8\", errors=\"replace\")\n        progress = int(data.groupdict()[\"prg\"].decode(\"utf-8\", errors=\"replace\")[:-1])\n        self._progess_bar(lib, size, progress)\n\n    def _print_pip(self, text: bytes) -> None:\n        pkg = self._re_pip_pkg.match(text)\n        if pkg:\n            logger.debug(\"Collected pip package '%s'\", pkg)\n            self._pip_pkg = pkg.groupdict()[\"lib\"].decode(\"utf-8\", errors=\"replace\")\n            return\n        data = self._re_pip.search(text)\n        if not data:\n            return\n        done = float(data.groupdict()[\"done\"].decode(\"utf-8\", errors=\"replace\"))\n        size = data.groupdict()[\"tot\"].decode(\"utf-8\", errors=\"replace\")\n        progress = int(round(done / float(size.split()[0]) * 100, 0))\n        self._progess_bar(self._pip_pkg, size, progress)\n\n    def _non_gui_print(self, text: bytes) -> None:\n        if self._is_gui:\n            return\n        if self._is_conda:\n            self._print_conda(text)\n        else:\n            self._print_pip(text)\n\n    def _seen_line_log(self, text: str) -> None:\n        if text in self._seen_lines:\n            return\n        logger.debug(text)\n        self._seen_lines.add(text)"}
{"Repository": "Snowfakery", "input": "A value with no sub-structure (although it could hold a formula) className SimpleValue(FieldDefinition) Method __init__ Method evaluator Method render Method __repr__ Attribute filename Attribute line_num Attribute _evaluator", "label": "class SimpleValue(FieldDefinition):\n    def __init__(self, definition: Scalar, filename: str, line_num: int):\n        self.filename = filename\n        self.line_num = line_num\n        self.definition: Scalar = definition\n        assert isinstance(filename, str)\n        assert isinstance(line_num, int), line_num\n        self._evaluator = None\n\n    def evaluator(self, context):\n        if self._evaluator is None:\n            if isinstance(self.definition, str):\n                with self.exception_handling(\"Cannot parse value {}\", self.definition):\n                    self._evaluator = context.get_evaluator(self.definition)\n            else:\n                self._evaluator = False\n        return self._evaluator\n\n    def render(self, context: RuntimeContext) -> FieldValue:\n        old_context_identifier = context.unique_context_identifier\n        context.unique_context_identifier = str(id(self))\n        evaluator = self.evaluator(context)\n        if evaluator:\n            try:\n                val = evaluator(context)\n                if hasattr(val, \"render\"):\n                    val = val.render()\n            except jinja2.exceptions.UndefinedError as e:\n                raise DataGenNameError(e.message, self.filename, self.line_num) from e\n            except Exception as e:\n                raise DataGenValueError(str(e), self.filename, self.line_num) from e\n        else:\n            val = self.definition\n        context.unique_context_identifier = old_context_identifier\n        if isinstance(val, str) and not context.interpreter.native_types:\n            val = look_for_number(val)\n        return val\n\n    def __repr__(self):\n        return f\"<{self.__class__.__name__ , self.definition}>\""}
{"Repository": "MANIQA", "input": "LARS for PyTorch Paper: `Large batch training of Convolutional Networks` - https://arxiv. className Lars(Optimizer) Method __setstate__ Method step", "label": "class Lars(Optimizer):\n    def __init__(\n        self,\n        params,\n        lr=1.0,\n        momentum=0,\n        dampening=0,\n        weight_decay=0,\n        nesterov=False,\n        trust_coeff=0.001,\n        eps=1e-8,\n        trust_clip=False,\n        always_adapt=False,\n    ):\n        if lr < 0.0:\n            raise ValueError(f\"Invalid learning rate: {lr}\")\n        if momentum < 0.0:\n            raise ValueError(f\"Invalid momentum value: {momentum}\")\n        if weight_decay < 0.0:\n            raise ValueError(f\"Invalid weight_decay value: {weight_decay}\")\n        if nesterov and (momentum <= 0 or dampening != 0):\n            raise ValueError(\"Nesterov momentum requires a momentum and zero dampening\")\n\n        defaults = dict(\n            lr=lr,\n            momentum=momentum,\n            dampening=dampening,\n            weight_decay=weight_decay,\n            nesterov=nesterov,\n            trust_coeff=trust_coeff,\n            eps=eps,\n            trust_clip=trust_clip,\n            always_adapt=always_adapt,\n        )\n        super().__init__(params, defaults)\n\n    def __setstate__(self, state):\n        super().__setstate__(state)\n        for group in self.param_groups:\n            group.setdefault(\"nesterov\", False)\n\n    @torch.no_grad()\n    def step(self, closure=None):\n        loss = None\n        if closure is not None:\n            with torch.enable_grad():\n                loss = closure()\n\n        device = self.param_groups[0]['params'][0].device\n        one_tensor = torch.tensor(1.0, device=device)  # because torch.where doesn't handle scalars correctly\n\n        for group in self.param_groups:\n            weight_decay = group['weight_decay']\n            momentum = group['momentum']\n            dampening = group['dampening']\n            nesterov = group['nesterov']\n            trust_coeff = group['trust_coeff']\n            eps = group['eps']\n\n            for p in group['params']:\n                if p.grad is None:\n                    continue\n                grad = p.grad\n\n                # apply LARS LR adaptation, LARC clipping, weight decay\n                # ref: https://github.com/NVIDIA/apex/blob/master/apex/parallel/LARC.py\n                if weight_decay != 0 or group['always_adapt']:\n                    w_norm = p.norm(2.0)\n                    g_norm = grad.norm(2.0)\n                    trust_ratio = trust_coeff * w_norm / (g_norm + w_norm * weight_decay + eps)\n                    # FIXME nested where required since logical and/or not working in PT XLA\n                    trust_ratio = torch.where(\n                        w_norm > 0,\n                        torch.where(g_norm > 0, trust_ratio, one_tensor),\n                        one_tensor,\n                    )\n                    if group['trust_clip']:\n                        trust_ratio = torch.minimum(trust_ratio / group['lr'], one_tensor)\n                    grad.add_(p, alpha=weight_decay)\n                    grad.mul_(trust_ratio)\n\n                # apply SGD update https://github.com/pytorch/pytorch/blob/1.7/torch/optim/sgd.py#L100\n                if momentum != 0:\n                    param_state = self.state[p]\n                    if 'momentum_buffer' not in param_state:\n                        buf = param_state['momentum_buffer'] = torch.clone(grad).detach()\n                    else:\n                        buf = param_state['momentum_buffer']\n                        buf.mul_(momentum).add_(grad, alpha=1. - dampening)\n                    if nesterov:\n                        grad = grad.add(buf, alpha=momentum)\n                    else:\n                        grad = buf\n\n                p.add_(grad, alpha=-group['lr'])\n\n        return loss"}
{"Repository": "EarthSim", "input": "Custom installation for development mode. className CustomDevelopCommand(develop) Method run", "label": "class CustomDevelopCommand(develop):\n    def run(self):\n        try:\n            print(\"Building custom models:\")\n            build_custom_models()\n        except ImportError as e:\n            print(\"Custom model compilation failed with: %s\" % e)\n        develop.run(self)"}
{"Repository": "Gnip-Trend-Detection", "input": "Derived \"list\" class, with ability to return className TopicSeries(list) Method get_subseries", "label": "class TopicSeries(list):\n    def get_subseries(self,length):\n        index = 0\n        while index + length <= len(self):\n            yield self[index:index+length]\n            index += 1"}
{"Repository": "CAPEv2", "input": "Cuckoo custom dict. className Dictionary(dict) Method __deepcopy__ Method __getattr__", "label": "class Dictionary(dict):\n    def __deepcopy__(self, memo=None):\n        new = self.__class__()\n        for key, value in self.items():\n            new[key] = copy.deepcopy(value, memo=memo)\n        return new\n\n    def __getattr__(self, key):\n        return self.get(key)\n\n    __setattr__ = dict.__setitem__\n    __delattr__ = dict.__delitem__"}
{"Repository": "albert_vi", "input": "Processor for the MRPC data set (GLUE version). className MrpcProcessor(DataProcessor) Method get_train_examples Method get_dev_examples Method get_test_examples Method get_labels Method _create_examples", "label": "class MrpcProcessor(DataProcessor):\n  def get_train_examples(self, data_dir):\n    return self._create_examples(\n        self._read_tsv(os.path.join(data_dir, \"MRPC\", \"train.tsv\")), \"train\")\n\n  def get_dev_examples(self, data_dir):\n    return self._create_examples(\n        self._read_tsv(os.path.join(data_dir, \"MRPC\", \"dev.tsv\")), \"dev\")\n\n  def get_test_examples(self, data_dir):\n    return self._create_examples(\n        self._read_tsv(os.path.join(data_dir, \"MRPC\", \"test.tsv\")), \"test\")\n\n  def get_labels(self):\n    return [\"0\", \"1\"]\n\n  def _create_examples(self, lines, set_type):\n    examples = []\n    for (i, line) in enumerate(lines):\n      if i == 0:\n        continue\n      guid = \"%s-%s\" % (set_type, i)\n      text_a = self.process_text(line[3])\n      text_b = self.process_text(line[4])\n      if set_type == \"test\":\n        guid = line[0]\n        label = \"0\"\n      else:\n        label = self.process_text(line[0])\n      examples.append(\n          InputExample(guid=guid, text_a=text_a, text_b=text_b, label=label))\n    return examples"}
{"Repository": "VintageEx", "input": "Does it make sense? className test(Command) Method initialize_options Method finalize_options Method run", "label": "class test(Command):\n    user_options = [('aa', 'a', 'aa')]\n\n\n\n    def initialize_options(self):\n\n        pass\n\n\n\n    def finalize_options(self):\n\n        pass\n\n\n\n    def run(self):\n\n        if os.name == 'nt':\n\n            subprocess.call([\"py.test.exe\"])"}
{"Repository": "multilingual-kd-pytorch", "input": "Takes a text file as input and binarizes it in memory at instantiation. className IndexedRawTextDataset(IndexedDataset) Method __init__ Method read_data Method __getitem__ Method get_original_text Method __del__ Method __len__ Method exists Attribute tokens_list Attribute lines Attribute sizes Attribute append_eos Attribute reverse_order Attribute size", "label": "class IndexedRawTextDataset(IndexedDataset):\n    def __init__(self, path, dictionary, append_eos=True, reverse_order=False):\n        self.tokens_list = []\n        self.lines = []\n        self.sizes = []\n        self.append_eos = append_eos\n        self.reverse_order = reverse_order\n        self.read_data(path, dictionary)\n        self.size = len(self.tokens_list)\n\n    def read_data(self, path, dictionary):\n        with open(path, 'r') as f:\n            for line in f:\n                self.lines.append(line.strip('\\n'))\n                tokens = Tokenizer.tokenize(\n                    line, dictionary, add_if_not_exist=False,\n                    append_eos=self.append_eos, reverse_order=self.reverse_order,\n                ).long()\n                self.tokens_list.append(tokens)\n                self.sizes.append(len(tokens))\n        self.sizes = np.array(self.sizes)\n\n    def __getitem__(self, i):\n        self.check_index(i)\n        return self.tokens_list[i]\n\n    def get_original_text(self, i):\n        self.check_index(i)\n        return self.lines[i]\n\n    def __del__(self):\n        pass\n\n    def __len__(self):\n        return self.size\n\n    @staticmethod\n    def exists(path):\n        return os.path.exists(path)"}
{"Repository": "Option-Pricing", "input": "Valuation of European call options in Black-Scholes-Merton Model (for forex) Garman, M. className GarmanKohlhagenForex(BSMOptionValuation) Method __init__ Method call_value Method put_value Attribute S0 Attribute K Attribute T Attribute rf Attribute rd Attribute sigma Attribute d1 Attribute d2", "label": "class GarmanKohlhagenForex(BSMOptionValuation):\n    def __init__(self, S0, K, T, rd, rf, sigma):\n        BSMOptionValuation.__init__(S0, S0, K, T, rd, rf, sigma)\n        assert sigma >= 0, 'volatility cannot be less than zero'\n        assert S0 >= 0, 'initial stock price cannot be less than zero'\n        assert T >= 0, 'time to maturity cannot be less than zero'\n\n        self.S0 = float(S0)\n        self.K = float(K)\n        self.T = float(T)\n        self.rf = float(rf)\n        self.rd = float(rd)\n        self.sigma = float(sigma)\n\n        self.d1 = ((log(self.S0 / self.K) + (self.rd - self.rf + 0.5 * self.sigma ** 2) * self.T) / (\n                self.sigma * sqrt(self.T)))\n\n        self.d2 = self.d1 - self.sigma * sqrt(self.T)\n\n    def call_value(self, empirical_put_price=None):\n        if empirical_put_price is None:\n            call_value = (self.S0 * exp(- self.rf * self.T) * stats.norm.cdf(self.d1, 0.0, 1.0) - self.K * exp(\n                - self.rd * self.T) * stats.norm.cdf(self.d2, 0.0, 1.0))\n        else:\n            call_value = empirical_put_price + exp(-self.div_yield * self.T) * self.S0 - exp(-self.r * self.T) * self.K\n\n        return call_value\n\n    def put_value(self, empirical_call_price=None):\n        if empirical_call_price is None:\n            put_value = self.K * exp(- self.rd * self.T) * stats.norm.cdf(- self.d2, 0.0, 1.0) - self.S0 * exp(\n                - self.rf * self.T) * stats.norm.cdf(- self.d1, 0.0, 1.0)\n        else:\n            put_value = empirical_call_price + exp(-self.r * self.T) * self.K - exp(-self.div_yield * self.T) * self.S0\n\n        return put_value"}
{"Repository": "STANet", "input": "This dataset class can load a set of images specified by the path --dataroot /path/to/data. className ChangeDetectionDataset(BaseDataset) Method __init__ Method __getitem__ Method __len__ Attribute istest Attribute istest Attribute A_paths Attribute B_paths Attribute L_paths", "label": "class ChangeDetectionDataset(BaseDataset):\n    def __init__(self, opt):\n        BaseDataset.__init__(self, opt)\n        folder_A = 'A'\n        folder_B = 'B'\n        folder_L = 'label'\n        self.istest = False\n        if opt.phase == 'test':\n            self.istest = True\n        self.A_paths = sorted(make_dataset(os.path.join(opt.dataroot, folder_A), opt.max_dataset_size))\n        self.B_paths = sorted(make_dataset(os.path.join(opt.dataroot, folder_B), opt.max_dataset_size))\n        if not self.istest:\n            self.L_paths = sorted(make_dataset(os.path.join(opt.dataroot, folder_L), opt.max_dataset_size))\n\n        print(self.A_paths)\n\n    def __getitem__(self, index):\n        A_path = self.A_paths[index]\n        B_path = self.B_paths[index]\n        A_img = Image.open(A_path).convert('RGB')\n        B_img = Image.open(B_path).convert('RGB')\n\n        transform_params = get_params(self.opt, A_img.size, test=self.istest)\n        # apply the same transform to A B L\n        transform = get_transform(self.opt, transform_params, test=self.istest)\n\n        A = transform(A_img)\n        B = transform(B_img)\n\n        if self.istest:\n            return {'A': A, 'A_paths': A_path, 'B': B, 'B_paths': B_path}\n\n        L_path = self.L_paths[index]\n        tmp = np.array(Image.open(L_path), dtype=np.uint32)/255\n        L_img = Image.fromarray(tmp)\n        transform_L = get_transform(self.opt, transform_params, method=Image.NEAREST, normalize=False,\n                                    test=self.istest)\n        L = transform_L(L_img)\n\n\n        return {'A': A, 'A_paths': A_path,\n                'B': B, 'B_paths': B_path,\n                'L': L, 'L_paths': L_path}\n\n    def __len__(self):\n        return len(self.A_paths)"}
{"Repository": "SA-BERT", "input": "Runs WordPiece tokenziation. className WordpieceTokenizer(object) Method __init__ Method tokenize Attribute vocab Attribute unk_token Attribute max_input_chars_per_word", "label": "class WordpieceTokenizer(object):\n  def __init__(self, vocab, unk_token=\"[UNK]\", max_input_chars_per_word=200):\n    self.vocab = vocab\n    self.unk_token = unk_token\n    self.max_input_chars_per_word = max_input_chars_per_word\n\n  def tokenize(self, text):\n    text = convert_to_unicode(text)\n\n    output_tokens = []\n    for token in whitespace_tokenize(text):\n      chars = list(token)\n      if len(chars) > self.max_input_chars_per_word:\n        output_tokens.append(self.unk_token)\n        continue\n\n      is_bad = False\n      start = 0\n      sub_tokens = []\n      while start < len(chars):\n        end = len(chars)\n        cur_substr = None\n        while start < end:\n          substr = \"\".join(chars[start:end])\n          if start > 0:\n            substr = \"##\" + substr\n          if substr in self.vocab:\n            cur_substr = substr\n            break\n          end -= 1\n        if cur_substr is None:\n          is_bad = True\n          break\n        sub_tokens.append(cur_substr)\n        start = end\n\n      if is_bad:\n        output_tokens.append(self.unk_token)\n      else:\n        output_tokens.extend(sub_tokens)\n    return output_tokens"}
{"Repository": "observable", "input": "A property that can be observed easily by listening for some special, auto-generated events. className ObservableProperty(property) Method __delete__ Method __get__ Method __set__", "label": "class ObservableProperty(property):\n    def __init__(\n            self, *args: T.Any,\n            event: str = None, observable: T.Union[Observable, str] = None,\n            **kwargs: T.Any\n    ) -> None:\n        super().__init__(*args, **kwargs)\n        self.event = event\n        self.observable = observable\n\n    def __delete__(self, instance: T.Any) -> None:\n        if self.fdel is not None:\n            self._trigger_event(instance, self.fdel.__name__, \"before_del\")\n        super().__delete__(instance)\n        self._trigger_event(instance, self.fdel.__name__, \"after_del\")\n\n    def __get__(self, instance: T.Any, owner: T.Any = None) -> T.Any:\n        if instance is None:\n            return super().__get__(instance, owner)\n        if self.fget is not None:\n            self._trigger_event(instance, self.fget.__name__, \"before_get\")\n        value = super().__get__(instance, owner)\n        if instance is None:\n            return value\n        self._trigger_event(instance, self.fget.__name__, \"after_get\", value)\n        return value\n\n    def __set__(self, instance: T.Any, value: T.Any) -> None:\n        if self.fset is not None:\n            self._trigger_event(instance, self.fset.__name__,\n                                \"before_set\", value)\n        super().__set__(instance, value)\n        self._trigger_event(instance, self.fset.__name__, \"after_set\", value)\n\n    def _trigger_event(\n            self, holder: T.Any, alt_name: str, action: str, *event_args: T.Any\n    ) -> None:\n        if isinstance(self.observable, Observable):\n            observable = self.observable\n        elif isinstance(self.observable, str):\n            observable = getattr(holder, self.observable)\n        elif isinstance(holder, Observable):\n            observable = holder\n        else:\n            raise TypeError(\n                \"This ObservableProperty is no member of an Observable \"\n                \"object. Specify where to find the Observable object for \"\n                \"triggering events with the observable keyword argument \"\n                \"when initializing the ObservableProperty.\"\n            )\n\n        name = alt_name if self.event is None else self.event\n        event = \"{}_{}\".format(action, name)\n        observable.trigger(event, *event_args)\n\n    deleter = _preserve_settings(property.deleter)\n    getter = _preserve_settings(property.getter)\n    setter = _preserve_settings(property.setter)\n\n    @classmethod\n    def create_with(\n            cls, event: str = None, observable: T.Union[str, Observable] = None\n    ) -> T.Callable[..., \"ObservableProperty\"]:\n        return functools.partial(cls, event=event, observable=observable)"}
{"Repository": "snakes", "input": "A :class:`NodeVisitor` subclass that walks the abstract syntax tree and allows modification of nodes. className NodeTransformer(NodeVisitor) Method visit_Name Method generic_visit", "label": "class NodeTransformer(NodeVisitor):\n    def generic_visit(self, node):\n        for field, old_value in iter_fields(node):\n            old_value = getattr(node, field, None)\n            if isinstance(old_value, list):\n                new_values = []\n                for value in old_value:\n                    if isinstance(value, AST):\n                        value = self.visit(value)\n                        if value is None:\n                            continue\n                        elif not isinstance(value, AST):\n                            new_values.extend(value)\n                            continue\n                    new_values.append(value)\n                old_value[:] = new_values\n            elif isinstance(old_value, AST):\n                new_node = self.visit(old_value)\n                if new_node is None:\n                    delattr(node, field)\n                else:\n                    setattr(node, field, new_node)\n        return node"}
{"Repository": "Expy-Kit", "input": "Retarget preset for deformation bones className VIEW3D_MT_DeformPreset(Menu) Method draw", "label": "class VIEW3D_MT_DeformPreset(Menu):\n    bl_label = \"\"\n    item_operator = MenuItemOperator.bl_idname\n\n    target_object = \"context.object.data.expykit_retarget\"\n    target_attr = \"deform_preset\"\n\n    def draw(self, context):\n        items = list(preset_handler.iterate_presets(context.scene, context))\n\n        layout = self.layout\n        col = layout.column(align=True)\n\n        for (raw, display, _) in items:\n            row = col.row(align=True)\n            name = display or raw\n            props = row.operator(\n                self.item_operator,\n                text=name,\n                translate=False,\n            )\n            props.menu_idname = self.bl_idname\n            props.target_object = self.target_object\n            props.target_attr = self.target_attr\n            props.item_value = raw"}
{"Repository": "Tor", "input": "class to setup and start tor sevices className TorServiceSetup(object) Method StartService", "label": "class TorServiceSetup(object):\n\tdef StartService(self):\n\t\tTorProxy = Proxy()\n\t\tprint(torBanner)\n\t\tTorProxy.ConfigureTor"}
{"Repository": "PyHive", "input": "Attributes: className GetInfo_args(object) Method __init__ Method read Method write Method validate Method __repr__ Method __eq__ Method __ne__ Attribute req", "label": "class GetInfo_args(object):\n    thrift_spec = (\n        None,  # 0\n        (1, TType.STRUCT, 'req', (TGetInfoReq, TGetInfoReq.thrift_spec), None, ),  # 1\n    )\n\n    def __init__(self, req=None,):\n        self.req = req\n\n    def read(self, iprot):\n        if iprot._fast_decode is not None and isinstance(iprot.trans, TTransport.CReadableTransport) and self.thrift_spec is not None:\n            iprot._fast_decode(self, iprot, (self.__class__, self.thrift_spec))\n            return\n        iprot.readStructBegin()\n        while True:\n            (fname, ftype, fid) = iprot.readFieldBegin()\n            if ftype == TType.STOP:\n                break\n            if fid == 1:\n                if ftype == TType.STRUCT:\n                    self.req = TGetInfoReq()\n                    self.req.read(iprot)\n                else:\n                    iprot.skip(ftype)\n            else:\n                iprot.skip(ftype)\n            iprot.readFieldEnd()\n        iprot.readStructEnd()\n\n    def write(self, oprot):\n        if oprot._fast_encode is not None and self.thrift_spec is not None:\n            oprot.trans.write(oprot._fast_encode(self, (self.__class__, self.thrift_spec)))\n            return\n        oprot.writeStructBegin('GetInfo_args')\n        if self.req is not None:\n            oprot.writeFieldBegin('req', TType.STRUCT, 1)\n            self.req.write(oprot)\n            oprot.writeFieldEnd()\n        oprot.writeFieldStop()\n        oprot.writeStructEnd()\n\n    def validate(self):\n        return\n\n    def __repr__(self):\n        L = ['%s=%r' % (key, value)\n             for key, value in self.__dict__.items()]\n        return '%s(%s)' % (self.__class__.__name__, ', '.join(L))\n\n    def __eq__(self, other):\n        return isinstance(other, self.__class__) and self.__dict__ == other.__dict__\n\n    def __ne__(self, other):\n        return not (self == other)"}
{"Repository": "openseg.pytorch", "input": "Crop the given numpy. className RandomCrop(_BaseTransform) Method __init__ Method get_lefttop Method _crop Method _process_img Method _process_labelmap Method _process_region_indexmap Method _process_maskmap Method _process_distance_map Method _process_angle_map Method _process_boundary_map Method _process_multi_label_direction_map Method _process_offsetmap Method __call__ Attribute ratio Attribute method Attribute grid Attribute allow_outside_center Attribute size Attribute size", "label": "class RandomCrop(_BaseTransform):\n    def __init__(self, crop_size, crop_ratio=0.5, method='random', grid=None, allow_outside_center=True):\n        self.ratio = crop_ratio\n        self.method = method\n        self.grid = grid\n        self.allow_outside_center = allow_outside_center\n\n        if isinstance(crop_size, float):\n            self.size = (crop_size, crop_size)\n        elif isinstance(crop_size, collections.Iterable) and len(crop_size) == 2:\n            self.size = crop_size\n        else:\n            raise TypeError('Got inappropriate size arg: {}'.format(crop_size))\n\n    def get_lefttop(self, crop_size, img_size):\n        if self.method == 'center':\n            return [(img_size[0] - crop_size[0]) // 2, (img_size[1] - crop_size[1]) // 2]\n\n        elif self.method == 'random':\n            x = random.randint(0, img_size[0] - crop_size[0])\n            y = random.randint(0, img_size[1] - crop_size[1])\n            return [x, y]\n\n        elif self.method == 'grid':\n            grid_x = random.randint(0, self.grid[0] - 1)\n            grid_y = random.randint(0, self.grid[1] - 1)\n            x = grid_x * ((img_size[0] - crop_size[0]) // (self.grid[0] - 1))\n            y = grid_y * ((img_size[1] - crop_size[1]) // (self.grid[1] - 1))\n            return [x, y]\n\n        else:\n            Log.error('Crop method {} is invalid.'.format(self.method))\n            exit(1)\n\n    def _crop(self, x, offset_up, offset_left, target_size):\n        return x[offset_up:offset_up + target_size[1], offset_left:offset_left + target_size[0]]\n\n    def _process_img(self, img, *args):\n        return self._crop(img, *args)\n\n    def _process_labelmap(self, x, *args):\n        return self._crop(x, *args)\n\n    def _process_region_indexmap(self, x, *args):\n        return self._crop(x, *args)\n\n    def _process_maskmap(self, x, *args):\n        return self._crop(x, *args)\n\n    def _process_distance_map(self, x, *args):\n        return self._crop(x, *args)\n\n    def _process_angle_map(self, x, *args):\n        return self._crop(x, *args)\n\n    def _process_boundary_map(self, x, *args):\n        return self._crop(x, *args)\n\n    def _process_multi_label_direction_map(self, x, *args):\n        return self._crop(x, *args)\n\n    # def _process_offsetmap_h(self, x, *args):\n    #     return self._crop(x, *args)\n\n    # def _process_offsetmap_w(self, x, *args):\n    #     return self._crop(x, *args)\n\n    def _process_offsetmap(self, x, *args):\n        return self._crop(x, *args)\n\n    def __call__(self, img, **kwargs):\n        img, data_dict = super().__call__(img, **kwargs)\n\n        height, width, _ = img.shape\n        target_size = [min(self.size[0], width), min(self.size[1], height)]\n\n        offset_left, offset_up = self.get_lefttop(target_size, [width, height])\n        return self._process(\n            img, data_dict,\n            random.random() > self.ratio,\n            offset_up, offset_left, target_size\n        )"}
{"Repository": "yadisk", "input": "A request to revoke the token. className RevokeTokenRequest(APIRequest) Method process_args Method process_json", "label": "class RevokeTokenRequest(APIRequest):\n    url = \"https://oauth.yandex.ru/revoke_token\"\n    method = \"POST\"\n\n    def __init__(self,\n                 session: \"AnySession\",\n                 token: str,\n                 client_id: str,\n                 client_secret: str, **kwargs):\n        APIRequest.__init__(self, session, {\"token\":         token,\n                                            \"client_id\":     client_id,\n                                            \"client_secret\": client_secret}, **kwargs)\n\n    def process_args(self, token: str, client_id: str, client_secret: str) -> None:\n        self.data = urlencode({\n            \"access_token\":  token,\n            \"client_id\":     client_id,\n            \"client_secret\": client_secret\n        }).encode(\"utf8\")\n\n    def process_json(self, js: JSON, **kwargs) -> TokenRevokeStatusObject:\n        if not isinstance(js, dict):\n            raise InvalidResponseError(\"Yandex.Disk did not return valid JSON\")\n\n        return TokenRevokeStatusObject(js)"}
{"Repository": "Sublime-Tweet", "input": "Response from a twitter request. className TwitterResponse(object) Method __init__ Method rate_limit_remaining Method rate_limit_limit Method rate_limit_reset Attribute headers", "label": "class TwitterResponse(object):\n    def __init__(self, headers):\n        self.headers = headers\n\n    @property\n    def rate_limit_remaining(self):\n        return int(self.headers.get('X-Rate-Limit-Remaining', \"0\"))\n\n    @property\n    def rate_limit_limit(self):\n        return int(self.headers.get('X-Rate-Limit-Limit', \"0\"))\n\n    @property\n    def rate_limit_reset(self):\n        return int(self.headers.get('X-Rate-Limit-Reset', \"0\"))"}
{"Repository": "F-LSeSim", "input": "A dataset class for paired image dataset. className AlignedDataset(BaseDataset) Method __init__ Method __getitem__ Method __len__ Attribute dir_AB Attribute AB_paths Attribute input_nc Attribute output_nc", "label": "class AlignedDataset(BaseDataset):\n    def __init__(self, opt):\n        BaseDataset.__init__(self, opt)\n        self.dir_AB = os.path.join(opt.dataroot, opt.phase)  # get the image directory\n        self.AB_paths = sorted(make_dataset(self.dir_AB, opt.max_dataset_size))  # get image paths\n        assert(self.opt.load_size >= self.opt.crop_size)   # crop_size should be smaller than the size of loaded image\n        self.input_nc = self.opt.output_nc if self.opt.direction == 'BtoA' else self.opt.input_nc\n        self.output_nc = self.opt.input_nc if self.opt.direction == 'BtoA' else self.opt.output_nc\n\n    def __getitem__(self, index):\n        # read a image given a random integer index\n        AB_path = self.AB_paths[index]\n        AB = Image.open(AB_path).convert('RGB')\n        # split AB image into A and B\n        w, h = AB.size\n        w2 = int(w / 2)\n        A = AB.crop((0, 0, w2, h))\n        B = AB.crop((w2, 0, w, h))\n\n        # apply the same transform to both A and B\n        transform_params = get_params(self.opt, A.size)\n        A_transform = get_transform(self.opt, transform_params, grayscale=(self.input_nc == 1))\n        B_transform = get_transform(self.opt, transform_params, grayscale=(self.output_nc == 1))\n\n        A = A_transform(A)\n        B = B_transform(B)\n\n        return {'A': A, 'B': B, 'A_paths': AB_path, 'B_paths': AB_path}\n\n    def __len__(self):\n        return len(self.AB_paths)"}
{"Repository": "BridgeTower", "input": "Reference : https://github.com/quark0/darts/blob/master/cnn/utils.py className CutoutDefault(object) Method __init__ Method __call__ Attribute length", "label": "class CutoutDefault(object):\n    def __init__(self, length):\n        self.length = length\n\n    def __call__(self, img):\n        h, w = img.size(1), img.size(2)\n        mask = np.ones((h, w), np.float32)\n        y = np.random.randint(h)\n        x = np.random.randint(w)\n\n        y1 = np.clip(y - self.length // 2, 0, h)\n        y2 = np.clip(y + self.length // 2, 0, h)\n        x1 = np.clip(x - self.length // 2, 0, w)\n        x2 = np.clip(x + self.length // 2, 0, w)\n\n        mask[y1:y2, x1:x2] = 0.0\n        mask = torch.from_numpy(mask)\n        mask = mask.expand_as(img)\n        img *= mask\n        return img"}
{"Repository": "django-elasticsearch-dsl-drf", "input": "Highlight backend. className HighlightBackend(BaseFilterBackend) Method prepare_highlight_fields Method get_highlight_query_params Method filter_queryset", "label": "class HighlightBackend(BaseFilterBackend):\n    highlight_param = 'highlight'\n\n    @classmethod\n    def prepare_highlight_fields(cls, view):\n        highlight_fields = view.highlight_fields\n\n        for field, options in highlight_fields.items():\n            if 'enabled' not in highlight_fields[field]:\n                highlight_fields[field]['enabled'] = False\n\n            if 'options' not in highlight_fields[field]:\n                highlight_fields[field]['options'] = {}\n\n        return highlight_fields\n\n    def get_highlight_query_params(self, request):\n        query_params = request.query_params.copy()\n        return query_params.getlist(self.highlight_param, [])\n\n    def filter_queryset(self, request, queryset, view):\n        highlight_query_params = self.get_highlight_query_params(request)\n        highlight_fields = self.prepare_highlight_fields(view)\n        for __field, __options in highlight_fields.items():\n            if __field in highlight_query_params or __options['enabled']:\n                queryset = queryset.highlight(__field, **__options['options'])\n\n        return queryset"}
{"Repository": "pipeline-ChatGLM", "input": "Preprocess document input from image/image url/image bytestream to ocr outputs className DocOCRProcessor(BaseComponent) Method __init__ Method _check_input_text Method run Attribute _lang Attribute _use_gpu Attribute _ocr", "label": "class DocOCRProcessor(BaseComponent):\n    return_no_answers: bool\n    outgoing_edges = 1\n    query_count = 0\n    query_time = 0\n\n    def __init__(self, use_gpu: bool = True, lang: str = \"ch\"):\n        self._lang = lang\n        self._use_gpu = False if paddle.get_device() == \"cpu\" else use_gpu\n        self._ocr = PaddleOCR(use_angle_cls=True, show_log=False, use_gpu=self._use_gpu, lang=self._lang)\n\n    def _check_input_text(self, inputs):\n        if isinstance(inputs, dict):\n            inputs = [inputs]\n        if isinstance(inputs, list):\n            input_list = []\n            for example in inputs:\n                data = {}\n                if isinstance(example, dict):\n                    if \"doc\" not in example.keys():\n                        raise ValueError(\n                            \"Invalid inputs, the inputs should contain an url to an image or a local path.\"\n                        )\n                    else:\n                        if isinstance(example[\"doc\"], str):\n\n                            if example[\"doc\"].startswith(\"http://\") or example[\"doc\"].startswith(\"https://\"):\n                                download_file(\"./\", example[\"doc\"].rsplit(\"/\", 1)[-1], example[\"doc\"])\n                                data[\"doc\"] = example[\"doc\"].rsplit(\"/\", 1)[-1]\n                            elif os.path.isfile(example[\"doc\"]):\n                                data[\"doc\"] = example[\"doc\"]\n                            else:\n                                img = base64.b64decode(example[\"doc\"].encode(\"utf-8\"))\n                                img = np.frombuffer(bytearray(img), dtype=\"uint8\")\n                                img = np.array(Image.open(BytesIO(img)).convert(\"RGB\"))\n                                img = Image.fromarray(img)\n                                img.save(\"./tmp.jpg\")\n                                data[\"doc\"] = \"./tmp.jpg\"\n                        else:\n                            raise ValueError(\"Incorrect path or url, URLs must start with `http://` or `https://`\")\n                    if \"prompt\" not in example.keys():\n                        raise ValueError(\"Invalid inputs, the inputs should contain the prompt.\")\n                    else:\n                        if isinstance(example[\"prompt\"], str):\n                            data[\"prompt\"] = [example[\"prompt\"]]\n                        elif isinstance(example[\"prompt\"], list) and all(\n                            isinstance(s, str) for s in example[\"prompt\"]\n                        ):\n                            data[\"prompt\"] = example[\"prompt\"]\n                        else:\n                            raise TypeError(\"Incorrect prompt, prompt should be string or list of string.\")\n                    if \"word_boxes\" in example.keys():\n                        data[\"word_boxes\"] = example[\"word_boxes\"]\n                    input_list.append(data)\n                else:\n                    raise TypeError(\n                        \"Invalid inputs, input for document intelligence task should be dict or list of dict, but type of {} found!\".format(\n                            type(example)\n                        )\n                    )\n        else:\n            raise TypeError(\n                \"Invalid inputs, input for document intelligence task should be dict or list of dict, but type of {} found!\".format(\n                    type(inputs)\n                )\n            )\n        return input_list\n\n    def run(self, meta: dict):\n        example = self._check_input_text(meta)[0]\n\n        if \"word_boxes\" in example.keys():\n            ocr_result = example[\"word_boxes\"]\n            example[\"ocr_type\"] = \"word_boxes\"\n        else:\n            ocr_result = self._ocr.ocr(example[\"doc\"], cls=True)\n            example[\"ocr_type\"] = \"ppocr\"\n            # Compatible with paddleocr>=2.6.0.2\n            ocr_result = ocr_result[0] if len(ocr_result) == 1 else ocr_result\n        example[\"ocr_result\"] = ocr_result\n        output = {\"example\": example}\n        return output, \"output_1\""}
{"Repository": "adversarial", "input": "Hard \"sigmoid\" (note: shifted along the x axis) className HardSigmoid(Linear) Method __init__ Method fprop Method cost Attribute left_slope", "label": "class HardSigmoid(Linear):\n    def __init__(self, left_slope=0.0, **kwargs):\n        super(HardSigmoid, self).__init__(**kwargs)\n        self.left_slope = left_slope\n\n    @wraps(Layer.fprop)\n    def fprop(self, state_below):\n\n        p = self._linear_part(state_below)\n        # Original: p = p * (p > 0.) + self.left_slope * p * (p < 0.)\n        # T.switch is faster.\n        # For details, see benchmarks in\n        # pylearn2/scripts/benchmark/time_relu.py\n        p = T.clip(p, 0., 1.)\n        return p\n\n    @wraps(Layer.cost)\n    def cost(self, *args, **kwargs):\n\n        raise NotImplementedError()"}
{"Repository": "pyinfra", "input": "an SSHConfig that supports includes directives https://github. className SSHConfig(ParamikoSSHConfig) Method parse", "label": "class SSHConfig(ParamikoSSHConfig):\n    def parse(self, file_obj):\n        file_obj = _expand_include_statements(file_obj)\n        return super().parse(file_obj)"}
{"Repository": "GA-DDPG", "input": "Wrapper class for agent training and logging className AgentWrapper(object) Method get_agent_update_step Method get_agent_incr_update_step Method load_weight Method get_weight Method save_model Method get_agent Method update_parameter Method get_agent_lr_info", "label": "class AgentWrapper(object):\n    def __init__(self, args_, config_,  pretrained_path=None, input_dim=512,\n                       logdir=None, set_init_step=False, model_surfix='latest', model_path=None, buffer_id=None):\n\n        from core.bc import BC\n        from core.ddpg import DDPG\n        from core import networks\n        self.args = args_\n        self.config = config_.RL_TRAIN\n        self.cfg = config_ # the global one\n        torch.manual_seed(self.args.seed)\n        POLICY = self.args.policy\n\n        self.agent = eval(POLICY)(input_dim, PandaTaskSpace6D(), self.config)\n        net_dict = make_nets_opts_schedulers(self.cfg.RL_MODEL_SPEC,  self.config)\n        self.agent.setup_feature_extractor(net_dict)\n        self.buffer_id = buffer_id\n\n        self.model_path = model_path\n        self.updates = self.agent.load_model(pretrained_path, set_init_step=set_init_step, surfix=model_surfix)\n        self.initial_updates = self.updates\n        self.epoch = 0\n\n\n    def get_agent_update_step(self):\n        return self.agent.update_step\n\n    def get_agent_incr_update_step(self):\n        return self.agent.update_step - self.initial_updates\n\n    def load_weight(self, weights):\n        self.agent.load_weight(weights)\n        return [0]\n\n    def get_weight(self):\n        return self.agent.get_weight()\n\n    def save_model(self, surfix='latest'):\n        self.agent.save_model(self.agent.update_step, output_dir=self.config.model_output_dir, surfix=surfix)\n\n    def get_agent(self):\n        return self.agent\n\n    def select_action(self, state, actions=None, goal_state=None, remain_timestep=1,\n                      gt_goal_rollout=True, curr_joint=None, gt_traj=None):\n        action, traj, extra_pred, aux_pred = self.agent.select_action(state, actions=actions, goal_state=goal_state, remain_timestep=remain_timestep)\n        return action, traj, extra_pred, aux_pred\n\n    def update_parameter(self, batch_data, updates, i):\n        return self.agent.update_parameters(batch_data,  updates, i)\n\n    def get_agent_lr_info(self):\n        self.agent.step_scheduler(self.agent.update_step)\n        return (self.agent.update_step, self.agent.get_lr())"}
{"Repository": "sphinx-toolbox", "input": "Represents an employee. className Employee(NamedTuple) Method __repr__ Method is_executive", "label": "class Employee(NamedTuple):\n\t#: The employee's name\n\tname: str\n\n\tid: int = 3\n\n\tdef __repr__(self) -> str:\n\t\treturn f'<Employee {self.name}, id={self.id}>'\n\n\tdef is_executive(self) -> bool:"}
{"Repository": "pytorch-mula", "input": "Computes and stores the average and current value className AverageMeter(object) Method __init__ Method reset Method update", "label": "class AverageMeter(object):\n    def __init__(self):\n\n        self.reset()\n\n\n\n    def reset(self):\n\n        self.val = 0\n\n        self.avg = 0\n\n        self.sum = 0\n\n        self.count = 0\n\n\n\n    def update(self, val, n=1):\n\n        self.val = val\n\n        self.sum += val * n\n\n        self.count += n\n\n        self.avg = self.sum / self.count"}
{"Repository": "gapic-generator-python", "input": "A named collection with a list of attributes. className Map(Element) Method render Method get", "label": "class Map(Element):\n    name: str\n    anchor_name: Optional[str]\n    elements: List[Element]\n\n    def render(self, spaces: int = 0):\n        maybe_anchor = (\" &\" + self.anchor_name) if self.anchor_name else \"\"\n        element_str = \"\\n\".join(\n            e.render(spaces=spaces + self.INDENT_SPACES) for e in self.elements\n        )\n        whitespace = \" \" * spaces\n        return f\"{whitespace}{self.name}:{maybe_anchor}\\n{element_str}\"\n\n    def get(self, key, default=None):\n        # Use iter([]) instead of a generator expression due to a bug in pytest.\n        # See https://github.com/pytest-dev/pytest-cov/issues/310 for details.\n        return next(\n            iter(\n                [e.val  # type: ignore\n                 for e in self.elements\n                 if e.key == key]  # type: ignore\n            ),\n            default\n        )"}
{"Repository": "improved-diffusion", "input": "A diffusion process which can skip steps in a base diffusion process. className SpacedDiffusion(GaussianDiffusion) Method __init__ Method _wrap_model Method _scale_timesteps Attribute use_timesteps Attribute timestep_map Attribute original_num_steps", "label": "class SpacedDiffusion(GaussianDiffusion):\n    def __init__(self, use_timesteps, **kwargs):\n        self.use_timesteps = set(use_timesteps)\n        self.timestep_map = []\n        self.original_num_steps = len(kwargs[\"betas\"])\n\n        base_diffusion = GaussianDiffusion(**kwargs)  # pylint: disable=missing-kwoa\n        last_alpha_cumprod = 1.0\n        new_betas = []\n        for i, alpha_cumprod in enumerate(base_diffusion.alphas_cumprod):\n            if i in self.use_timesteps:\n                new_betas.append(1 - alpha_cumprod / last_alpha_cumprod)\n                last_alpha_cumprod = alpha_cumprod\n                self.timestep_map.append(i)\n        kwargs[\"betas\"] = np.array(new_betas)\n        super().__init__(**kwargs)\n\n    def p_mean_variance(\n        self, model, *args, **kwargs\n    ):  # pylint: disable=signature-differs\n        return super().p_mean_variance(self._wrap_model(model), *args, **kwargs)\n\n    def training_losses(\n        self, model, *args, **kwargs\n    ):  # pylint: disable=signature-differs\n        return super().training_losses(self._wrap_model(model), *args, **kwargs)\n\n    def _wrap_model(self, model):\n        if isinstance(model, _WrappedModel):\n            return model\n        return _WrappedModel(\n            model, self.timestep_map, self.rescale_timesteps, self.original_num_steps\n        )\n\n    def _scale_timesteps(self, t):\n        # Scaling is done by the wrapped model.\n        return t"}
{"Repository": "flask-basicauth", "input": "A Flask extension for adding HTTP basic access authentication to the application. className BasicAuth(object) Method __init__ Method init_app Method require_basic_auth Method check_credentials Method authenticate Method challenge Method required Method wrapper Attribute app Attribute app", "label": "class BasicAuth(object):\n    def __init__(self, app=None):\n        if app is not None:\n            self.app = app\n            self.init_app(app)\n        else:\n            self.app = None\n\n    def init_app(self, app):\n        app.config.setdefault('BASIC_AUTH_FORCE', False)\n        app.config.setdefault('BASIC_AUTH_REALM', '')\n\n        @app.before_request\n        def require_basic_auth():\n            if not current_app.config['BASIC_AUTH_FORCE']:\n                return\n            if not self.authenticate():\n                return self.challenge()\n\n    def check_credentials(self, username, password):\n        correct_username = current_app.config['BASIC_AUTH_USERNAME']\n        correct_password = current_app.config['BASIC_AUTH_PASSWORD']\n        return username == correct_username and password == correct_password\n\n    def authenticate(self):\n        auth = request.authorization\n        return (\n            auth and auth.type == 'basic' and\n            self.check_credentials(auth.username, auth.password)\n        )\n\n    def challenge(self):\n        realm = current_app.config['BASIC_AUTH_REALM']\n        return Response(\n            status=401,\n            headers={'WWW-Authenticate': 'Basic realm=\"%s\"' % realm}\n        )\n\n    def required(self, view_func):\n        @wraps(view_func)\n        def wrapper(*args, **kwargs):\n            if self.authenticate():\n                return view_func(*args, **kwargs)\n            else:\n                return self.challenge()\n        return wrapper"}
{"Repository": "voicefixer", "input": "Support setup.py upload. className UploadCommand(Command) Method status Method initialize_options Method finalize_options Method run", "label": "class UploadCommand(Command):\n    description = \"Build and publish the package.\"\n    user_options = []\n\n    @staticmethod\n    def status(s):\n        print(\"\\033[1m{0}\\033[0m\".format(s))\n\n    def initialize_options(self):\n        pass\n\n    def finalize_options(self):\n        pass\n\n    def run(self):\n        try:\n            self.status(\"Removing previous builds\")\n            rmtree(os.path.join(here, \"dist\"))\n        except OSError:\n            pass\n\n        self.status(\"Building Source and Wheel (universal) distribution\")\n        os.system(\"{0} setup.py sdist bdist_wheel --universal\".format(sys.executable))\n\n        self.status(\"Uploading the package to PyPI via Twine\")\n        os.system(\"twine upload dist/*\")\n\n        self.status(\"Pushing git tags\")\n        os.system(\"git tag v{0}\".format(about[\"__version__\"]))\n        os.system(\"git push --tags\")\n\n        sys.exit()"}
{"Repository": "jeeves", "input": "This test use a configuration file Conf.conf. className Jcourse(FunkLoadTestCase) Method setUp Method login Method logout Method test_show_all_assignments", "label": "class Jcourse(FunkLoadTestCase):\n    def setUp(self):\n        self.server_url = self.conf_get('main', 'url')\n\n        credential_host = self.conf_get('credential', 'host')\n        credential_port = self.conf_getInt('credential', 'port')\n        self.username, self.pwd = xmlrpc_get_credential(\n            credential_host, credential_port, \"group1\")\n\n\n    def login(self, page=\"/index\"):\n        # The description should be set in the configuration file\n        server_url = self.server_url\n\n        reply = self.get(server_url + \"/index\",\n            description=\"Get index\")\n\n        csrftoken = extract_token(self.getBody(), \"name='csrfmiddlewaretoken' value='\", \"' />\")\n        self.post(server_url + \"/accounts/login/?next=\" + page,\n            params=[['csrfmiddlewaretoken', csrftoken],\n            ['redirect_to', page],\n            ['username', self.username],\n            ['password', self.pwd]],\n            description=\"Post /accounts/login/\")\n\n    def logout(self):\n        self.get(self.server_url + \"/accounts/logout/\",\n                    description=\"Get /accounts/logout/\")\n\n    def test_show_all_assignments(self):\n        page = \"/courses\"\n        self.login(page)\n        self.assert_(page == self.getLastUrl(), \"Error in login\")\n        reply = self.get(self.server_url + page, description=\"Get assignments\")\n        self.logout()"}
{"Repository": "drf-extensions", "input": "A subclass of unicode used for the return value of conversion to possibly attach some attributes. className UnicodeWithAttrs(unicode) Method toc_html Method indent", "label": "class UnicodeWithAttrs(unicode):\n    metadata = None\n    _toc = None\n    def toc_html(self):\n        if self._toc is None:\n            return None\n\n        def indent():\n            return '  ' * (len(h_stack) - 1)\n        lines = []\n        h_stack = [0]   # stack of header-level numbers\n        for level, id, name in self._toc:\n            if level > h_stack[-1]:\n                lines.append(\"%s<ul>\" % indent())\n                h_stack.append(level)\n            elif level == h_stack[-1]:\n                lines[-1] += \"</li>\"\n            else:\n                while level < h_stack[-1]:\n                    h_stack.pop()\n                    if not lines[-1].endswith(\"</li>\"):\n                        lines[-1] += \"</li>\"\n                    lines.append(\"%s</ul></li>\" % indent())\n            lines.append('%s<li><a href=\"#%s\">%s</a>' % (\n                indent(), id, name))\n        while len(h_stack) > 1:\n            h_stack.pop()\n            if not lines[-1].endswith(\"</li>\"):\n                lines[-1] += \"</li>\"\n            lines.append(\"%s</ul>\" % indent())\n        return '\\n'.join(lines) + '\\n'\n    toc_html = property(toc_html)"}
{"Repository": "openmmtools", "input": "A mix-in providing the interface for harmonic restraints. className HarmonicRestraintForceMixIn(object) Method __init__ Method spring_constant Method distance_at_energy", "label": "class HarmonicRestraintForceMixIn(object):\n    def __init__(self, spring_constant, *args, **kwargs):\n        energy_function = '(K/2)*distance(g1,g2)^2'\n        restraint_parameters = collections.OrderedDict([('K', spring_constant)])\n        super(HarmonicRestraintForceMixIn, self).__init__(energy_function, restraint_parameters,\n                                                          *args, **kwargs)\n\n    @property\n    def spring_constant(self):\n        # This works for both CustomBondForce and CustomCentroidBondForce.\n        parameters = self.getBondParameters(0)[-1]\n        return parameters[0] * unit.kilojoule_per_mole/unit.nanometers**2\n\n    def distance_at_energy(self, potential_energy):\n        return _compute_harmonic_radius(self.spring_constant, potential_energy)\n\n    def _compute_restraint_volume(self, thermodynamic_state, square_well,\n                                  radius_cutoff, energy_cutoff):\n        # If there is not a cutoff, integrate up to 100kT\n        if energy_cutoff is None:\n            energy_cutoff = 100.0  # kT\n        radius = self.distance_at_energy(energy_cutoff * thermodynamic_state.kT)\n        if radius_cutoff is not None:\n            radius = min(radius, radius_cutoff)\n        if square_well:\n            return _compute_sphere_volume(radius)\n        return _compute_harmonic_volume(radius, self.spring_constant,\n                                        thermodynamic_state.beta)"}
{"Repository": "dionaea", "input": "Our own Timer class because some attributes we have to user are undocumented in the Python stub files. className SubTimer(Thread) Method cancel Method run", "label": "class SubTimer(Thread):\n    def __init__(self, interval: float, function: Callable, delay: Optional[float] = None, repeat=False,\n                 args: Optional[list] = None, kwargs: Optional[dict] = None):\n        Thread.__init__(self)\n        self.interval = interval\n        self.function = function\n        self.delay = delay\n        if self.delay is None:\n            self.delay = self.interval\n        self.repeat = repeat\n        self.args = args if args is not None else []\n        self.kwargs = kwargs if kwargs is not None else {}\n        self.finished = Event()\n\n    def cancel(self):\n        self.finished.set()\n\n    def run(self) -> None:\n        self.finished.wait(self.delay)\n        if not self.finished.is_set():\n            self.function(*self.args, **self.kwargs)\n        while self.repeat and not self.finished.wait(self.interval):\n            if not self.finished.is_set():\n                self.function(*self.args, **self.kwargs)"}
{"Repository": "HoymilesZeroExport", "input": "This class reads the configuration from the fixed config file. className ConfigFileConfigProvider(ConfigProvider) Method __init__ Method get_powermeter_target_point Method get_powermeter_max_point Method get_powermeter_tolerance Method on_grid_usage_jump_to_limit_percent Method get_min_wattage_in_percent Method get_normal_wattage Method get_reduce_wattage Method get_battery_priority Attribute config", "label": "class ConfigFileConfigProvider(ConfigProvider):\n    def __init__(self, config: ConfigParser):\n        self.config = config\n\n    def get_powermeter_target_point(self):\n        return self.config.getint('CONTROL', 'POWERMETER_TARGET_POINT')\n\n    def get_powermeter_max_point(self):\n        return self.config.getint('CONTROL', 'POWERMETER_MAX_POINT')\n\n    def get_powermeter_tolerance(self):\n        return self.config.getint('CONTROL', 'POWERMETER_TOLERANCE')\n\n    def on_grid_usage_jump_to_limit_percent(self):\n        return self.config.getint('COMMON', 'ON_GRID_USAGE_JUMP_TO_LIMIT_PERCENT')\n\n    def get_min_wattage_in_percent(self, inverter_idx):\n        return self.config.getint('INVERTER_' + str(inverter_idx + 1), 'HOY_MIN_WATT_IN_PERCENT')\n\n    def get_normal_wattage(self, inverter_idx):\n        return self.config.getint('INVERTER_' + str(inverter_idx + 1), 'HOY_BATTERY_NORMAL_WATT')\n\n    def get_reduce_wattage(self, inverter_idx):\n        return self.config.getint('INVERTER_' + str(inverter_idx + 1), 'HOY_BATTERY_REDUCE_WATT')\n\n    def get_battery_priority(self, inverter_idx):\n        return self.config.getint('INVERTER_' + str(inverter_idx + 1), 'HOY_BATTERY_PRIORITY')"}
{"Repository": "otrace", "input": "Wrapper to make an object appear like a dict. className ObjectDict(MappingDict) Method __init__ Method copy Method keys Method __contains__ Method __getitem__ Method __iter__ Method __len__ Method __setitem__ Method __delitem__ Attribute _obj", "label": "class ObjectDict(MappingDict):\n    def __init__(self, obj):\n        self._obj = obj\n\n    def copy(self):\n        return ObjectDict(self._obj)\n\n    def keys(self):\n        return dir(self._obj)\n\n    def __contains__(self, key):\n        return hasattr(self._obj, key)\n\n    def __getitem__(self, key):\n        if not hasattr(self._obj, key):\n            raise KeyError(key)\n        return getattr(self._obj, key)\n\n    def __iter__(self):\n        return self.keys().__iter__\n\n    def __len__(self):\n        return len(self.keys())\n\n    def __setitem__(self, key, value):\n        setattr(self._obj, key, value)\n\n    def __delitem__(self, key):\n        if not hasattr(self._obj, key):\n            raise KeyError(key)\n        delattr(self._obj, key)"}
{"Repository": "annotated-types", "input": "Len() implies that ``min_length <= len(value) <= max_length``. className Len(GroupedMetadata) Method __iter__", "label": "class Len(GroupedMetadata):\n    min_length: Annotated[int, Ge(0)] = 0\n    max_length: Optional[Annotated[int, Ge(0)]] = None\n\n    def __iter__(self) -> Iterator[BaseMetadata]:\n        if self.min_length > 0:\n            yield MinLen(self.min_length)\n        if self.max_length is not None:\n            yield MaxLen(self.max_length)"}
{"Repository": "Im2txt", "input": "Helper class for decoding images in TensorFlow. className ImageDecoder(object) Method __init__ Method decode_jpeg Attribute _sess Attribute _encoded_jpeg Attribute _decode_jpeg", "label": "class ImageDecoder(object):\n  def __init__(self):\n    # Create a single TensorFlow Session for all image decoding calls.\n    self._sess = tf.Session()\n\n    # TensorFlow ops for JPEG decoding.\n    self._encoded_jpeg = tf.placeholder(dtype=tf.string)\n    self._decode_jpeg = tf.image.decode_jpeg(self._encoded_jpeg, channels=3)\n\n  def decode_jpeg(self, encoded_jpeg):\n    image = self._sess.run(self._decode_jpeg,\n                           feed_dict={self._encoded_jpeg: encoded_jpeg})\n    assert len(image.shape) == 3\n    assert image.shape[2] == 3\n    return image"}
{"Repository": "XVFI", "input": "For convenience of printing diverse values by using \"AverageClass\" className ProgressMeter(object) Method __init__ Method print Method _get_batch_fmtstr Attribute batch_fmtstr Attribute meters Attribute prefix", "label": "class ProgressMeter(object):\n    \"\"\" refer from \"https://github.com/pytorch/examples/blob/master/imagenet/main.py\" \"\"\"\n\n    def __init__(self, num_batches, *meters, prefix=\"\"):\n        self.batch_fmtstr = self._get_batch_fmtstr(num_batches)\n        self.meters = meters\n        self.prefix = prefix\n\n    def print(self, batch):\n        entries = [self.prefix + self.batch_fmtstr.format(batch)]\n        # # Epoch: [0][  0/196]\n        entries += [str(meter) for meter in self.meters]\n        print('\\t'.join(entries))\n\n    def _get_batch_fmtstr(self, num_batches):\n        num_digits = len(str(num_batches // 1))\n        fmt = '{:' + str(num_digits) + 'd}'\n        return '[' + fmt + '/' + fmt.format(num_batches) + ']'"}
{"Repository": "SemanticGuidedHumanMatting", "input": "ResNetEncoder inherits from torchvision's official ResNet. className ResNetEncoder(ResNet) Method __init__ Method forward Attribute conv1", "label": "class ResNetEncoder(ResNet):\n    layers = {\n        'resnet50':  [3, 4, 6, 3],\n        'resnet101': [3, 4, 23, 3],\n    }\n    \n    def __init__(self, in_channels, variant='resnet101', norm_layer=None):\n        super().__init__(\n            block=Bottleneck,\n            layers=self.layers[variant],\n            replace_stride_with_dilation=[False, False, True],\n            norm_layer=norm_layer)\n        \n        # Replace first conv layer if in_channels doesn't match.\n        if in_channels != 3:\n            self.conv1 = nn.Conv2d(in_channels, 64, 7, 2, 3, bias=False)\n            \n        # Delete fully-connected layer\n        del self.avgpool\n        del self.fc\n    \n    def forward(self, x):\n        x0 = x  # 1/1\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x1 = x  # 1/2\n        x = self.maxpool(x)\n        x = self.layer1(x)\n        x2 = x  # 1/4\n        x = self.layer2(x)\n        x3 = x  # 1/8\n        x = self.layer3(x)\n        x = self.layer4(x)\n        x4 = x  # 1/16\n        return x4, x3, x2, x1, x0"}
{"Repository": "QuestionGeneration", "input": "Simple logger to insert stuff into a file className FileLogger(object) Method __init__ Method write Attribute file", "label": "class FileLogger(object):\n    def __init__(self, path):\n        self.file = open(path, 'w')\n\n    def write(self, text, print_text=True):\n        if print_text:\n            print(\"FILE LOGGER: %s\" % text)\n        self.file.write(str(text) + \"\\n\")\n        self.file.flush()"}
{"Repository": "pyrpipe", "input": "This class represents Stringtie program for transcript assembly. className Stringtie(Assembly) Method __init__ Method perform_assembly Attribute _command Attribute _deps Attribute _param_yaml Attribute _valid_args", "label": "class Stringtie(Assembly):\n    def __init__(self,*args,threads=None,guide=None,**kwargs):\n        super().__init__(*args,**kwargs)\n        self._command='stringtie'\n        self._deps=[self._command]\n        self._param_yaml='stringtie.yaml'\n        self._valid_args=valid_args._args_STRINGTIE\n        \n        #resolve threads to use\n        self.resolve_parameter(\"-p\",threads,_threads,'_threads')\n        self.resolve_parameter(\"-G\",guide,None,'_guide')\n        \n\n                         \n    def perform_assembly(self,bam_file,out_dir=None,out_suffix=\"_stringtie\",objectid=\"NA\"):\n        #create path to output file\n        fname=pu.get_file_basename(bam_file)\n        \n        if not out_dir:\n            out_dir=pu.get_file_directory(bam_file)\n        \n        if not pu.check_paths_exist(out_dir):\n            pu.mkdir(out_dir)\n            \n        out_gtf_file=os.path.join(out_dir,fname+out_suffix+\".gtf\")\n\n        #Add output file name and input bam\n        internal_args=(bam_file,)\n        internal_kwargs={\"-o\":out_gtf_file}\n        #add positional args\n        internal_kwargs['--']=internal_args\n\n        #call stringtie\n        status=self.run(None,objectid=objectid,target=out_gtf_file,**internal_kwargs)\n        \n        if status:\n            #check if sam file is present in the location directory of sraOb\n            if not pu.check_files_exist(out_gtf_file) and not _dryrun:\n                return \"\"\n            return out_gtf_file\n        \n        return \"\""}
{"Repository": "nfstream", "input": "NFRequest HTTP server className NFRequestServer(HTTPServer) Method __init__ Attribute stopped Attribute channel", "label": "class NFRequestServer(HTTPServer):\n    def __init__(self, *args):\n        HTTPServer.__init__(self, *args)\n        self.stopped = False\n        # self.channel = channel"}
{"Repository": "TheOrgBook", "input": "Class to manage creation of indy proofs. className ProofManager(object) Method __init__ Method add_filter Method construct_proof", "label": "class ProofManager(object):\n    def __init__(self, proof_request: dict, credential_ids: set = None) -> None:\n        self.proof_request = proof_request\n        self.credential_ids = credential_ids\n        self.filters = []\n\n    def add_filter(self, claim_name: str, claim_value: str):\n        self.filters.append(Filter(claim_name, claim_value))\n\n    def construct_proof(self):\n        return run_coro(self.construct_proof_async())\n\n    async def construct_proof_async(self):\n        proof = await indy_client().construct_proof(\n            indy_holder_id(),\n            self.proof_request,\n            None, # wql filters\n            self.credential_ids,\n        )\n\n        return proof.proof"}
{"Repository": "fairscale", "input": "Baseline that does an output projection, a softmax & a NLL loss (cross-entropy). className BaselineSoftmaxNllLoss(BaselineSoftmax) Method forward", "label": "class BaselineSoftmaxNllLoss(BaselineSoftmax):\n    def __init__(\n        self,\n        proj_weight: nn.Parameter,\n        tile_factor: int = 0,\n        log_softmax: bool = True,\n        margin: float = 0.35,\n        scale: Optional[float] = None,\n    ):\n        super().__init__(proj_weight, tile_factor, log_softmax, margin, scale)\n\n    def forward(self, input: torch.Tensor, target: torch.Tensor) -> torch.Tensor:  # type: ignore\n        assert isinstance(input, torch.Tensor)\n        assert isinstance(target, torch.Tensor)\n        input, target = _reshape_inputs(input, target)\n        x = super().forward(input, target)\n        return F.nll_loss(x, target, reduction=\"sum\")"}
{"Repository": "MutexMatch4SSL", "input": "refer: https://github.com/pytorch/examples/blob/master/imagenet/main.py className AverageMeter(object) Method __init__ Method reset Method update", "label": "class AverageMeter(object):\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count"}
{"Repository": "gantryd", "input": "A health check which tries to connect to a port via TCP. className TcpCheck(HealthCheck) Method __init__ Method run Attribute config", "label": "class TcpCheck(HealthCheck):\n  def __init__(self, config):\n    super(TcpCheck, self).__init__()\n    self.config = config\n\n  def run(self, container, report):\n    container_port = self.config.getExtraField('port')\n    container_ip = self.getContainerIPAddress(container)\n\n    report('Checking TCP port in container ' + container['Id'][0:12] + ': ' + str(container_port),\n      level = ReportLevels.EXTRA)\n    try:\n      sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n      sock.connect((container_ip, container_port))\n      sock.close()\n    except Exception as e:\n      print e\n      return False\n\n    return True"}
{"Repository": "brython", "input": "Visitor that invokes a provieded callmaker visitor with just the NamedItem nodes className RuleCollectorVisitor(GrammarVisitor) Method __init__ Method visit_Rule Method visit_NamedItem", "label": "class RuleCollectorVisitor(GrammarVisitor):\n    def __init__(self, rules: Dict[str, Rule], callmakervisitor: GrammarVisitor) -> None:\n        self.rulses = rules\n        self.callmaker = callmakervisitor\n\n    def visit_Rule(self, rule: Rule) -> None:\n        self.visit(rule.flatten())\n\n    def visit_NamedItem(self, item: NamedItem) -> None:\n        self.callmaker.visit(item)"}
{"Repository": "fast-ode", "input": "A distribution over timesteps in the diffusion process, intended to reduce variance of the objective. className ScheduleSampler(ABC) Method weights Method sample", "label": "class ScheduleSampler(ABC):\n    @abstractmethod\n    def weights(self):\n    def sample(self, batch_size, device):\n        w = self.weights()\n        p = w / np.sum(w) # loss weight normalize\n        indices_np = np.random.choice(len(p), size=(batch_size,), p=p)[0]\n        if indices_np == 0: indices_np += 1\n        assert indices_np > 0, f\"indices_np : {indices_np}\"\n        indices_np = np.repeat(indices_np, batch_size)\n        indices = th.from_numpy(indices_np).long().to(device)\n        weights_np = 1 / (len(p) * p[indices_np])\n        weights = th.from_numpy(weights_np).float().to(device)\n        return indices, weights"}
{"Repository": "home-assistant-on-echo-show", "input": "Handler for Open Page Intent. className OpenPageIntentHandler(AbstractRequestHandler) Method can_handle Method handle", "label": "class OpenPageIntentHandler(AbstractRequestHandler):\n    def can_handle(self, handler_input):\n        # type: (HandlerInput) -> bool\n        return ask_utils.is_intent_name(\"OpenPageIntent\")(handler_input)\n\n    def handle(self, handler_input):\n        # type: (HandlerInput) -> Response\n\n        response_builder = handler_input.response_builder\n        number = ask_utils.request_util.get_slot(handler_input, \"page\").value\n\n        # Render empty template, needed for OpenURL command\n        # see https://amazon.developer.forums.answerhub.com/questions/220506/alexa-open-a-browser.html\n        response_builder.add_directive(\n            RenderDocumentDirective(\n                token=TOKEN,\n                document=_load_apl_document(\"template.json\")\n            )\n        )\n\n        # Open respective page of dashboard\n        response_builder.add_directive(\n            ExecuteCommandsDirective(\n                token=TOKEN,\n                commands=[open_page(number)]\n            )\n        )\n        \n        return response_builder.response"}
{"Repository": "forte", "input": "A dataset representing data packs. className DataPackDataset(DatasetBase) Method process Method collate", "label": "class DataPackDataset(DatasetBase):\n    def __init__(\n        self,\n        data_source: DataPackIterator,\n        feature_schemes: Dict,\n        hparams: Union[Dict, HParams] = None,\n        device: Optional[torch.device] = None,\n    ):\n        self._data_source: DataPackIterator = data_source\n        self._feature_scheme: Dict = feature_schemes\n\n        super().__init__(self._data_source, hparams, device)\n\n    def process(self, raw_example: RawExample) -> FeatureCollection:\n        tid: int = raw_example[0]\n        data_pack: DataPack = raw_example[1]\n        instance_entry: EntryType = data_pack.get_entry(tid)  # type:ignore\n        feature_collection: FeatureCollection = {}\n\n        for tag, scheme in self._feature_scheme.items():\n            extractor: BaseExtractor = scheme[\"extractor\"]\n            feature: Feature = extractor.extract(data_pack, instance_entry)\n            feature_collection[tag] = feature\n\n        return feature_collection\n\n    def collate(self, examples: List[FeatureCollection]) -> Batch:\n        batch_size = len(examples)\n\n        example_collection: Dict[str, List] = {}\n        for example in examples:\n            for tag, feature in example.items():\n                if tag not in example_collection:\n                    example_collection[tag] = []\n                example_collection[tag].append(feature)\n\n        tensor_collection: Dict[str, Dict[str, Any]] = {}\n        for tag, features in example_collection.items():\n            if tag not in tensor_collection:\n                tensor_collection[tag] = {}\n\n            converter: Converter = self._feature_scheme[tag][\"converter\"]\n            data, masks = converter.convert(features)\n            tensor_collection[tag][\"data\"] = data\n            tensor_collection[tag][\"masks\"] = masks\n            tensor_collection[tag][\"features\"] = features\n\n        return Batch(batch_size, **tensor_collection)"}
{"Repository": "pytools", "input": "Raised when a topological ordering cannot be computed due to a cycle. className CycleError(Exception) Method __init__", "label": "class CycleError(Exception):\n    def __init__(self, node: NodeT) -> None:\n        self.node = node"}
{"Repository": "tabnet", "input": "Wrapper for most torch scheduler functions. className LRSchedulerCallback(Callback) Method on_batch_end Method on_epoch_end", "label": "class LRSchedulerCallback(Callback):\n    scheduler_fn: Any\n    optimizer: Any\n    scheduler_params: dict\n    early_stopping_metric: str\n    is_batch_level: bool = False\n\n    def __post_init__(\n        self,\n    ):\n        self.is_metric_related = hasattr(self.scheduler_fn, \"is_better\")\n        self.scheduler = self.scheduler_fn(self.optimizer, **self.scheduler_params)\n        super().__init__()\n\n    def on_batch_end(self, batch, logs=None):\n        if self.is_batch_level:\n            self.scheduler.step()\n        else:\n            pass\n\n    def on_epoch_end(self, epoch, logs=None):\n        current_loss = logs.get(self.early_stopping_metric)\n        if current_loss is None:\n            return\n        if self.is_batch_level:\n            pass\n        else:\n            if self.is_metric_related:\n                self.scheduler.step(current_loss)\n            else:\n                self.scheduler.step()"}
{"Repository": "modelstore", "input": "Model persistence for Annoy models: https://github. className AnnoyManager(ModelManager) Method __init__ Method required_dependencies Method _required_kwargs Method matches_with Method _get_functions Method get_params Method load", "label": "class AnnoyManager(ModelManager):\n    NAME = \"annoy\"\n\n    def __init__(self, storage: CloudStorage = None):\n        super().__init__(self.NAME, storage)\n\n    def required_dependencies(self) -> list:\n        return [\"annoy\"]\n\n    def _required_kwargs(self):\n        return [\"model\", \"metric\", \"num_trees\"]\n\n    def matches_with(self, **kwargs) -> bool:\n        # pylint: disable=import-outside-toplevel\n        from annoy import AnnoyIndex\n\n        return isinstance(kwargs.get(\"model\"), AnnoyIndex)\n\n    def _get_functions(self, **kwargs) -> list:\n        if not self.matches_with(**kwargs):\n            raise TypeError(\"Model is not an AnnoyIndex!\")\n\n        return [\n            partial(\n                save_model,\n                model=kwargs[\"model\"],\n            ),\n        ]\n\n    def get_params(self, **kwargs) -> dict:\n        return {\n            \"num_dimensions\": kwargs[\"model\"].f,\n            \"num_trees\": kwargs[\"num_trees\"],\n            \"metric\": kwargs[\"metric\"],\n        }\n\n    def load(self, model_path: str, meta_data: metadata.Summary) -> Any:\n        super().load(model_path, meta_data)\n\n        # pylint: disable=import-outside-toplevel\n        from annoy import AnnoyIndex\n\n        # Extract index size & metric from the meta_data\n        params = meta_data.model.parameters\n        num_dimensions = int(params[\"num_dimensions\"])\n        metric = params[\"metric\"]\n\n        model = AnnoyIndex(num_dimensions, metric)\n        model.load(_model_file_path(model_path))\n        return model"}
{"Repository": "nglod", "input": "Base class for single mesh datasets. className MeshDataset(Dataset) Method resample Method __getitem__ Method __len__ Method num_shapes", "label": "class MeshDataset(Dataset):\n    def __init__(self, \n        args=None, \n        dataset_path = None,\n        raw_obj_path = None,\n        sample_mode = None,\n        get_normals = None,\n        seed = None,\n        num_samples = None,\n        trim = None,\n        sample_tex = None\n    ):\n        self.args = args\n        self.dataset_path = setparam(args, dataset_path, 'dataset_path')\n        self.raw_obj_path = setparam(args, raw_obj_path, 'raw_obj_path')\n        self.sample_mode = setparam(args, sample_mode, 'sample_mode')\n        self.get_normals = setparam(args, get_normals, 'get_normals')\n        self.num_samples = setparam(args, num_samples, 'num_samples')\n        self.trim = setparam(args, trim, 'trim')\n        self.sample_tex = setparam(args, sample_tex, 'sample_tex')\n\n        # Possibly remove... or fix trim obj\n        #if self.raw_obj_path is not None and not os.path.exists(self.dataset_path):\n        #    _, _, self.mesh = trim_obj_to_file(self.raw_obj_path, self.dataset_path)\n        #elif not os.path.exists(self.dataset_path):\n        #    assert False and \"Data does not exist and raw obj file not specified\"\n        #else:\n        \n        if self.sample_tex:\n            out = load_obj(self.dataset_path, load_materials=True)\n            self.V, self.F, self.texv, self.texf, self.mats = out\n        else:\n            self.V, self.F = load_obj(self.dataset_path)\n\n        self.V, self.F = normalize(self.V, self.F)\n        self.mesh = self.V[self.F]\n        self.resample()\n\n    def resample(self):\n        self.nrm = None\n        if self.get_normals:\n            self.pts, self.nrm = sample_surface(self.V, self.F, self.num_samples*5)\n            self.nrm = self.nrm.cpu()\n        else:\n            self.pts = point_sample(self.V, self.F, self.sample_mode, self.num_samples)\n\n        self.d = compute_sdf(self.V.cuda(), self.F.cuda(), self.pts.cuda())   \n\n        self.d = self.d[...,None]\n        self.d = self.d.cpu()\n        self.pts = self.pts.cpu()\n\n    def __getitem__(self, idx: int):\n        if self.get_normals:\n            return self.pts[idx], self.d[idx], self.nrm[idx]\n        elif self.sample_tex:\n            return self.pts[idx], self.d[idx], self.rgb[idx]\n        else:\n            return self.pts[idx], self.d[idx]\n            \n    def __len__(self):\n        return self.pts.size()[0]\n\n    def num_shapes(self):\n        return 1"}
{"Repository": "openap", "input": "SavitzkyGolay Filter Parameters ---------- window_size : int the length of the window. className SavitzkyGolay(BaseFilter) Method __init__ Method filter Attribute window_size Attribute order Attribute deriv", "label": "class SavitzkyGolay(BaseFilter):\n    def __init__(self, window_size=11, order=2, deriv=0, i=False):\n        super(SavitzkyGolay, self).__init__(i=i)\n\n        try:\n            window_size = np.abs(np.int(window_size))\n            order = np.abs(np.int(order))\n        except ValueError:\n            raise ValueError(\"window_size and order have to be of type int\")\n        if window_size % 2 != 1 or window_size < 1:\n            raise TypeError(\"win size size must be a positive odd number\")\n        if window_size < order + 2:\n            raise TypeError(\"win size is too small for the polynomials order\")\n\n        self.window_size = window_size\n        self.order = order\n        self.deriv = deriv\n\n    def filter(self, X, Y):\n        if self.interpolate:\n            X, Y = self.simplefill(X, Y)\n        else:\n            X, Y = self.sortxy(X, Y)\n\n        order_range = list(range(self.order+1))\n        half_window = (self.window_size - 1) // 2\n        # precompute coefficients\n        b = np.mat([[k**i for i in order_range]\n                    for k in range(-half_window, half_window+1)])\n        m = np.linalg.pinv(b).A[self.deriv]\n        # pad the signal at the extremes with\n        # values taken from the signal itself\n        firstvals = Y[0] - np.abs(Y[1:half_window+1][::-1] - Y[0])\n        lastvals = Y[-1] + np.abs(Y[-half_window-1:-1][::-1] - Y[-1])\n        Y1 = np.concatenate((firstvals, Y, lastvals))\n        Y2 = np.convolve(m, Y1, mode='valid')\n\n        return X, Y2"}
{"Repository": "ubelt", "input": "A dictionary subclass where all set operations are defined. className SetDict(dict) Method __or__ Method __and__ Method __sub__ Method __xor__ Method union Method intersection Method difference Method symmetric_difference Method subdict", "label": "class SetDict(dict):\n    # We could just use the builtin variant for this specific operation\n    def __or__(self, other):\n        return self.union(other)\n\n    def __and__(self, other):\n        return self.intersection(other)\n\n    def __sub__(self, other):\n        return self.difference(other)\n\n    def __xor__(self, other):\n        return self.symmetric_difference(other)\n\n    ### Main set operations\n\n    def union(self, *others):\n        cls = self.__class__\n        args = it.chain([self], others)\n        new = cls(it.chain.from_iterable(d.items() for d in args))\n        return new\n\n    def intersection(self, *others):\n        cls = self.__class__\n        isect_keys = set(self.keys())\n        for v in others:\n            isect_keys.intersection_update(v)\n        new = cls((k, self[k]) for k in self if k in isect_keys)\n        return new\n\n    def difference(self, *others):\n        cls = self.__class__\n        other_keys = set()\n        for v in others:\n            other_keys.update(v)\n        # Looping over original keys is important to maintain partial order.\n        new = cls((k, self[k]) for k in self.keys() if k not in other_keys)\n        return new\n\n    def symmetric_difference(self, *others):\n        from collections import defaultdict\n        cls = self.__class__\n        accum_count = defaultdict(lambda: 0)\n        accum_refs = {}\n        for d in it.chain([self], others):\n            for k in d.keys():\n                accum_count[k] += 1\n                accum_refs[k] = d\n        new = cls((k, accum_refs[k][k]) for k, count in accum_count.items()\n                  if count % 2 == 1)\n        return new\n\n    ### Extra set operations\n\n    def subdict(self, keys, default=NoParam):\n        cls = self.__class__\n        if default is NoParam:\n            new = cls([(k, self[k]) for k in keys])\n        else:\n            new = cls([(k, self.get(k, default)) for k in keys])\n        return new"}
{"Repository": "puresnmp", "input": "Wrapper for network timeouts. className Timeout(SnmpError) Method __init__", "label": "class Timeout(SnmpError):\n    def __init__(self, message: str) -> None:\n        super().__init__(message)\n        self.message = message"}
{"Repository": "rasa_core", "input": "The first action in any turn - bot waits for a user message. className ActionListen(Action) Method name", "label": "class ActionListen(Action):\n    def name(self) -> Text:\n        return ACTION_LISTEN_NAME\n\n    async def run(self, dispatcher, tracker, domain):\n        return []"}
{"Repository": "FreeSim", "input": "This class controls the default input of the software and the state of the software (enabled/disabled). className DefaultInput(object) Method __init__", "label": "class DefaultInput(object):\n    def __init__(self) -> None:\n        super().__init__()\n\n        self.mechanism_parameters_0 = OrderedDict([(0,0),(1,'CV')])\n\n        self.cv_parameters_1 = OrderedDict([(0,0.4),(1,-0.4),(2,0.4),(3,1.0),(4,1),(5,0.0),(6,0.0),(7,298),(8,0.0),(9,0.0)])\n        self.cv_parameters_10 = OrderedDict([(0,1e-5),(1,0.0),(2,0.0),(3,0.0),(4,0.0)])\n        self.cv_parameters_11 = OrderedDict([(0,0),(1,1e-5),(2,1e-2)])\n        self.cv_parameters_12 = OrderedDict([(0,0)])\n        self.cv_parameters_13 = OrderedDict([(0,-1.0),(1,10),(2,0),(3,0),(4,298),(5,0.0),(6,0.0)])\n\n        self.chemical_parameters_2 = OrderedDict([(0,0),(1,0.0),(2,1.0),(3,1.0),(4,0.5),(5,1),(6,-0.2),(7,0.0),(8,1.0),(9,0.5),(10,0),(11,0)])\n        self.chemical_parameters_21 = OrderedDict([(0,''),(1,0.0),(2,''),(3,0.0),(4,False)])\n        self.chemical_parameters_22 = OrderedDict([(0,True),(1,1e-9),(2,1e-5),(3,1e-5),(4,True),(5,1e-9),(6,0.0),(7,0.0),(8,False),(9,1e-9),(10,0.0),(11,0.0),(12,False),(13,1e-9),(14,0.0),(15,0.0),(16,False),(17,1e-9),(18,0.0),(19,0.0)])\n\n\n\n        self.model_parameters_3 = OrderedDict([(0,0.05),(1,0),(2,6.0),(3,1e-8)])\n        self.model_parameters_30 = OrderedDict([(0,5e-3),(1,6),(2,0.0)])\n        self.model_parameters_31 = OrderedDict([(0,0.05)])\n        self.model_parameters_32 = OrderedDict([(0,1e-5),(1,0.02),(2,0.0)])\n\n        self.stochastic_process_parameters_4 = OrderedDict([(0,100),(1,500),])\n        self.stochastic_process_parameters_40 = OrderedDict([(0,0),(1,True)])\n\n        self.adsorption_parameters_5 = OrderedDict([(0,0.0),(1,0.0),(2,0.0),(3,0.0),(4,0.0),(5,0.0),(6,0.0),(7,0.0),(8,0.0)])\n        self.adsorption_parameters_50 = OrderedDict([(0,1.0),(1,0.5)])\n\n        self.AI_parameters_6 = OrderedDict([(0,1.0),(1,-1.0),(2,0.0),(3,1.0),(4,1),(5,1e-5),(6,0.0),(7,1e-9),(8,1e-9),(9,1.0),(10,0.5),(11,0),(12,298),(13,1e-5),(14,1)])\n        self.AI_parameters_60 = OrderedDict([(0,0.0),(1,0.0),(2,0.0),(3,0.0),(4,1),(5,1.0),(6,0.0),(7,1.0),(8,1.0),(9,1.0),(10,0.5),(11,0),(12,0)])\n\n\n\n        self.cv_parameters_enabled_1 = OrderedDict([(5,False),(6,False),(8,False),(9,False)])\n        self.cv_parameters_enabled_10 = OrderedDict()\n        self.cv_parameters_enabled_11 = OrderedDict([(2,False)])\n        self.cv_parameters_enabled_12 = OrderedDict()\n        self.cv_parameters_enabled_13 = OrderedDict([(2,False),(3,False)])\n\n\n        self.chemical_parameters_enabled_2 = OrderedDict()\n        self.chemical_parameters_enabled_21 = OrderedDict()\n        self.chemical_parameters_enabled_22 = OrderedDict()\n\n        self.model_parameters_enabled_3= OrderedDict([(1,False),(3,False)])\n        self.model_parameters_enabled_30 = OrderedDict()\n        self.model_parameters_enabled_31 = OrderedDict()\n        \n        self.stochastic_parameters_enabled_4= OrderedDict()\n        self.stochastic_parameters_enabled_40 = OrderedDict()\n        self.adsorption_parameters_enabled_5 = OrderedDict([(0,False),(1,False),(2,False),(3,False),(4,False),(5,False),(6,False),(7,False),(8,False)])\n        self.adsorption_parameters_enabled_50 = OrderedDict([(0,False),(1,False)])\n        self.AI_parameters_enabled_6 = OrderedDict([(0,False),(1,False),(2,False),(4,False),(6,False),(12,False)])\n        self.AI_parameters_enabled_60 = OrderedDict([(0,False),(1,False),(2,False),(3,False),(4,False),(5,False),(6,False),(7,False),(8,False),(9,False),(10,False),(11,False),(12,False)])\n\n        self.chemical_parameters_hided_21 = OrderedDict([(1,True)])\n        \n        self.file_options_parameters = OrderedDict([(0,True),(1,os.getcwd() + '\\Data'),(2,datetime.datetime.now().strftime(r'%Y-%m-%d')),(3,False),(4,False),(5,True),(9,'Simulation')])"}
{"Repository": "satellite_seg", "input": "Computes and stores the average and current value className AverageMeter(object) Method __init__ Method reset Method update", "label": "class AverageMeter(object):\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count"}
{"Repository": "pmu-tools", "input": "Human-readable output data in per-cpu columns. className OutputColumns(OutputHuman) Method __init__ Method set_cpus Method show Method flush Method reset Attribute nodes Attribute timestamp Attribute cpunames Attribute printed_header", "label": "class OutputColumns(OutputHuman):\n    def __init__(self, logfile, args, version, cpu):\n        OutputHuman.__init__(self, logfile, args, version, cpu)\n        self.nodes = {}\n        self.timestamp = None\n        self.cpunames = set()\n        self.printed_header = False\n\n    def set_cpus(self, cpus):\n        self.cpunames = cpus\n\n    def show(self, timestamp, title, area, hdr, val, unit, desc, sample, bn, below, idle):\n        if self.args.single_thread:\n            OutputHuman.show(self, timestamp, title, area, hdr, val, unit, desc, sample, bn, below, idle)\n            return\n        self.print_header()\n        self.timestamp = timestamp\n        key = (area, hdr)\n        if key not in self.nodes:\n            self.nodes[key] = {}\n        assert title not in self.nodes[key]\n        self.nodes[key][title] = (val, unit, desc, sample, bn, below, idle)\n\n    def flush(self):\n        VALCOL_LEN = 16\n        write = self.logf.write\n\n        cpunames = sorted(self.cpunames)\n\n        if not self.printed_header:\n            if self.timestamp:\n                write(\"%9s\" % \"\")\n            self.print_line_header(\"\", \"\")\n            for j in cpunames:\n                write(\"%*s  \" % (VALCOL_LEN, j))\n            write(\"\\n\")\n            self.printed_header = True\n\n        for key in sorted(sorted(self.nodes.keys(), key=lambda x: x[1]), key=lambda x: x[0] == \"\"):\n            node = self.nodes[key]\n            desc = None\n            sample = None\n            unit = None\n            if self.timestamp:\n                self.print_timestamp(self.timestamp)\n\n            self.print_line_header(key[0], key[1])\n            vlist = []\n            for cpuname in cpunames:\n                if cpuname in node:\n                    cpu = node[cpuname]\n                    uval, unit, desc, sample, bn, below, idle = cpu\n                    v = uval.format_value(unit)\n                    vlist.append(uval)\n                    write(\"%*s%s \" % (VALCOL_LEN, v, \"?\" if below else \"*\" if bn else \" \"))\n                else:\n                    write(\"%*s  \" % (VALCOL_LEN, \"\"))\n            if unit:\n                # XXX should move this to be per entry?\n                cval = combine_uval(vlist)\n                vs = (\" +- \" + cval.format_uncertainty() + \" \" + cval.format_mux()) if cval.stddev else \"\"\n                write(\" %-*s%s\" % (self.unitlen, (\"  \" if unit[0] != \"%\" else \"\") + unit, vs))\n            write(\"\\n\")\n            self.print_desc(desc, sample)\n        self.nodes = {}\n\n    def reset(self, name):\n        Output.reset(self, name)\n        self.printed_header = False"}
{"Repository": "tardis", "input": "Exception raised when creation of active mode of tardis is attempted. className TemporarilyUnavaliable(Exception) Method __init__ Method __str__ Attribute value", "label": "class TemporarilyUnavaliable(Exception):\n    def __init__(self, value):\n        self.value = value\n\n    def __str__(self):\n        return repr(self.value)"}
{"Repository": "dh-virtualenv", "input": "Special OptionParser for handling Debhelper options. className DebhelperOptionParser(OptionParser) Method parse_args", "label": "class DebhelperOptionParser(OptionParser):\n    def parse_args(self, args=None, values=None):\n        args = [o[2:] if (o.startswith('-O-') and self.has_option(o[2:])) else o\n                for o in self._get_args(args)]\n        args.extend(os.environ.get('DH_OPTIONS', '').split())\n        # Unfortunately OptionParser is an old style class :(\n        return OptionParser.parse_args(self, args, values)"}
{"Repository": "modelicagym", "input": "Wrapper class. className FMI1MEEnv(ModelicaMEEnv) Method __init__", "label": "class FMI1MEEnv(ModelicaMEEnv):\n    def __init__(self, model_path, config, log_level, simulation_start_time=0):\n        super().__init__(model_path, config, FMIStandardVersion.first, log_level,\n                         simulation_start_time=simulation_start_time)"}
{"Repository": "DCASE2018Task2", "input": "Computes and stores the average and current value className AverageMeter(object) Method __init__ Method reset Method update", "label": "class AverageMeter(object):\n    def  __init__(self):\n        self.reset()\n\n    def  reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count"}
{"Repository": "SRFormer", "input": "CUDA prefetcher. className CUDAPrefetcher() Method __init__ Method preload Method next Method reset Attribute ori_loader Attribute loader Attribute opt Attribute stream Attribute device", "label": "class CUDAPrefetcher():\n    def __init__(self, loader, opt):\n        self.ori_loader = loader\n        self.loader = iter(loader)\n        self.opt = opt\n        self.stream = torch.cuda.Stream()\n        self.device = torch.device('cuda' if opt['num_gpu'] != 0 else 'cpu')\n        self.preload()\n\n    def preload(self):\n        try:\n            self.batch = next(self.loader)  # self.batch is a dict\n        except StopIteration:\n            self.batch = None\n            return None\n        # put tensors to gpu\n        with torch.cuda.stream(self.stream):\n            for k, v in self.batch.items():\n                if torch.is_tensor(v):\n                    self.batch[k] = self.batch[k].to(device=self.device, non_blocking=True)\n\n    def next(self):\n        torch.cuda.current_stream().wait_stream(self.stream)\n        batch = self.batch\n        self.preload()\n        return batch\n\n    def reset(self):\n        self.loader = iter(self.ori_loader)\n        self.preload()"}
{"Repository": "Nero", "input": "As with previous ARM cores there is an instruction, SVC (formerly SWI) that generates a supervisor call. className ARMSupervisorCallExtension(Extension) Method __init__ Method __str__ Method mnemonics Attribute dsm Attribute arch", "label": "class ARMSupervisorCallExtension(Extension):\n    Archs = defaultdict(list,\n                        [(archinfo.ArchARM, ['SVC'])])\n\n    def __init__(self, arch, dsm):\n        self.dsm = dsm\n        self.arch = arch\n\n    def __str__(self):\n        return \"ARMSupervisorCallExtension({})\".format(self.dsm)\n\n    @classmethod\n    def mnemonics(cls, arch):\n        return cls.Archs[type(arch)]"}
{"Repository": "rl-teacher", "input": "Vanilla two hidden layer multi-layer perceptron className FullyConnectedMLP(object) Method __init__ Method run Attribute model", "label": "class FullyConnectedMLP(object):\n    def __init__(self, obs_shape, act_shape, h_size=64):\n        input_dim = np.prod(obs_shape) + np.prod(act_shape)\n\n        self.model = Sequential()\n        self.model.add(Dense(h_size, input_dim=input_dim))\n        self.model.add(LeakyReLU())\n\n        self.model.add(Dropout(0.5))\n        self.model.add(Dense(h_size))\n        self.model.add(LeakyReLU())\n\n        self.model.add(Dropout(0.5))\n        self.model.add(Dense(1))\n\n    def run(self, obs, act):\n        flat_obs = tf.contrib.layers.flatten(obs)\n        x = tf.concat([flat_obs, act], axis=1)\n        return self.model(x)"}
{"Repository": "kaggle-ndsb", "input": "This layer turns (n_views * batch_size, num_channels, r, c) into (n_views * batch_size, n_views * num_channels, r, c) by rolling and concatenating feature maps. className CyclicConvRollLayer(CyclicRollLayer) Method get_output_shape_for Method get_output_for", "label": "class CyclicConvRollLayer(CyclicRollLayer):\n    def get_output_shape_for(self, input_shape):\n        return (input_shape[0], 4*input_shape[1]) + input_shape[2:]\n\n    def get_output_for(self, input, *args, **kwargs):\n        return dihedral_ops.cyclic_convroll(input)"}
{"Repository": "MobileNetV3-SSD-Compact-Version", "input": "Keeps track of most recent, average, sum, and count of a metric. className AverageMeter(object) Method __init__ Method reset Method update", "label": "class AverageMeter(object):\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count"}
{"Repository": "china_southern_power_grid_stat", "input": "Generic API errors className CSGAPIError(Exception) Method __init__ Method __str__", "label": "class CSGAPIError(Exception):\n    def __init__(self, sta: str, msg: str | None = None) -> None:\n        Exception.__init__(self)\n        self.sta = sta\n        self.msg = msg\n\n    def __str__(self):\n        return f\"<CSGAPIError sta={self.sta} message={self.msg}>\""}
{"Repository": "opsdroid", "input": "A module for opsdroid to allow memory to persist in a mongo database. className DatabaseMongo(Database) Method __init__ Attribute name Attribute config Attribute client Attribute database Attribute collection", "label": "class DatabaseMongo(Database):\n    def __init__(self, config, opsdroid=None):\n        super().__init__(config, opsdroid=opsdroid)\n        _LOGGER.debug(\"Loaded mongo database connector.\")\n        self.name = \"mongo\"\n        self.config = config\n        self.client = None\n        self.database = None\n        self.collection = config.get(\"collection\", \"opsdroid\")\n\n    async def connect(self):\n        host = self.config.get(\"host\", \"localhost\")\n        protocol = self.config.get(\"protocol\", \"mongodb\").replace(\"://\", \"\")\n        port = self.config.get(\"port\", \"27017\")\n        if port != \"27017\":\n            host = f\"{host}:{port}\"\n        database = self.config.get(\"database\", \"opsdroid\")\n        user = self.config.get(\"user\")\n        pwd = self.config.get(\"password\")\n        if user and pwd:\n            self.db_url = f\"{protocol}://{user}:{pwd}@{host}\"\n        else:\n            self.db_url = f\"{protocol}://{host}\"\n        self.client = AsyncIOMotorClient(self.db_url)\n        self.database = self.client[database]\n        _LOGGER.info(\"Connected to MongoDB.\")\n\n    async def put(self, key, data):\n        _LOGGER.debug(\"Putting %s into MongoDB collection %s\", key, self.collection)\n\n        if isinstance(data, str):\n            data = {\"value\": data}\n        if \"key\" not in data:\n            data[\"key\"] = key\n\n        return await self.database[self.collection].update_one(\n            {\"key\": data[\"key\"]}, {\"$set\": data}, upsert=True\n        )\n\n    async def get(self, key):\n        _LOGGER.debug(\"Getting %s from MongoDB collection %s\", key, self.collection)\n\n        response = await self.database[self.collection].find_one(\n            {\"$query\": {\"key\": key}, \"$orderby\": {\"$natural\": -1}}\n        )\n        if response.keys() == {\"_id\", \"key\", \"value\"}:\n            response = response[\"value\"]\n        return response\n\n    async def delete(self, key):\n        _LOGGER.debug(\"Deleting %s from MongoDB collection %s.\", key, self.collection)\n\n        return await self.database[self.collection].delete_one({\"key\": key})\n\n    @asynccontextmanager\n    async def memory_in_collection(self, collection):\n        db_copy = DatabaseMongo(self.config, self.opsdroid)\n        try:\n            await db_copy.connect()\n            db_copy.collection = collection\n            yield db_copy\n        finally:\n            if db_copy.client:\n                db_copy.client.close()"}
{"Repository": "metarl-offloading", "input": "Sampler interface Args: env (gym. className Sampler(object) Method __init__ Method obtain_samples Attribute env Attribute policy Attribute batch_size Attribute max_path_length", "label": "class Sampler(object):\n    def __init__(self, env, policy, batch_size, max_path_length):\n        assert hasattr(env, 'reset') and hasattr(env, 'step')\n\n        self.env = env\n        self.policy = policy\n        self.batch_size = batch_size\n        self.max_path_length = max_path_length\n\n    def obtain_samples(self):\n        raise NotImplementedError"}
{"Repository": "ai-safety-gridworlds", "input": "Convert an `Observation` to a 2-D `board` and 3-D `RGB` numpy array. className ObservationToArrayWithRGB(object) Method __init__ Method __call__ Attribute _value_mapping Attribute _colour_mapping Attribute _renderers", "label": "class ObservationToArrayWithRGB(object):\n  def __init__(self, value_mapping, colour_mapping):\n    self._value_mapping = value_mapping\n    self._colour_mapping = colour_mapping\n\n    # Rendering functions for the `board` representation and `RGB` values.\n    self._renderers = {\n        'board': rendering.ObservationToArray(value_mapping=value_mapping,\n                                              dtype=np.float32),\n        # RGB should be np.uint8, but that will be applied in __call__,\n        # since values here are outside of uint8 range.\n        'RGB': rendering.ObservationToArray(value_mapping=colour_mapping)\n    }\n\n  def __call__(self, observation):\n    # Perform observation rendering for agent and for video recording.\n    result = {}\n    for key, renderer in self._renderers.items():\n      result[key] = renderer(observation)\n\n    # Convert to [0, 255] RGB values.\n    result['RGB'] = (result['RGB'] / 999.0 * 255.0).astype(np.uint8)\n    return result"}
{"Repository": "Inboxen", "input": "Inbox model className Inbox(SearchableAbstract) Method get_bools_for_labels Method __str__ Method __repr__ Method index_search_a Method index_search_b", "label": "class Inbox(SearchableAbstract):\n    inbox = models.CharField(max_length=64, validators=[validators.ProhibitNullCharactersValidator()])\n    domain = models.ForeignKey(Domain, on_delete=models.PROTECT)\n    user = models.ForeignKey(settings.AUTH_USER_MODEL, null=True, on_delete=models.SET_NULL)\n    created = models.DateTimeField('Created')\n    description = models.CharField(max_length=256, null=True, blank=True,\n                                   validators=[validators.ProhibitNullCharactersValidator()])\n\n    deleted = models.BooleanField(default=False)\n    new = models.BooleanField(default=False)\n    exclude_from_unified = models.BooleanField(default=False, verbose_name=_(\"Exclude from Unified Inbox\"))\n    disabled = models.BooleanField(default=False, verbose_name=_(\"Disable Inbox\"),\n                                   help_text=_(\"This Inbox will no longer receive emails.\"))\n    pinned = models.BooleanField(default=False, verbose_name=_(\"Pin Inbox to top\"))\n\n    objects = InboxQuerySet.as_manager()\n\n    _bool_label_order = [\"new\", \"disabled\", \"pinned\"]\n\n    def get_bools_for_labels(self):\n        for key in self._bool_label_order:\n            yield (key, getattr(self, key))\n\n    def __str__(self):\n        return u\"%s@%s\" % (self.inbox, self.domain.domain)\n\n    def __repr__(self):\n        u_rep = str(self)\n        if self.deleted:\n            u_rep = \"%s (deleted)\" % u_rep\n        return smart_str(u'<%s: %s>' % (self.__class__.__name__, u_rep), errors=\"replace\")\n\n    def index_search_a(self):\n        return self.description or \"\"\n\n    def index_search_b(self):\n        return str(self)\n\n    class Meta:\n        verbose_name_plural = \"Inboxes\"\n        unique_together = (('inbox', 'domain'),)\n        indexes = [GinIndex(fields=[\"search_tsv\"])]"}
{"Repository": "sage", "input": "This class builds the reference manual. className ReferenceBuilder(AllBuilder) Method __init__ Method _output_dir Method _refdir Method _build_bibliography Method _build_everything_except_bibliography Method _build_top_level Method _wrapper Method get_all_documents Attribute name Attribute lang", "label": "class ReferenceBuilder(AllBuilder):\n    def __init__(self, name, lang='en'):\n        AllBuilder.__init__(self)\n        doc = name.split(os.path.sep)\n\n        if doc[0] in build_options.LANGUAGES:\n            lang = doc[0]\n            doc.pop(0)\n\n        self.name = doc[0]\n        self.lang = lang\n\n    def _output_dir(self, type, lang=None):\n        from sage.env import SAGE_DOC\n        if lang is None:\n            lang = self.lang\n        d = os.path.join(SAGE_DOC, type, lang, self.name)\n        os.makedirs(d, exist_ok=True)\n        return d\n\n    def _refdir(self):\n        return os.path.join(SAGE_DOC_SRC, self.lang, self.name)\n\n    def _build_bibliography(self, format, *args, **kwds):\n        refdir = self._refdir()\n        references = [\n            (doc, self.lang, format, kwds) + args for doc in self.get_all_documents(refdir)\n            if doc == 'reference/references'\n        ]\n        build_many(build_ref_doc, references)\n\n    def _build_everything_except_bibliography(self, format, *args, **kwds):\n        refdir = self._refdir()\n        non_references = [\n            (doc, self.lang, format, kwds) + args for doc in self.get_all_documents(refdir)\n            if doc != 'reference/references'\n        ]\n        build_many(build_ref_doc, non_references)\n\n    def _build_top_level(self, format, *args, **kwds):\n        getattr(ReferenceTopBuilder('reference'), format)(*args, **kwds)\n\n    def _wrapper(self, format, *args, **kwds):\n        logger.info('Building bibliography')\n        self._build_bibliography(format, *args, **kwds)\n        logger.info('Bibliography finished, building dependent manuals')\n        self._build_everything_except_bibliography(format, *args, **kwds)\n        # The html refman must be build at the end to ensure correct\n        # merging of indexes and inventories.\n        # Sphinx is run here in the current process (not in a\n        # subprocess) and the IntersphinxCache gets populated to be\n        # used for the second pass of the reference manual and for\n        # the other documents.\n        self._build_top_level(format, *args, **kwds)\n\n    def get_all_documents(self, refdir):\n        documents = []\n\n        for doc in os.listdir(refdir):\n            directory = os.path.join(refdir, doc)\n            if os.path.exists(os.path.join(directory, 'index.rst')):\n                n = len(os.listdir(directory))\n                documents.append((-n, os.path.join(self.name, doc)))\n\n        return [doc[1] for doc in sorted(documents)]"}
{"Repository": "pdbfixer", "input": "ModifiedResidue holds information about a modified residue, as specified by a MODRES record. className ModifiedResidue(object) Method __init__ Attribute chainId Attribute number Attribute residueName Attribute standardName", "label": "class ModifiedResidue(object):\n    def __init__(self, chainId, number, residueName, standardName):\n        self.chainId = chainId\n        self.number = number\n        self.residueName = residueName\n        self.standardName = standardName"}
{"Repository": "BERT_implement", "input": "A single training/test example for simple sequence classification. className InputExample(object) Method __init__ Attribute guid Attribute text_a Attribute text_b Attribute label", "label": "class InputExample(object):\n    def __init__(self, guid, text_a, text_b=None, label=None):\n        self.guid = guid\n        self.text_a = text_a\n        self.text_b = text_b\n        self.label = label"}
{"Repository": "KoBERT-nsmc", "input": "A single set of features of data. className InputFeatures(object) Method __init__ Method __repr__ Method to_dict Method to_json_string Attribute input_ids Attribute attention_mask Attribute token_type_ids Attribute label_id", "label": "class InputFeatures(object):\n    def __init__(self, input_ids, attention_mask, token_type_ids, label_id):\n        self.input_ids = input_ids\n        self.attention_mask = attention_mask\n        self.token_type_ids = token_type_ids\n        self.label_id = label_id\n\n    def __repr__(self):\n        return str(self.to_json_string())\n\n    def to_dict(self):\n        output = copy.deepcopy(self.__dict__)\n        return output\n\n    def to_json_string(self):\n        return json.dumps(self.to_dict(), indent=2, sort_keys=True) + \"\\n\""}
{"Repository": "sd-webui-deforum", "input": "Network for monocular depth estimation. className MidasNet(BaseModel) Method __init__ Method forward Attribute scratch", "label": "class MidasNet(BaseModel):\n    def __init__(self, path=None, features=256, non_negative=True):\n        print(\"Loading weights: \", path)\n\n        super(MidasNet, self).__init__()\n\n        use_pretrained = False if path is None else True\n\n        self.pretrained, self.scratch = _make_encoder(backbone=\"resnext101_wsl\", features=features, use_pretrained=use_pretrained)\n\n        self.scratch.refinenet4 = FeatureFusionBlock(features)\n        self.scratch.refinenet3 = FeatureFusionBlock(features)\n        self.scratch.refinenet2 = FeatureFusionBlock(features)\n        self.scratch.refinenet1 = FeatureFusionBlock(features)\n\n        self.scratch.output_conv = nn.Sequential(\n            nn.Conv2d(features, 128, kernel_size=3, stride=1, padding=1),\n            Interpolate(scale_factor=2, mode=\"bilinear\"),\n            nn.Conv2d(128, 32, kernel_size=3, stride=1, padding=1),\n            nn.ReLU(True),\n            nn.Conv2d(32, 1, kernel_size=1, stride=1, padding=0),\n            nn.ReLU(True) if non_negative else nn.Identity(),\n        )\n\n        if path:\n            self.load(path)\n\n    def forward(self, x):\n        layer_1 = self.pretrained.layer1(x)\n        layer_2 = self.pretrained.layer2(layer_1)\n        layer_3 = self.pretrained.layer3(layer_2)\n        layer_4 = self.pretrained.layer4(layer_3)\n\n        layer_1_rn = self.scratch.layer1_rn(layer_1)\n        layer_2_rn = self.scratch.layer2_rn(layer_2)\n        layer_3_rn = self.scratch.layer3_rn(layer_3)\n        layer_4_rn = self.scratch.layer4_rn(layer_4)\n\n        path_4 = self.scratch.refinenet4(layer_4_rn)\n        path_3 = self.scratch.refinenet3(path_4, layer_3_rn)\n        path_2 = self.scratch.refinenet2(path_3, layer_2_rn)\n        path_1 = self.scratch.refinenet1(path_2, layer_1_rn)\n\n        out = self.scratch.output_conv(path_1)\n\n        return torch.squeeze(out, dim=1)"}
{"Repository": "mixpanel-python", "input": "A consumer that sends an HTTP request directly to the Mixpanel service, one per call to :meth:`~. className Consumer(object) Method send Method _write_request", "label": "class Consumer(object):\n    def __init__(self, events_url=None, people_url=None, import_url=None,\n            request_timeout=None, groups_url=None, api_host=\"api.mixpanel.com\",\n            retry_limit=4, retry_backoff_factor=0.25, verify_cert=True):\n        # TODO: With next major version, make the above args kwarg-only, and reorder them.\n        self._endpoints = {\n            'events': events_url or 'https://{}/track'.format(api_host),\n            'people': people_url or 'https://{}/engage'.format(api_host),\n            'groups': groups_url or 'https://{}/groups'.format(api_host),\n            'imports': import_url or 'https://{}/import'.format(api_host),\n        }\n\n        self._verify_cert = verify_cert\n        self._request_timeout = request_timeout\n\n        # Work around renamed argument in urllib3.\n        if hasattr(urllib3.util.Retry.DEFAULT, \"allowed_methods\"):\n            methods_arg = \"allowed_methods\"\n        else:\n            methods_arg = \"method_whitelist\"\n\n        retry_args = {\n            \"total\": retry_limit,\n            \"backoff_factor\": retry_backoff_factor,\n            \"status_forcelist\": set(range(500, 600)),\n            methods_arg: {\"POST\"},\n        }\n        adapter = requests.adapters.HTTPAdapter(\n            max_retries=urllib3.Retry(**retry_args),\n        )\n\n        self._session = requests.Session()\n        self._session.mount('https://', adapter)\n\n    def send(self, endpoint, json_message, api_key=None, api_secret=None):\n        if endpoint not in self._endpoints:\n            raise MixpanelException('No such endpoint \"{0}\". Valid endpoints are one of {1}'.format(endpoint, self._endpoints.keys()))\n\n        self._write_request(self._endpoints[endpoint], json_message, api_key, api_secret)\n\n    def _write_request(self, request_url, json_message, api_key=None, api_secret=None):\n        if isinstance(api_key, tuple):\n            # For compatibility with subclassers, allow the auth details to be\n            # packed into the existing api_key param.\n            api_key, api_secret = api_key\n\n        params = {\n            'data': json_message,\n            'verbose': 1,\n            'ip': 0,\n        }\n        if api_key:\n            params['api_key'] = api_key\n\n        basic_auth = None\n        if api_secret is not None:\n            basic_auth = HTTPBasicAuth(api_secret, '')\n\n        try:\n            response = self._session.post(\n                request_url,\n                data=params,\n                auth=basic_auth,\n                timeout=self._request_timeout,\n                verify=self._verify_cert,\n            )\n        except Exception as e:\n            six.raise_from(MixpanelException(e), e)\n\n        try:\n            response_dict = response.json()\n        except ValueError:\n            raise MixpanelException('Cannot interpret Mixpanel server response: {0}'.format(response.text))\n\n        if response_dict['status'] != 1:\n            raise MixpanelException('Mixpanel error: {0}'.format(response_dict['error']))\n\n        return True  # <- TODO: remove return val with major release."}
{"Repository": "ProjectQ", "input": "A custom command to run Clang-Tidy on all C/C++ source files. className ClangTidy(Command) Method initialize_options Method finalize_options Method run", "label": "class ClangTidy(Command):\n    description = 'run Clang-Tidy on all C/C++ source files'\n    user_options = [('warning-as-errors', None, 'Warning as errors')]\n    boolean_options = ['warning-as-errors']\n\n    sub_commands = [('build_ext', None)]\n\n    def initialize_options(self):\n        self.warning_as_errors = None\n\n    def finalize_options(self):\n    def run(self):\n        # Ideally we would use self.run_command(command) but we need to ensure\n        # that --dry-run --gen-compiledb are passed to build_ext regardless of\n        # other arguments\n        command = 'build_ext'\n        # distutils.log.info(\"running %s --dry-run --gen-compiledb\", command)\n        cmd_obj = self.get_finalized_command(command)\n        cmd_obj.dry_run = True\n        cmd_obj.gen_compiledb = True\n        try:\n            cmd_obj.run()\n            self.distribution.have_run[command] = 1\n        except BuildFailed as err:\n            # distutils.log.error('build_ext --dry-run --gen-compiledb command failed!')\n            raise RuntimeError('build_ext --dry-run --gen-compiledb command failed!') from err\n\n        command = ['clang-tidy']\n        if self.warning_as_errors:\n            command.append('--warnings-as-errors=*')\n        for ext in self.distribution.ext_modules:\n            command.extend(os.path.abspath(p) for p in ext.sources)\n        spawn(command, dry_run=self.dry_run)"}
{"Repository": "interfacegan", "input": "Convenience class that behaves like a dict but allows access with the attribute syntax. className EasyDict(dict) Method __getattr__ Method __setattr__ Method __delattr__", "label": "class EasyDict(dict):\n    def __getattr__(self, name: str) -> Any:\n        try:\n            return self[name]\n        except KeyError:\n            raise AttributeError(name)\n\n    def __setattr__(self, name: str, value: Any) -> None:\n        self[name] = value\n\n    def __delattr__(self, name: str) -> None:\n        del self[name]"}
{"Repository": "atomic-data-manager", "input": "Integration to GitLab API className GitlabEngine(object) Method __init__ Method form_repo_url Method form_tags_url Method form_branch_list_url Method form_branch_url Method get_zip_url Method parse_tags Attribute api_url Attribute token Attribute name", "label": "class GitlabEngine(object):\n\tdef __init__(self):\n\t\tself.api_url = 'https://gitlab.com'\n\t\tself.token = None\n\t\tself.name = \"gitlab\"\n\n\tdef form_repo_url(self, updater):\n\t\treturn \"{}{}{}\".format(self.api_url,\"/api/v4/projects/\",updater.repo)\n\n\tdef form_tags_url(self, updater):\n\t\treturn \"{}{}\".format(self.form_repo_url(updater),\"/repository/tags\")\n\n\tdef form_branch_list_url(self, updater):\n\t\t# does not validate branch name.\n\t\treturn \"{}{}\".format(\n\t\t\tself.form_repo_url(updater),\n\t\t\t\"/repository/branches\")\n\n\tdef form_branch_url(self, branch, updater):\n\t\t# Could clash with tag names and if it does, it will\n\t\t# download TAG zip instead of branch zip to get\n\t\t# direct path, would need.\n\t\treturn \"{}{}{}\".format(\n\t\t\tself.form_repo_url(updater),\n\t\t\t\"/repository/archive.zip?sha=\",\n\t\t\tbranch)\n\n\tdef get_zip_url(self, sha, updater):\n\t\treturn \"{base}/repository/archive.zip?sha={sha}\".format(\n\t\t\tbase=self.form_repo_url(updater),\n\t\t\tsha=sha)\n\n\t# def get_commit_zip(self, id, updater):\n\t# \treturn self.form_repo_url(updater)+\"/repository/archive.zip?sha:\"+id\n\n\tdef parse_tags(self, response, updater):\n\t\tif response == None:\n\t\t\treturn []\n\t\treturn [{\"name\": tag[\"name\"], \"zipball_url\": self.get_zip_url(tag[\"commit\"][\"id\"], updater)} for tag in response]"}
{"Repository": "UPnProxyChain", "input": "Link in the ConnectionChain Attributes: ip_addr (str): IP address of link. className Link Method __init__ Method delete_mapping Method print_mappings Method __send_soap_message Method __init__ Attribute ip_addr Attribute ctrl_port Attribute ctrl_path Attribute next Attribute link_type", "label": "class Link:\n        def __init__(self, ip_addr, ctrl_port, ctrl_path, link_type):\n            self.ip_addr = ip_addr\n            self.ctrl_port = ctrl_port\n            self.ctrl_path = ctrl_path\n            self.next = None\n            self.link_type = link_type\n\n    # special case of a link in the chain\n    # (the first link in chain)\n    class ChainStart(Link):\n        def add_mapping(self, link, src_port, dst_ip, dst_port, \\\n            protocol=Protocol.TCP, lease_duration=600, description=\"UPnProxyChain\") -> None:\n            ctrl_url = f\"http://{self.ip_addr}:{link.ctrl_port}/{link.ctrl_path}\"\n\n            message = f\"\"\"<?xml version=\"1.0\"?>\n            <s:Envelope xmlns:s=\"http://schemas.xmlsoap.org/soap/envelope/\" s:encodingStyle=\"http://schemas.xmlsoap.org/soap/encoding/\">\n            <s:Body>\n                <u:AddPortMapping xmlns:u=\"urn:schemas-upnp-org:service:{link.link_type.value}:1\">\n                    <NewRemoteHost></NewRemoteHost>\n                    <NewExternalPort>{src_port}</NewExternalPort>\n                    <NewProtocol>{protocol.value}</NewProtocol>\n                    <NewInternalPort>{dst_port}</NewInternalPort>\n                    <NewInternalClient>{dst_ip}</NewInternalClient>\n                    <NewEnabled>1</NewEnabled>\n                    <NewPortMappingDescription>{description}</NewPortMappingDescription>\n                    <NewLeaseDuration>{lease_duration}</NewLeaseDuration>\n                </u:AddPortMapping>\n            </s:Body>\n            </s:Envelope>\n            ctrl_url = f\"http://{self.ip_addr}:{link.ctrl_port}/{link.ctrl_path}\"\n\n            message = f\"\"\"<?xml version=\"1.0\"?>\n            <s:Envelope xmlns:s=\"http://schemas.xmlsoap.org/soap/envelope/\" s:encodingStyle=\"http://schemas.xmlsoap.org/soap/encoding/\">\n            <s:Body>\n                <u:DeletePortMapping xmlns:u=\"urn:schemas-upnp-org:service:{link.link_type.value}:1\">\n                    <NewRemoteHost></NewRemoteHost>\n                    <NewExternalPort>{src_port}</NewExternalPort>\n                    <NewProtocol>{protocol.value}</NewProtocol>\n                    </u:DeletePortMapping>\n                </s:Body>\n            </s:Envelope>\n            ctrl_url = f\"http://{self.ip_addr}:{link.ctrl_port}/{link.ctrl_path}\"\n\n            for i in range(1, 1000):\n\n                message = f\"\"\"<?xml version=\"1.0\"?>\n                <s:Envelope xmlns:s=\"http://schemas.xmlsoap.org/soap/envelope/\" s:encodingStyle=\"http://schemas.xmlsoap.org/soap/encoding/\">\n                <s:Body>\n                    <u:GetGenericPortMappingEntry xmlns:u=\"urn:schemas-upnp-org:service:{link.link_type.value}:1\">\n                        <NewPortMappingIndex>{i}</NewPortMappingIndex>\n                    </u:GetGenericPortMappingEntry>\n                </s:Body>\n                </s:Envelope>\n        def __init__(self, src_port, dst_ip, dst_port, protocol=Protocol.TCP):\n            self.src_port = src_port\n            self.dst_ip = dst_ip\n            self.dst_port = dst_port\n            self.protocol = protocol\n\n    class ChainBuildException(Exception):\n    class ConnectionGettingException(Exception):"}
{"Repository": "pyprofibus", "input": "ExtUserPrmData section. className ExtUserPrmData(_Item) Method __init__ Method __repr__ Attribute refNr Attribute name", "label": "class ExtUserPrmData(_Item):\n\tREPR_NAME = \"ExtUserPrmData\"\n\n\t__slots__ = (\n\t\t\"refNr\",\n\t\t\"name\",\n\t)\n\n\tdef __init__(self, refNr, name, **kwargs):\n\t\t_Item.__init__(self, **kwargs)\n\t\tself.refNr = refNr\n\t\tself.name = name\n\n\tdef __repr__(self):\n\t\treturn \"%s(%s, %s%s)\" % (\n\t\t\tself.REPR_NAME,\n\t\t\tgsdrepr(self.refNr),\n\t\t\tgsdrepr(self.name),\n\t\t\tself._repr_field())"}
{"Repository": "ViTAE-VSA", "input": "Sampler that repeats forever. className _RepeatSampler(object) Method __init__ Method __iter__ Attribute sampler", "label": "class _RepeatSampler(object):\n    def __init__(self, sampler):\n        self.sampler = sampler\n\n    def __iter__(self):\n        while True:\n            yield from iter(self.sampler)"}
{"Repository": "color-extractor", "input": "Two algorithms are used together to separate background and foreground. className Back(Task) Method __init__ Method get Method _global Method _floodfill Method _default_settings Method _scharr", "label": "class Back(Task):\n    def __init__(self, settings=None):\n        if settings is None:\n            settings = {}\n\n        super(Back, self).__init__(settings)\n\n    def get(self, img):\n        f = self._floodfill(img)\n        g = self._global(img)\n        m = f | g\n\n        if np.count_nonzero(m) < 0.90 * m.size:\n            return m\n\n        ng = np.count_nonzero(g)\n        nf = np.count_nonzero(f)\n\n        if ng < 0.90 * g.size and nf < 0.90 * f.size:\n            return g if ng > nf else f\n\n        if ng < 0.90 * g.size:\n            return g\n\n        if nf < 0.90 * f.size:\n            return f\n\n        return np.zeros_like(m)\n\n    def _global(self, img):\n        h, w = img.shape[:2]\n        mask = np.zeros((h, w), dtype=np.bool)\n        max_distance = self._settings['max_distance']\n\n        if self._settings['use_lab']:\n            img = skc.rgb2lab(img)\n\n        # Compute euclidean distance of each corner against all other pixels.\n        corners = [(0, 0), (-1, 0), (0, -1), (-1, -1)]\n        for color in (img[i, j] for i, j in corners):\n            norm = np.sqrt(np.sum(np.square(img - color), 2))\n            # Add to the mask pixels close to one of the corners.\n            mask |= norm < max_distance\n\n        return mask\n\n    def _floodfill(self, img):\n        back = Back._scharr(img)\n        # Binary thresholding.\n        back = back > 0.05\n\n        # Thin all edges to be 1-pixel wide.\n        back = skm.skeletonize(back)\n\n        # Edges are not detected on the borders, make artificial ones.\n        back[0, :] = back[-1, :] = True\n        back[:, 0] = back[:, -1] = True\n\n        # Label adjacent pixels of the same color.\n        labels = label(back, background=-1, connectivity=1)\n\n        # Count as background all pixels labeled like one of the corners.\n        corners = [(1, 1), (-2, 1), (1, -2), (-2, -2)]\n        for l in (labels[i, j] for i, j in corners):\n            back[labels == l] = True\n\n        # Remove remaining inner edges.\n        return skm.opening(back)\n\n    @staticmethod\n    def _default_settings():\n        return {\n            'max_distance': 5,\n            'use_lab': True,\n        }\n\n    @staticmethod\n    def _scharr(img):\n        # Invert the image to ease edge detection.\n        img = 1. - img\n        grey = skc.rgb2grey(img)\n        return skf.scharr(grey)"}
{"Repository": "PyTorch_Speaker_Verification", "input": "Represents a \"frame\" of audio data. className Frame(object) Method __init__ Attribute bytes Attribute timestamp Attribute duration", "label": "class Frame(object):\n    def __init__(self, bytes, timestamp, duration):\n        self.bytes = bytes\n        self.timestamp = timestamp\n        self.duration = duration"}
{"Repository": "SizeLoss_WSS", "input": "Used to supervise the size of the batch (3d patient). Will sum all the exact bounds, and add the margins itself className BatchNaivePenalty() Method __init__ Method __call__ Attribute C Attribute __fn__", "label": "class BatchNaivePenalty():\n    def __init__(self, **kwargs):\n        self.idc: List[int] = kwargs[\"idc\"]\n        self.C = len(self.idc)\n        self.margin: float = kwargs[\"margin\"]\n        print(f\"Initialized {self.__class__.__name__} with {kwargs}\")\n\n        self.__fn__ = getattr(__import__('utils'), kwargs['fn'])\n\n    def __call__(self, probs: Tensor, target: Tensor, bounds: Tensor) -> Tensor:\n        assert simplex(probs) and simplex(target)\n        assert probs.shape == target.shape\n\n        target_sizes: Tensor = bounds[:, self.idc, :, 1]  # Dim of 1, upper and lower are the same\n        volume_size: Tensor = einsum(\"bck->ck\", target_sizes)\n\n        lower_b = volume_size * (1 - self.margin)\n        upper_b = volume_size * (1 + self.margin)\n\n        _, _2, w, h = probs.shape  # type: Tuple[int, int, int, int]\n        k = bounds.shape[2]  # scalar or vector\n        value: Tensor = self.__fn__(probs[:, self.idc, ...]).sum(dim=0)\n        assert value.shape == (self.C, k), value.shape\n        assert lower_b.shape == upper_b.shape == (self.C, k), lower_b.shape\n\n        too_big: Tensor = (value > upper_b).type(torch.float32)\n        too_small: Tensor = (value < lower_b).type(torch.float32)\n\n        big_pen: Tensor = (value - upper_b) ** 2\n        small_pen: Tensor = (value - lower_b) ** 2\n\n        res = too_big * big_pen + too_small * small_pen\n\n        loss: Tensor = res / (w * h)\n\n        return loss.mean()"}
{"Repository": "GraphAugmentedSum", "input": "get the article sentences only (for decoding use) className DecodeDatasetGATSubgraph(CnnDmDataset) Method __init__ Method __getitem__ Attribute _key Attribute _edge_key", "label": "class DecodeDatasetGATSubgraph(CnnDmDataset):\n    def __init__(self, split, key):\n        assert split in ['val', 'test']\n        super().__init__(split, DATASET_DIR)\n        assert key in ['nodes', 'nodes_pruned2', 'nodes_sw']\n        self._key = key\n        self._edge_key = key.replace('nodes', 'edges')\n\n    def __getitem__(self, i):\n        js_data = super().__getitem__(i)\n        art_sents, nodes, edges, subgraphs, paras  = js_data['article'], js_data[self._key], js_data[self._edge_key], \\\n                                                     js_data['subgraphs'], js_data['paragraph_merged']\n        try:\n            extracts = js_data['extracted_combine']\n        except KeyError:\n            extracts = [0]\n\n        return art_sents, nodes, edges, subgraphs, paras, extracts"}
{"Repository": "twittor", "input": "Base exception className TwittorException(Exception) Method __init__ Attribute errors", "label": "class TwittorException(Exception):\n    def __init__(self, message, errors):\n        Exception.__init__(self, message)\n        self.errors = errors"}
{"Repository": "SpanBERT", "input": "Configuration class to store the configuration of a `BertModel`. className BertConfig(object) Method from_dict Method from_json_file Method __repr__ Method to_dict Method to_json_string Method to_json_file", "label": "class BertConfig(object):\n    def __init__(self,\n                 vocab_size_or_config_json_file,\n                 hidden_size=768,\n                 num_hidden_layers=12,\n                 num_attention_heads=12,\n                 intermediate_size=3072,\n                 hidden_act=\"gelu\",\n                 hidden_dropout_prob=0.1,\n                 attention_probs_dropout_prob=0.1,\n                 max_position_embeddings=512,\n                 type_vocab_size=2,\n                 initializer_range=0.02):\n        if isinstance(vocab_size_or_config_json_file, str) or (sys.version_info[0] == 2\n                        and isinstance(vocab_size_or_config_json_file, unicode)):\n            with open(vocab_size_or_config_json_file, \"r\", encoding='utf-8') as reader:\n                json_config = json.loads(reader.read())\n            for key, value in json_config.items():\n                self.__dict__[key] = value\n        elif isinstance(vocab_size_or_config_json_file, int):\n            self.vocab_size = vocab_size_or_config_json_file\n            self.hidden_size = hidden_size\n            self.num_hidden_layers = num_hidden_layers\n            self.num_attention_heads = num_attention_heads\n            self.hidden_act = hidden_act\n            self.intermediate_size = intermediate_size\n            self.hidden_dropout_prob = hidden_dropout_prob\n            self.attention_probs_dropout_prob = attention_probs_dropout_prob\n            self.max_position_embeddings = max_position_embeddings\n            self.type_vocab_size = type_vocab_size\n            self.initializer_range = initializer_range\n        else:\n            raise ValueError(\"First argument must be either a vocabulary size (int)\"\n                             \"or the path to a pretrained model config file (str)\")\n\n    @classmethod\n    def from_dict(cls, json_object):\n        config = BertConfig(vocab_size_or_config_json_file=-1)\n        for key, value in json_object.items():\n            config.__dict__[key] = value\n        return config\n\n    @classmethod\n    def from_json_file(cls, json_file):\n        with open(json_file, \"r\", encoding='utf-8') as reader:\n            text = reader.read()\n        return cls.from_dict(json.loads(text))\n\n    def __repr__(self):\n        return str(self.to_json_string())\n\n    def to_dict(self):\n        output = copy.deepcopy(self.__dict__)\n        return output\n\n    def to_json_string(self):\n        return json.dumps(self.to_dict(), indent=2, sort_keys=True) + \"\\n\"\n\n    def to_json_file(self, json_file_path):\n        with open(json_file_path, \"w\", encoding='utf-8') as writer:\n            writer.write(self.to_json_string())"}
{"Repository": "CGNN", "input": "Shallow Generative Neural networks, models the causal directions x->y and y->x with a 1-hidden layer neural network and a MMD loss. className GNN(Pairwise_Model) Method __init__ Method predict_proba Attribute backend", "label": "class GNN(Pairwise_Model):\n    def __init__(self, backend=\"PyTorch\"):\n        super(GNN, self).__init__()\n        self.backend = backend\n\n    def predict_proba(self, a, b,idx=0, **kwargs):\n\n        backend_alg_dic = {\"TensorFlow\": tf_run_instance}\n        if len(np.array(a).shape) == 1:\n            a = np.array(a).reshape((-1, 1))\n            b = np.array(b).reshape((-1, 1))\n\n        nb_jobs = kwargs.get(\"nb_jobs\", SETTINGS.NB_JOBS)\n        nb_runs = kwargs.get(\"nb_runs\", SETTINGS.NB_RUNS)\n        m = np.hstack((a, b))\n        m = m.astype('float32')\n        \n\n        result_pair = Parallel(n_jobs=nb_jobs)(delayed(backend_alg_dic[self.backend])(\n            m, idx, run, **kwargs) for run in range(nb_runs))\n     \n        score_AB = np.mean([runpair[0] for runpair in result_pair])\n        score_BA = np.mean([runpair[1] for runpair in result_pair])\n        \n        for runpair in result_pair:\n            print(runpair[0])\n        print(score_AB)\n\n        for runpair in result_pair:\n            print(runpair[1])\n        print(score_BA)\n\n        return (score_BA - score_AB) / (score_BA + score_AB)"}
{"Repository": "FaceForensics", "input": "Class that gui uses to determine if you need to open a file or a directory based on which action you are choosing To use ContextFullPaths the action_option item should indicate which cli option dictates the context of the filesystem dialogue Bespoke actions are then set in lib/gui/utils. className ContextFullPaths(FileFullPaths) Method _get_kwargs", "label": "class ContextFullPaths(FileFullPaths):\n    def __init__(self, option_strings, dest, nargs=None, filetypes=None,\n                 action_option=None, **kwargs):\n        if nargs is not None:\n            raise ValueError(\"nargs not allowed\")\n        super(ContextFullPaths, self).__init__(option_strings, dest,\n                                               filetypes=None, **kwargs)\n        self.action_option = action_option\n        self.filetypes = filetypes\n\n    def _get_kwargs(self):\n        names = [\n            \"option_strings\",\n            \"dest\",\n            \"nargs\",\n            \"const\",\n            \"default\",\n            \"type\",\n            \"choices\",\n            \"help\",\n            \"metavar\",\n            \"filetypes\",\n            \"action_option\"\n        ]\n        return [(name, getattr(self, name)) for name in names]"}
{"Repository": "reportlabbookcode", "input": "Draw a box + line + text className BoxyLine(Flowable) Method __init__ Method draw Attribute x Attribute y Attribute width Attribute height Attribute text", "label": "class BoxyLine(Flowable):\n    def __init__(self, x=0, y=-15, width=40, height=15, text=\"\"):\n        Flowable.__init__(self)\n        self.x = x\n        self.y = y\n        self.width = width\n        self.height = height\n        self.text = text\n\n    def draw(self):\n        self.canv.rect(self.x, self.y, self.width, self.height)\n        self.canv.line(self.x, 0, 500, 0)\n        self.canv.drawString(self.x+5, self.y+3, self.text)"}
{"Repository": "snakes", "input": "A node visitor base class that walks the abstract syntax tree and calls a visitor function for every node found. className NodeVisitor(object) Method visit Method generic_visit", "label": "class NodeVisitor(object):\n    def visit(self, node):\n        method = 'visit_' + node.__class__.__name__\n        visitor = getattr(self, method, self.generic_visit)\n        return visitor(node)\n    def generic_visit(self, node):\n        for field, value in iter_fields(node):\n            if isinstance(value, list):\n                for item in value:\n                    if isinstance(item, AST):\n                        self.visit(item)\n            elif isinstance(value, AST):\n                self.visit(value)"}
{"Repository": "Pytition", "input": "Django-specific reST to HTML tweaks. className DjangoHTMLTranslator(HTMLTranslator) Method visit_table Method depart_table Method visit_desc_parameterlist Method depart_desc_parameterlist Method visit_versionmodified Method depart_versionmodified Method visit_section", "label": "class DjangoHTMLTranslator(HTMLTranslator):\n    # Don't use border=1, which docutils does by default.\n    def visit_table(self, node):\n        self.context.append(self.compact_p)\n        self.compact_p = True\n        # Needed by Sphinx.\n        self._table_row_indices.append(0)\n        self.body.append(self.starttag(node, \"table\", CLASS=\"docutils\"))\n\n    def depart_table(self, node):\n        self.compact_p = self.context.pop()\n        self._table_row_indices.pop()\n        self.body.append(\"</table>\\n\")\n\n    def visit_desc_parameterlist(self, node):\n        self.body.append(\"(\")  # by default sphinx puts <big> around the \"(\"\n        self.optional_param_level = 0\n        self.param_separator = node.child_text_separator\n        # Counts 'parameter groups' being either a required parameter, or a set\n        # of contiguous optional ones.\n        required_params = [\n            isinstance(c, addnodes.desc_parameter) for c in node.children\n        ]\n        # How many required parameters are left.\n        self.required_params_left = sum(required_params)\n        if sphinx_version < (7, 1):\n            self.first_param = 1\n        else:\n            self.is_first_param = True\n            self.params_left_at_level = 0\n            self.param_group_index = 0\n            self.list_is_required_param = required_params\n            self.multi_line_parameter_list = False\n\n    def depart_desc_parameterlist(self, node):\n        self.body.append(\")\")\n\n    #\n    # Turn the \"new in version\" stuff (versionadded/versionchanged) into a\n    # better callout -- the Sphinx default is just a little span,\n    # which is a bit less obvious that I'd like.\n    #\n    # FIXME: these messages are all hardcoded in English. We need to change\n    # that to accommodate other language docs, but I can't work out how to make\n    # that work.\n    #\n    version_text = {\n        \"versionchanged\": \"Changed in Django %s\",\n        \"versionadded\": \"New in Django %s\",\n    }\n\n    def visit_versionmodified(self, node):\n        self.body.append(self.starttag(node, \"div\", CLASS=node[\"type\"]))\n        version_text = self.version_text.get(node[\"type\"])\n        if version_text:\n            title = \"%s%s\" % (version_text % node[\"version\"], \":\" if len(node) else \".\")\n            self.body.append('<span class=\"title\">%s</span> ' % title)\n\n    def depart_versionmodified(self, node):\n        self.body.append(\"</div>\\n\")\n\n    # Give each section a unique ID -- nice for custom CSS hooks\n    def visit_section(self, node):\n        old_ids = node.get(\"ids\", [])\n        node[\"ids\"] = [\"s-\" + i for i in old_ids]\n        node[\"ids\"].extend(old_ids)\n        super().visit_section(node)\n        node[\"ids\"] = old_ids"}
{"Repository": "BlackJack-Simulator", "input": "Represents the shoe, which consists of a number of card decks. className Shoe(object) Method __init__ Method __str__ Method init_cards Method init_count Method deal Method do_count Method truecount Method shoe_penetration Attribute count Attribute count_history Attribute ideal_count Attribute decks Attribute cards", "label": "class Shoe(object):\n    reshuffle = False\n\n    def __init__(self, decks):\n        self.count = 0\n        self.count_history = []\n        self.ideal_count = {}\n        self.decks = decks\n        self.cards = self.init_cards()\n        self.init_count()\n\n    def __str__(self):\n        s = \"\"\n        for c in self.cards:\n            s += \"%s\\n\" % c\n        return s\n\n    def init_cards(self):\n        self.count = 0\n        self.count_history.append(self.count)\n\n        cards = []\n        for d in range(self.decks):\n            for c in CARDS:\n                for i in range(0, 4):\n                    cards.append(Card(c, CARDS[c]))\n        shuffle(cards)\n        return cards\n\n    def init_count(self):\n        for card in CARDS:\n            self.ideal_count[card] = 4 * SHOE_SIZE\n\n    def deal(self):\n        if self.shoe_penetration() < SHOE_PENETRATION:\n            self.reshuffle = True\n        card = self.cards.pop()\n\n        assert self.ideal_count[card.name] > 0, \"Either a cheater or a bug!\"\n        self.ideal_count[card.name] -= 1\n\n        self.do_count(card)\n        return card\n\n    def do_count(self, card):\n        self.count += BASIC_OMEGA_II[card.name]\n        self.count_history.append(self.truecount())\n\n    def truecount(self):\n        return self.count / (self.decks * self.shoe_penetration())\n\n    def shoe_penetration(self):\n        return len(self.cards) / (DECK_SIZE * self.decks)"}
{"Repository": "django-relativity", "input": "Define some extra Field methods so this Rel acts more like a Field, which lets us use ReverseManyToOneDescriptor in both directions. className CustomForeignObjectRel(ForeignObjectRel) Method __init__ Method foreign_related_fields Method local_related_fields Method related_fields Method get_attname Method relationship_related_query_name Method _get_extra_restriction Method _get_extra_restriction_legacy Method _resolve_expression_local_references Method get_forward_related_filter Attribute multiple Attribute reverse_multiple", "label": "class CustomForeignObjectRel(ForeignObjectRel):\n    def __init__(self, *args, **kwargs):\n        super(CustomForeignObjectRel, self).__init__(*args, **kwargs)\n        self.multiple = self.field.reverse_multiple\n        self.reverse_multiple = self.field.multiple\n\n    @property\n    def foreign_related_fields(self):\n        return []\n\n    @property\n    def local_related_fields(self):\n        return []\n\n    @property\n    def related_fields(self):\n        return []\n\n    def get_attname(self):\n        return self.name\n\n    def relationship_related_query_name(self):\n        return self.remote_field.name\n\n    def _get_extra_restriction(self, alias, related_alias):\n        return Restriction(\n            forward=False,\n            local_model=self.related_model,\n            related_model=self.model,\n            local_alias=related_alias,\n            related_alias=alias,\n            predicate=self.field.predicate,\n        )\n\n    def _get_extra_restriction_legacy(self, where_class, alias, related_alias):\n        # this is a shim to maintain compatibility with django < 4.0\n        return self._get_extra_restriction(alias, related_alias)\n\n    # this is required to handle a change in Django 4.0\n    # https://docs.djangoproject.com/en/4.0/releases/4.0/#miscellaneous\n    # the signature of the (private) funtion was changed\n    if django.VERSION < (4, 0):\n        get_extra_restriction = _get_extra_restriction_legacy\n    else:\n        get_extra_restriction = _get_extra_restriction\n\n    @classmethod\n    def _resolve_expression_local_references(cls, expr, obj):\n        if isinstance(expr, L):\n            return expr._relativity_resolve_for_instance(obj)\n        elif hasattr(expr, \"get_source_expressions\"):\n            for source_expr in expr.get_source_expressions():\n                cls._resolve_expression_local_references(source_expr, obj)\n        else:\n            return expr\n        return expr\n\n    def get_forward_related_filter(self, obj):\n        q = self.field.predicate\n        q = q() if callable(q) else copy.deepcopy(q)\n\n        # If this is a simple restriction that can be expressed as an AND of\n        # two basic field lookups, we can return a dictionary of filters...\n        if q.connector == Q.AND and all(type(c) == tuple for c in q.children):\n            return {\n                lookup: self._resolve_expression_local_references(v, obj)\n                for lookup, v in q.children\n            }\n\n        # ...otherwise, we return this lookup and let the compiler figure it\n        # out. This will involve a join where the above method might not.\n        else:\n            return {self.name: obj}"}
{"Repository": "Stitch", "input": "Class representing a _UNICODE_STRING Adds the following behavior: * The Buffer attribute is presented as a Python string rather than a pointer to an unsigned short. className _UNICODE_STRING(Obj) Method __new__ Method __str__ Method getBuffer", "label": "class _UNICODE_STRING(Obj):\n    def __new__(typ, *args, **kwargs):\n        obj = object.__new__(typ)\n        return obj\n\n    def __str__(self):\n        return self.Buffer\n\n    # Custom Attributes\n    def getBuffer(self):\n        return read_unicode_string(self.space, types, [], self.address)\n    Buffer = property(fget=getBuffer)"}
{"Repository": "LSH_Memory", "input": "Omniglot dataset. className OmniglotDataset(Dataset) Method __init__ Method sample_episode_batch Attribute data Attribute num_categories Attribute category_size", "label": "class OmniglotDataset(Dataset):\n    def __init__(self, filepath):\n        with open(filepath, \"rb\") as f:\n            processed_data = pickle.load(f)\n\n        self.data = dict()\n        for image, label in zip(processed_data['images'], processed_data['labels']):\n            if label not in self.data:\n                self.data[label] = list()\n            img = np.expand_dims(image, axis=0).astype('float32')\n            #img /= 255.0\n            self.data[label].append(img)\n        self.num_categories = len(self.data)\n        self.category_size = len(self.data[processed_data['labels'][0]])\n\n    def sample_episode_batch(self, episode_length, episode_width, batch_size, N):\n        for rnd in range(N):\n            episodes_x = [list() for _ in range(episode_length)]\n            episodes_y = [list() for _ in range(episode_length)]\n            assert(self.num_categories >= episode_width)\n\n            for b in range(batch_size):\n                episode_labels = random.sample(self.data.keys(), episode_width)\n\n                # Evenly divide episode_length among episode_width\n                remainder = episode_length % episode_width\n                remainders = [0] * (episode_width - remainder) + [1] * remainder\n                quotient = int((episode_length - remainder) / episode_width)\n                episode_x = [random.sample(self.data[label], r + quotient) for label, r in zip(episode_labels, remainders)]\n                assert(quotient+1 <= self.category_size)\n\n                # Arrange episode so that each distinct label is seen before moving to 2nd showing\n                # Concatenate class episodes together into single list\n                episode = sum([[(example, label_id, example_id) for example_id, example in enumerate(examples_per_label)] for label_id, examples_per_label in enumerate(episode_x)], list())\n                random.shuffle(episode)\n                episode.sort(key=lambda elem: elem[2])\n                assert len(episode) == episode_length\n\n                # During training, the set of labels for each episode are considered distinct\n                # The memory is not emptied during each training episode\n                for idx in range(episode_length):\n                    episodes_x[idx].append(episode[idx][0])\n                    episodes_y[idx].append(episode[idx][1] + b * episode_width)\n\n            yield ([torch.from_numpy(np.array(xx)) for xx in episodes_x],\n                   [torch.from_numpy(np.array(yy)) for yy in episodes_y])"}
{"Repository": "lamb", "input": "Immutable reversible mappings from strings to integers. className Vocab(object) Method __init__ Method unk_index Method eos_index Method token Method __iter__ Method index_or_unk Method size Method decode Method encode Method index_frequency Attribute _token_to_index Attribute _token_to_frequency Attribute _unk Attribute _eos Attribute _index_to_token", "label": "class Vocab(object):\n  def __init__(self, tokens, unk=u'<UNK>', eos=u'\\u25bc'):\n    self._token_to_index = {}\n    self._token_to_frequency = {}\n    self._unk = unk\n    self._eos = eos\n    token_to_index = self._token_to_index\n    token_to_frequency = self._token_to_frequency\n    # Get the unique tokens from `tokens` that might be a generator.\n    for token in tokens:\n      token_to_index[token] = True\n      token_to_frequency[token] = token_to_frequency.get(token, 0) + 1\n    token_to_index[unk] = True\n    token_to_index[eos] = True\n    # Now that we have a smaller set of tokens, assign ids in sorted\n    # order for deterministic encoding.\n    self._index_to_token = [None] * len(token_to_index)\n    index_to_token = self._index_to_token\n    i = 0\n    for token in sorted(list(token_to_index)):\n      token_to_index[token] = i\n      index_to_token[i] = token\n      i += 1\n\n  def unk_index(self):\n    return self._token_to_index[self._unk]\n\n  def eos_index(self):\n    return self._token_to_index[self._eos]\n\n  def token(self, index_):\n    return self._index_to_token[index_]\n\n  def __iter__(self):\n    for i in range(self.size()):\n      yield self.token(i)\n\n  def index_or_unk(self, token):\n    if token in self._token_to_index:\n      return self._token_to_index[token]\n    else:\n      return self.unk_index()\n\n  def size(self):\n    return len(self._index_to_token)\n\n  def decode(self, ids):\n    assert all([0 <= x and x < len(self._index_to_token) for x in ids])\n    return [self.token(x) for x in ids]\n\n  def encode(self, tokens, add_eos=True):\n    ids = [self.index_or_unk(token) for token in tokens]\n\n    if add_eos:\n      ids += [self.eos_index()]\n\n    return ids\n\n  def index_frequency(self, index_):\n    return self._token_to_frequency.get(self.token(index_), 0)"}
{"Repository": "Siamese-Networks-for-One-Shot-Learning", "input": "Modified Stochastic gradient descent optimizer. className Modified_SGD(Optimizer) Method get_updates Method get_config", "label": "class Modified_SGD(Optimizer):\n    def __init__(self, lr=0.01, momentum=0., decay=0.,\n                 nesterov=False, lr_multipliers=None, momentum_multipliers=None, **kwargs):\n        super(Modified_SGD, self).__init__(**kwargs)\n        with K.name_scope(self.__class__.__name__):\n            self.iterations = K.variable(0, dtype='int64', name='iterations')\n            self.lr = K.variable(lr, name='lr')\n            self.momentum = K.variable(momentum, name='momentum')\n            self.decay = K.variable(decay, name='decay')\n        self.initial_decay = decay\n        self.nesterov = nesterov\n        self.lr_multipliers = lr_multipliers\n        self.momentum_multipliers = momentum_multipliers\n\n    @interfaces.legacy_get_updates_support\n    def get_updates(self, loss, params):\n        grads = self.get_gradients(loss, params)\n        self.updates = [K.update_add(self.iterations, 1)]\n\n        lr = self.lr\n        if self.initial_decay > 0:\n            lr *= (1. / (1. + self.decay * K.cast(self.iterations,\n                                                  K.dtype(self.decay))))\n        \n        \n        # momentum\n        shapes = [K.int_shape(p) for p in params]\n        moments = [K.zeros(shape) for shape in shapes]\n        self.weights = [self.iterations] + moments\n        for p, g, m in zip(params, grads, moments):\n\n            if self.lr_multipliers != None:\n                if p.name in self.lr_multipliers:\n                    new_lr = lr * self.lr_multipliers[p.name]\n                else:\n                    new_lr = lr\n            else:\n                new_lr = lr\n\n            if self.momentum_multipliers != None:\n                if p.name in self.momentum_multipliers:\n                    new_momentum = self.momentum * \\\n                        self.momentum_multipliers[p.name]\n                else:\n                    new_momentum = self.momentum\n            else:\n                new_momentum = self.momentum\n\n            v = new_momentum * m - new_lr * g  # velocity\n            self.updates.append(K.update(m, v))\n\n            if self.nesterov:\n                new_p = p + new_momentum * v - new_lr * g\n            else:\n                new_p = p + v\n\n            # Apply constraints.\n            if getattr(p, 'constraint', None) is not None:\n                new_p = p.constraint(new_p)\n\n            self.updates.append(K.update(p, new_p))\n        return self.updates\n\n    def get_config(self):\n        config = {'lr': float(K.get_value(self.lr)),\n                  'momentum': float(K.get_value(self.momentum)),\n                  'decay': float(K.get_value(self.decay)),\n                  'nesterov': self.nesterov,\n                  'lr_multipliers': float(K.get_value(self.lr_multipliers)),\n                  'momentum_multipliers': float(K.get_value(self.momentum_multipliers))}\n        base_config = super(Modified_SGD, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))"}
{"Repository": "python-aliexpress-api", "input": "Common base class for all AliExpress API exceptions. className AliexpressException(Exception) Method __init__ Method __str__ Attribute reason", "label": "class AliexpressException(Exception):\n    def __init__(self, reason: str):\n        super().__init__()\n        self.reason = reason\n\n    def __str__(self) -> str:\n        return '%s' % self.reason"}
{"Repository": "hAMRonization", "input": "Updated for ResFinder v4. className ResFinderIterator(hAMRonizedResultIterator) Method __init__ Method parse Attribute metadata Attribute field_mapping", "label": "class ResFinderIterator(hAMRonizedResultIterator):\n    def __init__(self, source, metadata):\n        metadata[\"reference_database_name\"] = \"resfinder\"\n        metadata[\"analysis_software_name\"] = \"resfinder\"\n        # even though resfinderv4 runs pointfinder\n        # parsing mutational resistance requires parsing a different file\n        # to get mutations (pointfinder)\n        metadata[\"genetic_variation_type\"] = GENE_PRESENCE\n        self.metadata = metadata\n\n        self.field_mapping = {\n            \"Resistance gene\": \"gene_symbol\",\n            \"Identity\": \"sequence_identity\",\n            \"Alignment Length/Gene Length\": None,\n            \"Position in reference\": None,\n            \"Contig\": \"input_sequence_id\",\n            \"Position in contig\": None,\n            \"Accession no.\": \"reference_accession\",\n            \"Phenotype\": \"drug_class\",\n            \"Coverage\": \"coverage_percentage\",\n            # decomposed from Position in contig e.g \"10432..11277\"\n            \"_start\": \"input_gene_start\",\n            \"_stop\": \"input_gene_stop\",\n            # infered from Position in contig field\n            \"_strand\": \"strand_orientation\",\n            # Resistance gene is mapped to both symbol and name\n            \"_gene_name\": \"gene_name\",\n            # decomposed from Alignment Length/Gene Length e.g 1176/1176\n            \"_reference_gene_length\": \"reference_gene_length\",\n        }\n\n        super().__init__(source, self.field_mapping, self.metadata)\n\n    def parse(self, handle):\n        reader = csv.DictReader(handle, delimiter=\"\\t\")\n        for result in reader:\n            result[\"_gene_name\"] = result[\"Resistance gene\"]\n            _start, _stop = result[\"Position in contig\"].split(\"..\")\n            if _start > _stop:\n                _strand = \"-\"\n            else:\n                _strand = \"+\"\n            result[\"_start\"] = _start\n            result[\"_stop\"] = _stop\n            result[\"_strand\"] = _strand\n            _reference_gene_length = result[\"Alignment Length/Gene Length\"].split(\"/\")[\n                0\n            ]\n            result[\"_reference_gene_length\"] = _reference_gene_length\n            yield self.hAMRonize(result, self.metadata)\n            result = {}"}
{"Repository": "amqtt", "input": "Asyncio Streams API protocol adapter This adapter relies on StreamReader to read from a TCP socket. className StreamReaderAdapter(ReaderAdapter) Method __init__ Method feed_eof Attribute _reader", "label": "class StreamReaderAdapter(ReaderAdapter):\n    def __init__(self, reader: StreamReader):\n        self._reader = reader\n\n    async def read(self, n=-1) -> bytes:\n        if n == -1:\n            data = await self._reader.read(n)\n        else:\n            data = await self._reader.readexactly(n)\n        return data\n\n    def feed_eof(self):\n        return self._reader.feed_eof()"}
{"Repository": "python-dali", "input": "All unaddressed control devices All control devices in the system that have no short address assigned. className DeviceBroadcastUnaddressed(DeviceAddress) Method from_frame Method add_to_frame Method __eq__ Method __str__", "label": "class DeviceBroadcastUnaddressed(DeviceAddress):\n    @classmethod\n    def from_frame(cls, f):\n        if len(f) == 24:\n            if f[16] and f[23:17] == 0x7E:\n                return cls()\n\n    def add_to_frame(self, f):\n        if len(f) == 24:\n            f[23:17] = 0x7E\n        else:\n            raise _bad_frame_length\n\n    def __eq__(self, other):\n        return isinstance(other, DeviceBroadcastUnaddressed)\n\n    def __str__(self):\n        return \"<broadcast unaddressed (control device)>\""}
{"Repository": "WoeUSB-ng", "input": "Post-installation for development mode. className PostDevelopCommand(develop) Method run", "label": "class PostDevelopCommand(develop):\n    def run(self):\n        # TODO\n        develop.run(self)"}
{"Repository": "ha-wyzeapi", "input": "A representation of the Wyze Home Monitoring system that works for wyze className WyzeHomeMonitoring(AlarmControlPanelEntity) Method __init__ Method alarm_arm_vacation Method state Method alarm_disarm Method alarm_arm_home Method alarm_arm_away Method alarm_arm_night Method alarm_trigger Method alarm_arm_custom_bypass Method supported_features Method device_info Method name Method unique_id Method extra_state_attributes Attribute _hms_service", "label": "class WyzeHomeMonitoring(AlarmControlPanelEntity):\n    DEVICE_MODEL = \"HMS\"\n    NAME = \"Wyze Home Monitoring System\"\n    AVAILABLE = True\n    _state = \"disarmed\"\n    _server_out_of_sync = False\n\n    def __init__(self, hms_service: HMSService):\n        self._hms_service = hms_service\n\n    def alarm_arm_vacation(self, code: Optional[str] = None) -> None:\n        raise NotImplementedError\n\n    @property\n    def state(self):\n        return self._state\n\n    def alarm_disarm(self, code: Optional[str] = None) -> None:\n        raise NotImplementedError\n\n    def alarm_arm_home(self, code: Optional[str] = None) -> None:\n        raise NotImplementedError\n\n    def alarm_arm_away(self, code: Optional[str] = None) -> None:\n        raise NotImplementedError\n\n    @token_exception_handler\n    async def async_alarm_disarm(self, code=None) -> None:\n        await self._hms_service.set_mode(HMSMode.DISARMED)\n        self._state = \"disarmed\"\n        self._server_out_of_sync = True\n\n    @token_exception_handler\n    async def async_alarm_arm_home(self, code=None):\n        await self._hms_service.set_mode(HMSMode.HOME)\n        self._state = \"armed_home\"\n        self._server_out_of_sync = True\n\n    @token_exception_handler\n    async def async_alarm_arm_away(self, code=None):\n        await self._hms_service.set_mode(HMSMode.AWAY)\n        self._state = \"armed_away\"\n        self._server_out_of_sync = True\n\n    def alarm_arm_night(self, code=None):\n        raise NotImplementedError\n\n    def alarm_trigger(self, code=None):\n        raise NotImplementedError\n\n    def alarm_arm_custom_bypass(self, code=None):\n        raise NotImplementedError\n\n    @property\n    def supported_features(self) -> int:\n        return AlarmControlPanelEntityFeature.ARM_HOME | AlarmControlPanelEntityFeature.ARM_AWAY\n\n    @property\n    def device_info(self):\n        return {\n            \"identifiers\": {\n                (DOMAIN, self.unique_id)\n            },\n            \"name\": self.NAME,\n            \"manufacturer\": \"WyzeLabs\",\n            \"model\": self.DEVICE_MODEL\n        }\n\n    @property\n    def name(self) -> str:\n        return self.NAME\n\n    @property\n    def unique_id(self):\n        return self._hms_service.hms_id\n\n    @property\n    def extra_state_attributes(self):\n        return {\n            ATTR_ATTRIBUTION: ATTRIBUTION,\n            \"device model\": self.DEVICE_MODEL,\n            \"mac\": self.unique_id\n        }\n\n    @token_exception_handler\n    async def async_update(self):\n        if not self._server_out_of_sync:\n            state = await self._hms_service.update(self._hms_service.hms_id)\n            if state is HMSMode.DISARMED:\n                self._state = \"disarmed\"\n            elif state is HMSMode.AWAY:\n                self._state = \"armed_away\"\n            elif state is HMSMode.HOME:\n                self._state = \"armed_home\"\n            elif state is HMSMode.CHANGING:\n                self._state = \"disarmed\"\n            else:\n                _LOGGER.warning(f\"Received {state} from server\")\n\n        self._server_out_of_sync = False"}
{"Repository": "Deeplab_pytorch", "input": "Normalize a tensor image with mean and standard deviation. className Normalize_cityscapes(object) Method __init__ Method __call__ Attribute mean", "label": "class Normalize_cityscapes(object):\n    def __init__(self, mean=(0., 0., 0.)):\n        self.mean = mean\n\n    def __call__(self, sample):\n        img = np.array(sample['image']).astype(np.float32)\n        mask = np.array(sample['label']).astype(np.float32)\n        img -= self.mean\n        img /= 255.0\n\n        return {'image': img,\n                'label': mask}"}
{"Repository": "Un-Mix", "input": "CIFAR10 Dataset. className CIFAR10Pair(CIFAR10) Method __getitem__", "label": "class CIFAR10Pair(CIFAR10):\n    def __getitem__(self, index):\n        img = self.data[index]\n        img = Image.fromarray(img)\n\n        if self.transform is not None:\n            im_1 = self.transform(img)\n            im_2 = self.transform(img)\n\n        return im_1, im_2"}
{"Repository": "youtube8mchallenge", "input": "Calculate the cross entropy loss between the predictions and labels. className MixedCrossEntropyLoss(BaseLoss) Method calculate_loss", "label": "class MixedCrossEntropyLoss(BaseLoss):\n  def calculate_loss(self, predictions, labels, orig_prediction, lambda_factor=0.5 ,**unused_params):\n    with tf.name_scope(\"loss_xent\"):\n      epsilon = 10e-6\n      float_labels = tf.cast(labels, tf.float32)\n      cross_entropy_loss = float_labels * tf.log(predictions + epsilon) + (\n          1 - float_labels) * tf.log(1 - predictions + epsilon)\n      cross_entropy_loss = tf.negative(cross_entropy_loss)\n\n      pred_cross_entropy_loss = orig_prediction * tf.log(predictions + epsilon) + (\n                                                                            1 - orig_prediction) * tf.log(\n          1 - predictions + epsilon)\n      pred_cross_entropy_loss = tf.negative(pred_cross_entropy_loss)\n      total_loss = lambda_factor * pred_cross_entropy_loss + (1 - lambda_factor) * cross_entropy_loss\n      return tf.reduce_mean(tf.reduce_sum(total_loss, 1))"}
{"Repository": "StyleFlow", "input": "Redirect stderr to stdout, optionally print stdout to a file, and optionally force flushing on both stdout and the file. className Logger(object) Method __init__ Method __enter__ Method __exit__ Method write Method flush Method close Attribute file Attribute file Attribute should_flush Attribute stdout Attribute stderr", "label": "class Logger(object):\n    def __init__(self, file_name: str = None, file_mode: str = \"w\", should_flush: bool = True):\n        self.file = None\n\n        if file_name is not None:\n            self.file = open(file_name, file_mode)\n\n        self.should_flush = should_flush\n        self.stdout = sys.stdout\n        self.stderr = sys.stderr\n\n        sys.stdout = self\n        sys.stderr = self\n\n    def __enter__(self) -> \"Logger\":\n        return self\n\n    def __exit__(self, exc_type: Any, exc_value: Any, traceback: Any) -> None:\n        self.close()\n\n    def write(self, text: str) -> None:\n        if len(text) == 0: # workaround for a bug in VSCode debugger: sys.stdout.write(''); sys.stdout.flush() => crash\n            return\n\n        if self.file is not None:\n            self.file.write(text)\n\n        self.stdout.write(text)\n\n        if self.should_flush:\n            self.flush()\n\n    def flush(self) -> None:\n        if self.file is not None:\n            self.file.flush()\n\n        self.stdout.flush()\n\n    def close(self) -> None:\n        self.flush()\n\n        # if using multiple loggers, prevent closing in wrong order\n        if sys.stdout is self:\n            sys.stdout = self.stdout\n        if sys.stderr is self:\n            sys.stderr = self.stderr\n\n        if self.file is not None:\n            self.file.close()"}
{"Repository": "tf-lstm-crf-batch", "input": "Gated recurrent unit (GRU). className GRU(object) Method __init__ Method link Method recurrence Method alloc Attribute input_dim Attribute hidden_dim Attribute with_batch Attribute name Attribute w_x Attribute w_xr Attribute w_hr Attribute w_xz Attribute w_hz Attribute w_h Attribute b_r Attribute b_z Attribute h_0", "label": "class GRU(object):\n    def __init__(self, input_dim, hidden_dim, with_batch=True, name='GRU'):\n        self.input_dim = input_dim\n        self.hidden_dim = hidden_dim\n        self.with_batch = with_batch\n        self.name = name\n\n        # Input weight tensor\n        self.w_x = shared((input_dim, hidden_dim), name + '__w_x')\n\n        # Reset weight tensor\n        self.w_xr = shared((input_dim, hidden_dim), name + '__w_xr')\n        self.w_hr = shared((hidden_dim, hidden_dim), name + '__w_hr')\n\n        # Update weight tensor\n        self.w_xz = shared((input_dim, hidden_dim), name + '__w_xz')\n        self.w_hz = shared((hidden_dim, hidden_dim), name + '__w_hz')\n\n        # Hidden weight tensor\n        self.w_h = shared((hidden_dim, hidden_dim), name + '__w_h')\n\n        # Initialize the bias vectors, h_0 to zero vectors\n        self.b_r = tf.Variable(tf.truncated_normal((hidden_dim,), mean=1), name=(name + '__b_r'))\n        self.b_z = tf.Variable(tf.truncated_normal((hidden_dim,), mean=1), name=(name + '__b_z'))\n        self.h_0 = shared((hidden_dim,), name + '__h_0')\n\n    def link(self, input):\n        def recurrence(previous_hidden_state, x_t):\n            if len(x_t.shape) == 1:\n                x_t = tf.reshape(x_t, [1, self.input_dim])\n            if len(previous_hidden_state.shape) == 1:\n                previous_hidden_state = tf.reshape(previous_hidden_state, [1, self.hidden_dim])\n\n            # update gate\n            z_t = tf.sigmoid(tf.matmul(x_t, self.w_xz) + tf.matmul(previous_hidden_state, self.w_hz) + self.b_z)\n            # reset gate\n            r_t = tf.sigmoid(tf.matmul(x_t, self.w_xr) + tf.matmul(previous_hidden_state, self.w_hr) + self.b_r)\n            # candidate activation\n            h_ = tf.tanh(tf.matmul(x_t, self.w_x) + tf.matmul(tf.multiply(previous_hidden_state, r_t), self.w_h))\n\n            h_t = tf.multiply((1 - z_t), previous_hidden_state) + tf.multiply(z_t, h_)\n\n            if self.with_batch == False:\n                h_t = tf.squeeze(h_t, axis=[0])\n            return h_t\n\n        # If we use batches, we have to permute the first and second dimension.\n        if self.with_batch:\n            batch_size = tf.shape(input)[0]\n            zeros = tf.ones([batch_size])\n\n            def alloc(prev, x):\n                return self.h_0\n\n            out_info = self.h_0\n            outputs_info = tf.scan(fn=alloc, elems=zeros, initializer=out_info, name='batch_init')\n            self.input = tf.transpose(input, (1, 0, 2))\n\n        else:\n            self.input = input\n            outputs_info = self.h_0\n\n        states = tf.scan(\n            fn=recurrence,\n            elems=self.input,\n            initializer=outputs_info,\n            name='state'\n        )\n        return states"}
{"Repository": "QMarkdowner", "input": "Escape HTML entities in $placeholders. className WebSafe(Filter) Method filter", "label": "class WebSafe(Filter):\n    def filter(self, val, **kw):\n        s = super(WebSafe, self).filter(val, **kw)\n        # These substitutions are copied from cgi.escape().\n        s = s.replace(\"&\", \"&amp;\") # Must be done first!\n        s = s.replace(\"<\", \"&lt;\")\n        s = s.replace(\">\", \"&gt;\")\n        # Process the additional transformations if any.\n        if 'also' in kw:\n            also = kw['also']\n            entities = webSafeEntities   # Global variable.\n            for k in also:\n                if k in entities:\n                    v = entities[k]\n                else:\n                    v = \"&#%s;\" % ord(k)\n                s = s.replace(k, v)\n        return s"}
{"Repository": "qgis-latlontools-plugin", "input": "Algorithm to convert a point layer to a Plus codes field. className ToPlusCodesAlgorithm(QgsProcessingAlgorithm) Method initAlgorithm Method processAlgorithm Method name Method icon Method displayName Method helpUrl Method shortHelpString Method createInstance", "label": "class ToPlusCodesAlgorithm(QgsProcessingAlgorithm):\n    # Constants used to refer to parameters and outputs. They will be\n    # used when calling the algorithm from another algorithm, or when\n    # calling from the QGIS console.\n    PrmInputLayer = 'InputLayer'\n    PrmPlusCodesFieldName = 'PlusCodesFieldName'\n    PrmPlusCodesLength = 'PlusCodesLength'\n    PrmOutputLayer = 'OutputLayer'\n\n    def initAlgorithm(self, config):\n        self.addParameter(\n            QgsProcessingParameterFeatureSource(\n                self.PrmInputLayer,\n                tr('Input point vector layer'),\n                [QgsProcessing.TypeVectorPoint])\n        )\n        self.addParameter(\n            QgsProcessingParameterString(\n                self.PrmPlusCodesFieldName,\n                tr('Plus Codes field name'),\n                defaultValue='pluscodes')\n        )\n        self.addParameter(\n            QgsProcessingParameterNumber(\n                self.PrmPlusCodesLength,\n                tr('Plus Codes length'),\n                type=QgsProcessingParameterNumber.Integer,\n                defaultValue=11,\n                optional=False,\n                minValue=10,\n                maxValue=olc.MAX_DIGIT_COUNT_)\n        )\n        self.addParameter(\n            QgsProcessingParameterFeatureSink(\n                self.PrmOutputLayer,\n                tr('Output layer'))\n        )\n\n    def processAlgorithm(self, parameters, context, feedback):\n        source = self.parameterAsSource(parameters, self.PrmInputLayer, context)\n        field_name = self.parameterAsString(parameters, self.PrmPlusCodesFieldName, context).strip()\n        plusCodesLength = self.parameterAsInt(parameters, self.PrmPlusCodesLength, context)\n\n        fieldsout = QgsFields(source.fields())\n\n        if fieldsout.append(QgsField(field_name, QVariant.String)) is False:\n            msg = \"{} '{}'\".format(tr('Plus Codes Field Name must be unique. There is already a field named'), field_name)\n            feedback.reportError(msg)\n            raise QgsProcessingException(msg)\n\n        layerCRS = source.sourceCrs()\n        (sink, dest_id) = self.parameterAsSink(\n            parameters, self.PrmOutputLayer,\n            context, fieldsout, source.wkbType(), layerCRS)\n\n        # The input to the plus codes conversions requires latitudes and longitudes\n        # If the layer is not EPSG:4326 we need to convert it.\n        epsg4326 = QgsCoordinateReferenceSystem('EPSG:4326')\n        if layerCRS != epsg4326:\n            transform = QgsCoordinateTransform(layerCRS, epsg4326, QgsProject.instance())\n\n        total = 100.0 / source.featureCount() if source.featureCount() else 0\n\n        iterator = source.getFeatures()\n        for cnt, feature in enumerate(iterator):\n            if feedback.isCanceled():\n                break\n            pt = feature.geometry().asPoint()\n            if layerCRS != epsg4326:\n                pt = transform.transform(pt)\n            try:\n                msg = olc.encode(pt.y(), pt.x(), plusCodesLength)\n            except Exception:\n                msg = ''\n            f = QgsFeature()\n            f.setGeometry(feature.geometry())\n            f.setAttributes(feature.attributes() + [msg])\n            sink.addFeature(f)\n\n            if cnt % 100 == 0:\n                feedback.setProgress(int(cnt * total))\n\n        return {self.PrmOutputLayer: dest_id}\n\n    def name(self):\n        return 'point2pluscodes'\n\n    def icon(self):\n        return QIcon(os.path.dirname(__file__) + '/images/pluscodes.svg')\n\n    def displayName(self):\n        return tr('Point layer to Plus Codes')\n\n    def helpUrl(self):\n        file = os.path.dirname(__file__) + '/index.html'\n        if not os.path.exists(file):\n            return ''\n        return QUrl.fromLocalFile(file).toString(QUrl.FullyEncoded)\n\n    def shortHelpString(self):\n        file = os.path.dirname(__file__) + '/doc/geom2pluscodes.help'\n        if not os.path.exists(file):\n            return ''\n        with open(file) as helpf:\n            help = helpf.read()\n        return help\n\n    def createInstance(self):\n        return ToPlusCodesAlgorithm()"}
{"Repository": "deep_rl_zoo", "input": "R2D2 e-greedy actor. className R2d2EpsilonGreedyActor(EpsilonGreedyActor) Method _select_action Method reset", "label": "class R2d2EpsilonGreedyActor(EpsilonGreedyActor):\n    def __init__(\n        self,\n        network: torch.nn.Module,\n        exploration_epsilon: float,\n        random_state: np.random.RandomState,  # pylint: disable=no-member\n        device: torch.device,\n    ):\n        super().__init__(\n            network,\n            exploration_epsilon,\n            random_state,\n            device,\n            'R2D2-greedy',\n        )\n        self._last_action = None\n        self._lstm_state = None\n\n    @torch.no_grad()\n    def _select_action(self, timestep: types_lib.TimeStep) -> types_lib.Action:\n        s_t = torch.tensor(timestep.observation[None, ...]).to(device=self._device, dtype=torch.float32)\n        a_tm1 = torch.tensor(self._last_action).to(device=self._device, dtype=torch.int64)\n        r_t = torch.tensor(timestep.reward).to(device=self._device, dtype=torch.float32)\n        hidden_s = tuple(s.to(device=self._device) for s in self._lstm_state)\n\n        network_output = self._network(\n            RnnDqnNetworkInputs(\n                s_t=s_t[None, ...],\n                a_tm1=a_tm1[None, ...],\n                r_t=r_t[None, ...],\n                hidden_s=hidden_s,\n            )\n        )\n        q_t = network_output.q_values\n        self._lstm_state = network_output.hidden_s\n\n        a_t = apply_egreedy_policy(q_t, self._exploration_epsilon, self._random_state)\n        self._last_action = a_t\n        return a_t\n\n    def reset(self) -> None:\n        self._last_action = 0  # During the first step of a new episode, use 'fake' previous action for network pass\n        self._lstm_state = self._network.get_initial_hidden_state(batch_size=1)"}
{"Repository": "KindleEar", "input": "Class to create ZIP archives with Python library files and packages. className PyZipFile(ZipFile) Method writepy Method _get_codename", "label": "class PyZipFile(ZipFile):\n    def writepy(self, pathname, basename=\"\"):\n        dir, name = os.path.split(pathname)\n        if os.path.isdir(pathname):\n            initname = os.path.join(pathname, \"__init__.py\")\n            if os.path.isfile(initname):\n                # This is a package directory, add it\n                if basename:\n                    basename = f\"{basename}/{name}\"\n                else:\n                    basename = name\n                if self.debug:\n                    print(\"Adding package in\", pathname, \"as\", basename)\n                fname, arcname = self._get_codename(initname[0:-3], basename)\n                if self.debug:\n                    print(\"Adding\", arcname)\n                self.write(fname, arcname)\n                dirlist = os.listdir(pathname)\n                dirlist.remove(\"__init__.py\")\n                # Add all *.py files and package subdirectories\n                for filename in dirlist:\n                    path = os.path.join(pathname, filename)\n                    ext = os.path.splitext(filename)[-1]\n                    if os.path.isdir(path):\n                        if os.path.isfile(os.path.join(path, \"__init__.py\")):\n                            # This is a package directory, add it\n                            self.writepy(path, basename)  # Recursive call\n                    elif ext == \".py\":\n                        fname, arcname = self._get_codename(path[0:-3],\n                                         basename)\n                        if self.debug:\n                            print(\"Adding\", arcname)\n                        self.write(fname, arcname)\n            else:\n                # This is NOT a package directory, add its files at top level\n                if self.debug:\n                    print(\"Adding files from directory\", pathname)\n                for filename in os.listdir(pathname):\n                    path = os.path.join(pathname, filename)\n                    ext = os.path.splitext(filename)[-1]\n                    if ext == \".py\":\n                        fname, arcname = self._get_codename(path[0:-3],\n                                         basename)\n                        if self.debug:\n                            print(\"Adding\", arcname)\n                        self.write(fname, arcname)\n        else:\n            if pathname[-3:] != \".py\":\n                raise RuntimeError(\n                      'Files added with writepy() must end with \".py\"')\n            fname, arcname = self._get_codename(pathname[0:-3], basename)\n            if self.debug:\n                print(\"Adding file\", arcname)\n            self.write(fname, arcname)\n\n    def _get_codename(self, pathname, basename):\n        file_py  = pathname + \".py\"\n        file_pyc = pathname + \".pyc\"\n        file_pyo = pathname + \".pyo\"\n        if os.path.isfile(file_pyo) and \\\n                            os.stat(file_pyo).st_mtime >= os.stat(file_py).st_mtime:\n            fname = file_pyo    # Use .pyo file\n        elif not os.path.isfile(file_pyc) or \\\n             os.stat(file_pyc).st_mtime < os.stat(file_py).st_mtime:\n            import py_compile\n            if self.debug:\n                print(\"Compiling\", file_py)\n            try:\n                py_compile.compile(file_py, file_pyc, None, True)\n            except py_compile.PyCompileError as err:\n                print(err.msg)\n            fname = file_pyc\n        else:\n            fname = file_pyc\n        archivename = os.path.split(fname)[1]\n        if basename:\n            archivename = f\"{basename}/{archivename}\"\n        return (fname, archivename)"}
{"Repository": "trelby", "input": "install_data command This specific install_data command only really installs trelby. className install_data(_install_data) Method run", "label": "class install_data(_install_data):\n    def run(self):\n        dataDir = self.install_dir\n\n        if self.root:\n            dataDir = dataDir[len(self.root):]\n\n        if (dataDir.rstrip(\"/\") in (\"/usr/share\", \"/usr/local/share\")) \\\n        or (sys.platform == \"win32\"):\n            _install_data.run(self)"}
{"Repository": "firedrake", "input": "Multifunction used for converting an expression into a weighted sum of coefficients. className CoefficientCollector(MultiFunction) Method product Method division Method sum Method power Method abs Method _scalar Method multi_index Method indexed Method component_tensor Method coefficient Method constant_value Method expr Method _is_scalar_equiv Method _as_scalar", "label": "class CoefficientCollector(MultiFunction):\n    def product(self, o, a, b):\n        scalars, vectors = split_by(self._is_scalar_equiv, [a, b])\n        # Case 1: scalar * scalar\n        if len(scalars) == 2:\n            # Compress the first argument (arbitrary)\n            scalar, vector = scalars\n        # Case 2: scalar * vector\n        elif len(scalars) == 1:\n            scalar, = scalars\n            vector, = vectors\n        # Case 3: vector * vector (invalid)\n        else:\n            raise ValueError(\"Expressions containing the product of two vector-valued \"\n                             \"subexpressions cannot be used for assignment. Consider using \"\n                             \"interpolate instead.\")\n        scaling = self._as_scalar(scalar)\n        return tuple((coeff, weight*scaling) for coeff, weight in vector)\n\n    def division(self, o, a, b):\n        # Division is only valid if b (the divisor) is a scalar\n        if self._is_scalar_equiv(b):\n            divisor = self._as_scalar(b)\n            return tuple((coeff, weight/divisor) for coeff, weight in a)\n        else:\n            raise ValueError(\"Expressions involving division by a vector-valued subexpression \"\n                             \"cannot be used for assignment. Consider using interpolate instead.\")\n\n    def sum(self, o, a, b):\n        # Note: a and b are tuples of (coefficient, weight) so addition is concatenation\n        return a + b\n\n    def power(self, o, a, b):\n        # Only valid if a and b are scalars\n        return ((Constant(self._as_scalar(a) ** self._as_scalar(b)), 1),)\n\n    def abs(self, o, a):\n        # Only valid if a is a scalar\n        return ((Constant(abs(self._as_scalar(a))), 1),)\n\n    def _scalar(self, o):\n        return ((Constant(o), 1),)\n\n    int_value = _scalar\n    float_value = _scalar\n    complex_value = _scalar\n    zero = _scalar\n\n    def multi_index(self, o):\n        pass\n\n    def indexed(self, o, a, _):\n        return a\n\n    def component_tensor(self, o, a, _):\n        return a\n\n    def coefficient(self, o):\n        return ((o, 1),)\n\n    def constant_value(self, o):\n        return ((o, 1),)\n\n    def expr(self, o, *operands):\n        raise NotImplementedError(f\"Handler not defined for {type(o)}\")\n\n    def _is_scalar_equiv(self, weighted_coefficients):\n        return all(_isconstant(c) and c.dat.dim == (1,) for (c, _) in weighted_coefficients)\n\n    def _as_scalar(self, weighted_coefficients):\n        return pytools.one(\n            functools.reduce(operator.add, (c.dat.data_ro*w for c, w in weighted_coefficients))\n        )"}
{"Repository": "spectrochempy", "input": "Post-installation for development mode. className PostDevelopCommand(_develop) Method run", "label": "class PostDevelopCommand(_develop):\n    def run(self):\n        _install_mpl()\n        _develop.run(self)"}
{"Repository": "nhentai", "input": "A metaclass that creates a Singleton base class when called. className _Singleton(type) Method __call__", "label": "class _Singleton(type):\n    _instances = {}\n\n    def __call__(cls, *args, **kwargs):\n        if cls not in cls._instances:\n            cls._instances[cls] = super(_Singleton, cls).__call__(*args, **kwargs)\n        return cls._instances[cls]"}
{"Repository": "dwave-hybrid", "input": "Produces a `hybrid. className HybridProblemRunnable(HybridRunnable) Method __init__", "label": "class HybridProblemRunnable(HybridRunnable):\n    def __init__(self, sampler, **sample_kwargs):\n        super(HybridProblemRunnable, self).__init__(\n            sampler, fields=('problem', 'samples'), **sample_kwargs)"}
{"Repository": "pyosc", "input": "RequestHandler class for the OSCServer className OSCRequestHandler(DatagramRequestHandler) Method setup Method _unbundle Method handle Method finish", "label": "class OSCRequestHandler(DatagramRequestHandler):\n\tdef setup(self):\n\t\t(self.packet, self.socket) = self.request\n\t\tself.replies = []\n\n\tdef _unbundle(self, decoded):\n\t\tif decoded[0] != \"#bundle\":\n\t\t\tself.replies += self.server.dispatchMessage(decoded[0], decoded[1][1:], decoded[2:], self.client_address)\n\t\t\treturn\n\t\t\n\t\tnow = time.time()\n\t\ttimetag = decoded[1]\n\t\tif (timetag > 0.) and (timetag > now):\n\t\t\ttime.sleep(timetag - now)\n\t\t\n\t\tfor msg in decoded[2:]:\n\t\t\tself._unbundle(msg)\n\t\t\n\tdef handle(self):\n\t\tdecoded = decodeOSC(self.packet)\n\t\tif not len(decoded):\n\t\t\treturn\n\t\t\n\t\tself._unbundle(decoded)\n\t\t\n\tdef finish(self):\n\t\tif self.server.return_port:\n\t\t\tself.client_address = (self.client_address[0], self.server.return_port)\n\t\t\n\t\tif len(self.replies) > 1:\n\t\t\tmsg = OSCBundle()\n\t\t\tfor reply in self.replies:\n\t\t\t\tmsg.append(reply)\n\t\telif len(self.replies) == 1:\n\t\t\tmsg = self.replies[0]\n\t\telse:\n\t\t\treturn\n\t\t\n\t\tself.server.client.sendto(msg, self.client_address)"}
{"Repository": "CLUENER2020", "input": "BERT model (\"Bidirectional Encoder Representations from Transformers\"). className BertModel(object) Method get_pooled_output Method get_sequence_output Method get_all_encoder_layers Method get_embedding_output Method get_embedding_table", "label": "class BertModel(object):\n    def __init__(self,\n                 config,\n                 is_training,\n                 input_ids,\n                 input_mask=None,\n                 token_type_ids=None,\n                 use_one_hot_embeddings=False,\n                 scope='bert'):\n        config = copy.deepcopy(config)\n        # / dropout \n        if not is_training:\n            config.hidden_dropout_prob = 0.0\n            config.attention_probs_dropout_prob = 0.0\n\n        input_shape = get_shape_list(input_ids, expected_rank=2)\n        batch_size = input_shape[0]\n        seq_length = input_shape[1]\n\n        if input_mask is None:\n            input_mask = tf.ones(shape=[batch_size, seq_length], dtype=tf.int32)\n\n        if token_type_ids is None:\n            token_type_ids = tf.zeros(shape=[batch_size, seq_length], dtype=tf.int32)\n\n        with tf.variable_scope(scope, default_name=\"bert\", reuse=tf.AUTO_REUSE):\n            with tf.variable_scope(\"embeddings\"):\n                # Perform embedding lookup on the word ids.\n                (self.embedding_output, self.embedding_table) = embedding_lookup(\n                    input_ids=input_ids,\n                    vocab_size=config.vocab_size,\n                    embedding_size=config.hidden_size,\n                    initializer_range=config.initializer_range,\n                    word_embedding_name=\"word_embeddings\",\n                    use_one_hot_embeddings=use_one_hot_embeddings)\n\n                # Add positional embeddings and token type embeddings, then layer\n                # normalize and perform dropout.\n                self.embedding_output = embedding_postprocessor(\n                    input_tensor=self.embedding_output,\n                    use_token_type=True,\n                    token_type_ids=token_type_ids,\n                    token_type_vocab_size=config.type_vocab_size,\n                    token_type_embedding_name=\"token_type_embeddings\",\n                    use_position_embeddings=True,\n                    position_embedding_name=\"position_embeddings\",\n                    initializer_range=config.initializer_range,\n                    max_position_embeddings=config.max_position_embeddings,\n                    dropout_prob=config.hidden_dropout_prob)\n\n            with tf.variable_scope(\"encoder\"):\n                # This converts a 2D mask of shape [batch_size, seq_length] to a 3D\n                # mask of shape [batch_size, seq_length, seq_length] which is used\n                # for the attention scores.\n                attention_mask = create_attention_mask_from_input_mask(\n                    input_ids, input_mask)\n\n                # Run the stacked transformer.\n                # `sequence_output` shape = [batch_size, seq_length, hidden_size].\n                self.all_encoder_layers = transformer_model(\n                    input_tensor=self.embedding_output,\n                    attention_mask=attention_mask,\n                    hidden_size=config.hidden_size,\n                    num_hidden_layers=config.num_hidden_layers,\n                    num_attention_heads=config.num_attention_heads,\n                    intermediate_size=config.intermediate_size,\n                    intermediate_act_fn=get_activation(config.hidden_act),\n                    hidden_dropout_prob=config.hidden_dropout_prob,\n                    attention_probs_dropout_prob=config.attention_probs_dropout_prob,\n                    initializer_range=config.initializer_range,\n                    do_return_all_layers=True)\n\n            self.sequence_output = self.all_encoder_layers[-1]\n            # The \"pooler\" converts the encoded sequence tensor of shape\n            # [batch_size, seq_length, hidden_size] to a tensor of shape\n            # [batch_size, hidden_size]. This is necessary for segment-level\n            # (or segment-pair-level) classification tasks where we need a fixed\n            # dimensional representation of the segment.\n            with tf.variable_scope(\"pooler\"):\n                # We \"pool\" the model by simply taking the hidden state corresponding\n                # to the first token. We assume that this has been pre-trained\n                first_token_tensor = tf.squeeze(self.sequence_output[:, 0:1, :], axis=1)\n                self.pooled_output = tf.layers.dense(\n                    first_token_tensor,\n                    config.hidden_size,\n                    activation=tf.tanh,\n                    kernel_initializer=create_initializer(config.initializer_range))\n\n    def get_pooled_output(self):\n        return self.pooled_output\n\n    def get_sequence_output(self):\n        return self.sequence_output\n\n    def get_all_encoder_layers(self):\n        return self.all_encoder_layers\n\n    def get_embedding_output(self):\n        return self.embedding_output\n\n    def get_embedding_table(self):\n        return self.embedding_table"}
{"Repository": "sensAI", "input": "Computes and stores the average and current value className AverageMeter(object) Method __init__ Method reset Method update Method __str__ Attribute name Attribute fmt", "label": "class AverageMeter(object):\n    def __init__(self, name, fmt=':f'):\n        self.name = name\n        self.fmt = fmt\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count\n\n    def __str__(self):\n        fmtstr = '{name} {val' + self.fmt + '} ({avg' + self.fmt + '})'\n        return fmtstr.format(**self.__dict__)"}
{"Repository": "numalogic", "input": "A stateless block that is used to postprocess the output of an ML model. className PostprocessBlock(StatelessBlock) Method __init__ Method run", "label": "class PostprocessBlock(StatelessBlock):\n    def __init__(self, postprocessor: transform_t, name: str = \"postprocess\"):\n        super().__init__(artifact=postprocessor, name=name)\n\n    def run(self, input_: npt.NDArray[float], **__) -> npt.NDArray[float]:\n        return self._artifact.transform(input_)"}
{"Repository": "construct", "input": "Adapter for length-value pairs. className LengthValueAdapter(Adapter) Method _encode Method _decode", "label": "class LengthValueAdapter(Adapter):\n    __slots__ = []\n    def _encode(self, obj, context):\n        return (len(obj), obj)\n    def _decode(self, obj, context):\n        return obj[1]"}
{"Repository": "evopose2d", "input": "A LearningRateSchedule that uses a cosine decay schedule. className WarmupCosineDecay(LearningRateSchedule) Method decayed_learning_rate Method __call__ Method compute_step Method get_config", "label": "class WarmupCosineDecay(LearningRateSchedule):\n    def __init__(\n            self,\n            initial_learning_rate,\n            decay_steps,\n            warmup_steps,\n            warmup_factor,\n            alpha=0.0,\n            name=None):\n        super(WarmupCosineDecay, self).__init__()\n\n        self.initial_learning_rate = initial_learning_rate\n        self.decay_steps = decay_steps\n        self.warmup_steps = warmup_steps\n        self.warmup_factor = warmup_factor\n        self.alpha = alpha\n        self.name = name\n\n    def __call__(self, step):\n        with ops.name_scope_v2(self.name or \"WarmupCosineDecay\"):\n            initial_learning_rate = ops.convert_to_tensor_v2(\n                self.initial_learning_rate, name=\"initial_learning_rate\")\n            dtype = initial_learning_rate.dtype\n            decay_steps = math_ops.cast(self.decay_steps, dtype)\n            warmup_steps = math_ops.cast(self.warmup_steps, dtype)\n            w_fac = math_ops.cast(self.warmup_factor, dtype)\n\n            global_step_recomp = math_ops.cast(step, dtype)\n            global_step_recomp = math_ops.minimum(global_step_recomp, decay_steps)\n\n            def compute_step(warming_up=False):\n                if warming_up:\n                    completed_fraction = global_step_recomp / warmup_steps\n                    gain = w_fac + (1 - w_fac) * completed_fraction\n                else:\n                    completed_fraction = (global_step_recomp - warmup_steps) / (decay_steps - warmup_steps)\n                    cosine_decayed = 0.5 * (1.0 + math_ops.cos(\n                        constant_op.constant(math.pi) * completed_fraction))\n                    gain = (1 - self.alpha) * cosine_decayed + self.alpha\n                return gain\n\n            gain = control_flow_ops.cond(math_ops.less(global_step_recomp, warmup_steps),\n                                         lambda: compute_step(warming_up=True),\n                                         lambda: compute_step(warming_up=False))\n\n            return math_ops.multiply(initial_learning_rate, gain)\n\n    def get_config(self):\n        return {\n            \"initial_learning_rate\": self.initial_learning_rate,\n            \"decay_steps\": self.decay_steps,\n            \"warmup_steps\": self.warmup_steps,\n            \"warmup_factor\": self.warmup_factor,\n            \"alpha\": self.alpha,\n            \"name\": self.name\n        }"}
{"Repository": "dateutil", "input": "Context manager for changing local time zone on Windows. className TZWinContext(TZContextBase) Method get_current_tz Method set_current_tz", "label": "class TZWinContext(TZContextBase):\n    def get_current_tz(self):\n        p = subprocess.Popen(['tzutil', '/g'], stdout=subprocess.PIPE)\n\n        ctzname, err = p.communicate()\n        ctzname = ctzname.decode()     # Popen returns\n\n        if p.returncode:\n            raise OSError('Failed to get current time zone: ' + err)\n\n        return ctzname\n\n    def set_current_tz(self, tzname):\n        p = subprocess.Popen('tzutil /s \"' + tzname + '\"')\n\n        out, err = p.communicate()\n\n        if p.returncode:\n            raise OSError('Failed to set current time zone: ' +\n                          (err or 'Unknown error.'))"}
{"Repository": "reclor", "input": "Processor for the SWAG data set. className SwagProcessor(DataProcessor) Method get_train_examples Method get_dev_examples Method get_test_examples Method get_labels Method _read_csv Method _create_examples", "label": "class SwagProcessor(DataProcessor):\n    def get_train_examples(self, data_dir):\n        logger.info(\"LOOKING AT {} train\".format(data_dir))\n        return self._create_examples(self._read_csv(os.path.join(data_dir, \"train.csv\")), \"train\")\n\n    def get_dev_examples(self, data_dir):\n        logger.info(\"LOOKING AT {} dev\".format(data_dir))\n        return self._create_examples(self._read_csv(os.path.join(data_dir, \"val.csv\")), \"dev\")\n\n    def get_test_examples(self, data_dir):\n        logger.info(\"LOOKING AT {} dev\".format(data_dir))\n        raise ValueError(\n            \"For swag testing, the input file does not contain a label column. It can not be tested in current code\"\n            \"setting!\"\n        )\n        return self._create_examples(self._read_csv(os.path.join(data_dir, \"test.csv\")), \"test\")\n\n    def get_labels(self):\n        return [\"0\", \"1\", \"2\", \"3\"]\n\n    def _read_csv(self, input_file):\n        with open(input_file, \"r\", encoding=\"utf-8\") as f:\n            return list(csv.reader(f))\n\n    def _create_examples(self, lines: List[List[str]], type: str):\n        if type == \"train\" and lines[0][-1] != \"label\":\n            raise ValueError(\"For training, the input file must contain a label column.\")\n\n        examples = [\n            InputExample(\n                example_id=line[2],\n                question=line[5],  # in the swag dataset, the\n                # common beginning of each\n                # choice is stored in \"sent2\".\n                contexts=[line[4], line[4], line[4], line[4]],\n                endings=[line[7], line[8], line[9], line[10]],\n                label=line[11],\n            )\n            for line in lines[1:]  # we skip the line with the column names\n        ]\n\n        return examples"}
{"Repository": "algobot", "input": "Backtest thread used during backtests in the GUI. className BacktestThread(QRunnable) Method __init__ Method get_configuration_details_to_setup_backtest Method get_configuration_dictionary_for_gui Method get_activity_dictionary Method setup_bot Method stop Method run_backtest Method run Attribute signals Attribute gui Attribute logger Attribute running Attribute caller", "label": "class BacktestThread(QRunnable):\n    def __init__(self, gui, logger):\n        super(BacktestThread, self).__init__()\n        self.signals = BacktestSignals()\n        self.gui = gui\n        self.logger = logger\n        self.running = True\n        self.caller = BACKTEST\n\n    def get_configuration_details_to_setup_backtest(self) -> Dict[str, Any]:\n        return get_config_helper(self.gui, BACKTEST)\n\n    def get_configuration_dictionary_for_gui(self) -> Dict[str, str]:\n        backtester = self.gui.backtester\n        if backtester.loss_strategy is not None:\n            stop_loss_percentage_string = f'{backtester.loss_percentage_decimal * 100}%'\n        else:\n            stop_loss_percentage_string = 'Configuration Required'\n\n        temp_dict = {\n            'starting_balance': f'${backtester.starting_balance}',\n            'interval': backtester.interval,\n            'margin_enabled': f'{backtester.margin_enabled}',\n            'stop_loss_percentage': stop_loss_percentage_string,\n            'stop_loss_strategy': backtester.get_stop_loss_strategy_string(),\n            'start_period': backtester.data[backtester.start_date_index][\"date_utc\"].strftime(\"%m/%d/%Y, %H:%M:%S\"),\n            'end_period': backtester.data[backtester.end_date_index][\"date_utc\"].strftime(\"%m/%d/%Y, %H:%M:%S\"),\n            'symbol': backtester.symbol,\n            'options': [('Configuration Required', 'Configuration Required') for _ in range(2)]}\n\n        return temp_dict\n\n    def get_activity_dictionary(self, period: Dict[str, datetime], index: int, length: int) -> dict:\n        backtester = self.gui.backtester\n        net = backtester.get_net()\n        profit = net - backtester.starting_balance\n        if profit < 0:\n            profit_percentage = round(100 - net / backtester.starting_balance * 100, 2)\n        else:\n            profit_percentage = round(net / backtester.starting_balance * 100 - 100, 2)\n\n        activity = {\n            'price': backtester.get_safe_rounded_string(backtester.current_price),\n            'net': round(net, backtester.precision),\n            'netString': f'${round(net, backtester.precision)}',\n            'balance': f'${round(backtester.balance, backtester.precision)}',\n            'commissionsPaid': f'${round(backtester.commissions_paid, backtester.precision)}',\n            'tradesMade': str(len(backtester.trades)),\n            'profit': f'${abs(round(profit, backtester.precision))}',\n            'profitPercentage': f'{profit_percentage}%',\n            'currentPeriod': period['date_utc'].strftime(\"%m/%d/%Y, %H:%M:%S\"),\n            'utc': period['date_utc'].timestamp(),\n            'percentage': int((index - backtester.start_date_index) / length * 100)\n        }\n\n        backtester.past_activity.append(activity)\n        return activity\n\n    def setup_bot(self):\n        self.gui.backtester = Backtester(**self.get_configuration_details_to_setup_backtest())\n        self.gui.backtester.apply_take_profit_settings(self.gui.configuration.get_take_profit_settings(BACKTEST))\n        self.gui.backtester.apply_loss_settings(self.gui.configuration.get_loss_settings(BACKTEST))\n        self.signals.started.emit(self.get_configuration_dictionary_for_gui())\n\n    def stop(self):\n        self.running = False\n\n    def run_backtest(self):\n        backtester = self.gui.backtester\n        backtester.start_backtest(thread=self)\n        self.signals.updateGraphLimits.emit(len(backtester.past_activity))\n        self.signals.activity.emit(self.get_activity_dictionary(period=backtester.data[backtester.end_date_index],\n                                                                index=1,\n                                                                length=1))\n\n    @pyqtSlot()\n    def run(self):\n        try:\n            self.setup_bot()\n            self.run_backtest()\n            self.signals.finished.emit()\n        except Exception as e:\n            self.logger.exception(repr(e))\n            self.signals.error.emit(BACKTEST, str(e))\n        finally:\n            self.signals.restore.emit()"}
{"Repository": "django-rest-framework-sso", "input": "Returns a JSON Web Token that can be used for authenticated requests. className ObtainAuthorizationTokenView(BaseAPIView) Method post", "label": "class ObtainAuthorizationTokenView(BaseAPIView):\n    permission_classes = (IsAuthenticated,)\n    serializer_class = AuthorizationTokenSerializer\n\n    def post(self, request, *args, **kwargs):\n        serializer = self.get_serializer(data=request.data)\n        serializer.is_valid(raise_exception=True)\n\n        if hasattr(request.auth, \"get\") and request.auth.get(claims.SESSION_ID):\n            try:\n                session_token = SessionToken.objects.active().get(\n                    pk=request.auth.get(claims.SESSION_ID), user=request.user\n                )\n            except SessionToken.DoesNotExist:\n                return Response({\"detail\": \"Invalid token.\"}, status=status.HTTP_401_UNAUTHORIZED)\n        else:\n            session_token = (\n                SessionToken.objects.active().filter(user=request.user).with_user_agent(request=request).first()\n            )\n            if session_token is None:\n                session_token = SessionToken(user=request.user, created_by=request.user)\n\n        session_token.update_attributes(request=request)\n        session_token.save()\n        payload = create_authorization_payload(\n            session_token=session_token, user=request.user, **serializer.validated_data\n        )\n        jwt_token = encode_jwt_token(payload=payload)\n        return Response({\"token\": jwt_token})"}
{"Repository": "fastkml", "input": "Base class for all KML objects. className _BaseObject(_XMLObject) Method __eq__ Method __repr__ Method _get_id Method _get_target_id", "label": "class _BaseObject(_XMLObject):\n    _default_ns = config.KMLNS\n\n    id = None\n    target_id = None\n\n    def __init__(\n        self,\n        ns: Optional[str] = None,\n        name_spaces: Optional[Dict[str, str]] = None,\n        id: Optional[str] = None,\n        target_id: Optional[str] = None,\n        **kwargs: Any,\n    ) -> None:\n        super().__init__(ns=ns, name_spaces=name_spaces, **kwargs)\n        self.id = id\n        self.target_id = target_id\n\n    def __eq__(self, other: object) -> bool:\n        try:\n            assert isinstance(other, type(self))\n            return (\n                super().__eq__(other)\n                and self.id == other.id\n                and self.target_id == other.target_id\n            )\n        except AssertionError:\n            return False\n\n    def __repr__(self) -> str:\n        return (\n            f\"{self.__class__.__module__}.{self.__class__.__name__}(\"\n            f\"ns={self.ns!r}, \"\n            f\"name_spaces={self.name_spaces!r}, \"\n            f\"id={self.id!r}, \"\n            f\"target_id={self.target_id!r}, \"\n            \")\"\n        )\n\n    def etree_element(\n        self,\n        precision: Optional[int] = None,\n        verbosity: Verbosity = Verbosity.normal,\n    ) -> Element:\n        element = super().etree_element(precision=precision, verbosity=verbosity)\n        if self.id:\n            element.set(\"id\", self.id)\n        if self.target_id:\n            element.set(\"targetId\", self.target_id)\n        return element\n\n    @classmethod\n    def _get_id(cls, element: Element, strict: bool) -> str:\n        return element.get(\"id\") or \"\"\n\n    @classmethod\n    def _get_target_id(cls, element: Element, strict: bool) -> str:\n        return element.get(\"targetId\") or \"\"\n\n    @classmethod\n    def _get_kwargs(\n        cls,\n        *,\n        ns: str,\n        name_spaces: Optional[Dict[str, str]] = None,\n        element: Element,\n        strict: bool,\n    ) -> Dict[str, Any]:\n        kwargs = super()._get_kwargs(\n            ns=ns,\n            name_spaces=name_spaces,\n            element=element,\n            strict=strict,\n        )\n        kwargs.update(\n            {\n                \"id\": cls._get_id(element=element, strict=strict),\n                \"target_id\": cls._get_target_id(element=element, strict=strict),\n            },\n        )\n        return kwargs"}
{"Repository": "apiscout", "input": "A simple chooser to be used as an embedded chooser className ApiChooser(Choose) Method __init__ Method filterDisplay Method populate Method OnClose Method getItems Method OnGetLine Method OnGetSize Attribute row_count Attribute base_address Attribute scout Attribute api_results Attribute all_items Attribute items Attribute icon Attribute selcount", "label": "class ApiChooser(Choose):\n    def __init__(self, title, api_results, flags=0):\n        Choose.__init__(self,\n                         title,\n                         [[\"#\", 6], [\"Offset\", 14], [\"API Address\", 14], [\"DLL\", 20], [\"API\", 35]],\n                         embedded=True, width=140, height=20, flags=flags)\n        self.row_count = 0\n        self.base_address = [ea for ea in idautils.Segments()][0]\n        self.scout = ApiScout()\n        self.scout.setBaseAddress(self.base_address)\n        self.api_results = api_results\n        self.all_items = self.populate(api_results)\n        self.items = self.populate(api_results)\n        self.icon = 4\n        self.selcount = 0\n\n    def filterDisplay(self, from_addr, to_addr, distance):\n        filtered_items = self.scout.filter(self.api_results, from_addr, to_addr, distance)\n        self.items = self.populate(filtered_items)\n\n    def populate(self, api_results):\n\n        api_rows = []\n        unified_results = set([])\n        new_api_results = {}\n\n        # replace sets with frozensets, so that the tuple is hashable\n        for key in api_results:\n            new_api_results[key] = []\n            for h in api_results[key]:\n                a1, a2, a3, a4, a5, a6, a7, a8 = h\n                new_api_results[key].append((a1, a2, a3, a4, a5, a6, a7, frozenset(a8)))\n\n        for key in new_api_results:\n            unified_results.update(new_api_results[key])\n        for index, entry in enumerate(sorted(unified_results)):\n            dll_name = \"{} ({}bit)\".format(entry[2], entry[4])\n            api_rows.append([\"%d\" % (index + 1), \"0x%x\" % (self.base_address + entry[0]), \"0x%x\" % entry[1], dll_name, entry[3]])\n            self.row_count += 1\n        return api_rows\n\n    def OnClose(self):\n        pass\n\n    def getItems(self, l):\n        items = []\n        for index in l:\n            items.append([int(self.items[index][1], 16), int(self.items[index][2], 16), self.items[index][3], str(self.items[index][4])])\n        return items\n\n    def OnGetLine(self, n):\n        return self.items[n]\n\n    def OnGetSize(self):\n        n = len(self.items)\n        return n"}
{"Repository": "TimeManager", "input": "Plugin information className timemanager_obj(object) Method __init__ Method getController Method initGui Method changeI18n Method unload Method animation_datetime Method animation_time_frame_size Method animation_time_frame_type Method animation_start_datetime Method animation_end_datetime", "label": "class timemanager_obj(object):\n    name = \"timemanager\"\n    longName = \"TimeManager Plugin for QGIS\"\n    description = \"Working with temporal vector data\"\n    author = \"Anita Graser, Karolina Alexiou\"\n    pluginUrl = \"https://github.com/anitagraser/TimeManager\"\n\n    def __init__(self, iface):\n        global control\n        try:\n            control\n        except NameError:\n            try:\n                overrideLocale = bool(QSettings().value(\"locale/overrideFlag\", False))\n                if not overrideLocale:\n                    lang = QLocale.system().name().split(\"_\")[0]\n                else:\n                    lang = QSettings().value(\"locale/userLocale\", \"\").split(\"_\")[0]\n            except Exception:\n                lang = \"en\"  # could not get locale, OSX may have this bug\n            info(\"Plugin language loaded: {}\".format(lang))\n            self.changeI18n(lang)\n            control = TimeManagerControl(iface)\n\n    def getController(self):\n        return control\n\n    def initGui(self):\n        control.load()\n\n    def changeI18n(self, new_lang):\n        # os.environ[\"LANG\"] = str(new_lang)\n        root = os.path.abspath(os.path.join(os.path.dirname(__file__), os.pardir))\n        translation_path = \"TimeManager:i18n/{}_{}.qm\".format(self.name, new_lang)\n        self.translator = QTranslator()\n        result = self.translator.load(translation_path)\n        if not result:\n            error(\n                \"Translation file {} for lang {} was not loaded properly,\" +\n                \"falling back to English\".format(translation_path, new_lang)\n            )\n            return\n        if qVersion() > \"4.3.3\":\n            QCoreApplication.installTranslator(self.translator)\n        else:\n            self.translator = None\n            warn(\"Translation not supported for Qt <= {}\".format(qVersion()))\n\n    def unload(self):\n        control.unload()\n        QgsExpression.unregisterFunction(\"$animation_datetime\")\n        QgsExpression.unregisterFunction(\"animation_datetime\")\n        QgsExpression.unregisterFunction(\"$animation_time_frame_size\")\n        QgsExpression.unregisterFunction(\"animation_time_frame_size\")\n        QgsExpression.unregisterFunction(\"$animation_time_frame_type\")\n        QgsExpression.unregisterFunction(\"animation_time_frame_type\")\n        QgsExpression.unregisterFunction(\"$animation_start_datetime\")\n        QgsExpression.unregisterFunction(\"animation_start_datetime\")\n        QgsExpression.unregisterFunction(\"$animation_end_datetime\")\n        QgsExpression.unregisterFunction(\"animation_end_datetime\")\n\n    @qgsfunction(0, \"TimeManager\")\n    def animation_datetime(values, feature, parent):\n        return time_util.datetime_to_str(control.getTimeLayerManager().getCurrentTimePosition(),\n                                         time_util.DEFAULT_FORMAT)\n\n    @qgsfunction(0, \"TimeManager\")\n    def animation_time_frame_size(values, feature, parent):\n        return control.getTimeLayerManager().getTimeFrameSize()\n\n    @qgsfunction(0, \"TimeManager\")\n    def animation_time_frame_type(values, feature, parent):\n        return control.getTimeLayerManager().getTimeFrameType()\n\n    @qgsfunction(0, \"TimeManager\")\n    def animation_start_datetime(values, feature, parent):\n        return time_util.datetime_to_str(control.getTimeLayerManager().getProjectTimeExtents()[0],\n                                         time_util.DEFAULT_FORMAT)\n\n    @qgsfunction(0, \"TimeManager\")\n    def animation_end_datetime(values, feature, parent):\n        return time_util.datetime_to_str(control.getTimeLayerManager().getProjectTimeExtents()[1],\n                                         time_util.DEFAULT_FORMAT)"}
{"Repository": "asyncssh", "input": "An octet string value className _OctetString(DERType) Method encode Method decode", "label": "class _OctetString(DERType):\n    @staticmethod\n    def encode(value: object) -> bytes:\n        return cast(bytes, value)\n\n    @classmethod\n    def decode(cls, constructed: bool, content: bytes) -> bytes:\n        if constructed:\n            raise ASN1DecodeError('OCTET STRING should not be constructed')\n\n        return content"}
{"Repository": "ScaffoldGraph", "input": "Abstract base class for generic rules where 'min' or 'max' filtering can be specified. className _MinMaxScaffoldFilterRule(BaseScaffoldFilterRule) Method __init__ Method filter Method get_property Method name Attribute func", "label": "class _MinMaxScaffoldFilterRule(BaseScaffoldFilterRule):\n    _f = {'min', 'max'}\n\n    def __init__(self, min_max='min'):\n        assert min_max in self._f, f'function must be min or max'\n        self.func = min if min_max == 'min' else max\n\n    def filter(self, child, parents):\n        props = [self.get_property(child, s) for s in parents]\n        val = self.func(props)\n        return list(compress(parents, [True if p == val else False for p in props]))\n\n    @abstractmethod\n    def get_property(self, child, parent):\n        raise NotImplementedError()\n\n    @property\n    def name(self):\n        return '{}'.format(\n            self.__class__.__name__\n        )"}
{"Repository": "unmasked_teacher", "input": "Get attributes >>> d = EasyDict({'foo':3}) >>> d['foo'] 3 >>> d. className EasyDict(dict) Method __init__ Method __setattr__ Method update Method pop", "label": "class EasyDict(dict):\n    def __init__(self, d=None, **kwargs):\n        if d is None:\n            d = {}\n        if kwargs:\n            d.update(**kwargs)\n        for k, v in d.items():\n            setattr(self, k, v)\n        # Class attributes\n        for k in self.__class__.__dict__.keys():\n            if not (k.startswith(\"__\") and k.endswith(\"__\")) and not k in (\"update\", \"pop\"):\n                setattr(self, k, getattr(self, k))\n\n    def __setattr__(self, name, value):\n        if isinstance(value, (list, tuple)):\n            value = [self.__class__(x) if isinstance(x, dict) else x for x in value]\n        elif isinstance(value, dict) and not isinstance(value, self.__class__):\n            value = self.__class__(value)\n        super(EasyDict, self).__setattr__(name, value)\n        super(EasyDict, self).__setitem__(name, value)\n\n    __setitem__ = __setattr__\n\n    def update(self, e=None, **f):\n        d = e or dict()\n        d.update(f)\n        for k in d:\n            setattr(self, k, d[k])\n\n    def pop(self, k, d=None):\n        if hasattr(self, k):\n            delattr(self, k)\n        return super(EasyDict, self).pop(k, d)"}
{"Repository": "bert_document_classification", "input": "BERT output over document into linear layer className DocumentBertMaxPool(BertPreTrainedModel) Method __init__ Method forward Method freeze_bert_encoder Method unfreeze_bert_encoder Method unfreeze_bert_encoder_last_layers Method unfreeze_bert_encoder_pooler_layer Attribute bert Attribute bert_batch_size Attribute dropout Attribute transformer_encoder Attribute transformer_encoder Attribute classifier", "label": "class DocumentBertMaxPool(BertPreTrainedModel):\n    def __init__(self, bert_model_config: BertConfig):\n        super(DocumentBertMaxPool, self).__init__(bert_model_config)\n        self.bert = BertModel(bert_model_config)\n        self.bert_batch_size= self.bert.config.bert_batch_size\n        self.dropout = nn.Dropout(p=bert_model_config.hidden_dropout_prob)\n\n        # self.transformer_encoder = TransformerEncoderLayer(d_model=bert_model_config.hidden_size,\n        #                                            nhead=6,\n        #                                            dropout=bert_model_config.hidden_dropout_prob)\n        #self.transformer_encoder = TransformerEncoder(encoder_layer, num_layers=6, norm=nn.LayerNorm(bert_model_config.hidden_size))\n        self.classifier = nn.Sequential(\n            nn.Dropout(p=bert_model_config.hidden_dropout_prob),\n            nn.Linear(bert_model_config.hidden_size, bert_model_config.num_labels),\n            nn.Tanh()\n        )\n\n    #input_ids, token_type_ids, attention_masks\n    def forward(self, document_batch: torch.Tensor, document_sequence_lengths: list, device='cuda'):\n\n        #contains all BERT sequences\n        #bert should output a (batch_size, num_sequences, bert_hidden_size)\n        bert_output = torch.zeros(size=(document_batch.shape[0],\n                                              min(document_batch.shape[1],self.bert_batch_size),\n                                              self.bert.config.hidden_size), dtype=torch.float, device='cuda')\n\n        #only pass through bert_batch_size numbers of inputs into bert.\n        #this means that we are possibly cutting off the last part of documents.\n\n        for doc_id in range(document_batch.shape[0]):\n            bert_output[doc_id][:self.bert_batch_size] = self.dropout(self.bert(document_batch[doc_id][:self.bert_batch_size,0],\n                                            token_type_ids=document_batch[doc_id][:self.bert_batch_size,1],\n                                            attention_mask=document_batch[doc_id][:self.bert_batch_size,2])[1])\n\n\n        prediction = self.classifier(bert_output.max(dim=1)[0])\n        assert prediction.shape[0] == document_batch.shape[0]\n        return prediction\n\n    def freeze_bert_encoder(self):\n        for param in self.bert.parameters():\n            param.requires_grad = False\n\n    def unfreeze_bert_encoder(self):\n        for param in self.bert.parameters():\n            param.requires_grad = True\n\n    def unfreeze_bert_encoder_last_layers(self):\n        for name, param in self.bert.named_parameters():\n            if \"encoder.layer.11\" in name or \"pooler\" in name:\n                param.requires_grad = True\n    def unfreeze_bert_encoder_pooler_layer(self):\n        for name, param in self.bert.named_parameters():\n            if \"pooler\" in name:\n                param.requires_grad = True"}
{"Repository": "sanskrit", "input": "Constructs a context in a variety of ways. className ContextTestCase(TestCase) Method compare_all Method testFile Method testModule Method testDict Method testOverride", "label": "class ContextTestCase(TestCase):\n    def compare_all(self, ctx):\n        self.assertEqual(ctx.config['DATABASE_URI'], cfg.DATABASE_URI)\n        self.assertEqual(ctx.config['DATA_PATH'], cfg.DATA_PATH)\n\n    def testFile(self):\n        path = os.path.join(os.path.dirname(__file__), 'config.py')\n        ctx = Context(path)\n        self.compare_all(ctx)\n\n    def testModule(self):\n        ctx = Context(cfg)\n        self.compare_all(ctx)\n\n    def testDict(self):\n        config = dict(DATABASE_URI=cfg.DATABASE_URI, DATA_PATH=cfg.DATA_PATH)\n        ctx = Context(config)\n        self.compare_all(ctx)\n\n    def testOverride(self):\n        config = dict(DATABASE_URI=cfg.DATABASE_URI, DATA_PATH=cfg.DATA_PATH,\n                      MONIER_XML_PATH='foo')\n        ctx = Context(config)\n        self.assertEqual(ctx.config['MONIER_XML_PATH'], 'foo')"}
{"Repository": "EDMarketConnector", "input": "CAPI Response. className CAPIData(UserDict) Method check_modules_ships", "label": "class CAPIData(UserDict):\n    def __init__(\n            self,\n            data: str | dict[str, Any] | 'CAPIData' | None = None,\n            source_host: str | None = None,\n            source_endpoint: str | None = None,\n            request_cmdr: str | None = None\n    ) -> None:\n        if data is None:\n            super().__init__()\n\n        elif isinstance(data, str):\n            super().__init__(json.loads(data))\n\n        else:\n            super().__init__(data)\n\n        self.original_data = self.data.copy()  # Just in case\n\n        self.source_host = source_host\n        self.source_endpoint = source_endpoint\n        self.request_cmdr = request_cmdr\n\n        if source_endpoint is None:\n            return\n\n        if source_endpoint == Session.FRONTIER_CAPI_PATH_SHIPYARD and self.data.get('lastStarport'):\n            # All the other endpoints may or may not have a lastStarport, but definitely won't have valid data\n            # for this check, which means it'll just make noise for no reason while we're working on other things\n            self.check_modules_ships()\n\n    def check_modules_ships(self) -> None:\n        modules: dict[str, Any] = self.data['lastStarport'].get('modules')\n        if modules is None or not isinstance(modules, dict):\n            if modules is None:\n                logger.debug('modules was None.  FC or Damaged Station?')\n\n            elif isinstance(modules, list):\n                if not modules:\n                    logger.debug('modules is empty list. Damaged Station?')\n\n                else:\n                    logger.error(f'modules is non-empty list: {modules!r}')\n\n            else:\n                logger.error(f'modules was not None, a list, or a dict! type: {type(modules)}, content: {modules}')\n\n            # Set a safe value\n            self.data['lastStarport']['modules'] = modules = {}\n\n        ships: dict[str, Any] = self.data['lastStarport'].get('ships')\n        if ships is None or not isinstance(ships, dict):\n            if ships is None:\n                logger.debug('ships was None')\n\n            else:\n                logger.error(f'ships was neither None nor a dict! type: {type(ships)}, content: {ships}')\n\n            # Set a safe value\n            self.data['lastStarport']['ships'] = {'shipyard_list': {}, 'unavailable_list': []}"}
{"Repository": "iPrompt", "input": "Text tokenizer for ASCII-256 Character Level Tokenization. className CharacterLevelTokenizer(TextTokenizer) Method __init__ Method __len__ Method exists Method Train Method tokens Method vocab Method EncodeAsIds Method EncodeAsTokens Method IdToToken Method TokenToId Method DecodeIds Method DecodeTokens Attribute num_text_tokens Attribute _tokens Attribute _vocab", "label": "class CharacterLevelTokenizer(TextTokenizer):\n    def __init__(self, **kwargs):\n\n        self.num_text_tokens = 256\n\n        super(CharacterLevelTokenizer, self).__init__()\n\n        self._tokens = [self.IdToToken(Id) for Id in range(self.num_text_tokens)]\n\n        self._vocab = {t: i for i,t in enumerate(self._tokens)}\n\n\n\n    def __len__(self):\n\n        return 256\n\n\n\n    @staticmethod\n\n    def exists(model_path):\n\n        return True\n\n\n\n    def Train(self, corpus):\n\n        pass\n\n\n\n    @property\n\n    def tokens(self):\n\n        return self._tokens\n\n\n\n    @property\n\n    def vocab(self):\n\n        return self._vocab\n\n\n\n    def EncodeAsIds(self, text, process_fn=None):\n        processed_text = text\n\n        if process_fn is not None:\n\n            processed_text = process_fn(processed_text)\n\n            processed_text = str(processed_text)\n\n        tokens = [self.TokenToId(c) for c in processed_text]\n\n        return Tokenization(tokens, processed_text, text)\n\n\n\n    def EncodeAsTokens(self, text, process_fn=None):\n        processed_text = text\n\n        if process_fn is not None:\n\n            processed_text = process_fn(processed_text)\n\n        processed_text = str(processed_text)\n\n        tokens = [c for c in processed_text]\n\n        return Tokenization(tokens, processed_text, text, asIds=False)\n\n\n\n    def IdToToken(self, Id):\n        return chr(Id)\n\n\n\n    def TokenToId(self, token):\n        return ord(token)\n\n\n\n    def DecodeIds(self, Ids):\n        if isinstance(Ids, Tokenization):\n\n            Ids = Ids.tokenization\n\n        return ''.join([self.IdToToken(tok) for tok in Ids])\n\n\n\n    def DecodeTokens(self, Tokens):\n        if isinstance(Tokens, Tokenization):\n\n            Tokens = Tokens.tokenization\n\n        return ''.join(Tokens)"}
{"Repository": "aws-parallelcluster", "input": "Identify a CloudFormation VPC stack. className CfnVpcStack(CfnStack) Method __init__ Method set_az_override Method get_public_subnet Method get_all_public_subnets Method get_private_subnet Method get_all_private_subnets Method _get_subnet Method _get_all_subnets Attribute default_az_id Attribute az_ids Attribute az_override Attribute __public_subnet_ids Attribute __private_subnet_ids", "label": "class CfnVpcStack(CfnStack):\n    # TODO add method to get N subnets\n    def __init__(self, default_az_id: str = None, az_ids: list = None, **kwargs):\n        super().__init__(**kwargs)\n        self.default_az_id = default_az_id\n        self.az_ids = az_ids\n        self.az_override = None\n        self.__public_subnet_ids = None\n        self.__private_subnet_ids = None\n        if \"CAPABILITY_NAMED_IAM\" not in self.capabilities:\n            self.capabilities.append(\"CAPABILITY_NAMED_IAM\")\n\n    def set_az_override(self, az_override):\n        self.az_override = az_override\n\n    def get_public_subnet(self):\n        return self._get_subnet(visibility=\"Public\")\n\n    def get_all_public_subnets(self):\n        if not self.__public_subnet_ids:\n            self.__public_subnet_ids, self.__private_subnet_ids = self._get_all_subnets()\n\n        return self.__public_subnet_ids\n\n    def get_private_subnet(self):\n        return self._get_subnet(visibility=\"Private\")\n\n    def get_all_private_subnets(self):\n        if not self.__private_subnet_ids:\n            self.__public_subnet_ids, self.__private_subnet_ids = self._get_all_subnets()\n\n        return self.__private_subnet_ids\n\n    def _get_subnet(self, visibility: str = \"Public\"):\n        if self.az_override is not None:\n            az_id_tag = to_pascal_from_kebab_case(self.az_override)\n        elif self.default_az_id:\n            az_id_tag = to_pascal_from_kebab_case(self.default_az_id)\n        else:\n            # get random subnet, if default is not set\n            assert_that(self.az_ids).is_not_none()\n            az_id_tag = to_pascal_from_kebab_case(random.choice(self.az_ids))\n\n        assert_that(az_id_tag).is_not_none()\n        return self.cfn_outputs[f\"{az_id_tag}{visibility}SubnetId\"]\n\n    def _get_all_subnets(self):\n        assert_that(self.az_ids).is_not_none()\n        public_subnet_ids = []\n        private_subnet_ids = []\n        for az_id in self.az_ids:\n            az_id_tag = to_pascal_from_kebab_case(az_id)\n            public_subnet_ids.append(self.cfn_outputs[f\"{az_id_tag}PublicSubnetId\"])\n            private_subnet_ids.append(self.cfn_outputs[f\"{az_id_tag}PrivateSubnetId\"])\n\n        # shuffle the two subnets list in unison\n        temp_subnets = list(zip(public_subnet_ids, private_subnet_ids))\n        random.shuffle(temp_subnets)\n        public_subnet_ids, private_subnet_ids = zip(*temp_subnets)\n\n        logging.info(\n            f\"Setting public subnets list to {public_subnet_ids} and private subnets list to {private_subnet_ids}\"\n        )\n        return list(public_subnet_ids), list(private_subnet_ids)"}
{"Repository": "py-ipfs-http-client", "input": "Raised when decoding a byte string to a Python object has failed due to some problem with the input data. className DecodingError(EncoderError) Method __init__", "label": "class DecodingError(EncoderError):\n\t__slots__ = (\"original\",)\n\n\toriginal: Exception\n\t\n\tdef __init__(self, encoder_name: str, original: Exception) -> None:\n\t\tself.original = original\n\t\t\n\t\tsuper().__init__(\"Object decoding error: {}\".format(original), encoder_name)"}
{"Repository": "tfdeploy", "input": "A tensor ensemble basically contains a list of tensors that correspond to models of an :py:class:`Ensemble` instance. className TensorEnsemble(object) Method __init__ Method eval Method __call__ Method func Method func_mean Method func_max Method func_min Method func_custom Attribute method Attribute tensors", "label": "class TensorEnsemble(object):\n    def __init__(self, tensors, method=METHOD_MEAN):\n        super(TensorEnsemble, self).__init__()\n\n        # check method\n        if method not in METHODS:\n            raise UnknownEnsembleMethodException(method)\n        self.method = method\n\n        self.tensors = list(tensors)\n\n    def eval(self, feed_dict=None):\n        # first, check that the length of all feed_dict keys match our own length\n        for tensor_ensemble in feed_dict:\n            if len(tensor_ensemble.tensors) != len(self.tensors):\n                raise EnsembleMismatchException(\"incompatible lengths of tensors: %d, %d\" \\\n                                                % (len(self.tensors), len(tensor_ensemble.tensors)))\n\n        # create a joined uuid\n        _uuid = uuid4()\n\n        # prepare feed_dicts\n        feed_dicts = [{} for _ in range(len(self.tensors))]\n        for tensor_ensemble, value in feed_dict.items():\n            for i, tensor in enumerate(tensor_ensemble.tensors):\n                if tensor is not None:\n                    feed_dicts[i][tensor] = value[i] if isinstance(value, (list, tuple)) else value\n\n        # eval all tensors\n        values = [t.eval(feed_dict=d, _uuid=_uuid) for t, d in zip(self.tensors, feed_dicts)]\n\n        # return the computed ensemble value\n        return self.func(values)\n\n    def __call__(self, *args, **kwargs):\n        return self.eval(*args, **kwargs)\n\n    def func(self, values):\n        if self.method == METHOD_MEAN:\n            return self.func_mean(values)\n        elif self.method == METHOD_MAX:\n            return self.func_max(values)\n        elif self.method == METHOD_MIN:\n            return self.func_min(values)\n        elif self.method == METHOD_CUSTOM:\n            return self.func_custom(values)\n        else:\n            raise UnknownEnsembleMethodException(self.method)\n\n    @staticmethod\n    def func_mean(values):\n        return np.mean(np.stack(values), axis=0)\n\n    @staticmethod\n    def func_max(values):\n        return np.amax(np.stack(values), axis=0)\n\n    @staticmethod\n    def func_min(values):\n        return np.amin(np.stack(values), axis=0)\n\n    @staticmethod\n    def func_custom(values):\n        raise NotImplementedError"}
{"Repository": "alfred-mailto", "input": "Decorator that postpones SIGTERM until wrapped function returns. className uninterruptible(object) Method __init__ Method signal_handler Method __call__ Method __get__ Attribute func Attribute _caught_signal", "label": "class uninterruptible(object):\n    def __init__(self, func, class_name=''):\n        self.func = func\n        functools.update_wrapper(self, func)\n        self._caught_signal = None\n\n    def signal_handler(self, signum, frame):\n        self._caught_signal = (signum, frame)\n\n    def __call__(self, *args, **kwargs):\n        self._caught_signal = None\n        # Register handler for SIGTERM, then call `self.func`\n        self.old_signal_handler = signal.getsignal(signal.SIGTERM)\n        signal.signal(signal.SIGTERM, self.signal_handler)\n\n        self.func(*args, **kwargs)\n\n        # Restore old signal handler\n        signal.signal(signal.SIGTERM, self.old_signal_handler)\n\n        # Handle any signal caught during execution\n        if self._caught_signal is not None:\n            signum, frame = self._caught_signal\n            if callable(self.old_signal_handler):\n                self.old_signal_handler(signum, frame)\n            elif self.old_signal_handler == signal.SIG_DFL:\n                sys.exit(0)\n\n    def __get__(self, obj=None, klass=None):\n        return self.__class__(self.func.__get__(obj, klass),\n                              klass.__name__)"}
{"Repository": "norfair", "input": "This class can be used either to change some parameters of the [KalmanFilter](https://filterpy. className FilterPyKalmanFilterFactory(FilterFactory) Method __init__ Method create_filter Attribute R Attribute Q Attribute P", "label": "class FilterPyKalmanFilterFactory(FilterFactory):\n    def __init__(self, R: float = 4.0, Q: float = 0.1, P: float = 10.0):\n        self.R = R\n        self.Q = Q\n        self.P = P\n\n    def create_filter(self, initial_detection: np.ndarray) -> KalmanFilter:\n        num_points = initial_detection.shape[0]\n        dim_points = initial_detection.shape[1]\n        dim_z = dim_points * num_points\n        dim_x = 2 * dim_z  # We need to accommodate for velocities\n\n        filter = KalmanFilter(dim_x=dim_x, dim_z=dim_z)\n\n        # State transition matrix (models physics): numpy.array()\n        filter.F = np.eye(dim_x)\n        dt = 1  # At each step we update pos with v * dt\n\n        filter.F[:dim_z, dim_z:] = dt * np.eye(dim_z)\n\n        # Measurement function: numpy.array(dim_z, dim_x)\n        filter.H = np.eye(\n            dim_z,\n            dim_x,\n        )\n\n        # Measurement uncertainty (sensor noise): numpy.array(dim_z, dim_z)\n        filter.R *= self.R\n\n        # Process uncertainty: numpy.array(dim_x, dim_x)\n        # Don't decrease it too much or trackers pay too little attention to detections\n        filter.Q[dim_z:, dim_z:] *= self.Q\n\n        # Initial state: numpy.array(dim_x, 1)\n        filter.x[:dim_z] = np.expand_dims(initial_detection.flatten(), 0).T\n        filter.x[dim_z:] = 0\n\n        # Estimation uncertainty: numpy.array(dim_x, dim_x)\n        filter.P[dim_z:, dim_z:] *= self.P\n\n        return filter"}
{"Repository": "borg", "input": "Cache, or information obtained from the security directory is newer than repository - this is either an attack or unsafe (multiple repos with same ID) className RepositoryReplay(Error) Method break_lock Method destroy Method local Method adhoc", "label": "class RepositoryReplay(Error):\n        exit_mcode = 64\n\n    @staticmethod\n    def break_lock(repository, path=None):\n        path = cache_dir(repository, path)\n        Lock(os.path.join(path, \"lock\"), exclusive=True).break_lock()\n\n    @staticmethod\n    def destroy(repository, path=None):\n        path = path or os.path.join(get_cache_dir(), repository.id_str)\n        config = os.path.join(path, \"config\")\n        if os.path.exists(config):\n            os.remove(config)  # kill config first\n            shutil.rmtree(path)\n\n    def __new__(\n        cls,\n        repository,\n        manifest,\n        path=None,\n        sync=True,\n        warn_if_unencrypted=True,\n        progress=False,\n        lock_wait=None,\n        permit_adhoc_cache=False,\n        cache_mode=FILES_CACHE_MODE_DISABLED,\n        iec=False,\n    ):\n        def local():\n            return LocalCache(\n                manifest=manifest,\n                path=path,\n                sync=sync,\n                warn_if_unencrypted=warn_if_unencrypted,\n                progress=progress,\n                iec=iec,\n                lock_wait=lock_wait,\n                cache_mode=cache_mode,\n            )\n\n        def adhoc():\n            return AdHocCache(manifest=manifest, lock_wait=lock_wait, iec=iec)\n\n        if not permit_adhoc_cache:\n            return local()\n\n        # ad-hoc cache may be permitted, but if the local cache is in sync it'd be stupid to invalidate\n        # it by needlessly using the ad-hoc cache.\n        # Check if the local cache exists and is in sync.\n\n        cache_config = CacheConfig(repository, path, lock_wait)\n        if cache_config.exists():\n            with cache_config:\n                cache_in_sync = cache_config.manifest_id == manifest.id\n            # Don't nest cache locks\n            if cache_in_sync:\n                # Local cache is in sync, use it\n                logger.debug(\"Cache: choosing local cache (in sync)\")\n                return local()\n        logger.debug(\"Cache: choosing ad-hoc cache (local cache does not exist or is not in sync)\")\n        return adhoc()"}
{"Repository": "FFTformer", "input": "Lmdb storage backend. className LmdbBackend(BaseStorageBackend) Method get Method get_text", "label": "class LmdbBackend(BaseStorageBackend):\n    def __init__(self,\n                 db_paths,\n                 client_keys='default',\n                 readonly=True,\n                 lock=False,\n                 readahead=False,\n                 **kwargs):\n        try:\n            import lmdb\n        except ImportError:\n            raise ImportError('Please install lmdb to enable LmdbBackend.')\n\n        if isinstance(client_keys, str):\n            client_keys = [client_keys]\n\n        if isinstance(db_paths, list):\n            self.db_paths = [str(v) for v in db_paths]\n        elif isinstance(db_paths, str):\n            self.db_paths = [str(db_paths)]\n        assert len(client_keys) == len(self.db_paths), (\n            'client_keys and db_paths should have the same length, '\n            f'but received {len(client_keys)} and {len(self.db_paths)}.')\n\n        self._client = {}\n\n        for client, path in zip(client_keys, self.db_paths):\n            self._client[client] = lmdb.open(\n                path,\n                readonly=readonly,\n                lock=lock,\n                readahead=readahead,\n                map_size=8*1024*10485760,\n                # max_readers=1,\n                **kwargs)\n\n    def get(self, filepath, client_key):\n        filepath = str(filepath)\n        assert client_key in self._client, (f'client_key {client_key} is not '\n                                            'in lmdb clients.')\n        client = self._client[client_key]\n        with client.begin(write=False) as txn:\n            value_buf = txn.get(filepath.encode('ascii'))\n        return value_buf\n\n    def get_text(self, filepath):\n        raise NotImplementedError"}
{"Repository": "MHN", "input": "Implementation should return indices of positive pairs and negative pairs that will be passed to compute className PairSelector(object) Method __init__ Method get_pairs", "label": "class PairSelector(object):\n    def __init__(self):\n        pass\n\n    def get_pairs(self, embeddings, labels):\n        raise NotImplementedError"}
{"Repository": "thrash-protect", "input": "Class containing one method for selecting a process to freeze, based on oom_score. className OOMScoreProcessSelector(ProcessSelector) Method scan", "label": "class OOMScoreProcessSelector(ProcessSelector):\n    def scan(self):\n        oom_scores = glob.glob('/proc/*/oom_score')\n        max = 0\n        worstpid = None\n        for fn in oom_scores:\n            try:\n                pid = int(fn.split('/')[2])\n            except ValueError:\n                continue\n            try:\n                with open(fn, 'r') as oom_score_file:\n                    oom_score = int(oom_score_file.readline())\n                stats = self.readStat(pid)\n                if not stats:\n                    continue\n                if 'T' in stats.state:\n                    logging.debug(\"oom_score: %s, cmd: %s, pid: %s, state: %s - no touch\" % (oom_score, stats.cmd, pid, stats.state))\n                    continue\n            except FileNotFoundError:\n                continue\n            if oom_score > 0:\n                logging.debug(\"oom_score: %s, cmd: %s, pid: %s\" % (oom_score, stats.cmd, pid))\n                if stats.cmd in config.cmd_whitelist:\n                    logging.debug(\"whitelisted process %s %s %s\" % (pid, stats.cmd, oom_score))\n                    oom_score /= config.whitelist_score_divider\n                if stats.cmd in config.cmd_blacklist:\n                    oom_score *= config.blacklist_score_multiplier\n                if oom_score > max:\n                    ## ignore self\n                    if pid in (getpid(), getppid()):\n                        continue\n                    max = oom_score\n                    worstpid = (pid, stats.ppid)\n        logging.debug(\"oom scan completed - selected pid: %s\" % (worstpid and worstpid[0]))\n        if worstpid != None:\n            return self.checkParents(*worstpid)\n        else:\n            return None"}
{"Repository": "GAN-AD", "input": "Gated Recurrent Unit cell (cf. http://arxiv.org/abs/1406.1078). className GRUCell(RNNCell) Method __init__ Method state_size Method output_size Method __call__ Attribute _num_units Attribute _activation Attribute _reuse", "label": "class GRUCell(RNNCell):\n  def __init__(self, num_units, input_size=None, activation=tanh, reuse=None):\n    if input_size is not None:\n      logging.warn(\"%s: The input_size parameter is deprecated.\", self)\n    self._num_units = num_units\n    self._activation = activation\n    self._reuse = reuse\n\n  @property\n  def state_size(self):\n    return self._num_units\n\n  @property\n  def output_size(self):\n    return self._num_units\n\n  def __call__(self, inputs, state, scope=None):\n    with _checked_scope(self, scope or \"gru_cell\", reuse=self._reuse):\n      with vs.variable_scope(\"gates\"):  # Reset gate and update gate.\n        # We start with bias of 1.0 to not reset and not update.\n        value = sigmoid(_linear(\n          [inputs, state], 2 * self._num_units, True, 1.0))\n        r, u = array_ops.split(\n            value=value,\n            num_or_size_splits=2,\n            axis=1)\n      with vs.variable_scope(\"candidate\"):\n        c = self._activation(_linear([inputs, r * state],\n                                     self._num_units, True))\n      new_h = u * state + (1 - u) * c\n    return new_h, new_h"}
{"Repository": "wifi-scripts", "input": "A fake Access Point that makes itself known by sending beacons className AccessPoint(object) Method __init__ Method run Method __init__ Method next_seq Attribute ap Attribute device Attribute interval Attribute beacon_pckt", "label": "class AccessPoint(object):\n    class AccessPointBeaconizer(threading.Thread):\n        def __init__(self, ap):\n            threading.Thread.__init__(self)\n            self.ap = ap\n            self.device = NetworkTransmitter(self.ap.iface)\n            self.interval = 0.1\n            self.beacon_pckt = dot11.Dot11(addr1='ff:ff:ff:ff:ff:ff',       \\\n                                           addr2=self.ap.bssid,             \\\n                                           addr3=self.ap.bssid)             \\\n                               / dot11.Dot11Beacon(cap='ESS+privacy')       \\\n                               / dot11.Dot11Elt(ID='SSID',                  \\\n                                                info=self.ap.essid)         \\\n                               / dot11.Dot11Elt(ID='DSset',                 \\\n                                                info=chr(self.ap.channel))  \\\n                               / dot11.Dot11Elt(ID='Rates',                 \\\n                                                info='\\x82\\x84\\x0b\\x16')    \\\n                               / dot11.Dot11Elt(ID='RSNinfo',\n                                                info='\\x01\\x00\\x00\\x0f\\xac' \\\n                                                     '\\x04\\x01\\x00\\x00\\x0f' \\\n                                                     '\\xac\\x04\\x01\\x00\\x00' \\\n                                                     '\\x0f\\xac\\x02\\x00\\x00')\n            self.setDaemon(True)\n    \n        def run(self):\n            while True:\n                self.beacon_pckt.SC = self.ap.next_seq()\n                now = time.time() - self.ap.starttime\n                self.beacon_pckt[dot11.Dot11Beacon].timestamp = now * 1000000\n                self.device.write(self.beacon_pckt)\n                time.sleep(self.interval)\n\n    \n    def __init__(self, iface, bssid, essid, channel):\n        self.channel = channel\n        self.bssid = bssid\n        self.essid = essid\n        self.iface = iface\n        self.seq = 0\n        self.starttime = time.time()\n        self.seq_lock = threading.Lock()\n        self.beaconizer = self.AccessPointBeaconizer(self)\n        self.beaconizer.start()\n\n    def next_seq(self):\n        with self.seq_lock:\n            self.seq = (self.seq + 16) % 65535\n            return self.seq"}
{"Repository": "toxygen", "input": "Inline image - save in buffer not in file system className ReceiveToBuffer(FileTransfer) Method __init__ Method get_data Method write_chunk Attribute _data Attribute _data_size", "label": "class ReceiveToBuffer(FileTransfer):\n    def __init__(self, tox, friend_number, size, file_number):\n        super(ReceiveToBuffer, self).__init__(None, tox, friend_number, size, file_number)\n        self._data = bytes()\n        self._data_size = 0\n\n    def get_data(self):\n        return self._data\n\n    def write_chunk(self, position, data):\n        if self._creation_time is None:\n            self._creation_time = time()\n        if data is None:\n            self.state = TOX_FILE_TRANSFER_STATE['FINISHED']\n            self.finished()\n        else:\n            data = bytes(data)\n            l = len(data)\n            if self._data_size < position:\n                self._data += (b'\\0' * (position - self._data_size))\n            self._data = self._data[:position] + data + self._data[position + l:]\n            if position + l > self._data_size:\n                self._data_size = position + l\n            self._done += l\n        self.signal()"}
{"Repository": "imageio", "input": "TEST DOCS className MyFormat(Format) Method _can_read Method _can_write Method _open Method _close Method _get_length Method _get_data Method _get_meta_data Method _open Method _close Method _append_data Method _set_meta_data", "label": "class MyFormat(Format):\n    _closed: List[int] = []\n\n    def _can_read(self, request):\n        return request.filename.lower().endswith(self.extensions + (\".haha\",))\n\n    def _can_write(self, request):\n        return request.filename.lower().endswith(self.extensions + (\".haha\",))\n\n    class Reader(Format.Reader):\n        _failmode = False\n        _stream_mode = False\n\n        def _open(self):\n            self._read_frames = 0\n\n        def _close(self):\n            self.format._closed.append(id(self))\n\n        def _get_length(self):\n            if self._stream_mode:\n                return np.inf\n            return 3\n\n        def _get_data(self, index):\n            if self._failmode == 2:\n                raise IndexError()\n            elif self._failmode:\n                return \"not an array\", {}\n            elif self._stream_mode and self._read_frames >= 5:\n                raise IndexError()  # Mark end of stream\n            else:\n                self._read_frames += 1\n                return np.ones((10, 10)) * index, self._get_meta_data(index)\n\n        def _get_meta_data(self, index):\n            if self._failmode:\n                return \"not a dict\"\n            return {\"index\": index}\n\n    class Writer(Format.Writer):\n        def _open(self):\n            self._written_data = []\n            self._written_meta = []\n            self._meta = None\n\n        def _close(self):\n            self.format._closed.append(id(self))\n\n        def _append_data(self, im, meta):\n            self._written_data.append(im)\n            self._written_meta.append(meta)\n\n        def _set_meta_data(self, meta):\n            self._meta = meta"}
{"Repository": "huaweicloud-sdk-python-v3", "input": "Attributes: openapi_types (dict): The key is attribute name and the value is attribute type. className AssociateIpToPolicyResponse(SdkResponse) Method __init__ Method to_dict Method to_str Method __repr__ Method __eq__ Method __ne__ Attribute discriminator", "label": "class AssociateIpToPolicyResponse(SdkResponse):\n    sensitive_list = []\n\n    openapi_types = {\n    }\n\n    attribute_map = {\n    }\n\n    def __init__(self):\n        super(AssociateIpToPolicyResponse, self).__init__()\n        self.discriminator = None\n\n    def to_dict(self):\n        result = {}\n\n        for attr, _ in six.iteritems(self.openapi_types):\n            value = getattr(self, attr)\n            if isinstance(value, list):\n                result[attr] = list(map(\n                    lambda x: x.to_dict() if hasattr(x, \"to_dict\") else x,\n                    value\n                ))\n            elif hasattr(value, \"to_dict\"):\n                result[attr] = value.to_dict()\n            elif isinstance(value, dict):\n                result[attr] = dict(map(\n                    lambda item: (item[0], item[1].to_dict())\n                    if hasattr(item[1], \"to_dict\") else item,\n                    value.items()\n                ))\n            else:\n                if attr in self.sensitive_list:\n                    result[attr] = \"****\"\n                else:\n                    result[attr] = value\n\n        return result\n\n    def to_str(self):\n        import simplejson as json\n        if six.PY2:\n            import sys\n            reload(sys)\n            sys.setdefaultencoding(\"utf-8\")\n        return json.dumps(sanitize_for_serialization(self), ensure_ascii=False)\n\n    def __repr__(self):\n        return self.to_str()\n\n    def __eq__(self, other):\n        if not isinstance(other, AssociateIpToPolicyResponse):\n            return False\n\n        return self.__dict__ == other.__dict__\n\n    def __ne__(self, other):\n        return not self == other"}
{"Repository": "nameko", "input": "Generic service that handles events. className HandlerService(object) Method __init__ Method handle Attribute events", "label": "class HandlerService(object):\n    name = \"handlerservice\"\n\n    def __init__(self):\n        self.events = []\n        services[self.name].append(self)\n\n    def handle(self, evt):\n        self.events.append(evt)\n        events.append(evt)"}
{"Repository": "dramatiq", "input": "A broker that can be used within unit tests. className StubBroker(Broker) Method __init__ Method dead_letters Method consume Method declare_queue Method enqueue Method flush Method flush_all Method join Attribute dead_letters_by_queue", "label": "class StubBroker(Broker):\n    def __init__(self, middleware=None):\n        super().__init__(middleware)\n\n        self.dead_letters_by_queue = defaultdict(list)\n\n    @property\n    def dead_letters(self):\n        return [message for messages in self.dead_letters_by_queue.values() for message in messages]\n\n    def consume(self, queue_name, prefetch=1, timeout=100):\n        try:\n            return _StubConsumer(\n                self.queues[queue_name],\n                self.dead_letters_by_queue[queue_name],\n                timeout,\n            )\n        except KeyError:\n            raise QueueNotFound(queue_name) from None\n\n    def declare_queue(self, queue_name):\n        if queue_name in self.queues:\n            return\n\n        self.emit_before(\"declare_queue\", queue_name)\n        self.queues[queue_name] = Queue()\n        self.emit_after(\"declare_queue\", queue_name)\n\n        delayed_name = dq_name(queue_name)\n        self.queues[delayed_name] = Queue()\n        self.delay_queues.add(delayed_name)\n        self.emit_after(\"declare_delay_queue\", delayed_name)\n\n    def enqueue(self, message, *, delay=None):\n        queue_name = message.queue_name\n        if delay is not None:\n            queue_name = dq_name(queue_name)\n            message_eta = current_millis() + delay\n            message = message.copy(\n                queue_name=queue_name,\n                options={\n                    \"eta\": message_eta,\n                },\n            )\n\n        if queue_name not in self.queues:\n            raise QueueNotFound(queue_name)\n\n        self.emit_before(\"enqueue\", message, delay)\n        self.queues[queue_name].put(message.encode())\n        self.emit_after(\"enqueue\", message, delay)\n        return message\n\n    def flush(self, queue_name):\n        for _ in iter_queue(self.queues[queue_name]):\n            self.queues[queue_name].task_done()\n\n    def flush_all(self):\n        for queue_name in chain(self.queues, self.delay_queues):\n            self.flush(queue_name)\n\n        self.dead_letters_by_queue.clear()\n\n    # TODO: Make fail_fast default to True.\n    def join(self, queue_name, *, fail_fast=False, timeout=None):\n        try:\n            queues = [\n                self.queues[queue_name],\n                self.queues[dq_name(queue_name)],\n            ]\n        except KeyError:\n            raise QueueNotFound(queue_name) from None\n\n        deadline = timeout and time.monotonic() + timeout / 1000\n        while True:\n            for queue in queues:\n                timeout = deadline and deadline - time.monotonic()\n                join_queue(queue, timeout=timeout)\n\n            # We cycle through $queue then $queue.DQ then $queue\n            # again in case the messages that were on the DQ got\n            # moved back on $queue.\n            for queue in queues:\n                if queue.unfinished_tasks:\n                    break\n            else:\n                if fail_fast:\n                    for message in self.dead_letters_by_queue[queue_name]:\n                        raise message._exception from None\n\n                return"}
{"Repository": "Human-Body-Measurements-using-Computer-Vision", "input": "Class to load deeplab model and run inference. className DeepLabModel(object) Method __init__ Method run Attribute graph", "label": "class DeepLabModel(object):\n\tINPUT_TENSOR_NAME = 'ImageTensor:0'\n\tOUTPUT_TENSOR_NAME = 'SemanticPredictions:0'\n\tINPUT_SIZE = 513\n\tFROZEN_GRAPH_NAME = 'frozen_inference_graph'\n\n\tdef __init__(self, tarball_path):\n\t\t#\"\"\"Creates and loads pretrained deeplab model.\"\"\"\n\t\tself.graph = tf.Graph()\n\t\tgraph_def = None\n\t\t# Extract frozen graph from tar archive.\n\t\ttar_file = tarfile.open(tarball_path)\n\t\tfor tar_info in tar_file.getmembers():\n\t\t\tif self.FROZEN_GRAPH_NAME in os.path.basename(tar_info.name):\n\t\t\t\tfile_handle = tar_file.extractfile(tar_info)\n\t\t\t\tgraph_def = tf.GraphDef.FromString(file_handle.read())\n\t\t\t\tbreak\n\n\t\ttar_file.close()\n\n\t\tif graph_def is None:\n\t\t\traise RuntimeError('Cannot find inference graph in tar archive.')\n\n\t\twith self.graph.as_default():\n\t\t\ttf.import_graph_def(graph_def, name='')\n\n\t\tself.sess = tf.Session(graph=self.graph)\n\n\tdef run(self, image):\n\t\twidth, height = image.size\n\t\tresize_ratio = 1.0 * self.INPUT_SIZE / max(width, height)\n\t\ttarget_size = (int(resize_ratio * width), int(resize_ratio * height))\n\t\tresized_image = image.convert('RGB').resize(target_size, Image.ANTIALIAS)\n\t\tbatch_seg_map = self.sess.run(\n\t\t\tself.OUTPUT_TENSOR_NAME,\n\t\t\tfeed_dict={self.INPUT_TENSOR_NAME: [np.asarray(resized_image)]})\n\t\tseg_map = batch_seg_map[0]\n\t\treturn resized_image, seg_map"}
{"Repository": "LateTemporalModeling3DCNN", "input": "Converts a numpy. className ToTensor3(object) Method __call__", "label": "class ToTensor3(object):\n    def __call__(self, clips):\n        if isinstance(clips, np.ndarray):\n            # handle numpy array\n            clips = torch.from_numpy(clips.transpose((3, 2, 0, 1)))\n            # backward compatibility\n            return clips.float().div(255.0)"}
{"Repository": "flask-principal", "input": "The context of an identity for a permission. className IdentityContext(object) Method __init__ Method identity Method can Method __call__ Method _decorated Method __enter__ Method __exit__ Attribute permission Attribute http_exception", "label": "class IdentityContext(object):\n    def __init__(self, permission, http_exception=None):\n        self.permission = permission\n        self.http_exception = http_exception\n    @property\n    def identity(self):\n        return g.identity\n\n    def can(self):\n        return self.identity.can(self.permission)\n\n    def __call__(self, f):\n        @wraps(f)\n        def _decorated(*args, **kw):\n            with self:\n                rv = f(*args, **kw)\n            return rv\n        return _decorated\n\n    def __enter__(self):\n        # check the permission here\n        if not self.can():\n            if self.http_exception:\n                abort(self.http_exception, self.permission)\n            raise PermissionDenied(self.permission)\n\n    def __exit__(self, *args):\n        return False"}
{"Repository": "avocado", "input": "Our custom command to get rid of junk files after build. className Clean(clean) Method run Method clean_optional_plugins", "label": "class Clean(clean):\n    description = \"Get rid of scratch, byte files and build stuff.\"\n\n    def run(self):\n        super().run()\n        cleaning_list = [\n            \"PYPI_UPLOAD\",\n            \"EGG_UPLOAD\",\n            \"./build\",\n            \"./dist\",\n            \"./man/avocado.1\",\n            \"./docs/build\",\n        ]\n\n        cleaning_list += list(Path(\"/tmp/\").glob(\".avocado-*\"))\n        cleaning_list += list(Path(\"/var/tmp/\").glob(\".avocado-*\"))\n        cleaning_list += list(Path(\".\").rglob(\"*.egg-info\"))\n        cleaning_list += list(Path(\".\").rglob(\"*.pyc\"))\n        cleaning_list += list(Path(\".\").rglob(\"__pycache__\"))\n        cleaning_list += list(Path(\"./docs/source/api/\").rglob(\"*.rst\"))\n\n        for e in cleaning_list:\n            if not os.path.exists(e):\n                continue\n            if os.path.isfile(e):\n                os.remove(e)\n            if os.path.isdir(e):\n                shutil.rmtree(e)\n\n        self.clean_optional_plugins()\n\n    @staticmethod\n    def clean_optional_plugins():\n        walk_plugins_setup_py([\"clean\", \"--all\"], directory=OPTIONAL_PLUGINS_PATH)\n        walk_plugins_setup_py([\"clean\", \"--all\"], directory=EXAMPLES_PLUGINS_TESTS_PATH)"}
{"Repository": "dino", "input": "Apply Gaussian Blur to the PIL image. className GaussianBlur(object) Method __init__ Method __call__ Attribute prob Attribute radius_min Attribute radius_max", "label": "class GaussianBlur(object):\n    def __init__(self, p=0.5, radius_min=0.1, radius_max=2.):\n        self.prob = p\n        self.radius_min = radius_min\n        self.radius_max = radius_max\n\n    def __call__(self, img):\n        do_it = random.random() <= self.prob\n        if not do_it:\n            return img\n\n        return img.filter(\n            ImageFilter.GaussianBlur(\n                radius=random.uniform(self.radius_min, self.radius_max)\n            )\n        )"}
{"Repository": "Neural-Scene-Flow-Fields", "input": "Network for monocular depth estimation. className MidasNet(BaseModel) Method __init__ Method forward Attribute scratch", "label": "class MidasNet(BaseModel):\n    def __init__(self, path=None, features=256, non_negative=True):\n        print(\"Loading weights: \", path)\n\n        super(MidasNet, self).__init__()\n\n        use_pretrained = False if path else True\n\n        self.pretrained, self.scratch = _make_encoder(features, use_pretrained)\n\n        self.scratch.refinenet4 = FeatureFusionBlock(features)\n        self.scratch.refinenet3 = FeatureFusionBlock(features)\n        self.scratch.refinenet2 = FeatureFusionBlock(features)\n        self.scratch.refinenet1 = FeatureFusionBlock(features)\n\n        self.scratch.output_conv = nn.Sequential(\n            nn.Conv2d(features, 128, kernel_size=3, stride=1, padding=1),\n            Interpolate(scale_factor=2, mode=\"bilinear\"),\n            nn.Conv2d(128, 32, kernel_size=3, stride=1, padding=1),\n            nn.ReLU(True),\n            nn.Conv2d(32, 1, kernel_size=1, stride=1, padding=0),\n            nn.ReLU(True) if non_negative else nn.Identity(),\n        )\n\n        if path:\n            self.load(path)\n\n    def forward(self, x):\n        layer_1 = self.pretrained.layer1(x)\n        layer_2 = self.pretrained.layer2(layer_1)\n        layer_3 = self.pretrained.layer3(layer_2)\n        layer_4 = self.pretrained.layer4(layer_3)\n\n        layer_1_rn = self.scratch.layer1_rn(layer_1)\n        layer_2_rn = self.scratch.layer2_rn(layer_2)\n        layer_3_rn = self.scratch.layer3_rn(layer_3)\n        layer_4_rn = self.scratch.layer4_rn(layer_4)\n\n        path_4 = self.scratch.refinenet4(layer_4_rn)\n        path_3 = self.scratch.refinenet3(path_4, layer_3_rn)\n        path_2 = self.scratch.refinenet2(path_3, layer_2_rn)\n        path_1 = self.scratch.refinenet1(path_2, layer_1_rn)\n\n        out = self.scratch.output_conv(path_1)\n\n        return torch.squeeze(out, dim=1)"}
{"Repository": "cross-adaptive-audio", "input": "AbstractFitness: Base class IS_FITNESS_RELATIVE: Whether or not fitness is relative, i. className AbstractFitness(object) Method __init__ Method evaluate_multiple Attribute target_sound", "label": "class AbstractFitness(object):\n    IS_FITNESS_RELATIVE = False\n\n    def __init__(self, target_sound):\n        self.target_sound = target_sound\n\n    def evaluate_multiple(self, individuals):\n        raise Exception('evaluate_multiple must be implemented by the subclass')"}
{"Repository": "Miyamoto", "input": "Level editor item to draw a line between two path nodes className PathEditorLineItem(LevelEditorItem) Method __init__ Method UpdateTooltip Method ListString Method nodePosChanged Method computeBoundRectAndPos Method paint Method delete Attribute objx Attribute objy Attribute nodelist Attribute loops", "label": "class PathEditorLineItem(LevelEditorItem):\n    BoundingRect = QtCore.QRectF(0, 0, 1, 1)  # compute later\n\n    def __init__(self, nodelist):\n        super().__init__()\n\n        self.objx = 0\n        self.objy = 0\n        self.nodelist = nodelist\n        self.loops = False\n        self.setFlag(self.ItemIsMovable, False)\n        self.setFlag(self.ItemIsSelectable, False)\n        self.computeBoundRectAndPos()\n        self.setZValue(25002)\n        self.UpdateTooltip()\n\n        self.setVisible(globals.PathsShown)\n\n    def UpdateTooltip(self):\n        self.setToolTip('')\n\n    def ListString(self):\n        return ''\n\n    def nodePosChanged(self):\n        self.computeBoundRectAndPos()\n        self.scene().update()\n\n    def computeBoundRectAndPos(self):\n        xcoords = []\n        ycoords = []\n        for node in self.nodelist:\n            xcoords.append(int(node['x']) + 8)\n            ycoords.append(int(node['y']) + 8)\n        self.objx = (min(xcoords) - 4)\n        self.objy = (min(ycoords) - 4)\n\n        mywidth = (8 + (max(xcoords) - self.objx)) * (globals.TileWidth / 16)\n        myheight = (8 + (max(ycoords) - self.objy)) * (globals.TileWidth / 16)\n        globals.DirtyOverride += 1\n        self.setPos(self.objx * (globals.TileWidth / 16), self.objy * (globals.TileWidth / 16))\n        globals.DirtyOverride -= 1\n        self.prepareGeometryChange()\n        self.BoundingRect = QtCore.QRectF(-4, -4, mywidth, myheight)\n\n    def paint(self, painter, option, widget):\n        painter.setRenderHint(QtGui.QPainter.Antialiasing)\n        painter.setClipRect(option.exposedRect)\n\n        color = globals.theme.color('path_connector')\n        painter.setBrush(QtGui.QBrush(color))\n        painter.setPen(QtGui.QPen(color, 3 * globals.TileWidth / 24, join=Qt.RoundJoin, cap=Qt.RoundCap))\n\n        mult = globals.TileWidth / 16\n        lines = []\n\n        snl = self.nodelist\n        for j in range(len(self.nodelist)):\n            if (j+1) < len(self.nodelist):\n                lines.append(QtCore.QLineF(\n                    float((snl[j  ]['x'] + 8) * mult) - self.x(),\n                    float((snl[j  ]['y'] + 8) * mult) - self.y(),\n                    float((snl[j+1]['x'] + 8) * mult) - self.x(),\n                    float((snl[j+1]['y'] + 8) * mult) - self.y()))\n\n        painter.drawLines(lines)\n\n        painter.setPen(QtGui.QPen(color, 3 * globals.TileWidth / 24, join=Qt.RoundJoin, cap=Qt.RoundCap, style=Qt.DotLine))\n        if self.loops:\n            painter.drawLine(QtCore.QLineF(\n                float((snl[-1]['x'] + 8) * mult) - self.x(),\n                float((snl[-1]['y'] + 8) * mult) - self.y(),\n                float((snl[ 0]['x'] + 8) * mult) - self.x(),\n                float((snl[ 0]['y'] + 8) * mult) - self.y()))\n\n    def delete(self):\n        self.scene().update()"}
{"Repository": "RT-ODLab", "input": "Track a series of values and provide access to smoothed values over a window or the global series average. className SmoothedValue(object) Method __init__ Method update Method synchronize_between_processes Method median Method avg Method global_avg Method max Method value Method __str__ Attribute deque Attribute total Attribute count Attribute fmt", "label": "class SmoothedValue(object):\n    def __init__(self, window_size=20, fmt=None):\n        if fmt is None:\n            fmt = \"{median:.4f} ({global_avg:.4f})\"\n        self.deque = deque(maxlen=window_size)\n        self.total = 0.0\n        self.count = 0\n        self.fmt = fmt\n\n    def update(self, value, n=1):\n        self.deque.append(value)\n        self.count += n\n        self.total += value * n\n\n    def synchronize_between_processes(self):\n        if not is_dist_avail_and_initialized():\n            return\n        t = torch.tensor([self.count, self.total], dtype=torch.float64, device='cuda')\n        dist.barrier()\n        dist.all_reduce(t)\n        t = t.tolist()\n        self.count = int(t[0])\n        self.total = t[1]\n\n    @property\n    def median(self):\n        d = torch.tensor(list(self.deque))\n        return d.median().item()\n\n    @property\n    def avg(self):\n        d = torch.tensor(list(self.deque), dtype=torch.float32)\n        return d.mean().item()\n\n    @property\n    def global_avg(self):\n        return self.total / self.count\n\n    @property\n    def max(self):\n        return max(self.deque)\n\n    @property\n    def value(self):\n        return self.deque[-1]\n\n    def __str__(self):\n        return self.fmt.format(\n            median=self.median,\n            avg=self.avg,\n            global_avg=self.global_avg,\n            max=self.max,\n            value=self.value)"}
{"Repository": "BracketHighlighter", "input": "Bracket Plugin base class. className BracketPluginCommand(object) Method run", "label": "class BracketPluginCommand(object):\n    def run(self, bracket, content, selection):\n        pass"}
{"Repository": "ancv", "input": "A runnable server for handling dynamic API requests. className APIHandler(Runnable) Method run", "label": "class APIHandler(Runnable):\n    def __init__(\n        self,\n        requester: str,\n        token: Optional[str],\n        terminal_landing_page: str,\n        browser_landing_page: str,\n    ) -> None:\n        self.requester = requester\n        self.token = token\n        self.terminal_landing_page = terminal_landing_page\n        self.browser_landing_page = browser_landing_page\n\n        LOGGER.debug(\"Instantiating web application.\")\n        self.app = web.Application()\n\n        LOGGER.debug(\"Adding routes.\")\n        self.app.add_routes(\n            [\n                # Order matters, see also https://www.grandmetric.com/2020/07/08/routing-order-in-aiohttp-library-in-python/\n                web.get(\"/\", self.root),\n                web.get(f\"/{_SHOWCASE_USERNAME}\", self.showcase),\n                web.get(\"/{username}\", self.username),\n            ]\n        )\n\n        self.app.cleanup_ctx.append(self.app_context)\n\n    def run(self, context: ServerContext) -> None:\n        LOGGER.info(\"Loaded, starting server...\")\n        web.run_app(self.app, host=context.host, port=context.port, path=context.path)\n\n    async def app_context(self, app: web.Application) -> AsyncGenerator[None, None]:\n        log = LOGGER.bind(app=app)\n        log.debug(\"App context initialization starting.\")\n\n        log.debug(\"Starting client session.\")\n        session = ClientSession()\n        log = log.bind(session=session)\n        log.debug(\"Started client session.\")\n\n        log.debug(\"Creating GitHub API instance.\")\n        github = GitHubAPI(\n            session,\n            requester=self.requester,\n            oauth_token=self.token,\n            cache=TTLCache(maxsize=1e2, ttl=60),\n        )\n        log = log.bind(github=github)\n        log.debug(\"Created GitHub API instance.\")\n\n        app[\"client_session\"] = session\n        app[\"github\"] = github\n\n        log.debug(\"App context initialization done, yielding.\")\n\n        yield\n\n        log.debug(\"App context teardown starting.\")\n\n        log.debug(\"Closing client session.\")\n        await app[\"client_session\"].close()\n        log.debug(\"Closed client session.\")\n\n        log.info(\"App context teardown done.\")\n\n    async def root(self, request: web.Request) -> web.Response:\n        user_agent = request.headers.get(\"User-Agent\", \"\")\n\n        if is_terminal_client(user_agent):\n            return web.Response(\n                text=f\"Visit {self.terminal_landing_page} to get started.\\n\"\n            )\n\n        raise web.HTTPFound(self.browser_landing_page)  # Redirect\n\n    async def showcase(self, request: web.Request) -> web.Response:\n        return web.Response(text=_SHOWCASE_RESUME)\n\n    async def username(self, request: web.Request) -> web.Response:\n        stopwatch = Stopwatch()\n        stopwatch(segment=\"Initialize Request\")\n\n        log = LOGGER.bind(request=request)\n        log.info(request.message.headers)\n\n        user = request.match_info[\"username\"]\n\n        if not is_valid_github_username(user):\n            raise web.HTTPBadRequest(reason=f\"Invalid username: {user}\")\n\n        # Implicit 'downcasting' from `Any` doesn't require an explicit `cast` call, just\n        # regular type hints:\n        # https://adamj.eu/tech/2021/07/06/python-type-hints-how-to-use-typing-cast/\n        session: ClientSession = request.app[\"client_session\"]\n        github: GitHubAPI = request.app[\"github\"]\n\n        log = log.bind(user=user)\n\n        stopwatch.stop()\n        try:\n            resume = await get_resume(\n                user=user, session=session, github=github, stopwatch=stopwatch\n            )\n        except ResumeLookupError as e:\n            stopwatch.stop()\n            log.warning(str(e))\n            return web.Response(text=str(e), status=HTTPStatus.NOT_FOUND)\n        else:\n            stopwatch(segment=\"Templating\")\n            try:\n                template = Template.from_model_config(resume)\n            except ResumeConfigError as e:\n                log.warning(str(e))\n                return web.Response(text=str(e))\n\n            stopwatch(segment=\"Rendering\")\n            resp = web.Response(text=template.render())\n            stopwatch.stop()\n\n            resp.headers[\"Server-Timing\"] = server_timing_header(stopwatch.timings)\n\n            log.debug(\"Serving rendered template.\")\n            return resp"}
{"Repository": "seahub", "input": "Draft models enable user save file as drafts, and publish later. className Draft(TimestampedModel) Method update Method delete Method to_dict", "label": "class Draft(TimestampedModel):\n    username = LowerCaseCharField(max_length=255, db_index=True)\n    status = models.CharField(max_length=20, default='open')\n    draft_file_path = models.CharField(max_length=1024)\n    origin_repo_id = models.CharField(max_length=36, db_index=True)\n    origin_file_uuid = models.UUIDField(unique=True)\n    origin_file_version = models.CharField(max_length=100)\n    publish_file_version = models.CharField(max_length=100, null=True)\n\n    objects = DraftManager()\n\n    # class Meta:\n    #     unique_together = (('username', 'draft_repo_id'), )\n\n    def update(self, publish_file_version, status='published'):\n        self.publish_file_version = publish_file_version\n        self.status = status\n        self.save()\n\n    def delete(self, operator):\n        draft_file_name = os.path.basename(self.draft_file_path)\n        draft_file_path = os.path.dirname(self.draft_file_path)\n        seafile_api.del_file(self.origin_repo_id, draft_file_path,\n                             json.dumps([draft_file_name]), operator)\n\n        super(Draft, self).delete()\n\n    def to_dict(self):\n        uuid = FileUUIDMap.objects.get_fileuuidmap_by_uuid(self.origin_file_uuid)\n        file_path = posixpath.join(uuid.parent_path, uuid.filename)\n\n        return {\n            'id': self.pk,\n            'owner': self.username,\n            'owner_nickname': email2nickname(self.username),\n            'origin_repo_id': self.origin_repo_id,\n            'origin_file_path': file_path,\n            'origin_file_version': self.origin_file_version,\n            'draft_file_path': self.draft_file_path,\n            'created_at': datetime_to_isoformat_timestr(self.created_at),\n            'updated_at': datetime_to_isoformat_timestr(self.updated_at),\n        }"}
{"Repository": "pypardis", "input": ":lower: lower bounds of bounding box className BoundingBox(object) Method __init__ Method intersection Method union Method split Method expand Method contains Method __repr__ Attribute lower Attribute upper Attribute lower Attribute upper Attribute lower Attribute upper Attribute lower Attribute upper", "label": "class BoundingBox(object):\n    def __init__(self, lower=None, upper=None, k=None, all_space=False):\n        if lower is not None:\n            self.lower = np.array(lower)\n            self.upper = np.array(upper) if upper is not None else self.lower\n        elif k is not None:\n            if all_space:\n                self.lower = np.full(k, sys.float_info.min)\n                self.upper = np.full(k, sys.float_info.max)\n            else:\n                self.lower = np.full(k, sys.float_info.max)\n                self.upper = np.full(k, sys.float_info.min)\n        else:\n            self.lower = None\n            self.upper = None\n\n    def intersection(self, other):\n        lower = np.maximum(self.lower, other.lower)\n        upper = np.minimum(self.upper, other.upper)\n        return BoundingBox(lower=lower, upper=upper)\n\n    def union(self, other):\n        lower = np.minimum(self.lower, other.lower)\n        upper = np.maximum(self.upper, other.upper)\n        return BoundingBox(lower=lower, upper=upper)\n\n    def split(self, dim, value):\n        left = BoundingBox(lower=np.copy(self.lower),\n                           upper=np.copy(self.upper))\n        left.upper[dim] = value\n        right = BoundingBox(lower=np.copy(self.lower),\n                            upper=np.copy(self.upper))\n        right.lower[dim] = value\n        return left, right\n\n    def expand(self, eps=0, how='add'):\n        if how == 'add':\n            return BoundingBox(self.lower - eps, self.upper + eps)\n        elif how == 'multiply':\n            span = self.upper - self.lower\n            return BoundingBox(self.lower - eps * span,\n                               self.upper + eps * span)\n\n    def contains(self, vector):\n        return np.all(self.lower <= vector) and np.all(self.upper >= vector)\n\n    def __repr__(self):\n        return 'BoundingBox(lower=%s\\n\\tupper=%s)' % (\n            str(self.lower), str(self.upper))"}
{"Repository": "python_data_structures_and_algorithms", "input": "Heaps: className MaxHeap(object) Method __init__ Method __len__ Method add Method _siftup Method extract Method _siftdown Attribute maxsize Attribute _elements Attribute _count", "label": "class MaxHeap(object):\n    def __init__(self, maxsize=None):\n        self.maxsize = maxsize\n        self._elements = Array(maxsize)\n        self._count = 0\n\n    def __len__(self):\n        return self._count\n\n    def add(self, value):\n        if self._count >= self.maxsize:\n            raise Exception('full')\n        self._elements[self._count] = value\n        self._count += 1\n        self._siftup(self._count-1)  # \n\n    def _siftup(self, ndx):\n        if ndx > 0:\n            parent = int((ndx-1)/2)\n            if self._elements[ndx] > self._elements[parent]:    #  parent\n                self._elements[ndx], self._elements[parent] = self._elements[parent], self._elements[ndx]\n                self._siftup(parent)    # \n\n    def extract(self):\n        if self._count <= 0:\n            raise Exception('empty')\n        value = self._elements[0]    #  root \n        self._count -= 1\n        self._elements[0] = self._elements[self._count]    # rootsiftDown\n        self._siftdown(0)    # \n        return value\n\n    def _siftdown(self, ndx):\n        left = 2 * ndx + 1\n        right = 2 * ndx + 2\n        # determine which node contains the larger value\n        largest = ndx\n        if (left < self._count and     # \n                self._elements[left] >= self._elements[largest] and\n                self._elements[left] >= self._elements[right]):  # largest\n            largest = left\n        elif right < self._count and self._elements[right] >= self._elements[largest]:\n            largest = right\n        if largest != ndx:\n            self._elements[ndx], self._elements[largest] = self._elements[largest], self._elements[ndx]\n            self._siftdown(largest)"}
{"Repository": "logicnn", "input": "Class for HiddenLayer className HiddenLayer(object) Method __init__ Attribute input Attribute activation Attribute W Attribute b Attribute output Attribute params", "label": "class HiddenLayer(object):\n    def __init__(self, rng, input, n_in, n_out, activation, W=None, b=None):\n\n        self.input = input\n        self.activation = activation\n\n        if W is None:            \n            if activation.func_name == \"ReLU\":\n                W_values = numpy.asarray(0.01 * rng.standard_normal(size=(n_in, n_out)), dtype=theano.config.floatX)\n            else:                \n                W_values = numpy.asarray(rng.uniform(low=-numpy.sqrt(6. / (n_in + n_out)), high=numpy.sqrt(6. / (n_in + n_out)),\n                                                     size=(n_in, n_out)), dtype=theano.config.floatX)\n            W = theano.shared(value=W_values, name='W')        \n        if b is None:\n            b_values = numpy.zeros((n_out,), dtype=theano.config.floatX)\n            b = theano.shared(value=b_values, name='b')\n\n        self.W = W\n        self.b = b\n\n        lin_output = T.dot(input, self.W) + self.b\n\n        self.output = (lin_output if activation is None else activation(lin_output))\n    \n        # parameters of the model\n        self.params = [self.W, self.b]"}
{"Repository": "anchore", "input": "Analyze only non-intermediate images. className NoIntermediateStrategy(SelectionStrategy) Method _should_analyze_image", "label": "class NoIntermediateStrategy(SelectionStrategy):\n    def _should_analyze_image(self, image, pos, count):\n        return (not image.is_intermediate()) or (pos == 0 or pos == count - 1)"}
{"Repository": "ZEN", "input": "ZEN model with next sentence prediction head. className ZenForNextSentencePrediction(ZenPreTrainedModel) Method __init__ Method forward Attribute output_attentions Attribute bert Attribute cls", "label": "class ZenForNextSentencePrediction(ZenPreTrainedModel):\n    def __init__(self, config, output_attentions=False, keep_multihead_output=False):\n        super(ZenForNextSentencePrediction, self).__init__(config)\n        self.output_attentions = output_attentions\n        self.bert = ZenModel(config, output_attentions=output_attentions,\n                              keep_multihead_output=keep_multihead_output)\n        self.cls = ZenOnlyNSPHead(config)\n        self.apply(self.init_bert_weights)\n\n    def forward(self, input_ids, input_ngram_ids, ngram_position_matrix, token_type_ids=None, attention_mask=None, next_sentence_label=None, head_mask=None):\n        outputs = self.bert(input_ids, input_ngram_ids, ngram_position_matrix, token_type_ids, attention_mask,\n                            output_all_encoded_layers=False,\n                            head_mask=head_mask)\n        if self.output_attentions:\n            all_attentions, _, pooled_output = outputs\n        else:\n            _, pooled_output = outputs\n        seq_relationship_score = self.cls(pooled_output)\n\n        if next_sentence_label is not None:\n            loss_fct = CrossEntropyLoss(ignore_index=-1)\n            next_sentence_loss = loss_fct(seq_relationship_score.view(-1, 2), next_sentence_label.view(-1))\n            return next_sentence_loss\n        elif self.output_attentions:\n            return all_attentions, seq_relationship_score\n        return seq_relationship_score"}
{"Repository": "RecSys2019_DeepLearning_Evaluation", "input": "BaseMatrixFactorizationRecommender className MatrixFactorizationCustomFactorsRecommender(BaseMatrixFactorizationRecommender) Method fit", "label": "class MatrixFactorizationCustomFactorsRecommender(BaseMatrixFactorizationRecommender):\n    RECOMMENDER_NAME = \"BaseMatrixFactorizationRecommender\"\n\n    def fit(self, USER_factors = None, ITEM_factors = None):\n\n        assert USER_factors is not None and ITEM_factors is not None\n\n        self.USER_factors = USER_factors.copy()\n        self.ITEM_factors = ITEM_factors.copy()"}
{"Repository": "conversational-datasets", "input": "A model that maps from text to dense vectors. className Encoder(object) Method encode_context Method encode_response", "label": "class Encoder(object):\n    __metaclass__ = abc.ABCMeta\n\n    @abc.abstractmethod\n    def encode_context(self, contexts):\n        pass\n\n    def encode_response(self, responses):\n        # Default to using the context encoding.\n        return self.encode_context(responses)"}
{"Repository": "nltools", "input": "Class to build the default MNI_Template dictionary. className MNI_Template_Factory(dict) Method resolution Method resolution Method mask_type Method mask_type Method mask Method mask Method plot Method plot Method brain Method brain", "label": "class MNI_Template_Factory(dict):\n    def __init__(\n        self,\n        resolution=\"2mm\",\n        mask_type=\"with_ventricles\",\n        mask=os.path.join(get_resource_path(), \"MNI152_T1_2mm_brain_mask.nii.gz\"),\n        plot=os.path.join(get_resource_path(), \"MNI152_T1_2mm.nii.gz\"),\n        brain=os.path.join(get_resource_path(), \"MNI152_T1_2mm_brain.nii.gz\"),\n    ):\n        self._resolution = resolution\n        self._mask_type = mask_type\n        self._mask = mask\n        self._plot = plot\n        self._brain = brain\n\n        self.update(\n            {\n                \"resolution\": self.resolution,\n                \"mask_type\": self.mask_type,\n                \"mask\": self.mask,\n                \"plot\": self.plot,\n                \"brain\": self.brain,\n            }\n        )\n\n    @property\n    def resolution(self):\n        return self._resolution\n\n    @resolution.setter\n    def resolution(self, resolution):\n        if isinstance(resolution, (int, float)):\n            resolution = f\"{int(resolution)}mm\"\n        if resolution not in [\"2mm\", \"3mm\"]:\n            raise NotImplementedError(\n                \"Only 2mm and 3mm resolutions are currently supported\"\n            )\n        self._resolution = resolution\n        self.update({\"resolution\": self._resolution})\n\n    @property\n    def mask_type(self):\n        return self._mask_type\n\n    @mask_type.setter\n    def mask_type(self, mask_type):\n        if mask_type not in [\"with_ventricles\", \"no_ventricles\"]:\n            raise NotImplementedError(\n                \"Only 'with_ventricles' and 'no_ventricles' mask_types are currently supported\"\n            )\n        self._mask_type = mask_type\n        self.update({\"mask_type\": self._mask_type})\n\n    @property\n    def mask(self):\n        return self._mask\n\n    @mask.setter\n    def mask(self, mask):\n        self._mask = mask\n        self.update({\"mask\": self._mask})\n\n    @property\n    def plot(self):\n        return self._plot\n\n    @plot.setter\n    def plot(self, plot):\n        self._plot = plot\n        self.update({\"plot\": self._plot})\n\n    @property\n    def brain(self):\n        return self._brain\n\n    @brain.setter\n    def brain(self, brain):\n        self._brain = brain\n        self.update({\"brain\": self._brain})"}
{"Repository": "DeepQA", "input": "Manage a single instance of the chatbot shared over the website className ChatbotManager(AppConfig) Method ready Method initBot Method callBot", "label": "class ChatbotManager(AppConfig):\n    name = 'chatbot_interface'\n    verbose_name = 'Chatbot Interface'\n\n    bot = None\n\n    def ready(self):\n        # Initialize the chatbot daemon (should be launched only once)\n        if (os.environ.get('RUN_MAIN') == 'true' and  # HACK: Avoid the autoreloader executing the startup code twice (could also use: python manage.py runserver --noreload) (see http://stackoverflow.com/questions/28489863/why-is-run-called-twice-in-the-django-dev-server)\n            not any(x in sys.argv for x in ['makemigrations', 'migrate'])):  # HACK: Avoid initialisation while migrate\n            ChatbotManager.initBot()\n\n    @staticmethod\n    def initBot():\n        if not ChatbotManager.bot:\n            logger.info('Initializing bot...')\n            ChatbotManager.bot = chatbot.Chatbot()\n            ChatbotManager.bot.main(['--modelTag', 'server', '--test', 'daemon', '--rootDir', chatbotPath])\n        else:\n            logger.info('Bot already initialized.')\n\n    @staticmethod\n    def callBot(sentence):\n        if ChatbotManager.bot:\n            return ChatbotManager.bot.daemonPredict(sentence)\n        else:\n            logger.error('Error: Bot not initialized!')"}
{"Repository": "superpixel_fcn", "input": "Converts a numpy.ndarray (H x W x C) to a torch.FloatTensor of shape (C x H x W). className ArrayToPILImage(object) Method __call__", "label": "class ArrayToPILImage(object):\n    def __call__(self, array):\n        assert(isinstance(array, np.ndarray))\n\n        img = Image.fromarray(array.astype(np.uint8))\n\n        return img"}
{"Repository": "ManimML", "input": "This is a wrapper class for the Manim ThreeDScene className ManimML3DScene(ThreeDScene) Method __init__ Method play", "label": "class ManimML3DScene(ThreeDScene):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n\n    def play(self):\n        pass"}
{"Repository": "volatility-plugins", "input": "A Scanner for Discontiguous scanning. className DiscontigSSDeepScanner(BaseSSDeepScanner) Method scan", "label": "class DiscontigSSDeepScanner(BaseSSDeepScanner):\n    def scan(self, start_offset = 0, maxlen = None):\n        contiguous_offset = 0\n        total_length = 0\n        for (offset, length) in self.address_space.get_available_addresses():\n            # Skip ranges before the start_offset\n            if self.address_space.address_compare(offset, start_offset) == -1:\n                continue\n\n            # Skip ranges that are too high (if maxlen is specified)\n            if maxlen != None:\n                if self.address_space.address_compare(offset, start_offset + maxlen) > 0:\n                    continue\n\n            # Try to join up adjacent pages as much as possible.\n            if offset == contiguous_offset + total_length:\n                total_length += length\n            else:\n                # Scan the last contiguous range.\n                for match in BaseSSDeepScanner.scan(self, contiguous_offset, total_length):\n                    yield match\n\n                # Reset the contiguous range.\n                contiguous_offset = offset\n                total_length = length\n\n        if total_length > 0:\n            # Do the last range.\n            for match in BaseSSDeepScanner.scan(self, contiguous_offset, total_length):\n                yield match"}
{"Repository": "SIGIR19-BERT-IR", "input": "Base class for data converters for sequence classification data sets. className DataProcessor(object) Method get_train_examples Method get_dev_examples Method get_test_examples Method get_labels Method _read_tsv", "label": "class DataProcessor(object):\n    def get_train_examples(self, data_dir):\n        raise NotImplementedError()\n\n    def get_dev_examples(self, data_dir):\n        raise NotImplementedError()\n\n    def get_test_examples(self, data_dir):\n        raise NotImplementedError()\n\n    def get_labels(self):\n        raise NotImplementedError()\n\n    @classmethod\n    def _read_tsv(cls, input_file, quotechar=None):\n        with tf.gfile.Open(input_file, \"r\") as f:\n            reader = csv.reader(f, delimiter=\"\\t\", quotechar=quotechar)\n            lines = []\n            for line in reader:\n                lines.append(line)\n            return lines"}
{"Repository": "semstyle", "input": "Splits and normalizes sentences className TextNormalizer(object) Method split_sentence Method normalize_word Method normalize_sentence Method normalize_text", "label": "class TextNormalizer(object):\n    _punc = ''.join([c for c in string.punctuation if c not in \",.\\\"\"])\n    _ttable = string.maketrans(\"\",\"\")\n\n    @classmethod\n    def split_sentence(cls, sent, options=None):\n        if options is not None and not options.isNLTKSplit():\n            return sent.split()\n        else:\n            return options.word_tokenize(sent)\n\n    @classmethod\n    def normalize_word(cls, w, options = TextNormalizerOptions()):\n        wout = str(w.decode('utf-8', 'ignore').encode('ascii','ignore').strip())\n        if options.isReplaceNumbers():\n            if wout.isdigit():\n                wout = \"NUMBER\"\n        if not options.isWordToPreserve(wout):\n            if options.getLowercase():\n                wout = str.lower(wout)\n            if options.getRemovePunct():\n                wout = wout.translate(cls._ttable, cls._punc + string.digits)\n            wout = wout.strip()\n\n        return str(wout)\n\n    @classmethod\n    def normalize_sentence(cls, sent, options = TextNormalizerOptions()):\n        #break the sentence into words\n        if type(sent) is list:\n            sent_splt = sent\n        else:\n            sent_enc = str(sent.decode('utf-8', 'ignore').encode('ascii','ignore').strip())\n            sent_splt = cls.split_sentence(sent_enc, options)\n\n        # normalize each of the words\n        sent_norm = []\n        for w in sent_splt:\n            wn = cls.normalize_word(w, options)\n            if not wn and options.getRemoveEmpty():\n                continue\n            sent_norm.append(wn)\n        return sent_norm\n\n    @classmethod\n    def normalize_text(cls, text, options = TextNormalizerOptions()):\n        text_norm = []\n        for sent in text:\n            sent_norm = cls.normalize_sentence(sent, options)\n            text_norm.append(sent_norm)\n\n        return text_norm"}
{"Repository": "im2latex", "input": "Simple Generator of tuples (img_path, formula_id) className DataGeneratorFile(object) Method __init__ Method __iter__ Attribute _filename", "label": "class DataGeneratorFile(object):\n    def __init__(self, filename):\n        self._filename = filename\n\n    def __iter__(self):\n        with open(self._filename) as f:\n            for line in f:\n                line = line.strip().split(' ')\n                path_img, id_formula = line[0], line[1]\n                yield path_img, id_formula"}
{"Repository": "linuxcnc", "input": "Meter - shows the value of a FLOAT with an analog meter className pyvcp_meter(Canvas) Method __init__ Method rad2deg Method value2angle Method p2c Method update Method draw_region Method draw_ticks Attribute size Attribute pad Attribute halpin Attribute min_ Attribute max_ Attribute min_alfa Attribute max_alfa Attribute circle Attribute mid Attribute r Attribute alfa Attribute minorscale Attribute minorscale Attribute majorscale Attribute majorscale Attribute line Attribute halpin Attribute value", "label": "class pyvcp_meter(Canvas):\n    # FIXME: logarithmic scale option\n    n=0\n    def __init__(self,root,pycomp,halpin=None, size=200,text=None,subtext=None,min_=0,max_=100,majorscale=None, minorscale=None,region1=None,region2=None,region3=None,**kw):\n        self.size = size\n        self.pad=10\n        Canvas.__init__(self,root,width=size,height=size)\n        self.halpin=halpin\n        self.min_=min_\n        self.max_=max_\n        range_=2.5\n        self.min_alfa=-math.pi/2-range_\n        self.max_alfa=-math.pi/2+range_\n        self.circle=self.create_oval(self.pad,self.pad,size-self.pad,size-self.pad, width=2)\n        self.itemconfig(self.circle,fill=\"white\")\n        self.mid=size/2\n        self.r=(size-2*self.pad)/2\n        self.alfa=0\n        if minorscale==None: \n            self.minorscale=0\n        else:\n            self.minorscale=minorscale\n        if majorscale==None: \n            self.majorscale=float((self.max_-self.min_)/10)\n        else: \n            self.majorscale=majorscale\n        if text!=None: t=self.create_text([self.mid,self.mid-size/12],font=\"Arial %d bold\" % (size/10),text=text)\n        if subtext!=None: t=self.create_text([self.mid,self.mid+size/12],font=\"Arial %d\" % (size/30+5),text=subtext)\n        if region1!=None: self.draw_region(region1)\n        if region2!=None: self.draw_region(region2)\n        if region3!=None: self.draw_region(region3)\n        self.draw_ticks()\n\n        self.line = self.create_line([self.mid,self.mid, self.mid+self.r*math.cos(self.alfa), self.mid+self.r*math.sin(self.alfa)],fill=\"red\", arrow=\"last\", arrowshape=(0.9*self.r,self.r,self.r/20))\n        self.itemconfig(self.line,width=3)\n\n        # create the hal pin\n        if halpin == None:\n            self.halpin = \"meter.\"+str(pyvcp_meter.n)+\".value\"\n            pyvcp_meter.n += 1\n        pycomp.newpin(self.halpin, HAL_FLOAT, HAL_IN)\n        self.value = pycomp[self.halpin]\n    \n    def rad2deg(self, rad): return rad*180/math.pi\n\n    def value2angle(self, value):\n            #returns angle for a given value\n            scale = (self.max_-self.min_)/(self.max_alfa-self.min_alfa)\n            alfa = self.min_alfa + (value-self.min_)/scale\n            if alfa > self.max_alfa:\n                alfa = self.max_alfa\n            elif alfa < self.min_alfa:\n                alfa = self.min_alfa            \n            return alfa\n    \n    def p2c(self, radius, angle): \n        #returns the cathesian coordinates (x,y) for given polar coordinates \n        #radius in percent of self.r; angle in radians\n        return self.mid+radius*self.r*math.cos(angle), self.mid+radius*self.r*math.sin(angle)\n\n    def update(self,pycomp):\n        self.value = pycomp[self.halpin]\n        self.alfa = self.value2angle(self.value)\n        x,y = self.p2c(0.8, self.alfa)\n        self.coords(self.line,self.mid,self.mid,x,y)\n\n    def draw_region(self, xxx_todo_changeme):\n            #Draws a colored region on the canvas between start and end\n            (start, end, color) = xxx_todo_changeme\n            start = self.value2angle(start)\n            start = -self.rad2deg(start)\n            end = self.value2angle(end)\n            end = -self.rad2deg(end)\n            extent = end-start\n            halfwidth = math.floor(0.1*self.r/2)+1\n            xy = self.pad+halfwidth, self.pad+halfwidth, self.size-self.pad-halfwidth, self.size-self.pad-halfwidth\n            self.create_arc(xy, start=start, extent=extent, outline=color, width=(halfwidth-1)*2, style=\"arc\")\n\n    def draw_ticks(self):\n        value = self.min_\n        while value <= self.max_:\n            alfa = self.value2angle(value)\n            xy1 = self.p2c(1,alfa)\n            xy2 = self.p2c(0.85,alfa)\n            xytext = self.p2c(0.75,alfa)\n            self.create_text(xytext,font=\"Arial %d\" % (self.size/30+5), text=\"%g\" % value)\n            self.create_line(xy1, xy2, width=2)\n            value = value + self.majorscale\n        #minor ticks\n        value = self.min_\n        if self.minorscale > 0:\n            while value <= self.max_:\n                if (value % self.majorscale) != 0:\n                    alfa = self.value2angle(value)\n                    xy1 = self.p2c(1,alfa)\n                    xy2 = self.p2c(0.9,alfa)\n                    self.create_line(xy1, xy2)\n                value = value + self.minorscale"}
{"Repository": "AnomalyDetectionCVPR2018-Pytorch", "input": "Loader for a single video. className SingleVideoIter(VideoIter) Method _get_video_list Method __getitem__", "label": "class SingleVideoIter(VideoIter):\n    def __init__(\n        self,\n        clip_length,\n        frame_stride,\n        video_path,\n        video_transform=None,\n        return_label=False,\n    ) -> None:\n        super().__init__(\n            clip_length, frame_stride, video_path, video_transform, return_label\n        )\n\n    def _get_video_list(self, dataset_path: str) -> List[str]:\n        return [dataset_path]\n\n    def __getitem__(self, idx: int) -> Tensor:\n        video, _, _, _ = self.video_clips.get_clip(idx)\n        in_clip_frames = list(\n            range(0, self.total_clip_length_in_frames, self.frames_stride)\n        )\n        video = video[in_clip_frames]\n        if self.video_transform is not None:\n            video = self.video_transform(video)\n\n        return video"}
{"Repository": "yadisk", "input": "A request to refresh an existing token. className RefreshTokenRequest(APIRequest) Method process_args Method process_json", "label": "class RefreshTokenRequest(APIRequest):\n    url = \"https://oauth.yandex.ru/token\"\n    method = \"POST\"\n\n    def __init__(self,\n                 session: \"AnySession\",\n                 refresh_token: str,\n                 client_id: str,\n                 client_secret: str, **kwargs):\n        APIRequest.__init__(self, session, {\"refresh_token\": refresh_token,\n                                            \"client_id\":     client_id,\n                                            \"client_secret\": client_secret}, **kwargs)\n\n    def process_args(self, refresh_token: str, client_id: str, client_secret: str) -> None:\n        self.data = urlencode({\n            \"grant_type\":    \"refresh_token\",\n            \"refresh_token\": refresh_token,\n            \"client_id\":     client_id,\n            \"client_secret\": client_secret,\n        }).encode(\"utf8\")\n\n    def process_json(self, js: JSON, **kwargs) -> TokenObject:\n        if not isinstance(js, dict):\n            raise InvalidResponseError(\"Yandex.Disk did not return valid JSON\")\n\n        return TokenObject(js)"}
{"Repository": "moderngl", "input": "Growing buffers className GrowingBuffers(Example) Method __init__ Method render Attribute points", "label": "class GrowingBuffers(Example):\n    gl_version = (3, 3)\n    title = \"Buffer Resize / Batch Draw\"\n    window_size = 720, 720\n    aspect_ratio = 1.0\n\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n\n        self.points = Points(self.ctx, 12 * 10)\n        self.points.add(10)\n\n    def render(self, time, frametime):\n        self.points.render(time)\n\n        # Add more points every 60 frames\n        if self.wnd.frames % 60 == 0:\n            self.points.add(5000)"}
{"Repository": "FSEventsParser", "input": "FSEvent file header structure. className FsEventFileHeader() Method __init__ Attribute src_fullpath Attribute signature Attribute unknown_raw Attribute unknown_hex Attribute unknown_int Attribute filesize", "label": "class FsEventFileHeader():\n    def __init__(self, buf, filename):\n        # Name and path of current source fsevent file\n\n        self.src_fullpath = filename\n\n        # Page header 'DLS1' or 'DLS2'\n\n        # Was written to disk using little-endian\n\n        # Byte stream contains either \"1SLD\" or \"2SLD\", reversing order\n\n        self.signature = buf[4] + buf[3] + buf[2] + buf[1]\n\n        # Unknown raw values in DLS header\n\n        # self.unknown_raw = buf[4:8]\n\n        # Unknown hex version\n\n        # self.unknown_hex = buf[4:8].encode(\"hex\")\n\n        # Unknown integer version\n\n        # self.unknown_int = struct.unpack(\"<I\", self.unknown_raw)[0]\n\n        # Size of current DLS page\n\n        self.filesize = struct.unpack(\"<I\", buf[8:12])[0]"}
{"Repository": "DynamicRouting", "input": "We use the \"CustomizedTrainer\" which contains a number pre-defined logic for standard training workflow. className Trainer(CustomizedTrainer) Method build_evaluator Method test_with_TTA", "label": "class Trainer(CustomizedTrainer):\n    @classmethod\n    def build_evaluator(cls, cfg, dataset_name, output_folder=None):\n        if output_folder is None:\n            output_folder = os.path.join(cfg.OUTPUT_DIR, \"inference\")\n        evaluator_list = []\n        evaluator_type = MetadataCatalog.get(dataset_name).evaluator_type\n        if evaluator_type in [\"sem_seg\", \"coco_panoptic_seg\"]:\n            evaluator_list.append(\n                SemSegEvaluator(\n                    dataset_name,\n                    distributed=True,\n                    num_classes=cfg.MODEL.SEM_SEG_HEAD.NUM_CLASSES,\n                    ignore_label=cfg.MODEL.SEM_SEG_HEAD.IGNORE_VALUE,\n                    output_dir=output_folder,\n                ))\n        if evaluator_type == \"cityscapes\":\n            assert (\n                torch.cuda.device_count() >= comm.get_rank()\n            ), \"CityscapesEvaluator currently do not work with multiple machines.\"\n            return CityscapesEvaluator(dataset_name)\n        if evaluator_type == \"pascal_voc\":\n            return PascalVOCDetectionEvaluator(dataset_name)\n        if hasattr(cfg, \"EVALUATORS\"):\n            for evaluator in cfg.EVALUATORS:\n                evaluator_list.append(\n                    evaluator(dataset_name, True, output_folder, dump=False))\n        if len(evaluator_list) == 0:\n            raise NotImplementedError(\n                \"no Evaluator for the dataset {} with the type {}\".format(\n                    dataset_name, evaluator_type))\n        if len(evaluator_list) == 1:\n            return evaluator_list[0]\n        return DatasetEvaluators(evaluator_list)\n\n    @classmethod\n    def test_with_TTA(cls, cfg, model):\n        logger = logging.getLogger(\"dl_lib.trainer\")\n        # In the end of training, run an evaluation with TTA\n        logger.info(\"Running inference with test-time augmentation ...\")\n        evaluator_type = MetadataCatalog.get(\n            cfg.DATASETS.TEST[0]).evaluator_type\n        if evaluator_type == \"sem_seg\":\n            model = SemanticSegmentorWithTTA(cfg, model)\n        evaluators = [\n            cls.build_evaluator(cfg,\n                                name,\n                                output_folder=os.path.join(\n                                    cfg.OUTPUT_DIR, \"inference_TTA\"))\n            for name in cfg.DATASETS.TEST\n        ]\n        res = cls.test(cfg, model, evaluators)\n        res = OrderedDict({k + \"_TTA\": v for k, v in res.items()})\n        return res"}
{"Repository": "openreplica", "input": "Unblocking Return className UnblockingReturn(ConCoordException) Method __init__ Method __str__ Attribute returnvalue Attribute unblocked", "label": "class UnblockingReturn(ConCoordException):\n    def __init__(self, returnvalue=None, unblockeddict={}):\n        self.returnvalue = returnvalue\n        self.unblocked = unblockeddict\n\n    def __str__(self):\n        return str(self.returnvalue) + \" \".join(unblockeddict.keys())"}
{"Repository": "e2e_lfmmi", "input": "Cyclic cosine annealing. className CyclicCosineScheduler(SchedulerInterface) Method _add_arguments Method scale", "label": "class CyclicCosineScheduler(SchedulerInterface):\n    alias = \"cosine\"\n\n    @staticmethod\n    def _add_arguments(parser: _PrefixParser):\n        parser.add_argument(\n            \"--warmup\", type=int, default=1000, help=\"Number of warmup iterations.\"\n        )\n        parser.add_argument(\n            \"--total\",\n            type=int,\n            default=100000,\n            help=\"Number of total annealing iterations.\",\n        )\n\n    def scale(self, n_iter):\n        import math\n\n        return 0.5 * (math.cos(math.pi * (n_iter - self.warmup) / self.total) + 1)"}
{"Repository": "pywebcopy", "input": "Support setup.py upload. className UploadCommand(Command) Method status Method initialize_options Method finalize_options Method run", "label": "class UploadCommand(Command):\n    description = 'Build and publish the package.'\n    user_options = []\n\n    @staticmethod\n    def status(s):\n        print('\\033[1m{0}\\033[1m'.format(s))\n\n    def initialize_options(self):\n        pass\n\n    def finalize_options(self):\n        pass\n\n    def run(self):\n        try:\n            self.status('Removing previous builds')\n            rmtree(os.path.join(here, 'dist'))\n        except OSError:\n            pass\n\n        self.status('Building Source and Wheel (universal) distribution')\n        os.system('{0} setup.py sdist bdist_wheel --universal'.format(\n            os.path.normpath(sys.executable)))\n\n        self.status('Uploading the package to PyPi via Twine')\n        os.system('twine upload dist/*')\n\n        self.status(\"Setting up git..\")\n        os.system('git init')\n        os.system('git add .')\n\n        self.status(\"Pushing to github master branch..\")\n        os.system('git push origin master')\n\n        self.status('Publishing git tags')\n        os.system('git tag v{0}'.format(VERSION))\n        os.system('git push origin --tags')\n\n        sys.exit()"}
{"Repository": "workflow", "input": "Wrapper for configuration value. className cfgval(dict) Method __repr__", "label": "class cfgval(dict):\n    __getattr__ = dict.__getitem__\n    __setattr__ = dict.__setitem__\n\n    def __repr__(self):\n        # return '{%s}' % ',\\n'.join(map(lambda o: \"'.%s': %s\" % (o[0],\n        # repr(o[1])), self.__dict__.items()))\n        return '%s\\n%s' % ('#cfgwrapper', repr(self))"}
{"Repository": "pycoin", "input": "This is a deterministic wallet that complies with BIP0049 (\"ypub\" on mainnet) [https://github. className BIP49Node(BIP32Node) Method address Method hwif Method ku_output_for_address", "label": "class BIP49Node(BIP32Node):\n    def address(self, is_compressed=True):\n        pk_hash = self.hash160(is_compressed=is_compressed)\n        underlying_script = self._network.contract.for_p2pkh_wit(pk_hash)\n        return self._network.address.for_p2s(underlying_script)\n\n    def hwif(self, as_private=False):\n        return self._network.bip49_as_string(\n            self.serialize(as_private=as_private), as_private=as_private\n        )\n\n    as_text = hwif\n\n    def ku_output_for_address(self):\n        yield (\"address\", self.address(), None)"}
{"Repository": "taxon", "input": "A Taxon instance provides methods to organize and query data by tag. className Taxon(object) Method __init__ Method backend Method tag Method untag Method remove Method tags Method items Method query Method find Method empty Method __str__ Method __unicode__ Attribute _backend", "label": "class Taxon(object):\n    def __init__(self, backend):\n        if not isinstance(backend, Backend):\n            raise ValueError(\"%r is not a valid backend\" % backend)\n        self._backend = backend\n\n    @property\n    def backend(self):\n        return self._backend\n\n    def tag(self, tag, *items):\n        return self.backend.tag_items(tag, *items)\n\n    def untag(self, tag, *items):\n        return self.backend.untag_items(tag, *items)\n\n    def remove(self, *items):\n        return self.backend.remove_items(*items)\n\n    def tags(self):\n        return self.backend.all_tags()\n\n    def items(self):\n        return self.backend.all_items()\n\n    def query(self, q):\n        if not isinstance(q, (tuple, Query)):\n            raise ValueError(\"%r is not a valid query\" % q)\n        return self.backend.query(q)\n\n    def find(self, q):\n        _, items = self.query(q)\n        return set(items)\n\n    def empty(self):\n        return self.backend.empty()\n\n    def __str__(self):\n        return unicode(self).encode('utf-8')\n\n    def __unicode__(self):\n        return u\"%s(%r)\" % (self.__class__.__name__, self.backend)"}
{"Repository": "tf-cpn", "input": "Apply a mapper/filter on the DataFlow. className MapData(object) Method __init__ Method get_data Method reset_state Attribute ds Attribute func", "label": "class MapData(object):\n    def __init__(self, ds, func):\n        self.ds = ds\n\n        self.func = func\n\n\n\n    def get_data(self):\n\n        for dp in self.ds.get_data():\n\n            ret = self.func(copy(dp))  # shallow copy the list\n\n            if ret is not None:\n\n                yield ret\n\n\n\n    def reset_state(self):\n\n        pass"}
{"Repository": "pursuedpybear", "input": "Asset that does stuff in the background, after other assets have loaded. className ChainingMixin(BackgroundMixin) Method _start", "label": "class ChainingMixin(BackgroundMixin):\n    def _start(self, *assets):\n        self._future = _executor.gather([\n            asset._future\n            for asset in assets\n            if hasattr(asset, '_future')\n        ], self._background, _asset=self)"}
{"Repository": "Meta-SGD-pytorch", "input": "An abstract class for defining a single few-shot task. className Task(object) Method __init__ Method get_class Attribute character_folders Attribute num_classes Attribute support_num Attribute query_num Attribute train_roots Attribute test_roots Attribute meta_roots Attribute train_labels Attribute test_labels Attribute meta_labels", "label": "class Task(object):\n    def __init__(self, character_folders, num_classes, support_num, query_num):\n        self.character_folders = character_folders\n        self.num_classes = num_classes\n        self.support_num = support_num\n        self.query_num = query_num\n\n        class_folders = random.sample(self.character_folders, self.num_classes)\n        labels = list(range(len(class_folders)))\n        labels = dict(zip(class_folders, labels))\n        samples = dict()\n\n        self.train_roots = []\n        self.test_roots = []\n        for c in class_folders:\n            temp = [os.path.join(c, x) for x in os.listdir(c)]\n            samples[c] = random.sample(temp, len(temp))\n            self.train_roots += samples[c][:support_num]\n            self.test_roots += samples[c][support_num:support_num + query_num]\n\n        samples = dict()\n        self.meta_roots = []\n        for c in class_folders:\n            temp = [os.path.join(c, x) for x in os.listdir(c)]\n            samples[c] = random.sample(temp, len(temp))\n            self.meta_roots += samples[c][:support_num]\n\n        self.train_labels = [\n            labels[self.get_class(x)] for x in self.train_roots\n        ]\n        self.test_labels = [labels[self.get_class(x)] for x in self.test_roots]\n        self.meta_labels = [labels[self.get_class(x)] for x in self.meta_roots]\n\n    def get_class(self, sample):\n        # raise NotImplementedError(\"This is abstract class\")\n        return os.path.join(*sample.split('/')[:-1])"}
{"Repository": "Live-Sports-Arbitrage-Bet-Finder", "input": "docstring for ClassName className ArbFinder(object) Method __init__ Method set_type Attribute URL Attribute driver Attribute sport", "label": "class ArbFinder(object):\n    def __init__(self, URL):\n        try:\n            # Setup ChromeDriver\n            self.URL = URL\n            self.driver = uc.Chrome()\n            # elf.driver.set_window_size(1400, 5000)\n            self.driver.implicitly_wait(5)\n            self.driver.get(URL)\n        except:\n            pass\n        self.sport = 'Tennis'\n\n    def set_type(self, ASK=0, BID=1):\n        # time.sleep(1)\n        # Logging in\n\n        try:\n            if (ASK):\n                # Change to ask view since the default is the bid view\n                self.type = \"ASK\"\n                self.driver.find_element(By.XPATH, \"//a[contains(@href,'/live')]//span[text()='\" + self.sport + \"']\").click()\n\n            elif (BID):\n                # Nothing to update since the default is the bid view\n                self.type = \"BID\"\n                self.driver.find_element(By.XPATH, \"//span[@class='pill-title' and text()='\" + self.sport + \"']\").click()\n        except Exception as e:\n            print(e)"}
{"Repository": "tileGAN", "input": "Dialog window for requesting a dataset selection from a drop-down box. className GetDatasetDialog(QDialog) Method __init__ Method value Method getValue Attribute dL Attribute dInput Attribute OKBtn", "label": "class GetDatasetDialog(QDialog):\n\tdef __init__(self, datasets, dataset, parent=None):\n\t\tsuper(GetDatasetDialog, self).__init__(parent)\n\t\tself.setWindowFlags(QtCore.Qt.WindowSystemMenuHint | QtCore.Qt.WindowTitleHint)\n\t\t## Default values\n\t\tself.dL = QLabel(\"Choose dataset:\")\n\t\tself.dInput = QComboBox(self)\n\t\tfor d in datasets:\n\t\t\tself.dInput.addItem(d)\n\n\t\tself.dInput.setCurrentIndex(self.dInput.findText(dataset))\n\t\tself.OKBtn = QPushButton(\"OK\")\n\t\tself.OKBtn.clicked.connect(self.accept)\n\n\t\t## Set layout, add buttons\n\t\tlayout = QGridLayout()\n\t\tlayout.setColumnStretch(1, 1)\n\t\tlayout.setColumnMinimumWidth(1, 250)\n\n\t\tlayout.addWidget(self.dL, 0, 0)\n\t\tlayout.addWidget(self.dInput, 0, 1)\n\t\tlayout.addWidget(self.OKBtn, 1, 1)\n\n\t\tself.setLayout(layout)\n\t\tself.setWindowTitle(\"Select dataset\")\n\n\tdef value(self):\n\t\treturn self.dInput.currentText()\n\n\t@staticmethod\n\tdef getValue(ds=None, d='', parent=None):\n\t\tdialog = GetDatasetDialog(ds, d, parent)\n\t\tresult = dialog.exec_()\n\t\treturn dialog.value(), result == QDialog.Accepted"}
{"Repository": "gps", "input": "Collection of trajectory-related variables. className TrajectoryInfo(BundleType) Method __init__", "label": "class TrajectoryInfo(BundleType):\n    def __init__(self):\n        variables = {\n            'dynamics': None,  # Dynamics object for the current iteration.\n            'x0mu': None,  # Mean for the initial state, used by the dynamics.\n            'x0sigma': None,  # Covariance for the initial state distribution.\n            'cc': None,  # Cost estimate constant term.\n            'cv': None,  # Cost estimate vector term.\n            'Cm': None,  # Cost estimate matrix term.\n            'last_kl_step': float('inf'),  # KL step of the previous iteration.\n        }\n        BundleType.__init__(self, variables)"}
{"Repository": "isort", "input": "Should be raised when a file is skipped for any reason className FileSkipped(ISortError) Method __init__ Attribute message Attribute file_path", "label": "class FileSkipped(ISortError):\n    def __init__(self, message: str, file_path: str):\n        super().__init__(message)\n        self.message = message\n        self.file_path = file_path"}
{"Repository": "Action_Recognition_Zoo", "input": "Computes and stores the average and current value className AverageMeter(object) Method __init__ Method reset Method update", "label": "class AverageMeter(object):\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count"}
{"Repository": "ODConv", "input": "Computes and stores the average and current value Imported from https://github. className AverageMeter(object) Method __init__ Method reset Method update", "label": "class AverageMeter(object):\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count"}
{"Repository": "leaguedirector", "input": "This class is responsible for creating a global keyboard hook and forwarding key events to QT when the game process has focus. className KeyboardHook(QThread) Method __init__ Method stop Method eventFilter Method setPid Method run Method stop_windows Method run_windows Method callback Method stop_mac Method run_mac Method callback Attribute tid Attribute pid Attribute running Attribute window", "label": "class KeyboardHook(QThread):\n    def __init__(self, window):\n        QThread.__init__(self)\n        self.tid = None\n        self.pid = None\n        self.running = True\n        self.window = window\n        self.window.installEventFilter(self)\n        QCoreApplication.instance().aboutToQuit.connect(self.stop)\n\n    def stop(self):\n        self.window.removeEventFilter(self)\n        self.running = False\n        if platform.system() == 'Windows':\n            self.stop_windows()\n        elif platform.system() == 'Darwin':\n            self.stop_mac()\n\n    def eventFilter(self, object, event):\n        if event.type() == QEvent.ActivationChange:\n            self.window.setFocus(Qt.OtherFocusReason)\n            QApplication.setActiveWindow(self.window)\n        return False\n\n    def setPid(self, pid):\n        self.pid = pid\n\n    def run(self):\n        self.tid = threading.get_ident()\n        if platform.system() == 'Windows':\n            self.run_windows()\n        elif platform.system() == 'Darwin':\n            self.run_mac()\n\n    def stop_windows(self):\n        windll.user32.PostThreadMessageA(self.tid, 18, 0, 0)\n\n    def run_windows(self):\n        from ctypes.wintypes import DWORD, WPARAM, LPARAM, MSG, HANDLE, HMODULE, LPCWSTR\n\n        class KBDLLHOOKSTRUCT(Structure):\n            _fields_ = [\n                (\"vk_code\", DWORD),\n                (\"scan_code\", DWORD),\n                (\"flags\", DWORD),\n                (\"time\", c_int),\n                (\"dwExtraInfo\", POINTER(DWORD))\n            ]\n\n        def callback(nCode, wParam, lParam):\n            pid = c_ulong()\n            windll.user32.GetWindowThreadProcessId(windll.user32.GetForegroundWindow(), byref(pid))\n            if pid.value == self.pid:\n                windll.user32.SendMessageA(self.window.winId(), wParam, lParam.contents.vk_code, 0)\n            return windll.user32.CallNextHookEx(None, nCode, wParam, lParam)\n\n        function = CFUNCTYPE(c_int, WPARAM, LPARAM, POINTER(KBDLLHOOKSTRUCT))(callback)\n        windll.kernel32.GetModuleHandleW.restype = HMODULE\n        windll.kernel32.GetModuleHandleW.argtypes = [LPCWSTR]\n        windll.user32.SetWindowsHookExA.argtypes = (c_int, HANDLE, HMODULE, DWORD)\n        hook = windll.user32.SetWindowsHookExA(13, function, windll.kernel32.GetModuleHandleW(None), 0)\n\n        msg = POINTER(MSG)()\n        while self.running:\n            try:\n                windll.user32.GetMessageW(msg, 0, 0, 0)\n                windll.user32.TranslateMessage(msg)\n                windll.user32.DispatchMessageA(msg)\n            except: pass\n\n        windll.user32.UnhookWindowsHookEx(hook)\n\n    def stop_mac(self):\n        from Quartz import CFRunLoopStop\n        if hasattr(self, 'runLoop'):\n            CFRunLoopStop(self.runLoop)\n\n    def run_mac(self):\n        from Quartz import (\n            CGEventTapCreate,\n            CFMachPortCreateRunLoopSource,\n            CFRunLoopAddSource,\n            CFRunLoopGetCurrent,\n            CGEventTapEnable,\n            CGEventMaskBit,\n            CFRunLoopRun,\n            CGEventGetIntegerValueField,\n            CGEventPostToPid,\n            kCGEventKeyDown,\n            kCGEventKeyUp,\n            kCGEventFlagsChanged,\n            kCGSessionEventTap,\n            kCGHeadInsertEventTap,\n            kCGEventTargetUnixProcessID,\n            kCFAllocatorDefault,\n            kCFRunLoopDefaultMode,\n        )\n        pid = QCoreApplication.applicationPid()\n\n        def callback(proxy, type, event, refcon):\n            if self.pid == CGEventGetIntegerValueField(event, kCGEventTargetUnixProcessID):\n                CGEventPostToPid(pid, event)\n            return event\n\n        tap = CGEventTapCreate(\n            kCGSessionEventTap,\n            kCGHeadInsertEventTap,\n            0,\n            CGEventMaskBit(kCGEventKeyDown) |\n            CGEventMaskBit(kCGEventKeyUp) |\n            CGEventMaskBit(kCGEventFlagsChanged),\n            callback,\n            None\n        )\n        if tap:\n            source = CFMachPortCreateRunLoopSource(kCFAllocatorDefault, tap, 0)\n            self.runLoop = CFRunLoopGetCurrent()\n            CFRunLoopAddSource(self.runLoop, source, kCFRunLoopDefaultMode)\n            CGEventTapEnable(tap, True)\n            CFRunLoopRun()"}
{"Repository": "pytest-watch", "input": "Listens for changes to files and re-runs tests after each change. className EventListener(FileSystemEventHandler) Method __init__ Method on_any_event Attribute event_queue Attribute extensions", "label": "class EventListener(FileSystemEventHandler):\n    def __init__(self, extensions=[], event_queue=None):\n        super(EventListener, self).__init__()\n        self.event_queue = event_queue or Queue()\n        self.extensions = extensions or DEFAULT_EXTENSIONS\n\n    def on_any_event(self, event):\n        # Filter for allowed event types\n        if not isinstance(event, WATCHED_EVENTS):\n            return\n\n        src_path = os.path.relpath(event.src_path)\n        dest_path = None\n        if isinstance(event, FileMovedEvent):\n            dest_path = os.path.relpath(event.dest_path)\n\n        # Filter files that don't match the allowed extensions\n        if not event.is_directory and self.extensions != ALL_EXTENSIONS:\n            src_ext = os.path.splitext(src_path)[1].lower()\n            src_included = src_ext in self.extensions\n            dest_included = False\n            if dest_path:\n                dest_ext = os.path.splitext(dest_path)[1].lower()\n                dest_included = dest_ext in self.extensions\n            if not src_included and not dest_included:\n                return\n\n        self.event_queue.put((type(event), src_path, dest_path))"}
{"Repository": "TransReID", "input": "Triplet loss using HARDER example mining, className TripletLoss(object) Method __init__ Method __call__ Attribute margin Attribute hard_factor Attribute ranking_loss Attribute ranking_loss", "label": "class TripletLoss(object):\n    def __init__(self, margin=None, hard_factor=0.0):\n        self.margin = margin\n        self.hard_factor = hard_factor\n        if margin is not None:\n            self.ranking_loss = nn.MarginRankingLoss(margin=margin)\n        else:\n            self.ranking_loss = nn.SoftMarginLoss()\n\n    def __call__(self, global_feat, labels, normalize_feature=False):\n        if normalize_feature:\n            global_feat = normalize(global_feat, axis=-1)\n        dist_mat = euclidean_dist(global_feat, global_feat)\n        dist_ap, dist_an = hard_example_mining(dist_mat, labels)\n\n        dist_ap *= (1.0 + self.hard_factor)\n        dist_an *= (1.0 - self.hard_factor)\n\n        y = dist_an.new().resize_as_(dist_an).fill_(1)\n        if self.margin is not None:\n            loss = self.ranking_loss(dist_an, dist_ap, y)\n        else:\n            loss = self.ranking_loss(dist_an - dist_ap, y)\n        return loss, dist_ap, dist_an"}
{"Repository": "alfred-mstodo-workflow", "input": "Extends the Base class and refines it for the Hashtag data structure className Hashtag(BaseModel) Method sync Method hashtags_in_task", "label": "class Hashtag(BaseModel):\n    id = CharField(primary_key=True)\n    tag = CharField()\n    revision = IntegerField(default=0)\n\n    @classmethod\n    def sync(cls):\n        from mstodo.models.task import Task\n\n        tasks_with_hashtags = Task.select().where(Task.title.contains('#'))\n        hashtags = {}\n\n        for task in tasks_with_hashtags:\n            for hashtag in cls.hashtags_in_task(task):\n                tag = re.sub(_hashtag_trim_pattern, r'', hashtag)\n                hashtags[tag.lower()] = tag\n\n        if len(hashtags) > 0:\n            hashtag_data = [{'id': id, 'tag': tag, 'revision': 0} for (id, tag) in hashtags.items()]\n            instances = cls.select()\n\n            return cls._perform_updates(instances, hashtag_data)\n\n        return False\n\n    @classmethod\n    def hashtags_in_task(cls, task):\n        return set(re.findall(_hashtag_pattern, ' ' + task.title))"}
{"Repository": "Sony-PMCA-RE", "input": "Task to run infoCommand() className InfoTask(BackgroundTask) Method doBefore Method do Method doAfter", "label": "class InfoTask(BackgroundTask):\n def doBefore(self):\n  self.ui.infoButton.config(state=DISABLED)\n\n def do(self, arg):\n  try:\n   print('')\n   infoCommand()\n  except Exception:\n   traceback.print_exc()\n\n def doAfter(self, result):\n  self.ui.infoButton.config(state=NORMAL)"}
{"Repository": "python-myfitnesspal", "input": "Stores information about a particular meal. className Meal(MFPBase) Method __init__ Method __getitem__ Method __len__ Method entries Method name Method totals Method get_as_list Method __str__ Attribute _name Attribute _entries", "label": "class Meal(MFPBase):\n    def __init__(self, name: str, entries: List[Entry]):\n        self._name = name\n        self._entries = entries\n\n    def __getitem__(self, value: int) -> Entry:\n        if not isinstance(value, int):\n            raise ValueError(\"Index must be an integer\")\n        return self.entries[value]\n\n    def __len__(self) -> int:\n        return len(self.entries)\n\n    @property\n    def entries(self) -> List[Entry]:\n        return self._entries\n\n    @property\n    def name(self) -> str:\n        return self._name\n\n    @property\n    def totals(self) -> NutritionDict:\n        nutrition = {}\n        for entry in self.entries:\n            for k, v in entry.nutrition_information.items():\n                if k not in nutrition:\n                    nutrition[k] = v\n                else:\n                    nutrition[k] += v\n\n        return nutrition\n\n    def get_as_list(self) -> List[types.MealEntry]:\n        return [e.get_as_dict() for e in self.entries]\n\n    def __str__(self) -> str:\n        return f\"{self.name.title()} {self.totals}\""}
{"Repository": "turkish-deasciifier", "input": "New distutils command for unit tests. className TestCommand(Command) Method initialize_options Method finalize_options Method run", "label": "class TestCommand(Command):\n    user_options = []\n\n    def initialize_options(self):\n        pass\n\n    def finalize_options(self):\n        pass\n\n    def run(self):\n        loader = unittest.TestLoader()\n        suite = loader.loadTestsFromModule(tests)\n        result = unittest.TextTestRunner(verbosity=2).run(suite)\n        if result.errors or result.failures:\n            raise SystemExit(\"\")"}
{"Repository": "LIQE", "input": "Implementation of the unweighted version of the alg. in 'Adapting Auxiliary Losses Using Gradient Similarity' className GradCosine(WeightingMethod) Method __init__ Method _flattening Method get_grad_cos_sim Method get_grad Method backward Attribute main_task Attribute cosine_similarity", "label": "class GradCosine(WeightingMethod):\n    def __init__(self, main_task, **kwargs):\n        self.main_task = main_task\n        self.cosine_similarity = nn.CosineSimilarity(dim=0)\n\n    @staticmethod\n    def _flattening(grad):\n        return torch.cat(tuple(g.reshape(-1, ) for i, g in enumerate(grad)), axis=0)\n\n    def get_grad_cos_sim(self, grad1, grad2):\n        flat_grad1 = self._flattening(grad1)\n        flat_grad2 = self._flattening(grad2)\n\n        cosine = nn.CosineSimilarity(dim=0)(flat_grad1, flat_grad2)\n\n        return torch.clamp(cosine, -1, 1)\n\n    def get_grad(self, losses, shared_parameters):\n        main_loss = losses[self.main_task]\n        aux_losses = torch.stack(tuple(l for i, l in enumerate(losses) if i != self.main_task))\n\n        main_grad = torch.autograd.grad(main_loss, shared_parameters, retain_graph=True)\n        # copy\n        grad = tuple(g.clone() for g in main_grad)\n\n        for loss in aux_losses:\n            aux_grad = torch.autograd.grad(loss, shared_parameters, retain_graph=True)\n            cosine = self.get_grad_cos_sim(main_grad, aux_grad)\n\n            if cosine > 0:\n                grad = tuple(g + ga for g, ga in zip(grad, aux_grad))\n\n        return grad\n\n    def backward(self, losses, shared_parameters, returns=True, **kwargs):\n        shared_grad = self.get_grad(\n            losses,\n            shared_parameters=shared_parameters\n        )\n        loss = torch.sum(torch.stack(losses))\n        loss.backward()\n        # update grads for shared weights\n        for p, g in zip(shared_parameters, shared_grad):\n            p.grad = g\n\n        if returns:\n            return loss"}
{"Repository": "metaseq", "input": "Source: https://stackoverflow.com/a/54128391 className NumpyExtension(Extension) Method __init__ Method include_dirs Method include_dirs Attribute __include_dirs", "label": "class NumpyExtension(Extension):\n    def __init__(self, *args, **kwargs):\n        self.__include_dirs = []\n        super().__init__(*args, **kwargs)\n\n    @property\n    def include_dirs(self):\n        import numpy\n\n        return self.__include_dirs + [numpy.get_include()]\n\n    @include_dirs.setter\n    def include_dirs(self, dirs):\n        self.__include_dirs = dirs"}
{"Repository": "docker-build-ami", "input": "ParserDelegate that creates an AMI using an AmiBuilder className Docker2AmiParserDelegate(AbstractParserDelegate) Method __init__ Method run_run Method run_copy Method run_add Method run_unknown Attribute _ami_builder Attribute _parser_state", "label": "class Docker2AmiParserDelegate(AbstractParserDelegate):\n    def __init__(self, ami_builder, parser_state):\n        self._ami_builder = ami_builder\n        self._parser_state = parser_state\n\n    def run_run(self, cmds):\n        self._ami_builder.run_cmd(self._parser_state.env, cmds)\n\n    def run_copy(self, src, dst):\n        self._ami_builder.run_cmd(self._parser_state.env,\n                                  f'cp -rf /tmp/docker-build-ami/{src} {dst}')\n\n    def run_add(self, src, dst):\n        if is_url_arg(src):\n            dst = os.path.basename(src) if dst == '.' else dst\n            self._ami_builder.run_cmd(\n                self._parser_state.env, f'curl {src} -o {dst}')\n        else:\n            if re.match(r'.*\\.(tgz|tar|tar\\.gz|tar\\.bz|tar\\.xz)$', src):\n                self._ami_builder.run_cmd(\n                    self._parser_state.env,\n                    f'tar -xpvf /tmp/docker-build-ami/{src} -C {dst}')\n            else:\n                self._ami_builder.run_cmd(\n                    self._parser_state.env,\n                    f'cp -rf /tmp/docker-build-ami/{src} {dst}')\n\n    def run_unknown(self, line):\n        print(f'{Color.YELLOW}Unknown Command: {line}{Color.CLEAR}')"}
{"Repository": "bert-Chinese-classification-task", "input": "A single set of features of data. className InputFeatures(object) Method __init__ Attribute input_ids Attribute input_mask Attribute segment_ids Attribute label_id", "label": "class InputFeatures(object):\n    def __init__(self, input_ids, input_mask, segment_ids, label_id):\n        self.input_ids = input_ids\n        self.input_mask = input_mask\n        self.segment_ids = segment_ids\n        self.label_id = label_id"}
{"Repository": "memcache-collections", "input": "Represents a memcache entry at a certain point in time. className Entry(object) Method __init__ Method key Method value Method cas_id Attribute _key Attribute _value Attribute _cas_id", "label": "class Entry(object):\n  def __init__(self, key, value, cas_id):\n    self._key = key\n    self._value = value\n    self._cas_id = cas_id\n\n  @property\n  def key(self):\n    return self._key\n\n  @property\n  def value(self):\n    return self._value\n\n  @property\n  def cas_id(self):\n    return self._cas_id"}
{"Repository": "nilearn", "input": "Implement search_light analysis using an arbitrary type of classifier. className SearchLight(BaseEstimator) Method fit", "label": "class SearchLight(BaseEstimator):\n    def __init__(\n        self,\n        mask_img,\n        process_mask_img=None,\n        radius=2.0,\n        estimator=\"svc\",\n        n_jobs=1,\n        scoring=None,\n        cv=None,\n        verbose=0,\n    ):\n        self.mask_img = mask_img\n        self.process_mask_img = process_mask_img\n        self.radius = radius\n        self.estimator = estimator\n        self.n_jobs = n_jobs\n        self.scoring = scoring\n        self.cv = cv\n        self.verbose = verbose\n\n    def fit(self, imgs, y, groups=None):\n        # check if image is 4D\n        imgs = check_niimg_4d(imgs)\n\n        # Get the seeds\n        process_mask_img = self.process_mask_img\n        if self.process_mask_img is None:\n            process_mask_img = self.mask_img\n\n        # Compute world coordinates of the seeds\n        process_mask, process_mask_affine = masking.load_mask_img(\n            process_mask_img\n        )\n        process_mask_coords = np.where(process_mask != 0)\n        process_mask_coords = coord_transform(\n            process_mask_coords[0],\n            process_mask_coords[1],\n            process_mask_coords[2],\n            process_mask_affine,\n        )\n        process_mask_coords = np.asarray(process_mask_coords).T\n\n        X, A = _apply_mask_and_get_affinity(\n            process_mask_coords,\n            imgs,\n            self.radius,\n            True,\n            mask_img=self.mask_img,\n        )\n\n        estimator = self.estimator\n        if estimator == \"svc\":\n            estimator = ESTIMATOR_CATALOG[estimator](dual=True)\n        elif isinstance(estimator, str):\n            estimator = ESTIMATOR_CATALOG[estimator]()\n\n        scores = search_light(\n            X,\n            y,\n            estimator,\n            A,\n            groups,\n            self.scoring,\n            self.cv,\n            self.n_jobs,\n            self.verbose,\n        )\n        scores_3D = np.zeros(process_mask.shape)\n        scores_3D[process_mask] = scores\n        self.scores_ = scores_3D\n        return self"}
{"Repository": "pyarmor", "input": "An error from creating or using an argument (optional or positional). className ArgumentError(Exception) Method __init__ Method __str__ Attribute argument_name Attribute message", "label": "class ArgumentError(Exception):\n    def __init__(self, argument, message):\n        self.argument_name = _get_action_name(argument)\n        self.message = message\n\n    def __str__(self):\n        if self.argument_name is None:\n            format = '%(message)s'\n        else:\n            format = 'argument %(argument_name)s: %(message)s'\n        return format % dict(message=self.message,\n                             argument_name=self.argument_name)"}
{"Repository": "ibm_zos_core", "input": "User does not have access to a directory. className DirectoryTraversalError(Exception) Method __init__ Attribute msg", "label": "class DirectoryTraversalError(Exception):\n    def __init__(self, path):\n        self.msg = \"Detected directory traversal, user does not have access to {0}\".format(path)\n        super().__init__(self.msg)"}
{"Repository": "MAT", "input": "Care about png files with help className PngStripper(ExiftoolStripper) Method _set_allowed", "label": "class PngStripper(ExiftoolStripper):\n    def _set_allowed(self):\n        self.allowed.update(['Bit Depth', 'Color Type',\n            'Compression', 'Filter', 'Interlace', 'Palette',\n            'Pixels Per Unit X',\n            'Pixels Per Unit Y', 'Pixel Units', 'Significant Bits',\n            'Background Color', 'SRGB Rendering'])"}
{"Repository": "MedSegDiff", "input": "Dice coeff for individual examples className DiceCoeff(Function) Method forward Method backward", "label": "class DiceCoeff(Function):\n    def forward(self, input, target):\n        self.save_for_backward(input, target)\n        eps = 0.0001\n        self.inter = torch.dot(input.view(-1), target.view(-1))\n        self.union = torch.sum(input) + torch.sum(target) + eps\n\n        t = (2 * self.inter.float() + eps) / self.union.float()\n        return t\n\n    # This function has only a single output, so it gets only one gradient\n    def backward(self, grad_output):\n\n        input, target = self.saved_variables\n        grad_input = grad_target = None\n\n        if self.needs_input_grad[0]:\n            grad_input = grad_output * 2 * (target * self.union - self.inter) \\\n                         / (self.union * self.union)\n        if self.needs_input_grad[1]:\n            grad_target = None\n\n        return grad_input, grad_target"}
{"Repository": "bert-multi-gpu", "input": "Configuration for `BertModel`. className BertConfig(object) Method from_dict Method from_json_file Method to_dict Method to_json_string", "label": "class BertConfig(object):\n    def __init__(self,\n                 vocab_size,\n                 hidden_size=768,\n                 num_hidden_layers=12,\n                 num_attention_heads=12,\n                 intermediate_size=3072,\n                 hidden_act=\"gelu\",\n                 hidden_dropout_prob=0.1,\n                 attention_probs_dropout_prob=0.1,\n                 max_position_embeddings=512,\n                 type_vocab_size=16,\n                 initializer_range=0.02):\n        self.vocab_size = vocab_size\n        self.hidden_size = hidden_size\n        self.num_hidden_layers = num_hidden_layers\n        self.num_attention_heads = num_attention_heads\n        self.hidden_act = hidden_act\n        self.intermediate_size = intermediate_size\n        self.hidden_dropout_prob = hidden_dropout_prob\n        self.attention_probs_dropout_prob = attention_probs_dropout_prob\n        self.max_position_embeddings = max_position_embeddings\n        self.type_vocab_size = type_vocab_size\n        self.initializer_range = initializer_range\n\n    @classmethod\n    def from_dict(cls, json_object):\n        config = BertConfig(vocab_size=None)\n        for (key, value) in six.iteritems(json_object):\n            config.__dict__[key] = value\n        return config\n\n    @classmethod\n    def from_json_file(cls, json_file):\n        with tf.gfile.GFile(json_file, \"r\") as reader:\n            text = reader.read()\n        return cls.from_dict(json.loads(text))\n\n    def to_dict(self):\n        output = copy.deepcopy(self.__dict__)\n        return output\n\n    def to_json_string(self):\n        return json.dumps(self.to_dict(), indent=2, sort_keys=True) + \"\\n\""}
{"Repository": "dropzone4-actions", "input": "Attaches HTTP Basic Authentication to the given Request object. className HTTPBasicAuth(AuthBase) Method __init__ Method __call__ Attribute username Attribute password", "label": "class HTTPBasicAuth(AuthBase):\n    def __init__(self, username, password):\n        self.username = username\n        self.password = password\n\n    def __call__(self, r):\n        r.headers['Authorization'] = _basic_auth_str(self.username, self.password)\n        return r"}
{"Repository": "squeeze-alexa", "input": "Opinionated Client subclass that configures from passed settings, className CustomClient(Client) Method __init__ Method _configure_tls Method connect Method disconnect Method _conf_file_of Method __del__ Method __str__ Attribute _keepalive Attribute settings Attribute _host Attribute _port Attribute connected", "label": "class CustomClient(Client):\n    def __init__(self, settings: MqttSettings):\n        super().__init__()\n        # self._keepalive = 5\n        self.settings = settings\n        self._host = settings.hostname\n        self._port = settings.port\n        self._configure_tls()\n        self.connected = False\n\n    def _configure_tls(self):\n        self.tls_set(certfile=self._conf_file_of(\"*-certificate.pem.crt\"),\n                     keyfile=self._conf_file_of(\"*-private.pem.key\"),\n                     tls_version=PROTOCOL_TLSv1_2)\n\n    def connect(self, host=None, port=None, keepalive=30, bind_address=\"\"):\n        print_d(\"Connecting {client}...\", client=self)\n        host = host or self._host\n        port = port or self._port\n\n        check_listening(host, port, msg=\"check your MQTT settings\")\n        print_d(\"Remote socket is listening, let's continue.\")\n        try:\n            ret = super().connect(host=host,\n                                  port=port,\n                                  keepalive=keepalive,\n                                  bind_address=bind_address)\n        except ssl.SSLError as e:\n            if 'SSLV3_ALERT_CERTIFICATE_UNKNOWN' in str(e):\n                raise Error(\"Certificate problem with MQTT. \"\n                            \"Is the certificate enabled in AWS?\")\n        else:\n            if ret == MQTT_ERR_SUCCESS:\n                print_d(\"Connected to {settings}\", settings=self.settings)\n                self.connected = True\n                return ret\n        raise Error(\"Couldn't connect to {settings}\".format(\n            settings=self.settings))\n\n    def disconnect(self):\n        ret = super().disconnect()\n        self.connected = False\n        if ret != MQTT_ERR_SUCCESS and ret != MQTT_ERR_NO_CONN:\n            raise Error(\"Failed to disconnect (%s)\" % error_string(ret))\n        return ret\n\n    def _conf_file_of(self, rel_glob: str) -> str:\n        full_glob = os.path.join(self.settings.cert_dir, rel_glob)\n        results = glob(full_glob)\n        try:\n            return results[0]\n        except IndexError:\n            raise Error(\"Can't find {glob} within dir {base}\".format(\n                base=self.settings.cert_dir, glob=rel_glob))\n\n    def __del__(self):\n        print_d(\"Disconnecting {what}\", what=self)\n        self.disconnect()\n        self.loop_stop()\n\n    def __str__(self) -> str:\n        return \"client to {host}:{port}\".format(host=self._host,\n                                                port=self._port)"}
{"Repository": "dbt-sqlserver", "input": "Custom command to verify that the git tag matches our version className VerifyVersionCommand(install) Method run", "label": "class VerifyVersionCommand(install):\n    description = \"Verify that the git tag matches our version\"\n\n    def run(self):\n        tag = os.getenv(\"GITHUB_REF_NAME\")\n        tag_without_prefix = tag[1:]\n\n        if tag_without_prefix != package_version:\n            info = \"Git tag: {0} does not match the version of this app: {1}\".format(\n                tag_without_prefix, package_version\n            )\n            sys.exit(info)"}
{"Repository": "SOLIDER", "input": "Apply Gaussian Blur to the PIL image. className GaussianBlur(object) Method __init__ Method __call__ Attribute prob Attribute radius_min Attribute radius_max", "label": "class GaussianBlur(object):\n    def __init__(self, p=0.5, radius_min=0.1, radius_max=2.):\n        self.prob = p\n        self.radius_min = radius_min\n        self.radius_max = radius_max\n\n    def __call__(self, img):\n        do_it = random.random() <= self.prob\n        if not do_it:\n            return img\n\n        return img.filter(\n            ImageFilter.GaussianBlur(\n                radius=random.uniform(self.radius_min, self.radius_max)\n            )\n        )"}
{"Repository": "lennoxs30", "input": "Represent an Auxiliary Unit like a heat exhanger className S30AuxiliaryUnit(Device) Method unique_name Method device_model Method register_device", "label": "class S30AuxiliaryUnit(Device):\n    def __init__(\n        self,\n        hass: HomeAssistant,\n        config_entry: ConfigEntry,\n        system: lennox_system,\n        s30_device: S30ControllerDevice,\n        equipment: lennox_equipment,\n    ):\n        super().__init__(equipment)\n        self._hass = hass\n        self._system = system\n        self._config_entry = config_entry\n        self._s30_controller_device: S30ControllerDevice = s30_device\n\n    @property\n    def unique_name(self) -> str:\n        # Not sure if every device has a serial number.\n        if self.equipment.unit_serial_number is None:\n            suffix = self.equipment.equipment_id\n        else:\n            suffix = self.equipment.unit_serial_number\n        return f\"{self._system.unique_id}_{suffix}\"\n\n    @property\n    def device_model(self):\n        if self.equipment is not None:\n            return self.equipment.unit_model_number\n        return None\n\n    def register_device(self):\n        device_registry = dr.async_get(self._hass)\n        name = f\"{self._system.name} {self.equipment.equipment_type_name}\"\n\n        device_registry.async_get_or_create(\n            config_entry_id=self._config_entry.entry_id,\n            identifiers={(LENNOX_DOMAIN, self.unique_name)},\n            manufacturer=LENNOX_MFG,\n            suggested_area=\"basement\",\n            name=name,\n            model=self.device_model,\n            hw_version=self.hw_version,\n            via_device=(LENNOX_DOMAIN, self._s30_controller_device.unique_name),\n        )"}
{"Repository": "EndoscopyDepthEstimation-Pytorch", "input": "Sets the learning rate of each parameter group according to cyclical learning rate policy (CLR). className CyclicLR(object) Method batch_step Method _triangular_scale_fn Method _triangular2_scale_fn Method _exp_range_scale_fn Method get_lr", "label": "class CyclicLR(object):\n    def __init__(self, optimizer, base_lr=1e-3, max_lr=6e-3,\n                 step_size=2000, mode='triangular', gamma=1.,\n                 scale_fn=None, scale_mode='cycle', last_batch_iteration=-1):\n\n        if not isinstance(optimizer, Optimizer):\n            raise TypeError('{} is not an Optimizer'.format(\n                type(optimizer).__name__))\n        self.optimizer = optimizer\n\n        if isinstance(base_lr, list) or isinstance(base_lr, tuple):\n            if len(base_lr) != len(optimizer.param_groups):\n                raise ValueError(\"expected {} base_lr, got {}\".format(\n                    len(optimizer.param_groups), len(base_lr)))\n            self.base_lrs = list(base_lr)\n        else:\n            self.base_lrs = [base_lr] * len(optimizer.param_groups)\n\n        if isinstance(max_lr, list) or isinstance(max_lr, tuple):\n            if len(max_lr) != len(optimizer.param_groups):\n                raise ValueError(\"expected {} max_lr, got {}\".format(\n                    len(optimizer.param_groups), len(max_lr)))\n            self.max_lrs = list(max_lr)\n        else:\n            self.max_lrs = [max_lr] * len(optimizer.param_groups)\n\n        self.step_size = step_size\n\n        if mode not in ['triangular', 'triangular2', 'exp_range'] \\\n                and scale_fn is None:\n            raise ValueError('mode is invalid and scale_fn is None')\n\n        self.mode = mode\n        self.gamma = gamma\n\n        if scale_fn is None:\n            if self.mode == 'triangular':\n                self.scale_fn = self._triangular_scale_fn\n                self.scale_mode = 'cycle'\n            elif self.mode == 'triangular2':\n                self.scale_fn = self._triangular2_scale_fn\n                self.scale_mode = 'cycle'\n            elif self.mode == 'exp_range':\n                self.scale_fn = self._exp_range_scale_fn\n                self.scale_mode = 'iterations'\n        else:\n            self.scale_fn = scale_fn\n            self.scale_mode = scale_mode\n\n        self.batch_step(last_batch_iteration + 1)\n        self.last_batch_iteration = last_batch_iteration\n\n    def batch_step(self, batch_iteration=None):\n        if batch_iteration is None:\n            batch_iteration = self.last_batch_iteration + 1\n        self.last_batch_iteration = batch_iteration\n        for param_group, lr in zip(self.optimizer.param_groups, self.get_lr()):\n            param_group['lr'] = lr\n\n    def _triangular_scale_fn(self, x):\n        return 1.\n\n    def _triangular2_scale_fn(self, x):\n        return 1 / (2. ** (x - 1))\n\n    def _exp_range_scale_fn(self, x):\n        return self.gamma ** (x)\n\n    def get_lr(self):\n        step_size = float(self.step_size)\n        cycle = np.floor(1 + self.last_batch_iteration / (2 * step_size))\n        x = np.abs(self.last_batch_iteration / step_size - 2 * cycle + 1)\n\n        lrs = []\n        param_lrs = zip(self.optimizer.param_groups, self.base_lrs, self.max_lrs)\n        for param_group, base_lr, max_lr in param_lrs:\n            base_height = (max_lr - base_lr) * np.maximum(0, (1 - x))\n            if self.scale_mode == 'cycle':\n                lr = base_lr + base_height * self.scale_fn(cycle)\n            else:\n                lr = base_lr + base_height * self.scale_fn(self.last_batch_iteration)\n            lrs.append(lr)\n        return lrs"}
{"Repository": "data2vec-pytorch", "input": "A Dataset instance for WikiText dataset loaded from HuggingFace datasets. className WikiText(Dataset) Method __init__ Method clean_dataset Method __len__ Method __getitem__ Method _mask_tokens Method collate_fn Attribute cfg Attribute path Attribute mlm_probability Attribute data Attribute tokenizer", "label": "class WikiText(Dataset):\n    def __init__(self, cfg, split, tokenizer, **kwargs):\n        super(WikiText, self).__init__()\n        self.cfg = cfg\n        self.path = cfg.dataset.name\n        self.mlm_probability = cfg.dataset.mlm_probability\n        raw_data = load_dataset('wikitext', self.path)[split]\n        self.data = self.clean_dataset(raw_data) if self.cfg.dataset.clean_dataset else raw_data\n        self.tokenizer = tokenizer\n        self.__dict__.update(kwargs)\n\n    def clean_dataset(self, data):\n        print('Cleaning dataset ...')\n        min_seq_len, max_seq_len = self.cfg.data.valid_seq_lenghts\n        texts = []\n        with tqdm(data, desc='Removing invalid sized inputs: ') as tbar:\n            for i, x in enumerate(tbar):\n                if len(x['text']) in range(min_seq_len, max_seq_len + 1):\n                    texts.append(x)\n        return texts\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, index):\n        raw_text = self.data[index]['text']\n        tokens = self.tokenizer(raw_text, return_attention_mask=False)\n        return tokens\n\n    def _mask_tokens(self, inputs, special_tokens_mask=None):\n        labels = inputs.clone()\n        # We sample a few tokens in each sequence for MLM training (with probability `self.mlm_probability`)\n        probability_matrix = torch.full(labels.shape, self.mlm_probability)\n        if special_tokens_mask is None:\n            special_tokens_mask = [\n                self.tokenizer.get_special_tokens_mask(val, already_has_special_tokens=True) for val in\n                labels.tolist()\n            ]\n            special_tokens_mask = torch.tensor(special_tokens_mask, dtype=torch.bool)\n        else:\n            special_tokens_mask = special_tokens_mask.bool()\n\n        probability_matrix.masked_fill_(special_tokens_mask, value=0.0)\n        masked_indices = torch.bernoulli(probability_matrix).bool()\n        labels[~masked_indices] = self.tokenizer.pad_token_id\n        # 80% of the time, we replace masked input tokens with tokenizer.mask_token ([MASK])\n        indices_replaced = torch.bernoulli(torch.full(labels.shape, 0.8)).bool() & masked_indices\n        inputs[indices_replaced] = self.tokenizer.convert_tokens_to_ids(self.tokenizer.mask_token)\n\n        # 10% of the time, we replace masked input tokens with random word\n        indices_random = torch.bernoulli(torch.full(labels.shape, 0.5)).bool() & masked_indices & ~indices_replaced\n        random_words = torch.randint(len(self.tokenizer), labels.shape, dtype=torch.long)\n        inputs[indices_random] = random_words[indices_random]\n\n        # The rest of the time (10% of the time) we keep the masked input tokens unchanged\n        return inputs, labels, masked_indices\n\n    def collate_fn(self, batch):\n        batch = self.tokenizer.pad(batch, return_tensors=\"pt\")\n        # If special token mask has been preprocessed, pop it from the dict.\n        special_tokens_mask = batch.pop(\"special_tokens_mask\", None)\n        src, trg, masked_indices = self._mask_tokens(\n            batch[\"input_ids\"], special_tokens_mask=special_tokens_mask\n        )\n        return src, trg, masked_indices"}
{"Repository": "cryptopals", "input": "This oracle uses the key also as IV, which makes it insecure. className LazyOracle(Oracle) Method __init__ Method decrypt_and_check_admin Attribute _iv", "label": "class LazyOracle(Oracle):\n    def __init__(self):\n        super().__init__()\n        self._iv = self._key    # Let's be lazy here\n\n    def decrypt_and_check_admin(self, ciphertext):\n        plaintext = aes_cbc_decrypt(ciphertext, self._key, self._iv)\n\n        if not check_ascii_compliance(plaintext):\n            raise Exception(\"The message is not valid\", plaintext)\n\n        return b';admin=true;' in plaintext"}
{"Repository": "BoilerPlate", "input": "Crops the given PIL Image on the long edge with a random start point. className RandomCropLongEdge(object) Method __call__ Method __repr__", "label": "class RandomCropLongEdge(object):\n  def __call__(self, img):\n    size = (min(img.size), min(img.size))\n    # Only step forward along this edge if it's the long edge\n    i = 0 if size[0] == img.size[0] else np.random.randint(low=0,high=img.size[0] - size[0])\n    j = 0 if size[1] == img.size[1] else np.random.randint(low=0,high=img.size[1] - size[1])    \n    return transforms.functional.crop(img, i, j, size[0], size[1])\n\n  def __repr__(self):\n    return self.__class__.__name__"}
{"Repository": "py-staticmaps", "input": "Render a circle using different renderers className Circle(Area) Method compute_circle", "label": "class Circle(Area):\n    def __init__(\n        self,\n        center: s2sphere.LatLng,\n        radius_km: float,\n        fill_color: Color = RED,\n        color: Color = TRANSPARENT,\n        width: int = 0,\n    ) -> None:\n        Area.__init__(self, list(Circle.compute_circle(center, radius_km)), fill_color, color, width)\n\n    @staticmethod\n    def compute_circle(center: s2sphere.LatLng, radius_km: float) -> typing.Iterator[s2sphere.LatLng]:\n        first = None\n        delta_angle = 0.1\n        angle = 0.0\n        geod = Geodesic.WGS84\n        while angle < 360.0:\n            d = geod.Direct(\n                center.lat().degrees,\n                center.lng().degrees,\n                angle,\n                radius_km * 1000.0,\n                Geodesic.LONGITUDE | Geodesic.LATITUDE | Geodesic.LONG_UNROLL,\n            )\n            latlng = create_latlng(d[\"lat2\"], d[\"lon2\"])\n            if first is None:\n                first = latlng\n            yield latlng\n            angle = angle + delta_angle\n        if first:\n            yield first"}
{"Repository": "hindley-milner-python", "input": "Letrec binding className Letrec(object) Method __init__ Method __str__ Attribute v Attribute defn Attribute body", "label": "class Letrec(object):\n    def __init__(self, v, defn, body):\n        self.v = v\n        self.defn = defn\n        self.body = body\n\n    def __str__(self):\n        return \"(letrec {v} = {defn} in {body})\".format(v=self.v, defn=self.defn, body=self.body)"}
{"Repository": "ilmsdump", "input": "No Permission! Read permission : Only open for teacher and TA className NoPermission(ILMSError) Method check", "label": "class NoPermission(ILMSError):\n    @classmethod\n    def check(cls, html: lxml.html.HtmlElement):\n        no_permission = html.xpath(\n            '//div[contains(@style, \"color:#F00;\") and '\n            '(starts-with(text(), \"!\") or starts-with(text(), \"No Permission!\"))]'\n            '/text()'\n        )\n        if no_permission:\n            raise cls(*no_permission)"}
{"Repository": "frescobaldi", "input": "Version information. className Version(QTextBrowser) Method __init__", "label": "class Version(QTextBrowser):\n    def __init__(self, parent=None):\n        super().__init__(parent)\n        self.setPlainText(debuginfo.version_info_string())"}
{"Repository": "Error-Detection", "input": "Medium config. className MediumConfig(object) Method __str__", "label": "class MediumConfig(object):\n  batch_size = 128\n\n  max_grad_norm = 3\n\n  learning_rate = 0.0001\n\n\n\n  keep_prob = 0.8\n\n  lr_decay = 0.98\n\n  init_scale = 0.05\n\n  hidden_size = 400\n\n  embedding_size = 400\n\n\n\n  max_epoch = 8\n\n  max_max_epoch = 1\n\n  max_max_max_epoch = 100\n\n\n\n  num_layers = 1\n\n  num_steps = 47\n\n  vocab_size = 9174\n\n  rnn_mode = BLOCK\n\n\n\n  def __str__(self):\n\n    return (\"batch_size: {}, learning_rate: {}, keep_prob: {}, max_grad_norm: {}, init_scale: {}, hidden_size: {}, embedding_size: {}, num_layers: {}\".format(self.batch_size,self.learning_rate,self.keep_prob,self.max_grad_norm,self.init_scale,self.hidden_size,self.embedding_size,self.num_layers))"}
{"Repository": "genshin_auto_fish", "input": "This class is a subclass of the base :class:`torch. className Dataset(torchDataset) Method __init__ Method input_dim Method mosaic_getitem Method wrapper Attribute __input_dim Attribute enable_mosaic", "label": "class Dataset(torchDataset):\n    def __init__(self, input_dimension, mosaic=True):\n        super().__init__()\n        self.__input_dim = input_dimension[:2]\n        self.enable_mosaic = mosaic\n\n    @property\n    def input_dim(self):\n        if hasattr(self, \"_input_dim\"):\n            return self._input_dim\n        return self.__input_dim\n\n    @staticmethod\n    def mosaic_getitem(getitem_fn):\n        @wraps(getitem_fn)\n        def wrapper(self, index):\n            if not isinstance(index, int):\n                self.enable_mosaic = index[0]\n                index = index[1]\n\n            ret_val = getitem_fn(self, index)\n\n            return ret_val\n\n        return wrapper"}
{"Repository": "steamcontroller", "input": "Gamepad uinput class, create a Xbox360 gamepad device className XBoxGamepad(UInput) Method __init__", "label": "class XBoxGamepad(UInput):\n    def __init__(self):\n        super(XBoxGamepad, self).__init__(\n            vendor=0x045e,\n            product=0x028e,\n            version=0x110,\n            name=b\"Microsoft X-Box 360 pad\",\n            keys=[Keys.BTN_START,\n                  Keys.BTN_MODE,\n                  Keys.BTN_SELECT,\n                  Keys.BTN_A,\n                  Keys.BTN_B,\n                  Keys.BTN_X,\n                  Keys.BTN_Y,\n                  Keys.BTN_TL,\n                  Keys.BTN_TR,\n                  Keys.BTN_THUMBL,\n                  Keys.BTN_THUMBR],\n            axes=[(Axes.ABS_X, -32768, 32767, 16, 128),\n                  (Axes.ABS_Y, -32768, 32767, 16, 128),\n                  (Axes.ABS_RX, -32768, 32767, 16, 128),\n                  (Axes.ABS_RY, -32768, 32767, 16, 128),\n                  (Axes.ABS_Z, 0, 255, 0, 0),\n                  (Axes.ABS_RZ, 0, 255, 0, 0),\n                  (Axes.ABS_HAT0X, -1, 1, 0, 0),\n                  (Axes.ABS_HAT0Y, -1, 1, 0, 0)],\n            rels=[]\n        )"}
{"Repository": "Blender-Texel-Density-Checker", "input": "Select Faces with same TD className Select_Same_TD(Operator) Method execute", "label": "class Select_Same_TD(Operator):\n\tbl_idname = \"object.select_same_texel\"\n\tbl_label = \"Select Faces with same TD\"\n\tbl_options = {'REGISTER', 'UNDO'}\n\n\tdef execute(self, context):\n\t\ttd = context.scene.td\n\t\t\n\t\t#save current mode and active object\n\t\tstart_active_obj = bpy.context.active_object\n\t\tstart_selected_obj = bpy.context.selected_objects\n\t\tstart_selected_faces_mode = td.selected_faces\n\n\t\t#select mode faces and set \"Selected faces\" for TD Operations\n\t\tbpy.ops.mesh.select_mode(use_extend=False, use_expand=False, type='FACE')\n\t\ttd.selected_faces = True\n\n\t\t#Calculate TD for search\n\t\tbpy.ops.object.texel_density_check()\n\t\tsearch_td_value = float(td.density)\n\n\t\tthreshold_filtered = td.select_td_threshold.replace(',', '.')\n\t\ttry:\n\t\t\tthreshold_td_value = float(threshold_filtered)\n\t\texcept:\n\t\t\tthreshold_td_value = 0.1\n\t\t\ttd.select_td_threshold = \"0.1\"\n\n\t\tbpy.ops.object.mode_set(mode='OBJECT')\n\t\tfor x in start_selected_obj:\n\t\t\tbpy.ops.object.select_all(action='DESELECT')\n\t\t\tif (x.type == 'MESH' and len(x.data.uv_layers) > 0):\n\t\t\t\tx.select_set(True)\n\t\t\t\tbpy.context.view_layer.objects.active = x\n\t\t\t\tface_count = len(bpy.context.active_object.data.polygons)\n\t\t\t\t\n\t\t\t\tsearched_faces=[]\n\n\t\t\t\tif bpy.context.area.spaces.active.type == \"IMAGE_EDITOR\" and bpy.context.scene.tool_settings.use_uv_select_sync == False:\n\t\t\t\t\t#save start selected in 3d view faces\n\t\t\t\t\tstart_selected_faces = []\n\t\t\t\t\tfor id in range (0, face_count):\n\t\t\t\t\t\tif bpy.context.active_object.data.polygons[id].select == True:\n\t\t\t\t\t\t\tstart_selected_faces.append(id)\n\t\t\t\t\tbpy.ops.object.mode_set(mode='EDIT')\n\n\t\t\t\t\tfor faceid in start_selected_faces:\n\t\t\t\t\t\tmesh = bpy.context.active_object.data\n\t\t\t\t\t\tbm_local = bmesh.from_edit_mesh(mesh)\n\t\t\t\t\t\tbm_local.faces.ensure_lookup_table()\n\t\t\t\t\t\tuv_layer = bm_local.loops.layers.uv.active\n\t\t\t\t\t\t\n\t\t\t\t\t\tfor uvid in range(0, len(bm_local.faces)):\n\t\t\t\t\t\t\tfor loop in bm_local.faces[uvid].loops:\n\t\t\t\t\t\t\t\tloop[uv_layer].select = False\n\t\t\t\t\t\t\n\t\t\t\t\t\tfor loop in bm_local.faces[faceid].loops:\n\t\t\t\t\t\t\tloop[uv_layer].select = True\n\t\t\t\t\t\t\n\t\t\t\t\t\tbpy.ops.object.texel_density_check()\n\t\t\t\t\t\t\n\t\t\t\t\t\tcurrent_poly_td_value = float(td.density)\n\t\t\t\t\t\tif (current_poly_td_value > (search_td_value - threshold_td_value)) and (current_poly_td_value < (search_td_value + threshold_td_value)):\n\t\t\t\t\t\t\tsearched_faces.append(faceid)\n\t\t\t\t\t\n\t\t\t\t\tmesh = bpy.context.active_object.data\n\t\t\t\t\tbm_local = bmesh.from_edit_mesh(mesh)\n\t\t\t\t\tbm_local.faces.ensure_lookup_table()\n\t\t\t\t\tuv_layer = bm_local.loops.layers.uv.active\n\t\t\t\t\t\n\t\t\t\t\tfor uvid in range(0, len(bm_local.faces)):\n\t\t\t\t\t\tfor loop in bm_local.faces[uvid].loops:\n\t\t\t\t\t\t\tloop[uv_layer].select = False\n\n\t\t\t\t\tfor faceid in searched_faces:\n\t\t\t\t\t\tfor loop in bm_local.faces[faceid].loops:\n\t\t\t\t\t\t\tloop[uv_layer].select = True\n\n\t\t\t\t\tbpy.ops.object.mode_set(mode='OBJECT')\n\t\t\t\t\tfor id in start_selected_faces:\n\t\t\t\t\t\tbpy.context.active_object.data.polygons[id].select = True\n\t\t\t\t\n\t\t\t\telse:\n\t\t\t\t\t\n\t\t\t\t\tfor faceid in range(0, face_count):\n\t\t\t\t\t\tbpy.ops.object.mode_set(mode='EDIT')\n\t\t\t\t\t\tbpy.ops.mesh.reveal()\n\t\t\t\t\t\tbpy.ops.mesh.select_all(action='DESELECT')\n\t\t\t\t\t\tbpy.ops.object.mode_set(mode='OBJECT')\n\t\t\t\t\t\tbpy.context.active_object.data.polygons[faceid].select = True\n\t\t\t\t\t\tbpy.ops.object.mode_set(mode='EDIT')\n\t\t\t\t\t\tbpy.ops.object.texel_density_check()\n\t\t\t\t\t\tcurrent_poly_td_value = float(td.density)\n\t\t\t\t\t\tif (current_poly_td_value > (search_td_value - threshold_td_value)) and (current_poly_td_value < (search_td_value + threshold_td_value)):\n\t\t\t\t\t\t\tsearched_faces.append(faceid)\n\n\t\t\t\t\tbpy.ops.object.mode_set(mode='OBJECT')\n\t\t\t\t\tfor id in range(0, face_count):\n\t\t\t\t\t\tbpy.context.active_object.data.polygons[id].select = False\n\n\t\t\t\t\tfor id in searched_faces:\n\t\t\t\t\t\tbpy.context.active_object.data.polygons[id].select = True\n\n\t\t#Select Objects Again\n\t\tfor x in start_selected_obj:\n\t\t\tx.select_set(True)\n\t\tbpy.context.view_layer.objects.active = start_active_obj\n\t\ttd.selected_faces = start_selected_faces_mode\n\n\t\tbpy.ops.object.mode_set(mode='EDIT')\n\n\t\treturn {'FINISHED'}"}
{"Repository": "monolingual-word-aligner", "input": "Invalid method-parameters. (INVALID_METHOD_PARAMS) className RPCInvalidMethodParams(RPCFault) Method __init__", "label": "class RPCInvalidMethodParams(RPCFault):\n    def __init__(self, error_data=None):\n        RPCFault.__init__(self, INVALID_METHOD_PARAMS, ERROR_MESSAGE[INVALID_METHOD_PARAMS], error_data)"}
{"Repository": "furl", "input": "Mixin that defines proper __str__/__unicode__ methods in Python 2 or 3. className UnicodeMixin(object) Method __str__ Method __str__", "label": "class UnicodeMixin(object):\n    if sys.version_info[0] >= 3:  # Python 3\n        def __str__(self):\n            return self.__unicode__()\n    else:  # Python 2\n        def __str__(self):\n            return self.__unicode__().encode('utf8')"}
{"Repository": "execution-specs", "input": "Forks that occur when a specific block number has been reached. className ByBlockNumber(ForkCriteria) Method __init__ Method check Method __repr__ Attribute _internal Attribute block_number", "label": "class ByBlockNumber(ForkCriteria):\n    block_number: int\n\n    def __init__(self, block_number: int):\n        self._internal = (ForkCriteria.BLOCK_NUMBER, block_number)\n        self.block_number = block_number\n\n    def check(self, block_number: int, timestamp: int) -> bool:\n        return block_number >= self.block_number\n\n    def __repr__(self) -> str:\n        return f\"ByBlockNumber({self.block_number})\""}
{"Repository": "RepLKNet-pytorch", "input": "Track a series of values and provide access to smoothed values over a window or the global series average. className SmoothedValue(object) Method __init__ Method update Method synchronize_between_processes Method median Method avg Method global_avg Method max Method value Method __str__ Attribute deque Attribute total Attribute count Attribute fmt", "label": "class SmoothedValue(object):\n    def __init__(self, window_size=20, fmt=None):\n        if fmt is None:\n            fmt = \"{median:.4f} ({global_avg:.4f})\"\n        self.deque = deque(maxlen=window_size)\n        self.total = 0.0\n        self.count = 0\n        self.fmt = fmt\n\n    def update(self, value, n=1):\n        self.deque.append(value)\n        self.count += n\n        self.total += value * n\n\n    def synchronize_between_processes(self):\n        if not is_dist_avail_and_initialized():\n            return\n        t = torch.tensor([self.count, self.total], dtype=torch.float64, device='cuda')\n        dist.barrier()\n        dist.all_reduce(t)\n        t = t.tolist()\n        self.count = int(t[0])\n        self.total = t[1]\n\n    @property\n    def median(self):\n        d = torch.tensor(list(self.deque))\n        return d.median().item()\n\n    @property\n    def avg(self):\n        d = torch.tensor(list(self.deque), dtype=torch.float32)\n        return d.mean().item()\n\n    @property\n    def global_avg(self):\n        return self.total / self.count\n\n    @property\n    def max(self):\n        return max(self.deque)\n\n    @property\n    def value(self):\n        return self.deque[-1]\n\n    def __str__(self):\n        return self.fmt.format(\n            median=self.median,\n            avg=self.avg,\n            global_avg=self.global_avg,\n            max=self.max,\n            value=self.value)"}
{"Repository": "OpenCue", "input": "Implement policy for sleeping between API retries className SleepingPolicy(ABC) Method sleep", "label": "class SleepingPolicy(ABC):\n    @abc.abstractmethod\n    def sleep(self, attempt):\n        assert attempt >= 0"}
{"Repository": "plyer", "input": "Collect and print the results for the changed lines only. className DiffReport(StandardReport) Method __init__ Method error Attribute _selected", "label": "class DiffReport(StandardReport):\n    def __init__(self, options):\n        super().__init__(options)\n        self._selected = options.selected_lines\n\n    def error(self, line_number, offset, text, check):\n        if line_number not in self._selected[self.filename]:\n            return\n        return super().error(line_number, offset, text, check)"}
{"Repository": "UMT", "input": "BERT model with pre-training heads. className BertForPreTraining(PreTrainedBertModel) Method __init__ Method forward Attribute bert Attribute cls", "label": "class BertForPreTraining(PreTrainedBertModel):\n    def __init__(self, config):\n        super(BertForPreTraining, self).__init__(config)\n        self.bert = BertModel(config)\n        self.cls = BertPreTrainingHeads(config, self.bert.embeddings.word_embeddings.weight)\n        self.apply(self.init_bert_weights)\n\n    def forward(self, input_ids, token_type_ids=None, attention_mask=None, masked_lm_labels=None, next_sentence_label=None):\n        sequence_output, pooled_output = self.bert(input_ids, token_type_ids, attention_mask,\n                                                   output_all_encoded_layers=False)\n        prediction_scores, seq_relationship_score = self.cls(sequence_output, pooled_output)\n\n        if masked_lm_labels is not None and next_sentence_label is not None:\n            loss_fct = CrossEntropyLoss(ignore_index=-1)\n            masked_lm_loss = loss_fct(prediction_scores.view(-1, self.config.vocab_size), masked_lm_labels.view(-1))\n            next_sentence_loss = loss_fct(seq_relationship_score.view(-1, 2), next_sentence_label.view(-1))\n            total_loss = masked_lm_loss + next_sentence_loss\n            return total_loss\n        else:\n            return prediction_scores, seq_relationship_score"}
{"Repository": "Yolov5-on-Flask", "input": "An Event-like class that signals all active clients when a new frame is available. className CameraEvent(object) Method __init__ Method wait Method set Method clear Attribute events", "label": "class CameraEvent(object):\n    def __init__(self):\n        self.events = {}\n\n    def wait(self):\n        ident = get_ident()\n        if ident not in self.events:\n            # this is a new client\n            # add an entry for it in the self.events dict\n            # each entry has two elements, a threading.Event() and a timestamp\n            self.events[ident] = [threading.Event(), time.time()]\n        return self.events[ident][0].wait()\n\n    def set(self):\n        now = time.time()\n        remove = None\n        for ident, event in self.events.items():\n            if not event[0].isSet():\n                # if this client's event is not set, then set it\n                # also update the last set timestamp to now\n                event[0].set()\n                event[1] = now\n            else:\n                # if the client's event is already set, it means the client\n                # did not process a previous frame\n                # if the event stays set for more than 5 seconds, then assume\n                # the client is gone and remove it\n                if now - event[1] > 5:\n                    remove = ident\n        if remove:\n            del self.events[remove]\n\n    def clear(self):\n        self.events[get_ident()][0].clear()"}
{"Repository": "GaitGraph", "input": "Type of data augmentation. className DropOutJoints(object) Method __call__", "label": "class DropOutJoints(object):\n    def __init__(\n        self, prob=1, dropout_rate_range=0.1,\n    ):\n        self.dropout_rate_range = dropout_rate_range\n        self.prob = prob\n\n    def __call__(self, data):\n        if np.random.binomial(1, self.prob, 1) != 1:\n            return data\n\n        T, V, C = data.shape\n        data = data.reshape(T * V, C)\n        # Choose the dropout_rate randomly for every sample from 0 - dropout range\n        dropout_rate = np.random.uniform(0, self.dropout_rate_range, 1)\n        zero_indices = 1 - np.random.binomial(1, dropout_rate, T * V)\n        for i in range(3):\n            data[:, i] = zero_indices * data[:, i]\n        data = data.reshape(T, V, C)\n        return data"}
{"Repository": "roblox-blender-plugin", "input": "Operator for revoking an OAuth2 session className RBX_OT_oauth2_logout(Operator) Method execute Method task_complete Method poll", "label": "class RBX_OT_oauth2_logout(Operator):\n    bl_idname = \"rbx.oauth2logout\"\n    bl_label = \"Log out\"\n    bl_description = \"Log out of your Roblox account\"\n\n    def execute(self, context):\n        def task_complete(task):\n            try:\n                task.result()\n            except Exception as exception:\n                traceback.print_exception(exception)\n\n        from . import event_loop\n        from .oauth2_client import RbxOAuth2Client\n\n        rbx = context.window_manager.rbx\n        oauth2_client = RbxOAuth2Client(rbx)\n        event_loop.submit(oauth2_client.logout(), task_complete)\n        return {\"FINISHED\"}\n\n    @classmethod\n    def poll(cls, context):\n        rbx = context.window_manager.rbx\n        return rbx.is_logged_in and not rbx.is_processing_login_or_logout"}
{"Repository": "fastkml", "input": "<author> and <contributor> describe a person, corporation, or similar entity. className _Person(_AtomObject) Method __bool__ Method __eq__", "label": "class _Person(_AtomObject):\n    name: Optional[str]\n    # conveys a human-readable name for the person.\n\n    uri: Optional[str]\n    # contains a home page for the person.\n\n    email: Optional[str]\n    # contains an email address for the person.\n\n    def __init__(\n        self,\n        ns: Optional[str] = None,\n        name_spaces: Optional[Dict[str, str]] = None,\n        name: Optional[str] = None,\n        uri: Optional[str] = None,\n        email: Optional[str] = None,\n        **kwargs: Any,\n    ) -> None:\n        super().__init__(ns=ns, name_spaces=name_spaces, **kwargs)\n        self.name = name\n        self.uri = uri\n        self.email = email\n\n    def __bool__(self) -> bool:\n        return bool(self.name)\n\n    def __eq__(self, other: object) -> bool:\n        try:\n            assert isinstance(other, type(self))\n        except AssertionError:\n            return False\n        return (\n            super().__eq__(other)\n            and self.name == other.name\n            and self.uri == other.uri\n            and self.email == other.email\n        )"}
{"Repository": "thinkgpt", "input": "Prompts the LLM to request to remember memory as needed className RefineChain(LLMChain) Method __init__ Method predict", "label": "class RefineChain(LLMChain):\n    def __init__(self, **kwargs):\n        super().__init__(prompt=REFINE_PROMPT, **kwargs)\n\n\n    def predict(self, instruction_hint: str = '', **kwargs: Any) -> str:\n        return super().predict(instruction_hint=instruction_hint, **kwargs)"}
{"Repository": "flask-googleauth", "input": "Super simple Google Federated Auth for a given domain. className GoogleFederated(GoogleAuth) Method __init__ Attribute _OPENID_ENDPOINT", "label": "class GoogleFederated(GoogleAuth):\n    def __init__(self, domain, app=None, url_prefix=None, name='GoogleAuth'):\n        self._OPENID_ENDPOINT = \"https://www.google.com/a/%s/o8/ud?be=o8\" % domain\n        super(GoogleFederated, self).__init__(app, url_prefix, name)"}
{"Repository": "MusicBox", "input": "showPic | MusicDetails --------------------- comments. className CurrentMusicDetail(ScrollArea) Method __init__ Method addLyricLabel Method removeAllLyricLabels Attribute mainLayout Attribute topLayout Attribute topMainLayout Attribute topHeaderLayout Attribute lyricFrames Attribute lyricFramesLayout Attribute allLyrics Attribute detailText Attribute titleLabel Attribute recoveryButton", "label": "class CurrentMusicDetail(ScrollArea):\n    def __init__(self, parent):\n        super(CurrentMusicDetail, self).__init__()\n\n        self.setObjectName('detail')\n        self.hide()\n\n        self.mainLayout = VBoxLayout(self.frame)\n        self.topLayout = HBoxLayout()\n        self.topMainLayout = VBoxLayout()\n        self.topHeaderLayout = HBoxLayout()\n\n        self.lyricFrames = ScrollArea()\n        self.lyricFrames.setObjectName('lyricScroll')\n        self.lyricFrames.frame.setObjectName('lyricFrame')\n        self.lyricFramesLayout = VBoxLayout(self.lyricFrames.frame)\n        # self.lyricFrames.setMaximumHeight(500)\n        # \n        self.allLyrics = []\n\n        # self.detailText = QTextEdit()\n        # self.detailText.setObjectName('detailText')\n        # self.detailText.setReadOnly(True)\n\n        self.titleLabel = QLabel(\"\")\n        self.titleLabel.setObjectName('titleLabel')\n\n        self.recoveryButton = QPushButton()\n        self.recoveryButton.setObjectName('recoveryButton')\n        self.recoveryButton.setMinimumSize(24, 24)\n        self.recoveryButton.setMaximumSize(36, 36)\n\n        self.setLayouts()\n\n    def setLayouts(self):\n        self.mainLayout.addLayout(self.topLayout)\n\n        # showPic\n        # self.topLayout.addStretch(1)\n        \n        self.topLayout.addLayout(self.topMainLayout)\n        self.topMainLayout.addSpacing(25)\n        self.topMainLayout.addLayout(self.topHeaderLayout)\n        self.topHeaderLayout.addStretch(1)\n        self.topHeaderLayout.addSpacing(100)\n        self.topHeaderLayout.addWidget(self.titleLabel)\n        self.topHeaderLayout.addStretch(1)\n        self.topHeaderLayout.addSpacing(20)\n        self.topHeaderLayout.addWidget(self.recoveryButton)\n        self.topHeaderLayout.addSpacing(50)\n        self.topMainLayout.addSpacing(30)\n        self.topMainLayout.addWidget(self.lyricFrames)\n\n    def addLyricLabel(self, label):\n\n        HStretchBox(self.lyricFramesLayout, label)\n        \n        self.allLyrics.append(label)\n\n    def removeAllLyricLabels(self):\n        for i in self.allLyrics:\n            i.deleteLater()\n        self.allLyrics = []\n\n        for i in range(self.lyricFramesLayout.count()):\n            self.lyricFramesLayout.takeAt(i)"}
{"Repository": "o365spray", "input": "OneDrive Enumeration module class className EnumerateModule_rst(EnumeratorBase) Method __init__ Method _enumerate", "label": "class EnumerateModule_rst(EnumeratorBase):\n    def __init__(self, *args, **kwargs):\n        super(EnumerateModule_rst, self).__init__(*args, **kwargs)\n\n    def _enumerate(self, domain: str, user: str, password: str = \"Password1\"):\n        try:\n            # Build email if not already built\n            email = self.HELPER.check_email(user, domain)\n\n            # Write the tested user\n            tested = f\"{user} -> {email}\" if user != email else email\n            if self.writer:\n                self.tested_writer.write(tested)\n\n            time.sleep(0.250)\n\n            # Update content type for SOAP XML\n            headers = Defaults.HTTP_HEADERS\n            headers[\"Content-Type\"] = \"application/soap+xml\"\n\n            # Handle FireProx API URL\n            if self.proxy_url:\n                proxy_url = self.proxy_url.rstrip(\"/\")\n                url = f\"{proxy_url}/rst2.srf\"\n\n                # Update headers\n                headers = Helper.fireprox_headers(headers)\n\n            else:\n                url = \"https://login.microsoftonline.com/rst2.srf\"\n\n            data = f\"\"\""}
{"Repository": "Text2Human", "input": "None dict. It will return none if key is not in the dict. className NoneDict(dict) Method __missing__", "label": "class NoneDict(dict):\n    def __missing__(self, key):\n        return None"}
{"Repository": "deep-neuroevolution", "input": "This class is slightly adapted from the following Subscriber Content from the Stack Exchange Network https://stackoverflow. className DiscreteSlider(Slider) Method __init__ Method set_val Attribute inc", "label": "class DiscreteSlider(Slider):\n    \"\"\"A matplotlib slider widget with discrete steps.\"\"\"\n    def __init__(self, *args, **kwargs):\n        self.inc = kwargs.pop('increment', 0.5)\n        Slider.__init__(self, *args, **kwargs)\n        self.valtext.set_text('')\n\n    def set_val(self, val):\n        discrete_val = int(val / self.inc) * self.inc\n        # We can't just call Slider.set_val(self, discrete_val), because this\n        # will prevent the slider from updating properly (it will get stuck at\n        # the first step and not \"slide\"). Instead, we'll keep track of the\n        # the continuous value as self.val and pass in the discrete value to\n        # everything else.\n        xy = self.poly.xy\n        xy[2] = discrete_val, 1\n        xy[3] = discrete_val, 0\n        self.poly.xy = xy\n        if discrete_val >= 0:\n            self.valtext.set_text(self.valfmt % discrete_val)\n        else:\n            self.valtext.set_text('')\n        if self.drawon:\n            self.ax.figure.canvas.draw()\n        self.val = val\n        if not self.eventson:\n            return\n        for _, func in self.observers.items():\n            func(discrete_val)"}
{"Repository": "MSN-Point-Cloud-Completion", "input": "Computes and stores the average and current value className AverageValueMeter(object) Method __init__ Method reset Method update", "label": "class AverageValueMeter(object):\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0.0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count"}
{"Repository": "pydatastructs", "input": "Abstract class for arrays in pydatastructs. className Array(object) Method __str__", "label": "class Array(object):\n    def __str__(self) -> str:\n        return str(self._data)"}
{"Repository": "Misskey.py", "input": "This is the top-level Misskey class common to both synchronous and asynchronous processing. className BaseMisskey(object) Method address Method token Method _address_parse", "label": "class BaseMisskey(object):\n    _address: str\n    _token: Optional[str] = None\n\n    @property\n    def address(self) -> str:\n        return self._address\n\n    @property\n    def token(self) -> Optional[str]:\n        return self._token\n\n    def __init__(\n        self, *,\n        address: str,\n        token: Optional[str] = None,\n    ):\n        self._address = self._address_parse(address)\n\n        self._token = token\n\n    @staticmethod\n    def _address_parse(address: str) -> str:\n        parsed_address = urlparse(address)\n        if parsed_address.scheme == \"\":\n            parsed_address = urlparse(f\"https://{address}\")\n\n        if (parsed_address.scheme != \"http\" and\n           parsed_address.scheme != \"https\"):\n            raise ValueError(\n                f'Address protocol does not support \"{parsed_address.scheme}\"')\n\n        return parsed_address._replace(\n            path=\"\",\n            params=\"\",\n            query=\"\",\n            fragment=\"\",\n        ).geturl().rstrip(\"/\")\n\n    def _api_request(\n        self, *,\n        endpoint: str,\n        params: Optional[dict] = None,\n        **kwargs\n    ) -> Any:\n        raise NotImplementedError()"}
{"Repository": "Paython", "input": "Errors when data is missing in developer API call className MissingDataError(Exception) Method __init__ Method __str__ Attribute msg", "label": "class MissingDataError(Exception):\n    def __init__(self, msg):\n        self.msg = msg\n    def __str__(self):\n        return repr(self.msg)"}
{"Repository": "SubGNN", "input": "Computes attention between a vector and a matrix using an additive attention function. className AdditiveAttention(Attention) Method __init__ Method reset_parameters Method _forward_internal", "label": "class AdditiveAttention(Attention):\n    def __init__(self, vector_dim: int, matrix_dim: int, normalize: bool = True) -> None:\n        super().__init__(normalize)\n        self._w_matrix = Parameter(torch.Tensor(vector_dim, vector_dim))\n        self._u_matrix = Parameter(torch.Tensor(matrix_dim, vector_dim))\n        self._v_vector = Parameter(torch.Tensor(vector_dim, 1))\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        torch.nn.init.xavier_uniform_(self._w_matrix)\n        torch.nn.init.xavier_uniform_(self._u_matrix)\n        torch.nn.init.xavier_uniform_(self._v_vector)\n\n    def _forward_internal(self, vector: torch.Tensor, matrix: torch.Tensor) -> torch.Tensor:\n        intermediate = vector.matmul(self._w_matrix).unsqueeze(1) + matrix.matmul(self._u_matrix)\n        intermediate = torch.tanh(intermediate)\n        return intermediate.matmul(self._v_vector).squeeze(2)"}
{"Repository": "rackspace-monitoring-agent-plugins-contrib", "input": "Create an object for a Docker service. Assume it is stopped. className DockerService(object) Method __init__ Method docker_stats Attribute url Attribute docker_running", "label": "class DockerService(object):\n    def __init__(self, url):\n\n        self.url = url\n        self.docker_running = False\n\n    def docker_stats(self):\n        docker_conn = Client(base_url=self.url)\n\n        try:\n            docker_info = docker_conn.info()\n            self.docker_running = True\n        # Apologies for the broad exception, it just works here.\n        except Exception:\n            self.docker_running = False\n\n        if self.docker_running:\n            # Create a dict from the list of lists 'docker info' uses\n            # to report Driver Status stats.\n            driver_status = dict([(metric[0], metric[1]) for metric in \\\n                    docker_info['DriverStatus']])\n             \n            print 'metric images int64', docker_info['Images']\n            print 'metric containers int64', docker_info['Containers']\n            print 'metric go_routines int64', docker_info['NGoroutines']\n            print 'metric driver string', docker_info['Driver']\n            \n            data_space_used_scalar, data_space_used_unit = \\\n                    driver_status['Data Space Used'].split()\n            print 'metric data_space_used float', \\\n                   data_space_used_scalar, data_space_used_unit\n            \n            data_space_total_scalar, data_space_total_unit = \\\n                    driver_status['Data Space Total'].split()\n            print 'metric data_space_total float', \\\n                   data_space_total_scalar, data_space_total_unit\n            \n            metadata_space_used_scalar, metadata_space_used_unit = \\\n                    driver_status['Metadata Space Used'].split()\n            print 'metric metadata_space_used float', \\\n                   metadata_space_used_scalar, metadata_space_used_unit\n            \n            metadata_space_total_scalar, metadata_space_total_unit = \\\n                    driver_status['Metadata Space Total'].split()\n            print 'metric metadata_space_total float', \\\n                   metadata_space_total_scalar, metadata_space_total_unit\n            \n            print 'status ok succeeded in obtaining docker stats.'\n        else:\n            print 'status err failed to obtain docker stats.'\n            sys.exit(1)"}
{"Repository": "MPC_template-model_predictive_control_for_reinforcement_learning", "input": "Observe the [x, y, vx, vy] of nearby vehicles. className LocalCoordinateObservation(ObservationType) Method space Method normalize Method observe", "label": "class LocalCoordinateObservation(ObservationType):\n    # here feature is the layer\n    FEATURES = ['presence', 'vx', 'vy']\n    # this is where makes the observation shape 11\n    GRID_SIZE = [[-5.5*5, 5.5*5], [-5.5*5, 5.5*5]]\n    GRID_STEP = [5, 5]\n\n    def __init__(self,\n                 env,\n                 features=FEATURES,\n                 grid_size=GRID_SIZE,\n                 grid_step=GRID_STEP,\n                 features_range=None,\n                 absolute=False,\n                 **kwargs):\n        self.env = env\n        self.features = features\n        self.grid_size = np.array(grid_size)\n        self.grid_step = np.array(grid_step)\n        grid_shape = np.asarray(np.floor((self.grid_size[:, 1] - self.grid_size[:, 0]) / grid_step), dtype=np.int)\n        self.grid = np.zeros((len(self.features), *grid_shape))\n        self.features_range = features_range\n        self.absolute = absolute\n\n    def space(self):\n        return spaces.Box(shape=self.grid.shape, low=-1, high=1, dtype=np.float32)\n\n    def normalize(self, df):\n        if not self.features_range:\n            self.features_range = {\n                \"vx\": [-2*MDPVehicle.SPEED_MAX, 2*MDPVehicle.SPEED_MAX],\n                \"vy\": [-2*MDPVehicle.SPEED_MAX, 2*MDPVehicle.SPEED_MAX]\n            }\n        for feature, f_range in self.features_range.items():\n            if feature in df:\n                df[feature] = utils.remap(df[feature], [f_range[0], f_range[1]], [-1, 1])\n        return df\n\n    def observe(self):\n        index_list = []\n        if self.absolute:\n            raise NotImplementedError()\n        else:\n            # Add nearby traffic\n            self.grid.fill(0)\n            df = pandas.DataFrame.from_records(\n                [v.to_dict(self.env.vehicle) for v in self.env.road.vehicles])\n            # Normalize\n            df = self.normalize(df)\n            # Fill-in features\n            for layer, feature in enumerate(self.features):\n                for _, vehicle in df.iterrows():\n                    x, y = vehicle[\"x\"], vehicle[\"y\"]\n                    # Recover unnormalized coordinates for cell index\n                    if \"x\" in self.features_range:\n                        x = utils.remap(x, [-1, 1], [self.features_range[\"x\"][0], self.features_range[\"x\"][1]])\n                    if \"y\" in self.features_range:\n                        y = utils.remap(y, [-1, 1], [self.features_range[\"y\"][0], self.features_range[\"y\"][1]])\n                    cell = (int((x - self.grid_size[0, 0]) / self.grid_step[0]),\n                            int((y - self.grid_size[1, 0]) / self.grid_step[1]))\n                    if 0 <= cell[1] < self.grid.shape[-2] and 0 <= cell[0] < self.grid.shape[-1]:\n                        self.grid[layer, cell[1], cell[0]] = vehicle[feature]\n                        index_list.append(cell)\n            # Clip\n            obs = np.clip(self.grid, -1, 1)\n\n            obs_list = []\n            for index in index_list:\n                obs_list.append([index[0], index[1],\n                                 self.grid[1, index[1], index[0]],\n                                 self.grid[2, index[1], index[0]]])\n            return obs"}
{"Repository": "yolov3-tiny-onnx-TensorRT", "input": "Helper class used for loading the serialized weights of a binary file stream and returning the initializers and the input tensors required for populating the ONNX graph with weights. className WeightLoader(object) Method __init__ Method load_upsample_scales Method load_conv_weights Method _open_weights_file Method _create_param_tensors Method _load_one_param_type Attribute weights_file", "label": "class WeightLoader(object):\n    def __init__(self, weights_file_path):\n        self.weights_file = self._open_weights_file(weights_file_path)\n\n    def load_upsample_scales(self, upsample_params):\n        initializer = list()\n        inputs = list()\n        name = upsample_params.generate_param_name()\n        shape = upsample_params.value.shape\n        data = upsample_params.value\n        scale_init = helper.make_tensor(\n            name, TensorProto.FLOAT, shape, data)\n        scale_input = helper.make_tensor_value_info(\n            name, TensorProto.FLOAT, shape)\n        initializer.append(scale_init)\n        inputs.append(scale_input)\n        return initializer, inputs\n\n\n    def load_conv_weights(self, conv_params):\n        initializer = list()\n        inputs = list()\n        if conv_params.batch_normalize:\n            bias_init, bias_input = self._create_param_tensors(\n                conv_params, 'bn', 'bias')\n            bn_scale_init, bn_scale_input = self._create_param_tensors(\n                conv_params, 'bn', 'scale')\n            bn_mean_init, bn_mean_input = self._create_param_tensors(\n                conv_params, 'bn', 'mean')\n            bn_var_init, bn_var_input = self._create_param_tensors(\n                conv_params, 'bn', 'var')\n            initializer.extend(\n                [bn_scale_init, bias_init, bn_mean_init, bn_var_init])\n            inputs.extend([bn_scale_input, bias_input,\n                           bn_mean_input, bn_var_input])\n        else:\n            bias_init, bias_input = self._create_param_tensors(\n                conv_params, 'conv', 'bias')\n            initializer.append(bias_init)\n            inputs.append(bias_input)\n        conv_init, conv_input = self._create_param_tensors(\n            conv_params, 'conv', 'weights')\n        initializer.append(conv_init)\n        inputs.append(conv_input)\n        return initializer, inputs\n\n    def _open_weights_file(self, weights_file_path):\n        weights_file = open(weights_file_path, 'rb')\n        length_header = 5\n        np.ndarray(\n            shape=(length_header, ), dtype='int32', buffer=weights_file.read(\n                length_header * 4))\n        return weights_file\n\n    def _create_param_tensors(self, conv_params, param_category, suffix):\n        param_name, param_data, param_data_shape = self._load_one_param_type(\n            conv_params, param_category, suffix)\n\n        initializer_tensor = helper.make_tensor(\n            param_name, TensorProto.FLOAT, param_data_shape, param_data)\n        input_tensor = helper.make_tensor_value_info(\n            param_name, TensorProto.FLOAT, param_data_shape)\n        return initializer_tensor, input_tensor\n\n    def _load_one_param_type(self, conv_params, param_category, suffix):\n        param_name = conv_params.generate_param_name(param_category, suffix)\n        channels_out, channels_in, filter_h, filter_w = conv_params.conv_weight_dims\n        if param_category == 'bn':\n            param_shape = [channels_out]\n        elif param_category == 'conv':\n            if suffix == 'weights':\n                param_shape = [channels_out, channels_in, filter_h, filter_w]\n            elif suffix == 'bias':\n                param_shape = [channels_out]\n        param_size = np.product(np.array(param_shape))\n        param_data = np.ndarray(\n            shape=param_shape,\n            dtype='float32',\n            buffer=self.weights_file.read(param_size * 4))\n        param_data = param_data.flatten().astype(float)\n        return param_name, param_data, param_shape"}
{"Repository": "hedwig", "input": "A single training/test example for simple sequence classification. className InputExample(object) Method __init__ Attribute guid Attribute text_a Attribute text_b Attribute label", "label": "class InputExample(object):\n    def __init__(self, guid, text_a, text_b=None, label=None):\n        self.guid = guid\n        self.text_a = text_a\n        self.text_b = text_b\n        self.label = label"}
{"Repository": "pytorch-i3d", "input": "Horizontally flip the given seq Images randomly with a given probability. className RandomHorizontalFlip(object) Method __init__ Method __call__ Method __repr__ Attribute p", "label": "class RandomHorizontalFlip(object):\n    def __init__(self, p=0.5):\n        self.p = p\n\n    def __call__(self, imgs):\n        if random.random() < self.p:\n            # t x h x w\n            return np.flip(imgs, axis=2).copy()\n        return imgs\n\n    def __repr__(self):\n        return self.__class__.__name__ + '(p={})'.format(self.p)"}
{"Repository": "pymacaroons", "input": "Serializer used to produce JSON macaroon format v1. className JsonSerializer(object) Method serialize Method _serialize_v1 Method _serialize_v2 Method deserialize Method _deserialize_v1 Method _deserialize_v2", "label": "class JsonSerializer(object):\n    def serialize(self, m):\n        from pymacaroons import macaroon\n        if m.version == macaroon.MACAROON_V1:\n            return self._serialize_v1(m)\n        return self._serialize_v2(m)\n\n    def _serialize_v1(self, macaroon):\n        serialized = {\n            'identifier': utils.convert_to_string(macaroon.identifier),\n            'signature': macaroon.signature,\n        }\n        if macaroon.location:\n            serialized['location'] = macaroon.location\n        if macaroon.caveats:\n            serialized['caveats'] = [\n                _caveat_v1_to_dict(caveat) for caveat in macaroon.caveats\n            ]\n        return json.dumps(serialized)\n\n    def _serialize_v2(self, macaroon):\n        serialized = {}\n        _add_json_binary_field(macaroon.identifier_bytes, serialized, 'i')\n        _add_json_binary_field(binascii.unhexlify(macaroon.signature_bytes),\n                               serialized, 's')\n\n        if macaroon.location:\n            serialized['l'] = macaroon.location\n        if macaroon.caveats:\n            serialized['c'] = [\n                _caveat_v2_to_dict(caveat) for caveat in macaroon.caveats\n            ]\n        return json.dumps(serialized)\n\n    def deserialize(self, serialized):\n        deserialized = json.loads(serialized)\n        if deserialized.get('identifier') is None:\n            return self._deserialize_v2(deserialized)\n        else:\n            return self._deserialize_v1(deserialized)\n\n    def _deserialize_v1(self, serialized):\n        from pymacaroons.macaroon import Macaroon, MACAROON_V1\n        from pymacaroons.caveat import Caveat\n\n        caveats = []\n        for c in serialized.get('caveats', []):\n            caveat = Caveat(\n                caveat_id=c['cid'],\n                verification_key_id=(\n                    utils.raw_b64decode(c['vid']) if c.get('vid')\n                    else None\n                ),\n                location=(\n                    c['cl'] if c.get('cl') else None\n                ),\n                version=MACAROON_V1\n            )\n            caveats.append(caveat)\n\n        return Macaroon(\n            location=serialized.get('location'),\n            identifier=serialized['identifier'],\n            caveats=caveats,\n            signature=serialized['signature'],\n            version=MACAROON_V1\n        )\n\n    def _deserialize_v2(self, serialized):\n        from pymacaroons.macaroon import Macaroon, MACAROON_V2\n        from pymacaroons.caveat import Caveat\n        caveats = []\n        for c in serialized.get('c', []):\n            caveat = Caveat(\n                caveat_id=_read_json_binary_field(c, 'i'),\n                verification_key_id=_read_json_binary_field(c, 'v'),\n                location=_read_json_binary_field(c, 'l'),\n                version=MACAROON_V2\n            )\n            caveats.append(caveat)\n        return Macaroon(\n            location=_read_json_binary_field(serialized, 'l'),\n            identifier=_read_json_binary_field(serialized, 'i'),\n            caveats=caveats,\n            signature=binascii.hexlify(\n                _read_json_binary_field(serialized, 's')),\n            version=MACAROON_V2\n        )"}
{"Repository": "prjxray", "input": "Object that represents a Project X-ray ROI. className Roi(object) Method __init__ Method tile_in_roi Method gen_tiles Method gen_sites Attribute grid Attribute x1 Attribute x2 Attribute y1 Attribute y2", "label": "class Roi(object):\n    def __init__(self, db, x1, x2, y1, y2):\n        self.grid = db.grid()\n        self.x1 = x1\n        self.x2 = x2\n        self.y1 = y1\n        self.y2 = y2\n\n    def tile_in_roi(self, grid_loc):\n        x = grid_loc.grid_x\n        y = grid_loc.grid_y\n        return self.x1 <= x and x <= self.x2 and self.y1 <= y and y <= self.y2\n\n    def gen_tiles(self, tile_types=None):\n        ''' Yield tile names within ROI.\n\n        tile_types: list of tile types to keep, or None for all\n        '''\n\n        for tile_name in self.grid.tiles():\n            loc = self.grid.loc_of_tilename(tile_name)\n\n            if not self.tile_in_roi(loc):\n                continue\n\n            gridinfo = self.grid.gridinfo_at_loc(loc)\n\n            if tile_types is not None and gridinfo.tile_type not in tile_types:\n                continue\n\n            yield tile_name\n\n    def gen_sites(self, site_types=None):\n        ''' Yield (tile_name, site_name, site_type) within ROI.\n\n        site_types: list of site types to keep, or None for all\n\n        '''\n\n        for tile_name in self.grid.tiles():\n            loc = self.grid.loc_of_tilename(tile_name)\n\n            if not self.tile_in_roi(loc):\n                continue\n\n            gridinfo = self.grid.gridinfo_at_loc(loc)\n\n            for site_name, site_type in gridinfo.sites.items():\n                if site_types is not None and site_type not in site_types:\n                    continue\n\n                yield (tile_name, site_name, site_type)"}
{"Repository": "self_talk", "input": "Reads the CommonsenseQA dataset into a unified format with context, question, label, choices and clarifications. className CommonsenseqaInstanceReader(InstanceReader) Method to_uniform_fields Method fields_to_instance", "label": "class CommonsenseqaInstanceReader(InstanceReader):\n    @overrides\n    def to_uniform_fields(self, fields):\n        context = ''\n\n        question = fields['question']['stem']\n        label = ['A','B','C','D','E'].index(fields['answerKey']) if \"answerKey\" in fields else None\n        choices = [c['text'] for c in fields['question']['choices']]\n\n        clarifications = [c[1] if len(c[1].split()) > 1 else \" \".join((c)) for c in fields['clarifications']]\n        clarifications = [c[0].upper() + c[1:] for c in clarifications] + [\"\"]\n        \n        return context, question, label, choices, clarifications\n\n    @overrides\n    def fields_to_instance(self, fields):\n        context, question, label, choices, clarifications = self.to_uniform_fields(fields)\n        context_with_clarifications = [f\"{context} {question} [choice] {clarification}\"\n                                       for clarification in clarifications]\n        context_with_choice_and_clarifications = [\n            [context_with_clar.replace(\"[choice]\", choice[0].lower() + choice[1:]).strip()\n             for context_with_clar in context_with_clarifications]\n             for choice in choices]\n\n        return context, question, label, choices, clarifications, context_with_choice_and_clarifications"}
{"Repository": "nfl_data_py", "input": "Support setup.py upload. className UploadCommand(Command) Method status Method initialize_options Method finalize_options Method run", "label": "class UploadCommand(Command):\n    description = 'Build and publish the package.'\n    user_options = []\n\n    @staticmethod\n    def status(s):\n        print('\\033[1m{0}\\033[0m'.format(s))\n\n    def initialize_options(self):\n        pass\n\n    def finalize_options(self):\n        pass\n\n    def run(self):\n        try:\n            self.status('Removing previous builds')\n            rmtree(os.path.join(here, 'dist'))\n        except OSError:\n            pass\n\n        self.status('Building Source and Wheel (universal) distribution')\n        os.system('{0} setup.py sdist bdist_wheel --universal'.format(sys.executable))\n\n        self.status('Uploading the package to PyPI via Twine')\n        os.system('twine upload dist/*')\n\n        self.status('Pushing git tags')\n        os.system('git tag v{0}'.format(about['__version__']))\n        os.system('git push --tags')\n\n        sys.exit()"}
{"Repository": "wammu", "input": "Page offering text control input. className InputPage(SimplePage) Method __init__ Attribute edit Attribute edit Attribute body", "label": "class InputPage(SimplePage):\n    def __init__(self, parent, title, text, choices=None, help=''):\n        Wammu.Wizard.SimplePage.__init__(self, parent, title, text)\n        if type(choices) == str:\n            self.edit = wx.TextCtrl(self, -1, choices, size=(300, -1))\n        else:\n            self.edit = wx.ComboBox(\n                self, -1, '', choices=choices, size=(300, -1)\n            )\n        self.sizer.Add(self.edit, 0, wx.ALL, 5)\n        self.body = wx.StaticText(self, -1, help)\n        self.body.Wrap(400)\n        self.sizer.Add(self.body, 0, wx.ALL, 5)"}
{"Repository": "cppclean", "input": "Data container to represent a C++ token. className Token(object) Method __init__ Method __str__ Attribute token_type Attribute name Attribute start Attribute end", "label": "class Token(object):\n    def __init__(self, token_type, name, start, end):\n        self.token_type = token_type\n        self.name = name\n        self.start = start\n        self.end = end\n\n    def __str__(self):\n        return 'Token(%r, %s, %s)' % (self.name, self.start, self.end)\n\n    __repr__ = __str__"}
{"Repository": "nautilus_trader", "input": "Represents an example user-defined data class. className MyData(Data) Method ts_event Method ts_init", "label": "class MyData(Data):\n    def __init__(\n        self,\n        value,\n        ts_event=0,\n        ts_init=0,\n    ):\n        self.value = value\n        self._ts_event = ts_event\n        self._ts_init = ts_init\n\n    @property\n    def ts_event(self) -> int:\n        return self._ts_event\n\n    @property\n    def ts_init(self) -> int:\n        return self._ts_init"}
{"Repository": "colorcet", "input": "Ordered dictionary with attribute access (e.g. for tab completion) className AttrODict(OrderedDict) Method __dir__ Method __delattr__ Method __getattr__ Method __setattr__", "label": "class AttrODict(OrderedDict):\n    def __dir__(self): return self.keys()\n    def __delattr__(self, name): del self[name]\n    def __getattr__(self, name):\n        return self[name] if not name.startswith('_') else super(AttrODict, self).__getattr__(name)\n    def __setattr__(self, name, value):\n        if (name.startswith('_')): return super(AttrODict, self).__setattr__(name, value)\n        self[name] = value"}
{"Repository": "FcaNet", "input": "Computes and stores the average and current value className AverageMeter(object) Method __init__ Method reset Method update", "label": "class AverageMeter(object):\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count"}
{"Repository": "pykeen", "input": "Contains a lazy reference to a training, testing, and validation dataset. className PathDataset(LazyDataset) Method _load Method _load_validation Method __repr__", "label": "class PathDataset(LazyDataset):\n    def __init__(\n        self,\n        training_path: Union[str, pathlib.Path],\n        testing_path: Union[str, pathlib.Path],\n        validation_path: Union[None, str, pathlib.Path],\n        eager: bool = False,\n        create_inverse_triples: bool = False,\n        load_triples_kwargs: Optional[Mapping[str, Any]] = None,\n    ) -> None:\n        self.training_path = pathlib.Path(training_path)\n        self.testing_path = pathlib.Path(testing_path)\n        self.validation_path = pathlib.Path(validation_path) if validation_path else None\n\n        self._create_inverse_triples = create_inverse_triples\n        self.load_triples_kwargs = load_triples_kwargs\n\n        if eager:\n            self._load()\n            self._load_validation()\n\n    def _load(self) -> None:\n        self._training = TriplesFactory.from_path(\n            path=self.training_path,\n            create_inverse_triples=self._create_inverse_triples,\n            load_triples_kwargs=self.load_triples_kwargs,\n        )\n        self._testing = TriplesFactory.from_path(\n            path=self.testing_path,\n            entity_to_id=self._training.entity_to_id,  # share entity index with training\n            relation_to_id=self._training.relation_to_id,  # share relation index with training\n            # do not explicitly create inverse triples for testing; this is handled by the evaluation code\n            create_inverse_triples=False,\n            load_triples_kwargs=self.load_triples_kwargs,\n        )\n\n    def _load_validation(self) -> None:\n        # don't call this function by itself. assumes called through the `validation`\n        # property and the _training factory has already been loaded\n        assert self._training is not None\n        if self.validation_path is None:\n            self._validation = None\n        else:\n            self._validation = TriplesFactory.from_path(\n                path=self.validation_path,\n                entity_to_id=self._training.entity_to_id,  # share entity index with training\n                relation_to_id=self._training.relation_to_id,  # share relation index with training\n                # do not explicitly create inverse triples for testing; this is handled by the evaluation code\n                create_inverse_triples=False,\n                load_triples_kwargs=self.load_triples_kwargs,\n            )\n\n    def __repr__(self) -> str:  # noqa: D105\n        return (\n            f'{self.__class__.__name__}(training_path=\"{self.training_path}\", testing_path=\"{self.testing_path}\",'\n            f' validation_path=\"{self.validation_path}\")'\n        )"}
{"Repository": "avionix", "input": ":param webhook: webhook describes how to call the conversion webhook. className CustomResourceConversion(HelmYaml) Method __init__ Attribute webhook Attribute strategy", "label": "class CustomResourceConversion(HelmYaml):\n    def __init__(self, webhook: WebhookConversion, strategy: str):\n        self.webhook = webhook\n        self.strategy = strategy"}
{"Repository": "next-prediction", "input": "Trainer class for model. className Trainer(object) Method __init__ Method step Attribute config Attribute model Attribute global_step Attribute xyloss Attribute loss Attribute grads Attribute grads Attribute train_op", "label": "class Trainer(object):\n  def __init__(self, model, config):\n    self.config = config\n    self.model = model  # this is an model instance\n\n    self.global_step = model.global_step\n\n    learning_rate = config.init_lr\n\n    if config.learning_rate_decay is not None:\n      decay_steps = int(config.train_num_examples /\n                        config.batch_size * config.num_epoch_per_decay)\n\n      learning_rate = tf.train.exponential_decay(\n          config.init_lr,\n          self.global_step,\n          decay_steps,  # decay every k samples used in training\n          config.learning_rate_decay,\n          staircase=True)\n\n    if config.optimizer == 'momentum':\n      opt_emb = tf.train.MomentumOptimizer(\n          learning_rate*config.emb_lr, momentum=0.9)\n      opt_rest = tf.train.MomentumOptimizer(learning_rate, momentum=0.9)\n    elif config.optimizer == 'adadelta':\n      opt_emb = tf.train.AdadeltaOptimizer(learning_rate*config.emb_lr)\n      opt_rest = tf.train.AdadeltaOptimizer(learning_rate)\n    elif config.optimizer == 'adam':\n      opt_emb = tf.train.AdamOptimizer(learning_rate*config.emb_lr)\n      opt_rest = tf.train.AdamOptimizer(learning_rate)\n    else:\n      raise Exception('Optimizer not implemented')\n\n    # losses\n    self.xyloss = model.xyloss\n    self.loss = model.loss  # get the loss funcion\n\n    # valist for embding layer\n    var_emb = [var for var in tf.trainable_variables()\n               if 'emb' in var.name]\n    var_rest = [var for var in tf.trainable_variables()\n                if 'emb' not in var.name]\n\n    # for training, we get the gradients first, then apply them\n    self.grads = tf.gradients(self.loss, var_emb+var_rest)\n\n    if config.clip_gradient_norm is not None:\n      # pylint: disable=g-long-ternary\n      self.grads = [grad if grad is None else\n                    tf.clip_by_value(grad, -1*config.clip_gradient_norm,\n                                     config.clip_gradient_norm)\n                    for grad in self.grads]\n\n    grads_emb = self.grads[:len(var_emb)]\n    grads_rest = self.grads[len(var_emb):]\n\n    train_emb = opt_emb.apply_gradients(zip(grads_emb, var_emb))\n    train_rest = opt_rest.apply_gradients(\n        zip(grads_rest, var_rest), global_step=self.global_step)\n    self.train_op = tf.group(train_emb, train_rest)\n\n  def step(self, sess, batch):\n    config = self.config\n    # idxs is a tuple (23,123,33..) index for sample\n    _, batch_data = batch\n    feed_dict = self.model.get_feed_dict(batch_data, is_train=True)\n    act_loss = -1\n    grid_loss = -1\n    traj_class_loss = -1\n    inputs = [self.loss, self.train_op, self.xyloss]\n    num_out = 3\n    if config.add_activity:\n      inputs += [self.model.activity_loss]\n      num_out += 1\n    if config.multi_decoder:\n      inputs += [self.model.traj_class_loss]\n      num_out += 1\n    inputs += self.model.grid_loss\n\n    outputs = sess.run(inputs, feed_dict=feed_dict)\n\n    loss, train_op, xyloss = outputs[:3]\n\n    if config.add_activity:\n      act_loss = outputs[3]\n\n    if config.multi_decoder:\n      if config.add_activity:\n        traj_class_loss = outputs[4]\n      else:\n        traj_class_loss = outputs[3]\n\n    grid_loss = outputs[num_out:]\n\n    return loss, train_op, xyloss, act_loss, traj_class_loss, grid_loss"}
{"Repository": "Pix2NeRF", "input": "srnchairs Dataset className srnchairs(Dataset) Method __init__ Method __len__ Method __getitem__ Attribute data Attribute path Attribute data Attribute data Attribute data Attribute data Attribute data Attribute transform", "label": "class srnchairs(Dataset):\n    def __init__(self, dataset_path, img_size, split, **kwargs):\n        super().__init__()\n        self.data = []\n        self.path = os.path.join(dataset_path)\n        if split == 'train':\n            with open(os.path.join(dataset_path, 'srn_chairs_train_filted.csv'), newline='') as f:\n                reader = csv.reader(f)\n                train_data = list(reader)\n            print(train_data[0])\n            train_data = [os.path.join('chairs_train', x[0]) for x in train_data]\n            self.data = train_data\n        elif split == 'val':\n            with open(os.path.join(dataset_path, 'srn_chairs_val_filted.csv'), newline='') as f:\n                reader = csv.reader(f)\n                val_data = list(reader)\n            val_data = [os.path.join('chairs_val', x[0]) for x in val_data]\n            self.data = val_data\n        elif split == 'test':\n            with open(os.path.join(dataset_path, 'srn_chairs_test_filted.csv'), newline='') as f:\n                reader = csv.reader(f)\n                test_data = list(reader)\n            test_data = [os.path.join('chairs_test', x[0]) for x in test_data]\n            self.data = test_data\n        elif split == 'train+val':\n            with open(os.path.join(dataset_path, 'srn_chairs_train_filted.csv'), newline='') as f:\n                reader = csv.reader(f)\n                train_data = list(reader)\n            train_data = [os.path.join('chairs_train', x[0]) for x in train_data]\n            with open(os.path.join(dataset_path, 'srn_chairs_val_filted.csv'), newline='') as f:\n                reader = csv.reader(f)\n                val_data = list(reader)\n            val_data = [os.path.join('chairs_val', x[0]) for x in val_data]\n            self.data = train_data + val_data\n        elif split == 'val+test':\n            with open(os.path.join(dataset_path, 'srn_chairs_test_filted.csv'), newline='') as f:\n                reader = csv.reader(f)\n                test_data = list(reader)\n            test_data = [os.path.join('chairs_test', x[0]) for x in test_data]\n            with open(os.path.join(dataset_path, 'srn_chairs_val_filted.csv'), newline='') as f:\n                reader = csv.reader(f)\n                val_data = list(reader)\n            val_data = [os.path.join('chairs_val', x[0]) for x in val_data]\n            self.data = test_data + val_data\n        else:\n            raise RuntimeError(\"wrong split\")\n        random.shuffle(self.data)\n        print(len(self.data))\n        assert len(self.data) > 0, \"Can't find data; make sure you specify the path to your dataset\"\n        self.transform = transforms.Compose(\n                    [transforms.ToTensor(), transforms.Resize((img_size, img_size), interpolation=0), transforms.Normalize([0.5], [0.5])])\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, index):\n        X = imageio.imread(os.path.join(self.path, self.data[index]))[..., :3]\n        X = self.transform(X)\n\n        return X, 0"}
{"Repository": "Sorcar", "input": "Integration to Github API className GithubEngine(object) Method __init__ Method form_repo_url Method form_tags_url Method form_branch_list_url Method form_branch_url Method parse_tags Attribute api_url Attribute token Attribute name", "label": "class GithubEngine(object):\n\tdef __init__(self):\n\t\tself.api_url = 'https://api.github.com'\n\t\tself.token = None\n\t\tself.name = \"github\"\n\n\tdef form_repo_url(self, updater):\n\t\treturn \"{}{}{}{}{}\".format(self.api_url,\"/repos/\",updater.user,\n\t\t\t\t\t\t\t\t\"/\",updater.repo)\n\n\tdef form_tags_url(self, updater):\n\t\tif updater.use_releases:\n\t\t\treturn \"{}{}\".format(self.form_repo_url(updater),\"/releases\")\n\t\telse:\n\t\t\treturn \"{}{}\".format(self.form_repo_url(updater),\"/tags\")\n\n\tdef form_branch_list_url(self, updater):\n\t\treturn \"{}{}\".format(self.form_repo_url(updater),\"/branches\")\n\n\tdef form_branch_url(self, branch, updater):\n\t\treturn \"{}{}{}\".format(self.form_repo_url(updater),\n\t\t\t\t\t\t\t\"/zipball/\",branch)\n\n\tdef parse_tags(self, response, updater):\n\t\tif response == None:\n\t\t\treturn []\n\t\treturn response"}
{"Repository": "web2attack", "input": "Client Options as found in the MySQL sources mysql-src/include/mysql_com.h className ClientFlag(_constantflags) Method get_default", "label": "class ClientFlag(_constantflags):\n    LONG_PASSWD             = 1 << 0\n    FOUND_ROWS              = 1 << 1\n    LONG_FLAG               = 1 << 2\n    CONNECT_WITH_DB         = 1 << 3\n    NO_SCHEMA               = 1 << 4\n    COMPRESS                = 1 << 5\n    ODBC                    = 1 << 6\n    LOCAL_FILES             = 1 << 7\n    IGNORE_SPACE            = 1 << 8\n    PROTOCOL_41             = 1 << 9\n    INTERACTIVE             = 1 << 10\n    SSL                     = 1 << 11\n    IGNORE_SIGPIPE          = 1 << 12\n    TRANSACTIONS            = 1 << 13\n    RESERVED                = 1 << 14\n    SECURE_CONNECTION       = 1 << 15\n    MULTI_STATEMENTS        = 1 << 16\n    MULTI_RESULTS           = 1 << 17\n    SSL_VERIFY_SERVER_CERT  = 1 << 30\n    REMEMBER_OPTIONS        = 1 << 31\n    \n    desc = {\n        'LONG_PASSWD':        (1 <<  0, 'New more secure passwords'),\n        'FOUND_ROWS':         (1 <<  1, 'Found instead of affected rows'),\n        'LONG_FLAG':          (1 <<  2, 'Get all column flags'),\n        'CONNECT_WITH_DB':    (1 <<  3, 'One can specify db on connect'),\n        'NO_SCHEMA':          (1 <<  4, \"Don't allow database.table.column\"),\n        'COMPRESS':           (1 <<  5, 'Can use compression protocol'),\n        'ODBC':               (1 <<  6, 'ODBC client'),\n        'LOCAL_FILES':        (1 <<  7, 'Can use LOAD DATA LOCAL'),\n        'IGNORE_SPACE':       (1 <<  8, \"Ignore spaces before ''\"),\n        'PROTOCOL_41':        (1 <<  9, 'New 4.1 protocol'),\n        'INTERACTIVE':        (1 << 10, 'This is an interactive client'),\n        'SSL':                (1 << 11, 'Switch to SSL after handshake'),\n        'IGNORE_SIGPIPE':     (1 << 12, 'IGNORE sigpipes'),\n        'TRANSACTIONS':       (1 << 13, 'Client knows about transactions'),\n        'RESERVED':           (1 << 14, 'Old flag for 4.1 protocol'),\n        'SECURE_CONNECTION':  (1 << 15, 'New 4.1 authentication'),\n        'MULTI_STATEMENTS':   (1 << 16, 'Enable/disable multi-stmt support'),\n        'MULTI_RESULTS':      (1 << 17, 'Enable/disable multi-results'),\n        'SSL_VERIFY_SERVER_CERT':     (1 << 30, ''),\n        'REMEMBER_OPTIONS':           (1 << 31, ''),\n    }\n    \n    default = [\n        LONG_PASSWD,\n        LONG_FLAG,\n        CONNECT_WITH_DB,\n        PROTOCOL_41,\n        TRANSACTIONS,\n        SECURE_CONNECTION,\n        MULTI_STATEMENTS,\n        MULTI_RESULTS,\n    ]\n\n    @classmethod\n    def get_default(cls):\n        flags = 0\n        for f in cls.default:\n            flags |= f\n        return flags"}
{"Repository": "dgl-lifesci", "input": "Wrapper for optimization with multiprocess className MultiProcessOptimizer(Optimizer) Method __init__ Method _sync_gradient Method backward_and_step Attribute n_processes", "label": "class MultiProcessOptimizer(Optimizer):\n    def __init__(self, n_processes, lr, optimizer):\n        super(MultiProcessOptimizer, self).__init__(lr=lr, optimizer=optimizer)\n        self.n_processes = n_processes\n\n    def _sync_gradient(self):\n        for param_group in self.optimizer.param_groups:\n            for p in param_group['params']:\n                if p.requires_grad and p.grad is not None:\n                    dist.all_reduce(p.grad.data, op=dist.ReduceOp.SUM)\n                    p.grad.data /= self.n_processes\n\n    def backward_and_step(self, loss):\n        loss.backward()\n        self._sync_gradient()\n        self.optimizer.step()\n        self._reset()"}
{"Repository": "dygiepp", "input": "Computes precision, recall, and micro-averaged F1 from a list of predicted and gold spans. className RelationMetrics(Metric) Method __init__ Method __call__ Method get_metric Method reset", "label": "class RelationMetrics(Metric):\n    def __init__(self):\n        self.reset()\n\n    # TODO(dwadden) This requires decoding because the dataset reader gets rid of gold spans wider\n    # than the span width. So, I can't just compare the tensor of gold labels to the tensor of\n    # predicted labels.\n    @overrides\n    def __call__(self, predicted_relation_list, metadata_list):\n        for predicted_relations, metadata in zip(predicted_relation_list, metadata_list):\n            gold_relations = metadata.relation_dict\n            self._total_gold += len(gold_relations)\n            self._total_predicted += len(predicted_relations)\n            for (span_1, span_2), label in predicted_relations.items():\n                ix = (span_1, span_2)\n                if ix in gold_relations and gold_relations[ix] == label:\n                    self._total_matched += 1\n\n    @overrides\n    def get_metric(self, reset=False):\n        precision, recall, f1 = compute_f1(self._total_predicted, self._total_gold, self._total_matched)\n\n        # Reset counts if at end of epoch.\n        if reset:\n            self.reset()\n\n        return precision, recall, f1\n\n    @overrides\n    def reset(self):\n        self._total_gold = 0\n        self._total_predicted = 0\n        self._total_matched = 0"}
{"Repository": "minionn", "input": "The service definition for GRPC. className MinioNNServicer(object) Method __init__ Method Precomputation Method Computation Attribute model_client Attribute w_precomputed Attribute nodes Attribute ip Attribute mpc_port", "label": "class MinioNNServicer(object):\n  def __init__(self, model, w, nodes, ip, mpc_port):\n    self.model_client = model\n    self.w_precomputed = w\n    self.nodes = nodes\n    self.ip = ip\n    self.mpc_port = mpc_port\n    \n\n  def Precomputation(self, request, context):\n    logger.info(\"Got precomputation request. Responding...\")\n    return minionn_onnx_pb2.PrecomputationResponse(model=self.model_client, w=self.w_precomputed)\n\n  def Computation(self, request, context):\n    logger.info(\"Got computation request.\")\n    logger.debug(\"xs has length: \" + str((len(request.xs))))\n\n    logger.info(\"Opening MPC server port. Waiting for client to connect...\")\n    minionn_helper.init_mpc(self.ip, self.mpc_port, True)\n\n    # Perform last precomputation step on U\n    decU = minionn_helper.server_decrypt_u(request.u, config.server_skey)\n    logger.debug(\"U has length: \" + str(len(list(decU))) )\n\n    # Now system is ready to start NN\n    handler = operation_handler.OperationHandler(self.nodes, self.model_client.graph.input[0].name)\n    handler.init_server(decU)\n    result = handler.run_network(x_in = request.xs,\n        in_name = self.model_client.graph.input[0].name,\n        out_name = self.model_client.graph.output[0].name) \n\n    logger.info(\"Shutting down MPC server again.\")\n    minionn_helper.shutdown_mpc()\n\n    logger.info(\"Computation response:\" + str(result))\n\n    return minionn_onnx_pb2.ComputationResponse(ys=result)"}
{"Repository": "uncompyle2", "input": "Exception to be raised when consts differ. className CmpErrorConsts(VerifyCmpError) Method __init__ Method __str__ Attribute name Attribute index", "label": "class CmpErrorConsts(VerifyCmpError):\n    def __init__(self, name, index):\n        self.name = name\n        self.index = index\n\n    def __str__(self):\n        return 'Compare Error within Consts of %s at index %i' % \\\n               (repr(self.name), self.index)"}
{"Repository": "chinese-support-redux", "input": "MultiByteCharSetProber className MultiByteCharSetProber(CharSetProber) Method __init__ Method reset Method charset_name Method language Method feed Method get_confidence Attribute distribution_analyzer Attribute coding_sm Attribute _last_char", "label": "class MultiByteCharSetProber(CharSetProber):\n    def __init__(self, lang_filter=None):\n        super(MultiByteCharSetProber, self).__init__(lang_filter=lang_filter)\n        self.distribution_analyzer = None\n        self.coding_sm = None\n        self._last_char = [0, 0]\n\n    def reset(self):\n        super(MultiByteCharSetProber, self).reset()\n        if self.coding_sm:\n            self.coding_sm.reset()\n        if self.distribution_analyzer:\n            self.distribution_analyzer.reset()\n        self._last_char = [0, 0]\n\n    @property\n    def charset_name(self):\n        raise NotImplementedError\n\n    @property\n    def language(self):\n        raise NotImplementedError\n\n    def feed(self, byte_str):\n        for i in range(len(byte_str)):\n            coding_state = self.coding_sm.next_state(byte_str[i])\n            if coding_state == MachineState.ERROR:\n                self.logger.debug('%s %s prober hit error at byte %s',\n                                  self.charset_name, self.language, i)\n                self._state = ProbingState.NOT_ME\n                break\n            elif coding_state == MachineState.ITS_ME:\n                self._state = ProbingState.FOUND_IT\n                break\n            elif coding_state == MachineState.START:\n                char_len = self.coding_sm.get_current_charlen()\n                if i == 0:\n                    self._last_char[1] = byte_str[0]\n                    self.distribution_analyzer.feed(self._last_char, char_len)\n                else:\n                    self.distribution_analyzer.feed(byte_str[i - 1:i + 1],\n                                                    char_len)\n\n        self._last_char[0] = byte_str[-1]\n\n        if self.state == ProbingState.DETECTING:\n            if (self.distribution_analyzer.got_enough_data() and\n                    (self.get_confidence() > self.SHORTCUT_THRESHOLD)):\n                self._state = ProbingState.FOUND_IT\n\n        return self.state\n\n    def get_confidence(self):\n        return self.distribution_analyzer.get_confidence()"}
{"Repository": "turnstile", "input": "Stores configuration data. className Config(object) Method __init__ Method __getitem__ Method __contains__ Method __getattr__ Method get Method get_database Method to_bool Attribute _config", "label": "class Config(object):\n    def __init__(self, conf_dict=None, conf_file=None):\n        self._config = {\n            None: {\n                'status': '413 Request Entity Too Large',\n            },\n        }\n\n        # Handle passed-in dict (middleware)\n        if conf_dict:\n            for key, value in conf_dict.items():\n                outer, _sep, inner = key.partition('.')\n\n                # Deal with prefix-less keys\n                if not inner:\n                    outer, inner = None, outer\n\n                # Make sure we have a place to put them\n                self._config.setdefault(outer, {})\n                self._config[outer][inner] = value\n\n        conf_files = []\n\n        # Were we to look aside to a configuration file?\n        if 'config' in self._config[None]:\n            conf_files.append(self._config[None]['config'])\n\n        # Were we asked to load a specific file in addition?\n        if conf_file:\n            conf_files.append(conf_file)\n\n        # Parse configuration files\n        if conf_files:\n            cp = ConfigParser.SafeConfigParser()\n            cp.read(conf_files)\n\n            # Each section corresponds to a top-level in the config\n            for sect in cp.sections():\n                # Transform [turnstile] section\n                outer = None if sect == 'turnstile' else sect\n\n                self._config.setdefault(outer, {})\n\n                # Merge in the options from the section\n                self._config[outer].update(dict(cp.items(sect)))\n\n    def __getitem__(self, key):\n        return self._config.get(key, {})\n\n    def __contains__(self, key):\n        return key in self._config\n\n    def __getattr__(self, key):\n        try:\n            return self._config.get(None, {})[key]\n        except KeyError:\n            raise AttributeError('%r object has no attribute %r' %\n                                 (self.__class__.__name__, key))\n\n    def get(self, key, default=None):\n        return self._config.get(None, {}).get(key, default)\n\n    def get_database(self, override=None):\n        # Grab the database connection arguments\n        redis_args = self['redis']\n\n        # If we have an override, read some overrides from that\n        # section\n        if override:\n            redis_args = redis_args.copy()\n            for key, value in self[override].items():\n                if not key.startswith('redis.'):\n                    continue\n                key = key[len('redis.'):]\n                if value:\n                    redis_args[key] = value\n                else:\n                    redis_args.pop(key, None)\n\n        # Return the redis database connection\n        return database.initialize(redis_args)\n\n    @staticmethod\n    def to_bool(value, do_raise=True):\n        value = value.lower()\n\n        # Try it as an integer\n        if value.isdigit():\n            return bool(int(value))\n\n        # OK, check it against the true/false values...\n        if value in _str_true:\n            return True\n        elif value in _str_false:\n            return False\n\n        # Not recognized\n        if do_raise:\n            raise ValueError(\"invalid literal for to_bool(): %r\" % value)\n\n        return False"}
{"Repository": "thesis", "input": "A DataInlet represents an inlet with continuous, multi-channel data that should be plotted as multiple lines. className DataInlet(Inlet) Method __init__ Method pull Method pull_and_plot Attribute bufsize Attribute buffer Attribute curves Attribute curves", "label": "class DataInlet(Inlet):\n    dtypes = [[], np.float32, np.float64, None, np.int32, np.int16, np.int8, np.int64]\n\n    def __init__(self, info: pylsl.StreamInfo, plt: Optional[pg.PlotItem]):\n        super().__init__(info)\n        # calculate the size for our buffer, i.e. two times the displayed data\n        self.bufsize = (\n            2 * math.ceil(info.nominal_srate() * PLOT_DURATION),\n            info.channel_count(),\n        )\n        self.buffer = np.empty(self.bufsize, dtype=self.dtypes[info.channel_format()])\n        empty = np.array([])\n        # create one curve object for each channel/line that will handle displaying the data\n        self.curves = []\n        if plt:\n            self.curves = [\n                pg.PlotCurveItem(x=empty, y=empty, autoDownsample=True)\n                for _ in range(self.channel_count)\n            ]\n            for curve in self.curves:\n                plt.addItem(curve)\n\n    def pull(self, timeout=1.0):\n        # pull the data\n        samples, ts = self.inlet.pull_chunk(\n            timeout=timeout, max_samples=self.bufsize[0]\n        )\n        self.buffer = np.asarray(samples)\n        return samples, ts\n\n    def pull_and_plot(self, plot_time, plt):\n        samples, ts = self.pull(timeout=0)\n        # ts will be empty if no samples were pulled, a list of timestamps otherwise\n        if plt and ts:\n            ts = np.asarray(ts)\n            y = self.buffer[0 : ts.size, :]\n            this_x = None\n            old_offset = 0\n            new_offset = 0\n            for ch_ix in range(self.channel_count):\n                # we don't pull an entire screen's worth of data, so we have to\n                # trim the old data and append the new data to it\n                old_x, old_y = self.curves[ch_ix].getData()\n                # the timestamps are identical for all channels, so we need to do\n                # this calculation only once\n                if ch_ix == 0:\n                    # find the index of the first sample that's still visible,\n                    # i.e. newer than the left border of the plot\n                    old_offset = old_x.searchsorted(plot_time)\n                    # same for the new data, in case we pulled more data than\n                    # can be shown at once\n                    new_offset = ts.searchsorted(plot_time)\n                    # append new timestamps to the trimmed old timestamps\n                    this_x = np.hstack((old_x[old_offset:], ts[new_offset:]))\n                # append new data to the trimmed old data\n                this_y = np.hstack((old_y[old_offset:], y[new_offset:, ch_ix] - ch_ix))\n                # replace the old data\n                self.curves[ch_ix].setData(this_x, this_y)"}
{"Repository": "actinia-core", "input": "This class tests the api logging interface className ApiLoggingTestCase(ActiniaResourceTestCaseBase) Method setUp Method tearDown Method test_logging", "label": "class ApiLoggingTestCase(ActiniaResourceTestCaseBase):\n    def setUp(self):\n        # We need to set the application context\n        self.app_context = flask_app.app_context()\n        self.app_context.push()\n        # The test user\n        self.user_id = \"soeren\"\n        self.log = ApiLogger()\n        self.request_object = DummyHTTPRequest()\n\n    def tearDown(self):\n        self.app_context.pop()\n\n    def test_logging(self):\n        ret = self.log.add_entry(\n            user_id=self.user_id, http_request=self.request_object\n        )\n\n        self.assertTrue(ret, \"add_entry does not work\")\n\n        ret = self.log.add_entry(\n            user_id=self.user_id, http_request=self.request_object\n        )\n\n        self.assertTrue(ret, \"add_entry does not work\")\n\n        ret = self.log.add_entry(\n            user_id=self.user_id, http_request=self.request_object\n        )\n\n        self.assertTrue(ret, \"add_entry does not work\")\n\n        size = self.log.size(self.user_id)\n\n        self.assertEqual(size, 3, \"The size method does not work %i\" % size)\n\n        list_of_logs = self.log.list(self.user_id, start=0, end=-1)\n\n        for entry in list_of_logs:\n            self.assertEqual(entry[\"api_info\"][\"endpoint\"], \"endpoint\")\n            self.assertEqual(entry[\"api_info\"][\"method\"], \"GET\")\n\n        # Remove the first entry\n        self.log.trim(self.user_id, 1, -1)\n\n        size = self.log.size(self.user_id)\n\n        self.assertEqual(size, 2, \"The size method does not work %i\" % size)\n\n        list_of_logs = self.log.list(self.user_id, start=0, end=-1)\n\n        for entry in list_of_logs:\n            self.assertEqual(entry[\"api_info\"][\"endpoint\"], \"endpoint\")\n            self.assertEqual(entry[\"api_info\"][\"method\"], \"GET\")\n\n        self.assertTrue(self.log.delete(self.user_id))"}
{"Repository": "SLIP", "input": "Computes and stores the average and current value className AverageMeter(object) Method __init__ Method reset Method update Method __str__ Attribute name Attribute fmt", "label": "class AverageMeter(object):\n    def __init__(self, name, fmt=':f'):\n        self.name = name\n        self.fmt = fmt\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count\n\n    def __str__(self):\n        fmtstr = '{name} {val' + self.fmt + '} ({avg' + self.fmt + '})'\n        return fmtstr.format(**self.__dict__)"}
{"Repository": "scikit-tensor", "input": "Base tensor class from which all tensor classes are subclasses. className tensor_mixin(object) Method ttm Method ttv Method _ttm_compute Method _ttv_compute Method unfold Method uttkrp Method transpose", "label": "class tensor_mixin(object):\n    __metaclass__ = ABCMeta\n\n    def ttm(self, V, mode=None, transp=False, without=False):\n        if mode is None:\n            mode = range(self.ndim)\n        if isinstance(V, np.ndarray):\n            Y = self._ttm_compute(V, mode, transp)\n        elif is_sequence(V):\n            dims, vidx = check_multiplication_dims(mode, self.ndim, len(V), vidx=True, without=without)\n            Y = self._ttm_compute(V[vidx[0]], dims[0], transp)\n            for i in xrange(1, len(dims)):\n                Y = Y._ttm_compute(V[vidx[i]], dims[i], transp)\n        return Y\n\n    def ttv(self, v, modes=[], without=False):\n        if not isinstance(v, tuple):\n            v = (v, )\n        dims, vidx = check_multiplication_dims(modes, self.ndim, len(v), vidx=True, without=without)\n        for i in range(len(dims)):\n            if not len(v[vidx[i]]) == self.shape[dims[i]]:\n                raise ValueError('Multiplicant is wrong size')\n        remdims = np.setdiff1d(range(self.ndim), dims)\n        return self._ttv_compute(v, dims, vidx, remdims)\n\n    #@abstractmethod\n    #def ttt(self, other, modes=None):\n    #    pass\n\n    @abstractmethod\n    def _ttm_compute(self, V, mode, transp):\n        pass\n\n    @abstractmethod\n    def _ttv_compute(self, v, dims, vidx, remdims):\n        pass\n\n    @abstractmethod\n    def unfold(self, rdims, cdims=None, transp=False):\n        pass\n\n    @abstractmethod\n    def uttkrp(self, U, mode):\n        pass\n\n    @abstractmethod\n    def transpose(self, axes=None):\n        pass"}
{"Repository": "DAMO-ConvAI", "input": "Configuration for `BertModel`. className BertConfig(object) Method from_dict Method from_json_file Method to_dict Method to_json_string", "label": "class BertConfig(object):\n    def __init__(self,\n                 vocab_size,\n                 hidden_size=768,\n                 num_hidden_layers=12,\n                 num_attention_heads=12,\n                 intermediate_size=3072,\n                 hidden_act=\"gelu\",\n                 hidden_dropout_prob=0.1,\n                 attention_probs_dropout_prob=0.1,\n                 max_position_embeddings=512,\n                 type_vocab_size=16,\n                 initializer_range=0.02):\n        self.vocab_size = vocab_size\n        self.hidden_size = hidden_size\n        self.num_hidden_layers = num_hidden_layers\n        self.num_attention_heads = num_attention_heads\n        self.hidden_act = hidden_act\n        self.intermediate_size = intermediate_size\n        self.hidden_dropout_prob = hidden_dropout_prob\n        self.attention_probs_dropout_prob = attention_probs_dropout_prob\n        self.max_position_embeddings = max_position_embeddings\n        self.type_vocab_size = type_vocab_size\n        self.initializer_range = initializer_range\n\n    @classmethod\n    def from_dict(cls, json_object):\n        config = BertConfig(vocab_size=None)\n        for (key, value) in six.iteritems(json_object):\n            config.__dict__[key] = value\n        return config\n\n    @classmethod\n    def from_json_file(cls, json_file):\n        with tf.gfile.GFile(json_file, \"r\") as reader:\n            text = reader.read()\n        return cls.from_dict(json.loads(text))\n\n    def to_dict(self):\n        output = copy.deepcopy(self.__dict__)\n        return output\n\n    def to_json_string(self):\n        return json.dumps(self.to_dict(), indent=2, sort_keys=True) + \"\\n\""}
{"Repository": "fairseq", "input": "Source: https://stackoverflow.com/a/54128391 className NumpyExtension(Extension) Method __init__ Method include_dirs Method include_dirs Attribute __include_dirs", "label": "class NumpyExtension(Extension):\n    def __init__(self, *args, **kwargs):\n        self.__include_dirs = []\n        super().__init__(*args, **kwargs)\n\n    @property\n    def include_dirs(self):\n        import numpy\n\n        return self.__include_dirs + [numpy.get_include()]\n\n    @include_dirs.setter\n    def include_dirs(self, dirs):\n        self.__include_dirs = dirs"}
{"Repository": "M2Det", "input": "A facility for config and config files. className Config(object) Method fromfile Method auto_argparser Method __init__ Method filename Method text Method __repr__ Method __len__ Method __getattr__ Method __getitem__ Method __setattr__ Method __setitem__ Method __iter__", "label": "class Config(object):\n    @staticmethod\n\n    def fromfile(filename):\n\n        filename = osp.abspath(osp.expanduser(filename))\n\n        check_file_exist(filename)\n\n        if filename.endswith('.py'):\n\n            module_name = osp.basename(filename)[:-3]\n\n            if '.' in module_name:\n\n                raise ValueError('Dots are not allowed in config file path.')\n\n            config_dir = osp.dirname(filename)\n\n            sys.path.insert(0, config_dir)\n\n            mod = import_module(module_name)\n\n            sys.path.pop(0)\n\n            cfg_dict = {\n\n                name: value\n\n                for name, value in mod.__dict__.items()\n\n                if not name.startswith('__')\n\n            }\n\n        elif filename.endswith(('.yaml', '.json')):\n\n            import mmcv\n\n            cfg_dict = mmcv.load(filename)\n\n        else:\n\n            raise IOError('Only py/yaml/json type are supported now!')\n\n        return Config(cfg_dict, filename=filename)\n\n\n\n    @staticmethod\n\n    def auto_argparser(description=None):\n        partial_parser = ArgumentParser(description=description)\n\n        partial_parser.add_argument('config', help='config file path')\n\n        cfg_file = partial_parser.parse_known_args()[0].config\n\n        cfg = Config.from_file(cfg_file)\n\n        parser = ArgumentParser(description=description)\n\n        parser.add_argument('config', help='config file path')\n\n        add_args(parser, cfg)\n\n        return parser, cfg\n\n\n\n    def __init__(self, cfg_dict=None, filename=None):\n\n        if cfg_dict is None:\n\n            cfg_dict = dict()\n\n        elif not isinstance(cfg_dict, dict):\n\n            raise TypeError('cfg_dict must be a dict, but got {}'.format(\n\n                type(cfg_dict)))\n\n\n\n        super(Config, self).__setattr__('_cfg_dict', ConfigDict(cfg_dict))\n\n        super(Config, self).__setattr__('_filename', filename)\n\n        if filename:\n\n            with open(filename, 'r') as f:\n\n                super(Config, self).__setattr__('_text', f.read())\n\n        else:\n\n            super(Config, self).__setattr__('_text', '')\n\n\n\n    @property\n\n    def filename(self):\n\n        return self._filename\n\n\n\n    @property\n\n    def text(self):\n\n        return self._text\n\n\n\n    def __repr__(self):\n\n        return 'Config (path: {}): {}'.format(self.filename,\n\n                                              self._cfg_dict.__repr__())\n\n\n\n    def __len__(self):\n\n        return len(self._cfg_dict)\n\n\n\n    def __getattr__(self, name):\n\n        return getattr(self._cfg_dict, name)\n\n\n\n    def __getitem__(self, name):\n\n        return self._cfg_dict.__getitem__(name)\n\n\n\n    def __setattr__(self, name, value):\n\n        if isinstance(value, dict):\n\n            value = ConfigDict(value)\n\n        self._cfg_dict.__setattr__(name, value)\n\n\n\n    def __setitem__(self, name, value):\n\n        if isinstance(value, dict):\n\n            value = ConfigDict(value)\n\n        self._cfg_dict.__setitem__(name, value)\n\n\n\n    def __iter__(self):\n\n        return iter(self._cfg_dict)"}
{"Repository": "otrace", "input": "Class to execute code interactively using argument values as the local context. className TraceInterpreter(object) Method __init__ Method evaluate Method exec_source Attribute compile", "label": "class TraceInterpreter(object):\n    def __init__(self):\n        self.compile = codeop.CommandCompiler()\n\n    def evaluate(self, expression, locals_dict={}, globals_dict={}, print_out=False):\n        _stdout = StringIO.StringIO()\n        _stderr = StringIO.StringIO()\n        locals_dict[\"_stdout\"] = _stdout\n        locals_dict[\"_stderr\"] = _stderr\n\n        prefix = \"print >>_stdout, \" if print_out else \"\"\n        result = self.exec_source(prefix+expression, locals_dict=locals_dict, globals_dict=globals_dict)\n\n        del locals_dict[\"_stdout\"]\n        del locals_dict[\"_stderr\"]\n\n        out_str = _stdout.getvalue()\n        err_str = _stderr.getvalue()\n\n        _stdout.close()\n        _stderr.close()\n\n        if result is None:\n            return (\"<Incomplete Expression>\", None)\n\n        return (out_str, err_str + result)\n\n    def exec_source(self, source, filename=\"<input>\", symbol=\"single\", locals_dict={}, globals_dict={}):\n        try:\n            # Compile code\n            code = self.compile(source, filename, symbol)\n        except (OverflowError, SyntaxError, ValueError):\n            # Syntax error\n            etype, value, last_traceback = sys.exc_info()\n            return \".\".join(traceback.format_exception_only(etype, value))\n\n        if code is None:\n            # Incomplete code\n            return None\n\n        try:\n            if globals_dict is locals_dict:\n                exec code in locals_dict\n            else:\n                exec code in globals_dict, locals_dict\n            return \"\"     # Successful execution\n        except SystemExit:\n            raise\n        except Exception:\n            return format_traceback()    # Error in execution"}
{"Repository": "libfacedetection.train", "input": "Legacy Delta XYWH BBox coder used in MMDet V1. className LegacyDeltaXYWHBBoxCoder(BaseBBoxCoder) Method encode", "label": "class LegacyDeltaXYWHBBoxCoder(BaseBBoxCoder):\n    def __init__(self,\n                 target_means=(0., 0., 0., 0.),\n                 target_stds=(1., 1., 1., 1.)):\n        super(BaseBBoxCoder, self).__init__()\n        self.means = target_means\n        self.stds = target_stds\n\n    def encode(self, bboxes, gt_bboxes):\n        assert bboxes.size(0) == gt_bboxes.size(0)\n        assert bboxes.size(-1) == gt_bboxes.size(-1) == 4\n        encoded_bboxes = legacy_bbox2delta(bboxes, gt_bboxes, self.means,\n                                           self.stds)\n        return encoded_bboxes\n\n    def decode(self,\n               bboxes,\n               pred_bboxes,\n               max_shape=None,\n               wh_ratio_clip=16 / 1000):\n        assert pred_bboxes.size(0) == bboxes.size(0)\n        decoded_bboxes = legacy_delta2bbox(bboxes, pred_bboxes, self.means,\n                                           self.stds, max_shape, wh_ratio_clip)\n\n        return decoded_bboxes"}
{"Repository": "Soundcloud-Downloader", "input": "Support setup.py upload. className UploadCommand(Command) Method status Method initialize_options Method finalize_options Method run", "label": "class UploadCommand(Command):\n    description = 'Build and publish the package.'\n    user_options = []\n\n    @staticmethod\n    def status(s):\n        print('\\033[1m{0}\\033[0m'.format(s))\n\n    def initialize_options(self):\n        pass\n\n    def finalize_options(self):\n        pass\n\n    def run(self):\n        try:\n            self.status('Removing previous builds')\n            rmtree(os.path.join(here, 'dist'))\n        except OSError:\n            pass\n\n        self.status('Building Source and Wheel (universal) distribution')\n        os.system('{0} setup.py sdist bdist_wheel --universal'.format(sys.executable))\n\n        self.status('Uploading the package to PyPI via Twine')\n        os.system('twine upload dist/*')\n\n        self.status('Pushing git tags')\n        os.system('git tag v{0}'.format(about['__version__']))\n        os.system('git push --tags')\n        \n        sys.exit()"}
{"Repository": "StyleFlow", "input": "Convenience class that behaves like a dict but allows access with the attribute syntax. className EasyDict(dict) Method __getattr__ Method __setattr__ Method __delattr__", "label": "class EasyDict(dict):\n    def __getattr__(self, name: str) -> Any:\n        try:\n            return self[name]\n        except KeyError:\n            raise AttributeError(name)\n\n    def __setattr__(self, name: str, value: Any) -> None:\n        self[name] = value\n\n    def __delattr__(self, name: str) -> None:\n        del self[name]"}
{"Repository": "django-localized-fields", "input": "Tests whether expressions properly work with :see:LocalizedField. className LocalizedExpressionsTestCase(TestCase) Method setUpClass Method test_localized_ref Method create_queryset", "label": "class LocalizedExpressionsTestCase(TestCase):\n    TestModel1 = None\n    TestModel2 = None\n\n    @classmethod\n    def setUpClass(cls):\n        super(LocalizedExpressionsTestCase, cls).setUpClass()\n\n        cls.TestModel1 = get_fake_model(\n            {\"name\": models.CharField(null=False, blank=False, max_length=255)}\n        )\n\n        cls.TestModel2 = get_fake_model(\n            {\n                \"text\": LocalizedField(),\n                \"other\": models.ForeignKey(\n                    cls.TestModel1,\n                    related_name=\"features\",\n                    on_delete=models.CASCADE,\n                ),\n            }\n        )\n\n    @classmethod\n    def test_localized_ref(cls):\n        obj = cls.TestModel1.objects.create(name=\"bla bla\")\n        for i in range(0, 10):\n            cls.TestModel2.objects.create(\n                text=LocalizedValue(\n                    dict(\n                        en=\"text_%d_en\" % i,\n                        ro=\"text_%d_ro\" % i,\n                        nl=\"text_%d_nl\" % i,\n                    )\n                ),\n                other=obj,\n            )\n\n        def create_queryset(ref):\n            return cls.TestModel1.objects.annotate(mytexts=ref).values_list(\n                \"mytexts\", flat=True\n            )\n\n        # assert that it properly selects the currently active language\n        for lang_code, _ in settings.LANGUAGES:\n            translation.activate(lang_code)\n            queryset = create_queryset(LocalizedRef(\"features__text\"))\n\n            for index, value in enumerate(queryset):\n                assert translation.get_language() in value\n                assert str(index) in value\n\n        # ensure that the default language is used in case no\n        # language is active at all\n        translation.deactivate_all()\n        queryset = create_queryset(LocalizedRef(\"features__text\"))\n        for index, value in enumerate(queryset):\n            assert settings.LANGUAGE_CODE in value\n            assert str(index) in value\n\n        # ensures that overriding the language works properly\n        queryset = create_queryset(LocalizedRef(\"features__text\", \"ro\"))\n        for index, value in enumerate(queryset):\n            assert \"ro\" in value\n            assert str(index) in value\n\n        # ensures that using this in combination with ArrayAgg works properly\n        queryset = create_queryset(\n            ArrayAgg(LocalizedRef(\"features__text\", \"ro\"))\n        ).first()\n        assert isinstance(queryset, list)\n        for value in queryset:\n            assert \"ro\" in value"}
{"Repository": "pytest-watch", "input": "Listens for changes to a single file and re-runs tests after each change. className EventSingleFileListener(FileSystemEventHandler) Method __init__ Method on_any_event Attribute event_queue Attribute path", "label": "class EventSingleFileListener(FileSystemEventHandler):\n    def __init__(self, path, event_queue=None):\n        super(EventSingleFileListener, self).__init__()\n        self.event_queue = event_queue or Queue()\n        self.path = path\n\n    def on_any_event(self, event):\n        # Filter for allowed event types\n        if not isinstance(event, WATCHED_EVENTS):\n            return\n\n        dest_path = None\n        if isinstance(event, FileMovedEvent):\n            dest_path = os.path.relpath(event.dest_path)\n        src_path = os.path.relpath(event.src_path)\n\n        # Filter everything but our specific file\n        if os.path.abspath(src_path) != self.path:\n            return\n        if dest_path and os.path.abspath(dest_path) != self.path:\n            return\n\n        self.event_queue.put((type(event), src_path, dest_path))"}
{"Repository": "respeaker_python_library", "input": "This class provides basic functions to access className PyWinUSB(Interface) Method __init__ Method rx_handler Method open Method getAllConnectedInterface Method write Method read Method setPacketCount Method getSerialNumber Method close Attribute report Attribute rcv_data Attribute device", "label": "class PyWinUSB(Interface):\n    vid = 0\n    pid = 0\n\n    isAvailable = isAvailable\n\n    def __init__(self):\n        super(PyWinUSB, self).__init__()\n        # Vendor page and usage_id = 2\n        self.report = []\n        # deque used here instead of synchronized Queue\n        # since read speeds are ~10-30% faster and are\n        # comprable to a based list implmentation.\n        self.rcv_data = collections.deque()\n        self.device = None\n        return\n\n    # handler called when a report is received\n    def rx_handler(self, data):\n        #logging.debug(\"rcv: %s\", data[1:])\n        self.rcv_data.append(data[1:])\n\n    def open(self):\n        self.device.set_raw_data_handler(self.rx_handler)\n        self.device.open(shared=False)\n\n    @staticmethod\n    def getAllConnectedInterface():\n        all_devices = hid.find_all_hid_devices()\n\n        # find devices with good vid/pid\n        all_mbed_devices = []\n        for d in all_devices:\n            if (d.product_name.find(\"MicArray\") >= 0):\n                all_mbed_devices.append(d)\n\n        boards = []\n        for dev in all_mbed_devices:\n            try:\n                dev.open(shared=False)\n                report = dev.find_output_reports()\n                if (len(report) == 1):\n                    new_board = PyWinUSB()\n                    new_board.report = report[0]\n                    new_board.vendor_name = dev.vendor_name\n                    new_board.product_name = dev.product_name\n                    new_board.serial_number = dev.serial_number\n                    new_board.vid = dev.vendor_id\n                    new_board.pid = dev.product_id\n                    new_board.device = dev\n                    new_board.device.set_raw_data_handler(new_board.rx_handler)\n\n                    boards.append(new_board)\n            except Exception as e:\n                logging.error(\"Receiving Exception: %s\", e)\n                dev.close()\n\n        return boards\n\n    def write(self, data):\n        for _ in range(64 - len(data)):\n            data.append(0)\n        #logging.debug(\"send: %s\", data)\n        self.report.send(bytearray([0]) + data)\n        return\n\n\n    def read(self, timeout=1.0):\n        start = time()\n        while len(self.rcv_data) == 0:\n            if time() - start > timeout:\n                # Read operations should typically take ~1-2ms.\n                # If this exception occurs, then it could indicate\n                # a problem in one of the following areas:\n                # 1. Bad usb driver causing either a dropped read or write\n                # 2. CMSIS-DAP firmware problem cause a dropped read or write\n                # 3. CMSIS-DAP is performing a long operation or is being\n                #    halted in a debugger\n                raise Exception(\"Read timed out\")\n        return self.rcv_data.popleft()\n\n    def setPacketCount(self, count):\n        # No interface level restrictions on count\n        self.packet_count = count\n\n    def getSerialNumber(self):\n        return self.serial_number\n\n    def close(self):\n        logging.debug(\"closing interface\")\n        self.device.close()"}
{"Repository": "medicare_locator", "input": "Custom form action to fill all slots required to find specific type of healthcare facilities in a certain city or zip code. className FacilityForm(FormAction) Method name Method required_slots Method slot_mappings", "label": "class FacilityForm(FormAction):\n    def name(self) -> Text:\n        return \"facility_form\"\n\n    @staticmethod\n    def required_slots(tracker: Tracker) -> List[Text]:\n        return [\"facility_type\", \"location\"]\n\n    def slot_mappings(self) -> Dict[Text, Any]:\n        return {\"facility_type\": self.from_entity(entity=\"facility_type\",\n                                                  intent=[\"inform\",\n                                                          \"search_provider\"]),\n                \"location\": self.from_entity(entity=\"location\",\n                                             intent=[\"inform\",\n                                                     \"search_provider\"])}\n\n    def submit(self,\n               dispatcher: CollectingDispatcher,\n               tracker: Tracker,\n               domain: Dict[Text, Any]\n               ) -> List[Dict]:\n        location = tracker.get_slot('location')\n        facility_type = tracker.get_slot('facility_type')\n\n        results = _find_facilities(location, facility_type)\n        button_name = _resolve_name(FACILITY_TYPES, facility_type)\n        if len(results) == 0:\n            dispatcher.utter_message(\n                \"Sorry, we could not find a {} in {}.\".format(button_name,\n                                                              location.title()))\n            return []\n\n        buttons = []\n        # limit number of results to 3 for clear presentation purposes\n        for r in results[:3]:\n            if facility_type == FACILITY_TYPES[\"hospital\"][\"resource\"]:\n                facility_id = r.get(\"provider_id\")\n                name = r[\"hospital_name\"]\n            elif facility_type == FACILITY_TYPES[\"nursing_home\"][\"resource\"]:\n                facility_id = r[\"federal_provider_number\"]\n                name = r[\"provider_name\"]\n            else:\n                facility_id = r[\"provider_number\"]\n                name = r[\"provider_name\"]\n\n            payload = \"/inform{\\\"facility_id\\\":\\\"\" + facility_id + \"\\\"}\"\n            buttons.append(\n                {\"title\": \"{}\".format(name.title()), \"payload\": payload})\n\n        if len(buttons) == 1:\n            message = \"Here is a {} near you:\".format(button_name)\n        else:\n            if button_name == \"home health agency\":\n                button_name = \"home health agencie\"\n            message = \"Here are {} {}s near you:\".format(len(buttons),\n                                                         button_name)\n\n        # TODO: update rasa core version for configurable `button_type`\n        dispatcher.utter_button_message(message, buttons)\n\n        return []"}
{"Repository": "napalm-logs", "input": "publisher sub-process class. className NapalmLogsListenerProc(NapalmLogsProc) Method _exit_gracefully Method _setup_listener Method _setup_ipc Method start Method stop", "label": "class NapalmLogsListenerProc(NapalmLogsProc):\n    def __init__(\n        self,\n        opts,\n        address,\n        port,\n        listener_type,\n        # pipe,\n        listener_opts=None,\n    ):\n        self.__up = False\n        self.opts = opts\n        self.address = address\n        self.port = port\n        # self.pipe = pipe\n        self._listener_type = listener_type\n        self.listener_opts = {} or listener_opts\n\n    def _exit_gracefully(self, signum, _):\n        log.debug(\"Caught signal in the listener process\")\n        self.stop()\n\n    def _setup_listener(self):\n        listener_class = get_listener(self._listener_type)\n        self.address = self.listener_opts.pop(\"address\", self.address)\n        self.port = self.listener_opts.pop(\"port\", self.port)\n        self.listener = listener_class(self.address, self.port, **self.listener_opts)\n\n    def _setup_ipc(self):\n        log.debug(\"Setting up the listener IPC pusher\")\n        self.ctx = zmq.Context()\n        self.pub = self.ctx.socket(zmq.PUSH)\n        self.pub.connect(LST_IPC_URL)\n        log.debug(\"Setting HWM for the listener: %d\", self.opts[\"hwm\"])\n        self.pub.setsockopt(zmq.SNDHWM, self.opts[\"hwm\"])\n\n    def start(self):\n        # counter metrics for messages\n        c_logs_ingested = Counter(\n            \"napalm_logs_listener_logs_ingested\",\n            \"Count of ingested log messages\",\n            [\"listener_type\", \"address\", \"port\"],\n        )\n        c_messages_published = Counter(\n            \"napalm_logs_listener_messages_published\",\n            \"Count of published messages\",\n            [\"listener_type\", \"address\", \"port\"],\n        )\n        self._setup_ipc()\n        log.debug(\"Using the %s listener\", self._listener_type)\n        self._setup_listener()\n        self.listener.start()\n        # Start suicide polling thread\n        thread = threading.Thread(\n            target=self._suicide_when_without_parent, args=(os.getppid(),)\n        )\n        thread.start()\n        signal.signal(signal.SIGTERM, self._exit_gracefully)\n        self.__up = True\n        while self.__up:\n            try:\n                log_message, log_source = self.listener.receive()\n            except ListenerException as lerr:\n                if self.__up is False:\n                    log.info(\"Exiting on process shutdown\")\n                    return\n                else:\n                    log.error(lerr, exc_info=True)\n                    raise NapalmLogsExit(lerr)\n            log.debug(\n                \"Received %s from %s. Queueing to the server.\", log_message, log_source\n            )\n            if not log_message:\n                log.info(\n                    \"Empty message received from %s. Not queueing to the server.\",\n                    log_source,\n                )\n                continue\n            c_logs_ingested.labels(\n                listener_type=self._listener_type, address=self.address, port=self.port\n            ).inc()\n            self.pub.send(umsgpack.packb((log_message, log_source)))\n            c_messages_published.labels(\n                listener_type=self._listener_type, address=self.address, port=self.port\n            ).inc()\n\n    def stop(self):\n        log.info(\"Stopping the listener process\")\n        self.__up = False\n        self.pub.close()\n        self.ctx.term()\n        # self.pipe.close()\n        self.listener.stop()"}
{"Repository": "bio-lm", "input": "Processor for the HOC data set className MedNLIProcessor(DataProcessor) Method get_example_from_tensor_dict Method _read_jsonl Method get_train_examples Method get_dev_examples Method get_test_examples Method get_labels Method _create_examples", "label": "class MedNLIProcessor(DataProcessor):\n    def get_example_from_tensor_dict(self, tensor_dict):\n        return InputExample(\n            tensor_dict[\"idx\"].numpy(),\n            tensor_dict[\"sentence1\"].numpy().decode(\"utf-8\"),\n            tensor_dict[\"sentence2\"].numpy().decode(\"utf-8\"),\n            str(tensor_dict[\"label\"].numpy()),\n        )\n\n    def _read_jsonl(self, fi):\n        dps = []\n        for line in open(fi):\n            dps.append(json.loads(line))\n        return dps\n\n    def get_train_examples(self, data_dir):\n        return self._create_examples(\n            self._read_jsonl(os.path.join(data_dir, \"mli_train_v1.jsonl\")), 'train')\n\n    def get_dev_examples(self, data_dir):\n        return self._create_examples(\n            self._read_jsonl(os.path.join(data_dir, \"mli_dev_v1.jsonl\")), 'dev')\n\n    def get_test_examples(self, data_dir):\n        return self._create_examples(\n            self._read_jsonl(os.path.join(data_dir, \"mli_test_v1.jsonl\")), 'test')\n\n    def get_labels(self):\n        return [\"entailment\", 'neutral', \"contradiction\"]\n\n    def _create_examples(self, items, set_type):\n        examples = []\n        for item in items:\n            guid = set_type + '-' + item['pairID']\n            text_a = item['sentence1']\n            text_b =item['sentence2']\n            label = item['gold_label']\n            examples.append(InputExample(guid=guid, text_a=text_a, text_b=text_b, label=label))\n        return examples"}
{"Repository": "trunk", "input": "Raised when > 1 device is attached and no device serial was given. className MultipleDevicesError(Exception) Method CheckError", "label": "class MultipleDevicesError(Exception):\n  @staticmethod\n  def CheckError(s):\n    return re.search(\"more than one (device and emulator|device|emulator)\", s)"}
{"Repository": "quix-streams", "input": "Serialize data to JSON formatted according to Quix EventData format. className QuixEventsSerializer(QuixSerializer) Method __call__", "label": "class QuixEventsSerializer(QuixSerializer):\n    # Quix EventData data must have the following headers set\n    _extra_headers = {\n        QModelKey.HEADER_NAME: QModelKey.EVENTDATA,\n        QCodecId.HEADER_NAME: QCodecId.JSON_TYPED,\n    }\n    _legacy = {\n        QModelKey.HEADER_NAME: QModelKey.EVENTDATA,\n        QCodecId.HEADER_NAME: QCodecId.JSON_TYPED,\n    }\n\n    def __call__(self, value: Mapping, ctx: SerializationContext) -> Union[str, bytes]:\n        if not isinstance(value, Mapping):\n            raise SerializationError(f\"Expected Mapping, got {type(value)}\")\n\n        event_value = value.get(\"Value\", \"\")\n        if not isinstance(event_value, str):\n            raise SerializationError(\n                f'Field \"Value\" is expected to be of type \"str\", got {type(event_value)}'\n            )\n\n        event_id = value.get(\"Id\", \"\")\n        if not isinstance(event_id, str):\n            raise SerializationError(\n                f'Field \"Id\" is expected to be of type \"str\", got {type(event_id)}'\n            )\n        tags = value.get(\"Tags\") or {}\n        if not isinstance(tags, dict):\n            raise SerializationError(\n                f'Field \"Tags\" is expected to be of type \"dict\", got {type(tags)}'\n            )\n\n        result = {\n            \"Id\": event_id,\n            \"Value\": event_value,\n            \"Tags\": tags,\n        }\n\n        try:\n            result[Q_TIMESTAMP_KEY] = value[Q_TIMESTAMP_KEY]\n        except KeyError:\n            raise SerializationError(\n                f\"Missing required Quix field: '{Q_TIMESTAMP_KEY}'\"\n            )\n\n        return self._to_json(result)"}
{"Repository": "oauth2client", "input": "Signs messages with a private key. className RsaSigner(object) Method __init__ Method sign Method from_string Attribute _key", "label": "class RsaSigner(object):\n    def __init__(self, pkey):\n        self._key = pkey\n\n    def sign(self, message):\n        message = _helpers._to_bytes(message, encoding='utf-8')\n        return rsa.pkcs1.sign(message, self._key, 'SHA-256')\n\n    @classmethod\n    def from_string(cls, key, password='notasecret'):\n        key = _helpers._from_bytes(key)  # pem expects str in Py3\n        marker_id, key_bytes = pem.readPemBlocksFromFile(\n            six.StringIO(key), _PKCS1_MARKER, _PKCS8_MARKER)\n\n        if marker_id == 0:\n            pkey = rsa.key.PrivateKey.load_pkcs1(key_bytes,\n                                                 format='DER')\n        elif marker_id == 1:\n            key_info, remaining = decoder.decode(\n                key_bytes, asn1Spec=_PKCS8_SPEC)\n            if remaining != b'':\n                raise ValueError('Unused bytes', remaining)\n            pkey_info = key_info.getComponentByName('privateKey')\n            pkey = rsa.key.PrivateKey.load_pkcs1(pkey_info.asOctets(),\n                                                 format='DER')\n        else:\n            raise ValueError('No key could be detected.')\n\n        return cls(pkey)"}
{"Repository": "fast-depth", "input": "Convert a ``numpy. className ToTensor(object) Method __call__", "label": "class ToTensor(object):\n    def __call__(self, img):\n        if not(_is_numpy_image(img)):\n            raise TypeError('img should be ndarray. Got {}'.format(type(img)))\n\n        if isinstance(img, np.ndarray):\n            # handle numpy array\n            if img.ndim == 3:\n                img = torch.from_numpy(img.transpose((2, 0, 1)).copy())\n            elif img.ndim == 2:\n                img = torch.from_numpy(img.copy())\n            else:\n                raise RuntimeError('img should be ndarray with 2 or 3 dimensions. Got {}'.format(img.ndim))\n\n            # backward compatibility\n            # return img.float().div(255)\n            return img.float()"}
{"Repository": "autobot", "input": "A snippet extracted from source code. className Snippet(NamedTuple) Method from_node", "label": "class Snippet(NamedTuple):\n    text: str\n    padding: str\n    lineno: int\n\n    @classmethod\n    def from_node(cls, source_code: str, node: ast.AST) -> Snippet:\n        return decontextualize(source_code, node)"}
{"Repository": "amazon-redshift-python-driver", "input": "Abstract base class used to store AWS credentials provided by user. className ABCAWSCredentialsHolder(ABC) Method __init__ Method has_associated_session Method get_boto_session Method get_session_credentials Attribute boto_session", "label": "class ABCAWSCredentialsHolder(ABC):\n    def __init__(self: \"ABCAWSCredentialsHolder\", session: \"boto3.Session\"):\n        self.boto_session = session\n\n    @property\n    def has_associated_session(self: \"ABCAWSCredentialsHolder\") -> bool:\n        return True\n\n    def get_boto_session(self: \"ABCAWSCredentialsHolder\") -> \"boto3.Session\":\n        return self.boto_session\n\n    @abstractmethod\n    def get_session_credentials(self: \"ABCAWSCredentialsHolder\") -> typing.Dict[str, str]:\n        pass"}
{"Repository": "DAM", "input": "Rotate entire clip randomly by a random angle within given bounds Args: degrees (sequence or int): Range of degrees to select from If degrees is a number instead of sequence like (min, max), the range of degrees, will be (-degrees, +degrees). className RandomRotation(object) Method __init__ Method __call__ Attribute degrees", "label": "class RandomRotation(object):\n    def __init__(self, degrees):\n        if isinstance(degrees, numbers.Number):\n            if degrees < 0:\n                raise ValueError('If degrees is a single number,'\n                                 'must be positive')\n            degrees = (-degrees, degrees)\n        else:\n            if len(degrees) != 2:\n                raise ValueError('If degrees is a sequence,'\n                                 'it must be of len 2.')\n\n        self.degrees = degrees\n\n    def __call__(self, clip):\n        angle = random.uniform(self.degrees[0], self.degrees[1])\n        if isinstance(clip[0], np.ndarray):\n            rotated = [rotate(image=img, angle=angle, preserve_range=True, mode='edge') for img in clip]\n        elif isinstance(clip[0], PIL.Image.Image):\n            rotated = [img.rotate(angle) for img in clip]\n        else:\n            raise TypeError('Expected numpy.ndarray or PIL.Image' +\n                            'but got list of {0}'.format(type(clip[0])))\n\n        return rotated"}
{"Repository": "django-rest-framework-reactive", "input": "Consumer for polling observers. className MainConsumer(AsyncConsumer) Method get_observers", "label": "class MainConsumer(AsyncConsumer):\n    async def observer_orm_notify(self, message):\n        @database_sync_to_async\n        def get_observers(table):\n            # Find all observers with dependencies on the given table.\n            return list(\n                Observer.objects.filter(\n                    dependencies__table=table, subscribers__isnull=False\n                )\n                .distinct('pk')\n                .values_list('pk', flat=True)\n            )\n\n        observers_ids = await get_observers(message['table'])\n\n        for observer_id in observers_ids:\n            await self.channel_layer.send(\n                CHANNEL_WORKER, {'type': TYPE_EVALUATE, 'observer': observer_id}\n            )\n\n    async def observer_poll(self, message):\n        # Sleep until we need to notify the observer.\n        await asyncio.sleep(message['interval'])\n\n        # Dispatch task to evaluate the observable.\n        await self.channel_layer.send(\n            CHANNEL_WORKER, {'type': TYPE_EVALUATE, 'observer': message['observer']}\n        )"}
{"Repository": "weechat-notify-send", "input": "Tests for nick_that_sent_message(). className NickThatSentMessageTests(TestsBase) Method test_returns_prefix_when_there_are_no_tags_and_prefix_has_no_modes Method test_returns_prefix_without_mode_when_there_are_no_tags_and_prefix_has_mode Method test_removes_also_space_before_nick_when_obtained_from_prefix Method test_only_single_character_is_removed_as_mode_from_prefix Method test_returns_nick_from_tags_when_tags_only_contains_nick Method test_returns_nick_from_tags_when_tags_contain_also_other_info Method test_returns_empty_string_when_both_tags_and_prefix_are_empty", "label": "class NickThatSentMessageTests(TestsBase):\n    def test_returns_prefix_when_there_are_no_tags_and_prefix_has_no_modes(self):\n        self.assertEqual(nick_that_sent_message([], 'john'), 'john')\n\n    def test_returns_prefix_without_mode_when_there_are_no_tags_and_prefix_has_mode(self):\n        self.assertEqual(nick_that_sent_message([], '~john'), 'john')\n        self.assertEqual(nick_that_sent_message([], '&john'), 'john')\n        self.assertEqual(nick_that_sent_message([], '@john'), 'john')\n        self.assertEqual(nick_that_sent_message([], '%john'), 'john')\n        self.assertEqual(nick_that_sent_message([], '+john'), 'john')\n        self.assertEqual(nick_that_sent_message([], '-john'), 'john')\n\n    def test_removes_also_space_before_nick_when_obtained_from_prefix(self):\n        self.assertEqual(nick_that_sent_message([], ' john'), 'john')\n\n    def test_only_single_character_is_removed_as_mode_from_prefix(self):\n        # Some protocols (e.g. Matrix) may start prefixes with a space.\n        # However, any subsequent characters should be considered to be part of\n        # the nick (e.g. from ' @john', we want '@john', including the '@').\n        self.assertEqual(nick_that_sent_message([], ' @john'), '@john')\n\n    def test_returns_nick_from_tags_when_tags_only_contains_nick(self):\n        self.assertEqual(nick_that_sent_message(['nick_john'], '--'), 'john')\n\n    def test_returns_nick_from_tags_when_tags_contain_also_other_info(self):\n        tags = ['prefix_nick_lightcyan', 'nick_john', 'host_~user@domain.com']\n        self.assertEqual(nick_that_sent_message(tags, '--'), 'john')\n\n    def test_returns_empty_string_when_both_tags_and_prefix_are_empty(self):\n        self.assertEqual(nick_that_sent_message([], ''), '')"}
{"Repository": "edX-6.00.2x-Introduction-to-Computational-Thinking-and-Data-Science", "input": "Representation of a simplified patient. className Patient(object) Method __init__ Method getTotalPop Method update Attribute viruses Attribute maxPop", "label": "class Patient(object):\n\tdef __init__(self, viruses, maxPop):\n\t\t# TODO\n\t\tself.viruses = viruses\n\t\tself.maxPop = maxPop\n\n\tdef getTotalPop(self):\n\t\t# TODO\t\t\n\t\treturn len( self.viruses )\n\n\tdef update(self):\n\t\t# TODO\n\t\t# Determine whether each virus particle survives and updates the \n\t\t# list of virus particles accordingly.\n\t\tnewViruses = []\n\t\tfor index, virus in reversed( list( enumerate( self.viruses ) ) ):\n\t\t\tif virus.doesClear():\n\t\t\t\t# print \"Virus clears\"\n\t\t\t\t# pop virus from viruses list\n\t\t\t\tself.viruses.pop( index )\n\t\t\telse:\n\t\t\t\tpopDensity = self.getTotalPop()/float(self.maxPop)\n\t\t\t\ttry:\n\t\t\t\t\t# determine if surving virus reproduces\n\t\t\t\t\t# append any offspring to new virus list\n\t\t\t\t\tnewViruses.append( virus.reproduce( popDensity ) )\n\t\t\t\texcept NoChildException:\n\t\t\t\t\tcontinue\n\t\t# print \"self.viruses =\", self.viruses\n\t\t# print \"newViruses =\",  newViruses\n\t\t# add the new viruses to the list of patient viruses\n\t\tself.viruses = self.viruses + newViruses\n\t\t# print self.viruses\n\n\t\treturn self.getTotalPop()"}
{"Repository": "h2", "input": "The DataReceived event is fired whenever data is received on a stream from the remote peer. className DataReceived(Event) Method __init__ Method __repr__ Attribute stream_id Attribute data Attribute flow_controlled_length Attribute stream_ended", "label": "class DataReceived(Event):\n    def __init__(self):\n        #: The Stream ID for the stream this data was received on.\n        self.stream_id = None\n\n        #: The data itself.\n        self.data = None\n\n        #: The amount of data received that counts against the flow control\n        #: window. Note that padding counts against the flow control window, so\n        #: when adjusting flow control you should always use this field rather\n        #: than ``len(data)``.\n        self.flow_controlled_length = None\n\n        #: If this data chunk also completed the stream, the associated\n        #: :class:`StreamEnded <h2.events.StreamEnded>` event will be available\n        #: here.\n        #:\n        #: .. versionadded:: 2.4.0\n        self.stream_ended = None\n\n    def __repr__(self):\n        return (\n            \"<DataReceived stream_id:%s, \"\n            \"flow_controlled_length:%s, \"\n            \"data:%s>\" % (\n                self.stream_id,\n                self.flow_controlled_length,\n                _bytes_representation(self.data[:20]),\n            )\n        )"}
{"Repository": "SfmLearner-Pytorch", "input": "Randomly zooms images up to 15% and crop them to keep same size as before. className RandomScaleCrop(object) Method __call__", "label": "class RandomScaleCrop(object):\n    def __call__(self, images, intrinsics):\n        assert intrinsics is not None\n        output_intrinsics = np.copy(intrinsics)\n\n        in_h, in_w, _ = images[0].shape\n        x_scaling, y_scaling = np.random.uniform(1,1.15,2)\n        scaled_h, scaled_w = int(in_h * y_scaling), int(in_w * x_scaling)\n\n        output_intrinsics[0] *= x_scaling\n        output_intrinsics[1] *= y_scaling\n        scaled_images = [resize(im, (scaled_h, scaled_w)) for im in images]\n\n        offset_y = np.random.randint(scaled_h - in_h + 1)\n        offset_x = np.random.randint(scaled_w - in_w + 1)\n        cropped_images = [im[offset_y:offset_y + in_h, offset_x:offset_x + in_w] for im in scaled_images]\n\n        output_intrinsics[0,2] -= offset_x\n        output_intrinsics[1,2] -= offset_y\n\n        return cropped_images, output_intrinsics"}
{"Repository": "ert", "input": "Removes all unnecessary levels from the tree, className InstructionTransformer(Transformer) Method instruction Method regular_instruction Method job_instruction Method NEWLINE", "label": "class InstructionTransformer(Transformer):\n    @staticmethod\n    def instruction(children):\n        return children[0] if len(children) > 0 else Discard\n\n    @staticmethod\n    def regular_instruction(children):\n        return children\n\n    @staticmethod\n    def job_instruction(children):\n        return children\n\n    @staticmethod\n    def NEWLINE(_token):\n        return Discard"}
{"Repository": "The_Lightmapper", "input": "Implements Best Area Fit (BAF) section selection criteria for Guillotine algorithm. className GuillotineBaf(Guillotine) Method _section_fitness", "label": "class GuillotineBaf(Guillotine):\n    def _section_fitness(self, section, width, height):\n        if width > section.width or height > section.height:\n            return None\n        return section.area()-width*height"}
{"Repository": "Capsule", "input": "Shows the currently available locations that can be assigned to an object export. className CAPSULE_MT_PieLocationObject(Menu) Method draw", "label": "class CAPSULE_MT_PieLocationObject(Menu):\n    bl_idname = \"pie.location_object\"\n    bl_label = \"Select Location\"\n\n    def draw(self, context):\n        layout = self.layout\n        pie = layout.menu_pie()\n\n        obj = context.object.CAPObj\n        preferences = context.preferences\n        addon_prefs = preferences.addons[__package__].preferences\n        cap_file = bpy.data.objects[addon_prefs.default_datablock].CAPFile\n\n        i = 0\n        for loc in cap_file.location_presets:\n            pie.operator(\"capsule.location_select_object\", text=cap_file.location_presets[i].name, icon = \"FILE_FOLDER\").loc = i\n            i += 1"}
{"Repository": "iGibson", "input": "Post-installation for installation mode. className PostInstallCommand(install) Method run", "label": "class PostInstallCommand(install):\n        def run(self):\n                print('post installation')\n                check_call(\"bash realenv/envs/build.sh\".split())\n                install.run(self)"}
{"Repository": "Read_Bert_Code", "input": "Processor for the MRPC data set (GLUE version). className MrpcProcessor(DataProcessor) Method get_train_examples Method get_dev_examples Method get_labels Method _create_examples", "label": "class MrpcProcessor(DataProcessor):\n    def get_train_examples(self, data_dir):\n        logger.info(\"LOOKING AT {}\".format(os.path.join(data_dir, \"train.tsv\")))\n        return self._create_examples(\n            self._read_tsv(os.path.join(data_dir, \"train.tsv\")), \"train\")\n\n    def get_dev_examples(self, data_dir):\n        return self._create_examples(\n            self._read_tsv(os.path.join(data_dir, \"dev.tsv\")), \"dev\")\n\n    def get_labels(self):\n        return [\"0\", \"1\"]\n\n    def _create_examples(self, lines, set_type):\n        examples = []\n        for (i, line) in enumerate(lines):\n            if i == 0:\n                continue\n            guid = \"%s-%s\" % (set_type, i)\n            text_a = line[3]\n            text_b = line[4]\n            label = line[0]\n            examples.append(\n                InputExample(guid=guid, text_a=text_a, text_b=text_b, label=label))\n        return examples"}
{"Repository": "RFB_ESRGAN-PyTorch", "input": "A fast data prefetch dataloader. className PrefetchDataLoader(DataLoader) Method __init__ Method __iter__", "label": "class PrefetchDataLoader(DataLoader):\n    def __init__(self, num_data_prefetch_queue: int, **kwargs) -> None:\n        self.num_data_prefetch_queue = num_data_prefetch_queue\n        super(PrefetchDataLoader, self).__init__(**kwargs)\n\n    def __iter__(self):\n        return PrefetchGenerator(super().__iter__(), self.num_data_prefetch_queue)"}
{"Repository": "SecBERT", "input": "A single training/test example for token classification. className InputExample(object) Method __init__ Attribute guid Attribute words Attribute labels", "label": "class InputExample(object):\n    def __init__(self, guid, words, labels):\n        self.guid = guid\n        self.words = words\n        self.labels = labels"}
{"Repository": "pikaur", "input": "Criteria from arch wiki. className ArchWikiTest(PikaurDbTestCase) Method test_reliable_parser Method test_split_packages_1 Method test_split_packages_2 Method test_split_packages_3", "label": "class ArchWikiTest(PikaurDbTestCase):\n    def test_reliable_parser(self):\n        # Arch Wiki: Reliable parser #\n        pikaur(\"-S aws-cli-git\")\n        self.assertInstalled(\"aws-cli-git\")\n\n    # def test_reliable_solver(self):\n    #     # Arch Wiki: Reliable solver\n    #     # @TODO: investigate further `gtest and gmock are in conflict.`\n    #     # self.remove_if_installed(\"gmock\") - didn't helped\n    #     fake_pikaur(\"-S ros-melodic-desktop\")\n    #     self.assertInstalled(\"ros-melodic-desktop\")\n    #     # it's slow as hell even with mocked makepkg :(\n\n    def test_split_packages_1(self):\n        # Split packages 1:\n        # Multiple packages from the same package base,\n        # without rebuilding or reinstalling multiple times, such as clion.\n        fake_pikaur(\"-S clion\")\n        self.assertInstalled(\"clion\")\n\n    def test_split_packages_2(self):\n        # Split packages 2:\n        # Split packages which depend on a package from the same package base,\n        # such as samsung-unified-driver\n        fake_pikaur(\"-S samsung-unified-driver --mflags=--skippgpcheck\")\n        self.assertInstalled(\"samsung-unified-driver\")\n\n    def test_split_packages_3(self):\n        # Split packages 3:\n        # Split packages independently,\n        # such as nxproxy and nxagent\n        pkg_name_1 = \"nxproxy\"\n        pkg_name_2 = \"nxagent\"\n\n        fake_pikaur(f\"-S {pkg_name_1} --mflags=--skippgpcheck\")\n        self.assertInstalled(pkg_name_1)\n        self.assertNotInstalled(pkg_name_2)\n\n        self.remove_packages(pkg_name_1)\n        # Split packages 3: 2 split packages\n        fake_pikaur(f\"-S {pkg_name_1} {pkg_name_2} --mflags=--skippgpcheck\")\n        self.assertInstalled(pkg_name_1)\n        self.assertInstalled(pkg_name_2)\n\n    # def test_split_packages_3(self):\n    #     # Split packages 3:\n    #     # Split packages independently,\n    #     # such as nxproxy and nxagent\n    #     fake_pikaur(\"-S lua51-xmlrpc --mflags=--skippgpcheck\")\n    #     self.assertInstalled(\"lua51-xmlrpc\")\n    #     self.assertNotInstalled(\"lua52-xmlrpc\")\n\n    #     self.remove_packages(\"lua51-xmlrpc\")\n\n    #     # Split packages 3: 2 split packages\n    #     fake_pikaur(\"-S lua51-xmlrpc lua52-xmlrpc --mflags=--skippgpcheck\")\n    #     self.assertInstalled(\"lua51-xmlrpc\")\n    #     self.assertInstalled(\"lua52-xmlrpc\")"}
{"Repository": "gemnet_pytorch", "input": "Implements the DataContainer but for a single molecule. Requires custom init method. className Molecule(DataContainer) Method __init__ Method get Method update Method to Attribute index_keys Attribute triplets_only Attribute cutoff Attribute int_cutoff Attribute keys Attribute R Attribute Z Attribute N Attribute E Attribute F Attribute N_cumsum Attribute addID Attribute device", "label": "class Molecule(DataContainer):\n    def __init__(self, R, Z, cutoff, int_cutoff, triplets_only=False):\n        self.index_keys = [\n            \"batch_seg\",\n            \"id_undir\",\n            \"id_swap\",\n            \"id_c\",\n            \"id_a\",\n            \"id3_expand_ba\",\n            \"id3_reduce_ca\",\n            \"Kidx3\",\n        ]\n        if not triplets_only:\n            self.index_keys += [\n                \"id4_int_b\",\n                \"id4_int_a\",\n                \"id4_reduce_ca\",\n                \"id4_expand_db\",\n                \"id4_reduce_cab\",\n                \"id4_expand_abd\",\n                \"Kidx4\",\n                \"id4_reduce_intm_ca\",\n                \"id4_expand_intm_db\",\n                \"id4_reduce_intm_ab\",\n                \"id4_expand_intm_ab\",\n            ]\n        self.triplets_only = triplets_only\n        self.cutoff = cutoff\n        self.int_cutoff = int_cutoff\n        self.keys = [\"N\", \"Z\", \"R\", \"F\", \"E\"]\n\n        assert R.shape == (len(Z), 3)\n        self.R = R\n        self.Z = Z\n        self.N = np.array([len(Z)], dtype=np.int32)\n        self.E = np.zeros(1, dtype=np.float32).reshape(1, 1)\n        self.F = np.zeros((len(Z), 3), dtype=np.float32)\n\n        self.N_cumsum = np.concatenate([[0], np.cumsum(self.N)])\n        self.addID = False\n        self.dtypes, dtypes2 = self.get_dtypes()\n        self.dtypes.update(dtypes2)  # merge all dtypes in single dict\n        self.device = \"cpu\"\n\n    def get(self):\n        data = self.__getitem__(0)\n        for var in [\"E\", \"F\"]:\n            data.pop(var)  # not needed i.e.e not kown -> want to calculate this\n        # push to the selected device\n        for key in data:\n            data[key] = data[key].to(self.device)\n        return data\n\n    def update(self, R):\n        assert self.R.shape == R.shape\n        self.R = R\n\n    def to(self, device):\n        self.device = device"}
{"Repository": "bravado", "input": "Wraps an operation to make it callable and provides a docstring. className CallableOperation(object) Method __init__ Method __doc__ Method __getattr__ Method __call__ Attribute also_return_response Attribute operation", "label": "class CallableOperation(object):\n    def __init__(self, operation, also_return_response=False):\n        self.also_return_response = also_return_response\n        self.operation = operation\n\n    @docstring_property(__doc__)\n    def __doc__(self):\n        return create_operation_docstring(self.operation)\n\n    def __getattr__(self, name):\n        return getattr(self.operation, name)\n\n    def __call__(self, **op_kwargs):\n        log.debug(u'%s(%s)', self.operation.operation_id, op_kwargs)\n        warn_for_deprecated_op(self.operation)\n\n        # Get per-request config\n        request_options = op_kwargs.pop('_request_options', {})\n        request_config = RequestConfig(request_options, self.also_return_response)\n\n        request_params = construct_request(\n            self.operation, request_options, **op_kwargs)\n\n        http_client = self.operation.swagger_spec.http_client\n\n        return http_client.request(\n            request_params,\n            operation=self.operation,\n            request_config=request_config,\n        )"}
{"Repository": "scrapy-redis", "input": "Redis-based request duplicates filter. className RFPDupeFilter(BaseDupeFilter) Method __init__ Method from_settings Method from_crawler Method request_seen Method request_fingerprint Method from_spider Method close Method clear Method log Attribute server Attribute key Attribute debug Attribute logdupes", "label": "class RFPDupeFilter(BaseDupeFilter):\n    logger = logger\n\n    def __init__(self, server, key, debug=False):\n        self.server = server\n        self.key = key\n        self.debug = debug\n        self.logdupes = True\n\n    @classmethod\n    def from_settings(cls, settings):\n        server = get_redis_from_settings(settings)\n        # XXX: This creates one-time key. needed to support to use this\n        # class as standalone dupefilter with scrapy's default scheduler\n        # if scrapy passes spider on open() method this wouldn't be needed\n        # TODO: Use SCRAPY_JOB env as default and fallback to timestamp.\n        key = defaults.DUPEFILTER_KEY % {'timestamp': int(time.time())}\n        debug = settings.getbool('DUPEFILTER_DEBUG')\n        return cls(server, key=key, debug=debug)\n\n    @classmethod\n    def from_crawler(cls, crawler):\n        return cls.from_settings(crawler.settings)\n\n    def request_seen(self, request):\n        fp = self.request_fingerprint(request)\n        # This returns the number of values added, zero if already exists.\n        added = self.server.sadd(self.key, fp)\n        return added == 0\n\n    def request_fingerprint(self, request):\n        fingerprint_data = {\n            \"method\": to_unicode(request.method),\n            \"url\": canonicalize_url(request.url),\n            \"body\": (request.body or b\"\").hex(),\n        }\n        fingerprint_json = json.dumps(fingerprint_data, sort_keys=True)\n        return hashlib.sha1(fingerprint_json.encode()).hexdigest()\n    \n    @classmethod\n    def from_spider(cls, spider):\n        settings = spider.settings\n        server = get_redis_from_settings(settings)\n        dupefilter_key = settings.get(\"SCHEDULER_DUPEFILTER_KEY\", defaults.SCHEDULER_DUPEFILTER_KEY)\n        key = dupefilter_key % {'spider': spider.name}\n        debug = settings.getbool('DUPEFILTER_DEBUG')\n        return cls(server, key=key, debug=debug)\n\n    def close(self, reason=''):\n        self.clear()\n\n    def clear(self):\n        self.server.delete(self.key)\n\n    def log(self, request, spider):\n        if self.debug:\n            msg = \"Filtered duplicate request: %(request)s\"\n            self.logger.debug(msg, {'request': request}, extra={'spider': spider})\n        elif self.logdupes:\n            msg = (\"Filtered duplicate request %(request)s\"\n                   \" - no more duplicates will be shown\"\n                   \" (see DUPEFILTER_DEBUG to show all duplicates)\")\n            self.logger.debug(msg, {'request': request}, extra={'spider': spider})\n            self.logdupes = False"}
{"Repository": "delta", "input": "Configuration for image preprocessing. className ImagePreprocessConfig(DeltaConfigComponent) Method __init__ Method _load_dict Method function Method helper Attribute _functions", "label": "class ImagePreprocessConfig(DeltaConfigComponent):\n    def __init__(self):\n        super().__init__()\n        self._functions = []\n\n    def _load_dict(self, d, base_dir):\n        if d is None:\n            self._functions = []\n            return\n        if not d:\n            return\n        self._functions = []\n        assert isinstance(d, list), 'preprocess should be list of commands'\n        for func in d:\n            if isinstance(func, str):\n                self._functions.append((func, {}))\n            else:\n                assert isinstance(func, dict), 'preprocess items must be strings or dicts'\n                assert len(func) == 1, 'One preprocess item per list entry.'\n                name = list(func.keys())[0]\n                self._functions.append((name, func[name]))\n\n    def function(self, image_type):\n        prep = lambda data, _, dummy: data\n        for (name, args) in self._functions:\n            t = preprocess_function(name)\n            assert t is not None, 'Preprocess function %s not found.' % (name)\n            p = t(image_type=image_type, **args)\n            def helper(cur, prev):\n                return lambda data, roi, bands: cur(prev(data, roi, bands), roi, bands)\n            prep = helper(p, prep)\n        return prep"}
{"Repository": "TDANet", "input": "Inverse Short-time Fourier Transform as a Layer className iSTFT(STFTBase) Method __init__", "label": "class iSTFT(STFTBase):\n    def __init__(self, *args, **kwargs):\n        super(iSTFT, self).__init__(*args, inverse=True, **kwargs)\n\n    def forward(\n        self, transform: th.Tensor, return_polar: bool = False, eps: float = EPSILON\n    ) -> th.Tensor:\n        if self.mode == \"torch\":\n            return _pytorch_istft(\n                transform,\n                self.frame_len,\n                self.frame_hop,\n                n_fft=(self.num_bins - 1) * 2,\n                return_polar=return_polar,\n                window=self.w,\n                normalized=self.normalized,\n                onesided=self.onesided,\n                center=self.center,\n                eps=eps,\n            )\n        else:\n            return _inverse_stft(\n                transform,\n                self.K,\n                self.w,\n                return_polar=return_polar,\n                frame_hop=self.frame_hop,\n                onesided=self.onesided,\n                center=self.center,\n                eps=eps,\n            )"}
{"Repository": "ViT-CIFAR", "input": "Randomly choose one of the best 25 Sub-policies on CIFAR10. className CIFAR10Policy(object) Method __init__ Method __call__ Method __repr__ Attribute policies", "label": "class CIFAR10Policy(object):\n    def __init__(self, fillcolor=(128, 128, 128)):\n        self.policies = [\n            SubPolicy(0.1, \"invert\", 7, 0.2, \"contrast\", 6, fillcolor),\n            SubPolicy(0.7, \"rotate\", 2, 0.3, \"translateX\", 9, fillcolor),\n            SubPolicy(0.8, \"sharpness\", 1, 0.9, \"sharpness\", 3, fillcolor),\n            SubPolicy(0.5, \"shearY\", 8, 0.7, \"translateY\", 9, fillcolor),\n            SubPolicy(0.5, \"autocontrast\", 8, 0.9, \"equalize\", 2, fillcolor),\n\n            SubPolicy(0.2, \"shearY\", 7, 0.3, \"posterize\", 7, fillcolor),\n            SubPolicy(0.4, \"color\", 3, 0.6, \"brightness\", 7, fillcolor),\n            SubPolicy(0.3, \"sharpness\", 9, 0.7, \"brightness\", 9, fillcolor),\n            SubPolicy(0.6, \"equalize\", 5, 0.5, \"equalize\", 1, fillcolor),\n            SubPolicy(0.6, \"contrast\", 7, 0.6, \"sharpness\", 5, fillcolor),\n\n            SubPolicy(0.7, \"color\", 7, 0.5, \"translateX\", 8, fillcolor),\n            SubPolicy(0.3, \"equalize\", 7, 0.4, \"autocontrast\", 8, fillcolor),\n            SubPolicy(0.4, \"translateY\", 3, 0.2, \"sharpness\", 6, fillcolor),\n            SubPolicy(0.9, \"brightness\", 6, 0.2, \"color\", 8, fillcolor),\n            SubPolicy(0.5, \"solarize\", 2, 0.0, \"invert\", 3, fillcolor),\n\n            SubPolicy(0.2, \"equalize\", 0, 0.6, \"autocontrast\", 0, fillcolor),\n            SubPolicy(0.2, \"equalize\", 8, 0.6, \"equalize\", 4, fillcolor),\n            SubPolicy(0.9, \"color\", 9, 0.6, \"equalize\", 6, fillcolor),\n            SubPolicy(0.8, \"autocontrast\", 4, 0.2, \"solarize\", 8, fillcolor),\n            SubPolicy(0.1, \"brightness\", 3, 0.7, \"color\", 0, fillcolor),\n\n            SubPolicy(0.4, \"solarize\", 5, 0.9, \"autocontrast\", 3, fillcolor),\n            SubPolicy(0.9, \"translateY\", 9, 0.7, \"translateY\", 9, fillcolor),\n            SubPolicy(0.9, \"autocontrast\", 2, 0.8, \"solarize\", 3, fillcolor),\n            SubPolicy(0.8, \"equalize\", 8, 0.1, \"invert\", 3, fillcolor),\n            SubPolicy(0.7, \"translateY\", 9, 0.9, \"autocontrast\", 1, fillcolor)\n        ]\n\n    def __call__(self, img):\n        policy_idx = random.randint(0, len(self.policies) - 1)\n        return self.policies[policy_idx](img)\n\n    def __repr__(self):\n        return \"AutoAugment CIFAR10 Policy\""}
{"Repository": "virgin-media-hub3", "input": "Translates integers values to/from the router's representation. className IntTranslator(Translator) Method snmp Method pyvalue", "label": "class IntTranslator(Translator):\n    snmp_datatype = DataType.INT\n    @staticmethod\n    def snmp(python_value):\n        if python_value is None:\n            return \"\"\n        return str(int(python_value))\n\n    @staticmethod\n    def pyvalue(snmp_value):\n        if snmp_value == \"\":\n            return None\n        if snmp_value is None:\n            raise ValueError(\"This could not have come from SNMP...\")\n        return int(snmp_value)"}
{"Repository": "nucypher", "input": "Alice base class for blockchain operations, mocking up new policies! className PolicyAuthor(NucypherTokenActor) Method __init__ Method create_policy Attribute application_agent", "label": "class PolicyAuthor(NucypherTokenActor):\n    def __init__(self, eth_endpoint: str, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.application_agent = ContractAgency.get_agent(\n            TACoApplicationAgent,\n            registry=self.registry,\n            blockchain_endpoint=eth_endpoint,\n        )\n\n    def create_policy(self, *args, **kwargs):\n        from nucypher.policy.policies import Policy\n\n        blockchain_policy = Policy(publisher=self, *args, **kwargs)\n        return blockchain_policy"}
{"Repository": "pgadmin4", "input": "PreferenceModule represets the preferences of different modules to the user in UI. className PreferencesModule(PgAdminModule) Method get_own_menuitems Method get_exposed_url_endpoints", "label": "class PreferencesModule(PgAdminModule):\n    def get_own_menuitems(self):\n        return {}\n\n    def get_exposed_url_endpoints(self):\n        return [\n            'preferences.index',\n            'preferences.get_by_name',\n            'preferences.get_all',\n            'preferences.get_all_cli',\n            'preferences.update_pref'\n        ]"}
{"Repository": "dropdav", "input": "Consumer of OAuth authentication. className OAuthConsumer(object) Method __init__ Attribute key Attribute secret", "label": "class OAuthConsumer(object):\n    key = None\n    secret = None\n\n    def __init__(self, key, secret):\n        self.key = key\n        self.secret = secret"}
{"Repository": "shedskin", "input": "basic test runner className TestRunner(CMakeBuilder) Method __init__ Attribute options Attribute source_dir Attribute build_dir Attribute tests Attribute log", "label": "class TestRunner(CMakeBuilder):\n    def __init__(self, options):\n        self.options = options\n        self.source_dir = pathlib.Path.cwd()\n        self.build_dir = pathlib.Path(\"build\")\n        self.tests = sorted(glob.glob(\"./test_*/test_*.py\", recursive=True))\n        self.log = logging.getLogger(self.__class__.__name__)"}
{"Repository": "moz-sql-parser", "input": "exception thrown if anything called by the verify_formatting function raises an exception className VerificationException(Exception) Method __init__ Method __str__ Attribute expected_sql Attribute expected_json Attribute new_sql Attribute new_json", "label": "class VerificationException(Exception):\n    def __init__(self, expected_sql, expected_json, new_sql, new_json):\n        self.expected_sql = expected_sql\n        self.expected_json = expected_json\n        self.new_sql = new_sql\n        self.new_json = new_json\n\n    def __str__(self):\n        res = EXCEPTION_MESSAGE % (self.expected_sql, self.new_sql, pformat(self.expected_json), pformat(self.new_json))\n        return res"}
{"Repository": "motor", "input": "A method on the wrapped PyMongo object that does no I/O and can be called className DelegateMethod(ReadOnlyProperty) Method __init__ Method wrap Method create_attribute Method wrapper Attribute wrap_class", "label": "class DelegateMethod(ReadOnlyProperty):\n    def __init__(self, doc=None):\n        ReadOnlyProperty.__init__(self, doc)\n        self.wrap_class = None\n\n    def wrap(self, original_class):\n        self.wrap_class = original_class\n        return self\n\n    def create_attribute(self, cls, attr_name):\n        if self.wrap_class is None:\n            return ReadOnlyProperty.create_attribute(self, cls, attr_name)\n\n        method = getattr(cls.__delegate_class__, attr_name)\n        original_class = self.wrap_class\n\n        @functools.wraps(method)\n        def wrapper(self_, *args, **kwargs):\n            result = method(self_.delegate, *args, **kwargs)\n\n            # Don't call isinstance(), not checking subclasses.\n            if result.__class__ == original_class:\n                # Delegate to the current object to wrap the result.\n                return self_.wrap(result)\n            else:\n                return result\n\n        if self.doc:\n            wrapper.__doc__ = self.doc\n\n        wrapper.is_wrap_method = True  # For Synchro.\n        return wrapper"}
{"Repository": "ngram-language-model", "input": "An n-gram language model trained on a given corpus. className LanguageModel(object) Method __init__ Method _smooth Method smoothed_count Method _create_model Method _convert_oov Method perplexity Method _best_candidate Method generate_sentences Attribute n Attribute laplace Attribute tokens Attribute vocab Attribute model Attribute masks", "label": "class LanguageModel(object):\n    def __init__(self, train_data, n, laplace=1):\n        self.n = n\n        self.laplace = laplace\n        self.tokens = preprocess(train_data, n)\n        self.vocab  = nltk.FreqDist(self.tokens)\n        self.model  = self._create_model()\n        self.masks  = list(reversed(list(product((0,1), repeat=n))))\n\n    def _smooth(self):\n        vocab_size = len(self.vocab)\n\n        n_grams = nltk.ngrams(self.tokens, self.n)\n        n_vocab = nltk.FreqDist(n_grams)\n\n        m_grams = nltk.ngrams(self.tokens, self.n-1)\n        m_vocab = nltk.FreqDist(m_grams)\n\n        def smoothed_count(n_gram, n_count):\n            m_gram = n_gram[:-1]\n            m_count = m_vocab[m_gram]\n            return (n_count + self.laplace) / (m_count + self.laplace * vocab_size)\n\n        return { n_gram: smoothed_count(n_gram, count) for n_gram, count in n_vocab.items() }\n\n    def _create_model(self):\n        if self.n == 1:\n            num_tokens = len(self.tokens)\n            return { (unigram,): count / num_tokens for unigram, count in self.vocab.items() }\n        else:\n            return self._smooth()\n\n    def _convert_oov(self, ngram):\n        mask = lambda ngram, bitmask: tuple((token if flag == 1 else \"<UNK>\" for token,flag in zip(ngram, bitmask)))\n\n        ngram = (ngram,) if type(ngram) is str else ngram\n        for possible_known in [mask(ngram, bitmask) for bitmask in self.masks]:\n            if possible_known in self.model:\n                return possible_known\n\n    def perplexity(self, test_data):\n        test_tokens = preprocess(test_data, self.n)\n        test_ngrams = nltk.ngrams(test_tokens, self.n)\n        N = len(test_tokens)\n\n        known_ngrams  = (self._convert_oov(ngram) for ngram in test_ngrams)\n        probabilities = [self.model[ngram] for ngram in known_ngrams]\n\n        return math.exp((-1/N) * sum(map(math.log, probabilities)))\n\n    def _best_candidate(self, prev, i, without=[]):\n        blacklist  = [\"<UNK>\"] + without\n        candidates = ((ngram[-1],prob) for ngram,prob in self.model.items() if ngram[:-1]==prev)\n        candidates = filter(lambda candidate: candidate[0] not in blacklist, candidates)\n        candidates = sorted(candidates, key=lambda candidate: candidate[1], reverse=True)\n        if len(candidates) == 0:\n            return (\"</s>\", 1)\n        else:\n            return candidates[0 if prev != () and prev[-1] != \"<s>\" else i]\n     \n    def generate_sentences(self, num, min_len=12, max_len=24):\n        for i in range(num):\n            sent, prob = [\"<s>\"] * max(1, self.n-1), 1\n            while sent[-1] != \"</s>\":\n                prev = () if self.n == 1 else tuple(sent[-(self.n-1):])\n                blacklist = sent + ([\"</s>\"] if len(sent) < min_len else [])\n                next_token, next_prob = self._best_candidate(prev, i, without=blacklist)\n                sent.append(next_token)\n                prob *= next_prob\n                \n                if len(sent) >= max_len:\n                    sent.append(\"</s>\")\n\n            yield ' '.join(sent), -1/math.log(prob)"}
{"Repository": "home-assistant-custom-components", "input": "Get data from remarks files. className RemarksManager(object) Method _get_remark Method _get_temp_remark", "label": "class RemarksManager(object):\n    # pylint: disable=too-many-arguments\n    def __init__(self, remarksfile, temp_sensor,\n                 cold_threshold, freeze_threshold, hour, minute,\n                 temp_hour, temp_minute, hass):\n        self._hass = hass\n        self._cfgdir = self._hass.config.config_dir\n        self._remarksfile = self._cfgdir+'/remarks/'+remarksfile\n        self._temp_sensor = temp_sensor\n        self._cold_threshold = cold_threshold\n        self._cold_temp_file = self._cfgdir+'/remarks/list_temp_below_20.txt'\n        self._freeze_threshold = freeze_threshold\n        self._freeze_temp_file = self._cfgdir+'/remarks/list_temp_below_0.txt'\n        self._remark = None\n        self._hour = hour\n        self._minute = minute\n        self._temp_hour = temp_hour\n        self._temp_minute = temp_minute\n\n        track_time_change(hass, lambda now: self._get_remark(),\n                          hour=self._hour, minute=self._minute, second=0)\n\n        track_time_change(hass, lambda now: self._get_temp_remark(),\n                          hour=self._temp_hour, minute=self._temp_minute,\n                          second=0)\n\n    def _get_remark(self):\n        _LOGGER.debug('Fetching remark from file \"%s\"', self._remarksfile)\n\n        lines = open(str(self._remarksfile)).read().splitlines()\n        self._remark = random.choice(lines)\n\n        _LOGGER.debug('Fire event for new remark')\n        self._hass.bus.fire(EVENT_REMARKS, {ATTR_TEXT: self._remark})\n\n    def _get_temp_remark(self):\n        sensor = self._hass.states.get(self._temp_sensor)\n        unit = sensor.attributes.get('unit_of_measurement')\n\n        if float(sensor.state) < float(self._freeze_threshold):\n            _LOGGER.debug('Fetching remark from \"%s\"', self._freeze_temp_file)\n            lines = open(str(self._freeze_temp_file)).read().splitlines()\n            self._remark = 'It is currently ' + sensor.state + unit +\\\n                           ' outside. ' + random.choice(lines)\n            _LOGGER.debug('Event for freezetemp remark, temp is %s%s',\n                          sensor.state, unit)\n            self._hass.bus.fire(EVENT_REMARKS, {ATTR_TEXT: self._remark})\n        elif float(sensor.state) < float(self._cold_threshold):\n            _LOGGER.debug('Fetching remark from \"%s\"', self._cold_temp_file)\n            lines = open(str(self._cold_temp_file)).read().splitlines()\n            self._remark = 'It is currently ' + sensor.state + unit +\\\n                           ' outside. '+random.choice(lines)\n            _LOGGER.debug('Event for coldtemp remark, temp is%s%s',\n                          sensor.state, unit)\n            self._hass.bus.fire(EVENT_REMARKS, {ATTR_TEXT: self._remark})"}
{"Repository": "schnetpack", "input": "Strategy that splits the atoms dataset into predefined partitions as defined in the metadata. className SubsamplePartitions(SplittingStrategy) Method split", "label": "class SubsamplePartitions(SplittingStrategy):\n    def __init__(\n        self,\n        split_partition_sources: List[str],\n        split_id=0,\n        base_splitting: Optional[SplittingStrategy] = None,\n        partition_key: str = \"splits\",\n    ):\n        self.split_partition_sources = split_partition_sources\n        self.partition_key = partition_key\n        self.split_id = split_id\n\n        self._unique_sources, self._splits_indices = np.unique(\n            self.split_partition_sources, return_inverse=True\n        )\n        self.base_splitting = base_splitting or RandomSplit()\n\n    def split(self, dataset, *split_sizes):\n        if len(split_sizes) != len(self.split_partition_sources):\n            raise ValueError(\n                f\"The number of `split_sizes`({len(split_sizes)}) needs to match the \"\n                + f\"number of `partition_sources`({len(self.split_partition_sources)}).\"\n            )\n\n        split_partition_sizes = {src: [] for src in self.split_partition_sources}\n        split_partition_idx = {src: [] for src in self.split_partition_sources}\n        for i, split_size, src in zip(\n            range(len(split_sizes)), split_sizes, self.split_partition_sources\n        ):\n            split_partition_sizes[src].append(split_size)\n            split_partition_idx[src].append(i)\n\n        partitions = dataset.metadata[self.partition_key]\n\n        split_indices = [None] * len(split_sizes)\n        for src in self._unique_sources:\n            partition = partitions[src][self.split_id]\n            print(len(partition))\n            partition_split_indices = random_split(\n                len(partition), *split_partition_sizes[src]\n            )\n            for i, split_idx in zip(split_partition_idx[src], partition_split_indices):\n                split_indices[i] = np.array(partition)[split_idx].tolist()\n        return split_indices"}
{"Repository": "qiskit-ibm-runtime", "input": "Class that represents an account with channel 'ibm_quantum.' className QuantumAccount(Account) Method get_auth_handler Method _assert_valid_instance", "label": "class QuantumAccount(Account):\n    def __init__(\n        self,\n        token: str,\n        url: Optional[str] = None,\n        instance: Optional[str] = None,\n        proxies: Optional[ProxyConfiguration] = None,\n        verify: Optional[bool] = True,\n        channel_strategy: Optional[str] = None,\n    ):\n        super().__init__(token, instance, proxies, verify, channel_strategy)\n        resolved_url = url or IBM_QUANTUM_API_URL\n        self.channel = \"ibm_quantum\"\n        self.url = resolved_url\n\n    def get_auth_handler(self) -> AuthBase:\n        return QuantumAuth(access_token=self.token)\n\n    @staticmethod\n    def _assert_valid_instance(instance: str) -> None:\n        if instance is not None:\n            try:\n                from_instance_format(instance)\n            except:\n                raise InvalidAccountError(\n                    f\"Invalid `instance` value. Expected hub/group/project format, got {instance}\"\n                )"}
{"Repository": "python-fuse", "input": "Auxiliary class for carrying directory entry data. className Direntry(FuseStruct) Method __init__ Attribute name Attribute offset Attribute type Attribute ino", "label": "class Direntry(FuseStruct):\n    def __init__(self, name, **kw):\n\n        self.name   = name\n        self.offset = 0\n        self.type   = 0\n        self.ino    = 0\n\n        FuseStruct.__init__(self, **kw)"}
{"Repository": "validictory", "input": "Validation error that refers to a missing field className RequiredFieldValidationError(ValidationError) Method __init__", "label": "class RequiredFieldValidationError(ValidationError):\n    def __init__(self, message):\n        super().__init__(message)"}
{"Repository": "mlforecast", "input": "Combine two lag transformations using an operator Parameters ---------- tfm1 : LagTransform First transformation. className Combine(_BaseLagTransform) Method _set_core_tfm Method _get_name Method transform Method update", "label": "class Combine(_BaseLagTransform):\n    def __init__(\n        self, tfm1: _BaseLagTransform, tfm2: _BaseLagTransform, operator: Callable\n    ):\n        self.tfm1 = tfm1\n        self.tfm2 = tfm2\n        self.operator = operator\n\n    def _set_core_tfm(self, lag: int) -> \"Combine\":\n        self.tfm1 = clone(self.tfm1)._set_core_tfm(lag)\n        self.tfm2 = clone(self.tfm2)._set_core_tfm(lag)\n        return self\n\n    def _get_name(self, lag: int) -> str:\n        lag1 = getattr(self.tfm1, \"lag\", lag)\n        lag2 = getattr(self.tfm2, \"lag\", lag)\n        return f\"{self.tfm1._get_name(lag1)}_{self.operator.__name__}_{self.tfm2._get_name(lag2)}\"\n\n    def transform(self, ga: CoreGroupedArray) -> np.ndarray:\n        return self.operator(self.tfm1.transform(ga), self.tfm2.transform(ga))\n\n    def update(self, ga: CoreGroupedArray) -> np.ndarray:\n        return self.operator(self.tfm1.update(ga), self.tfm2.update(ga))"}
{"Repository": "DPGAN", "input": "Class representing a minibatch of train/val/test examples for text summarization. className Batch(object) Method __init__ Method init_decoder_seq Attribute pad_id", "label": "class Batch(object):\n  def __init__(self, example_list, hps, vocab):\n    self.pad_id = vocab.word2id(data.PAD_TOKEN) # id of the PAD token used to pad sequences\n    self.init_decoder_seq(example_list, hps)  # initialize the input to the encoder\n\n  def init_decoder_seq(self, example_list, hps):\n\n      for ex in example_list:\n          ex.pad_decoder_inp_targ(hps.max_enc_seq_len, hps.max_enc_sen_num, self.pad_id)\n\n      # Initialize the numpy arrays.\n      # Note: our decoder inputs and targets must be the same length for each batch (second dimension = max_dec_steps) because we do not use a dynamic_rnn for decoding. However I believe this is possible, or will soon be possible, with Tensorflow 1.0, in which case it may be best to upgrade to that.\n      self.dec_batch = np.zeros((hps.batch_size, hps.max_enc_sen_num, hps.max_enc_seq_len), dtype=np.int32)\n      self.target_batch = np.zeros((hps.batch_size, hps.max_enc_sen_num, hps.max_enc_seq_len), dtype=np.int32)\n      self.dec_padding_mask = np.zeros((hps.batch_size * hps.max_enc_sen_num, hps.max_enc_seq_len),\n                                       dtype=np.float32)\n      self.labels = np.zeros((hps.batch_size, hps.max_enc_sen_num, hps.max_enc_seq_len), dtype=np.int32)\n      self.dec_sen_lens = np.zeros((hps.batch_size, hps.max_enc_sen_num), dtype=np.int32)\n      self.dec_lens = np.zeros((hps.batch_size), dtype=np.int32)\n      self.review_sentenc_orig = []\n\n      for i, ex in enumerate(example_list):\n          #self.new_review_text = []\n          self.labels[i]=np.array([[ex.label for k in range(hps.max_enc_seq_len) ] for j in range(hps.max_enc_sen_num)])\n          self.review_sentenc_orig.append([sen for sen in ex.original_reivew])\n\n          self.dec_lens[i] = ex.dec_len\n          self.dec_batch[i, :, :] = np.array(ex.dec_input)\n          self.target_batch[i] = np.array(ex.target)\n          for j in range(len(ex.dec_sen_len)):\n              self.dec_sen_lens[i][j] = ex.dec_sen_len[j]\n\n      self.target_batch = np.reshape(self.target_batch,\n                                     [hps.batch_size * hps.max_enc_sen_num, hps.max_enc_seq_len])\n\n      for j in range(len(self.target_batch)):\n          for k in range(len(self.target_batch[j])):\n              if int(self.target_batch[j][k]) != self.pad_id:\n                  self.dec_padding_mask[j][k] = 1\n\n      self.dec_batch = np.reshape(self.dec_batch, [hps.batch_size * hps.max_enc_sen_num, hps.max_enc_seq_len])\n      self.dec_sen_lens = np.reshape(self.dec_sen_lens, [hps.batch_size * hps.max_enc_sen_num])\n      self.labels = np.reshape(self.labels, [hps.batch_size * hps.max_enc_sen_num, hps.max_enc_seq_len])"}
{"Repository": "geopackage-python", "input": "Ellipsoidal Mercator projection class that holds specific calculations and formulas for EPSG3395. className EllipsoidalMercator(Mercator) Method __init__ Method lat_to_northing Method tile_to_lat_lon Method f Method tile_to_meters", "label": "class EllipsoidalMercator(Mercator):\n    def __init__(self):\n        super(EllipsoidalMercator, self).__init__()\n\n    @staticmethod\n    def lat_to_northing(lat):\n        r = 6378137.0\n        e = 0.081819190842621\n        return r * log(tan((pi / 2 + lat) / 2) * ((1 - e * sin(lat)) /\n                                                  (1 + e * sin(lat)))**(e / 2))\n\n    @staticmethod\n    def tile_to_lat_lon(z, x, y):\n        n = 2.0**z\n        lon = x / n * 360.0 - 180.0\n        my = (y - 2**(z - 1)) * 6378137 * pi * 2 / 2**z\n\n        def f(phi):\n            return EllipsoidalMercator.lat_to_northing(phi) - my\n\n        lat = 0.0\n        oldLat = 1.0\n        diff = 1.0\n        while abs(diff) > 0.0001:\n            newLat = lat - f(lat) * (lat - oldLat) / (f(lat) - f(oldLat))\n            if newLat > 1.48499697138:\n                newLat = 1.48499697138\n            elif newLat < -1.48499697138:\n                newLat = -1.48499697138\n            oldLat = lat\n            lat = newLat\n            diff = lat - oldLat\n        lat = lat * 180.0 / pi\n        return lat, lon\n\n    def tile_to_meters(self, z, x, y):\n        lat, lon = self.tile_to_lat_lon(z, x, y)\n        meters_x = lon * self.origin_shift / 180.0\n        meters_y = self.lat_to_northing(lat * pi / 180.0)\n        return meters_x, meters_y"}
{"Repository": "grid-cells", "input": "Calculates the dist over HD cells given an absolute angle. className HeadDirectionCellEnsemble(CellEnsemble) Method unnor_logpdf", "label": "class HeadDirectionCellEnsemble(CellEnsemble):\n  def __init__(self, n_cells, concentration=20, seed=None,\n               soft_targets=None, soft_init=None):\n    super(HeadDirectionCellEnsemble, self).__init__(n_cells,\n                                                    soft_targets,\n                                                    soft_init)\n    # Create a random Von Mises with fixed cov over the position\n    rs = np.random.RandomState(seed)\n    self.means = rs.uniform(-np.pi, np.pi, (n_cells))\n    self.kappa = np.ones_like(self.means) * concentration\n\n  def unnor_logpdf(self, x):\n    return self.kappa * tf.cos(x - self.means[np.newaxis, np.newaxis, :])"}
{"Repository": "2021-composition-vs-inheritance", "input": "Contract type for an employee being paid a monthly salary. className SalariedContract(Contract) Method get_payment", "label": "class SalariedContract(Contract):\n    monthly_salary: float\n    percentage: float = 1\n\n    def get_payment(self) -> float:\n        return self.monthly_salary * self.percentage"}
{"Repository": "pydatastructs", "input": "Represents a multi-dimensional array. className MultiDimensionalArray(Array) Method __new__ Method methods Method __getitem__ Method __setitem__ Method _compare_shape Method fill Method shape", "label": "class MultiDimensionalArray(Array):\n    __slots__ = ['_sizes', '_data', '_dtype']\n\n    def __new__(cls, dtype: type = NoneType, *args, **kwargs):\n        raise_if_backend_is_not_python(\n            cls, kwargs.get('backend', Backend.PYTHON))\n        if dtype is NoneType:\n            raise ValueError(\"Data type is not defined.\")\n        elif not args:\n            raise ValueError(\"Too few arguments to create a \"\n                             \"multi dimensional array, pass dimensions.\")\n        if len(args) == 1:\n            obj = Array.__new__(cls)\n            obj._dtype = dtype\n            obj._sizes = (args[0], 1)\n            obj._data = [None] * args[0]\n            return obj\n\n        dimensions = args\n        for dimension in dimensions:\n            if dimension < 1:\n                raise ValueError(\"Size of dimension cannot be less than 1\")\n        n_dimensions = len(dimensions)\n        d_sizes = []\n        index = 0\n        while n_dimensions > 1:\n            size = dimensions[index]\n            for i in range(index+1,  len(dimensions)):\n                size = size * dimensions[i]\n            d_sizes.append(size)\n            n_dimensions -= 1\n            index += 1\n        d_sizes.append(dimensions[index])\n        d_sizes.append(1)\n        obj = Array.__new__(cls)\n        obj._dtype = dtype\n        obj._sizes = tuple(d_sizes)\n        obj._data = [None] * obj._sizes[1] * dimensions[0]\n        return obj\n\n    @classmethod\n    def methods(cls) -> list:\n        return ['__new__', '__getitem__', '__setitem__', 'fill', 'shape']\n\n    def __getitem__(self, indices):\n        self._compare_shape(indices)\n        if isinstance(indices, int):\n            return self._data[indices]\n        position = 0\n        for i in range(0, len(indices)):\n            position += self._sizes[i + 1] * indices[i]\n        return self._data[position]\n\n    def __setitem__(self, indices, element) -> None:\n        self._compare_shape(indices)\n        if isinstance(indices, int):\n            self._data[indices] = element\n        else:\n            position = 0\n            for i in range(0, len(indices)):\n                position += self._sizes[i + 1] * indices[i]\n            self._data[position] = element\n\n    def _compare_shape(self, indices) -> None:\n        indices = [indices] if isinstance(indices, int) else indices\n        if len(indices) != len(self._sizes) - 1:\n            raise IndexError(\"Shape mismatch, current shape is %s\" % str(self.shape))\n        if any(indices[i] >= self._sizes[i] for i in range(len(indices))):\n            raise IndexError(\"Index out of range.\")\n\n    def fill(self, element) -> None:\n        element = self._dtype(element)\n        for i in range(len(self._data)):\n            self._data[i] = element\n\n    @property\n    def shape(self) -> tuple:\n        shape = []\n        size = len(self._sizes)\n        for i in range(1, size):\n            shape.append(self._sizes[i-1]//self._sizes[i])\n        return tuple(shape)"}
{"Repository": "maro", "input": "Generate weather data bin for the specified topology from frontierweather. className WeatherToyPipeline(WeatherPipeline) Method __init__ Method download Method clean Method _weather Method _gen_weather Method _preprocess Attribute _start_time Attribute _end_time", "label": "class WeatherToyPipeline(WeatherPipeline):\n    def __init__(self, topology: str, start_time: str, end_time: str, is_temp: bool = False):\n        super().__init__(topology, \"\", is_temp)\n        self._start_time = start_time\n        self._end_time = end_time\n\n    def download(self, is_force: bool):\n    def clean(self):\n        logger.info_green(\"Cleaning weather data.\")\n        DataPipeline.clean(self)\n        self._new_file_list.append(self._clean_file)\n        self._preprocess(output_file=self._clean_file)\n\n    def _weather(self):\n        water = round(float(random.uniform(-1, 1)), 2)\n\n        snow = round(float(random.uniform(-1, 1)), 2)\n\n        if snow > 0.0 and water > 0:\n            return WeatherPipeline.WeatherEnum.SLEET.value\n        elif water > 0.0:\n            return WeatherPipeline.WeatherEnum.RAINY.value\n        elif snow > 0.0:\n            return WeatherPipeline.WeatherEnum.SNOWY.value\n        else:\n            return WeatherPipeline.WeatherEnum.SUNNY.value\n\n    def _gen_weather(self, tick):\n        date = tick.strftime(\"%m/%d/%Y %H:%M:%S\")\n        wh = self._weather()\n        temp = round(float(random.uniform(-1, 1) * 40), 2)\n\n        return {\"date\": date, \"weather\": wh, \"temp\": temp}\n\n    def _preprocess(self, output_file: str):\n        data: list = []\n\n        cur_tick = pd.to_datetime(self._start_time)\n        end_tick = pd.to_datetime(self._end_time)\n\n        while cur_tick <= end_tick:\n            new_weather = self._gen_weather(cur_tick)\n            data.append(new_weather)\n            cur_tick += pd.Timedelta(1, unit=\"day\")\n\n        with open(output_file, \"w+\") as fp:\n            writer = csv.DictWriter(fp, [\"date\", \"weather\", \"temp\"])\n\n            writer.writeheader()\n            writer.writerows(data)"}
{"Repository": "graph-index", "input": "Extremely fast webserver using libev. See http://www.fapws.org/ className FapwsServer(ServerAdapter) Method run Method app", "label": "class FapwsServer(ServerAdapter):\n    def run(self, handler): # pragma: no cover\n        import fapws._evwsgi as evwsgi\n        from fapws import base, config\n        port = self.port\n        if float(config.SERVER_IDENT[-2:]) > 0.4:\n            # fapws3 silently changed its API in 0.5\n            port = str(port)\n        evwsgi.start(self.host, port)\n        # fapws3 never releases the GIL. Complain upstream. I tried. No luck.\n        if 'BOTTLE_CHILD' in os.environ and not self.quiet:\n            _stderr(\"WARNING: Auto-reloading does not work with Fapws3.\\n\")\n            _stderr(\"         (Fapws3 breaks python thread support)\\n\")\n        evwsgi.set_base_module(base)\n        def app(environ, start_response):\n            environ['wsgi.multiprocess'] = False\n            return handler(environ, start_response)\n        evwsgi.wsgi_cb(('', app))\n        evwsgi.run()"}
{"Repository": "pyrequest", "input": "Wrapper to redirect stdout or stderr className OutputRedirector(object) Method __init__ Method write Method writelines Method flush Attribute fp", "label": "class OutputRedirector(object):\n    def __init__(self, fp):\n        self.fp = fp\n\n    def write(self, s):\n        self.fp.write(s)\n\n    def writelines(self, lines):\n        self.fp.writelines(lines)\n\n    def flush(self):\n        self.fp.flush()"}
{"Repository": "rarfile", "input": "RAR5 archive main record. className Rar5MainInfo(Rar5Info) Method _must_disable_hack", "label": "class Rar5MainInfo(Rar5Info):\n    type = RAR_BLOCK_MAIN\n    main_flags = None\n    main_volume_number = None\n\n    def _must_disable_hack(self):\n        if self.main_flags & RAR5_MAIN_FLAG_SOLID:\n            return True\n        return False"}
{"Repository": "dev-best-practices", "input": "The summary line for a class docstring should fit on one line. className ExampleClass(object) Method __init__ Method example_method Method __special__ Method __special_without_docstring__ Method __private Method __private_without_docstring Method _protected Method _protected_without_docstring Attribute attr1 Attribute attr2 Attribute attr3", "label": "class ExampleClass(object):\n    def __init__(self, param1, param2, param3=0):\n        self.attr1 = param1\n        self.attr2 = param2\n        self.attr3 = param3\n\n    def example_method(self, param1, param2):\n        return True\n\n    def __special__(self):\n        pass\n\n    def __special_without_docstring__(self):\n        pass\n\n    def __private(self):\n        pass\n\n    def __private_without_docstring(self):\n        pass\n      \n    def _protected(self):\n        pass\n      \n    def _protected_without_docstring(self):\n        pass"}
{"Repository": "TF-SimpleHumanPose", "input": "Apply a mapper/filter on the DataFlow. className MapData(object) Method __init__ Method get_data Method reset_state Attribute ds Attribute func", "label": "class MapData(object):\n    def __init__(self, ds, func):\n        self.ds = ds\n\n        self.func = func\n\n\n\n    def get_data(self):\n\n        for dp in self.ds.get_data():\n\n            ret = self.func(copy(dp))  # shallow copy the list\n\n            if ret is not None:\n\n                yield ret\n\n\n\n    def reset_state(self):\n\n        pass"}
{"Repository": "Siamese-LSTM", "input": "Keras Custom Layer that calculates Manhattan Distance. className ManDist(Layer) Method __init__ Method build Method call Method compute_output_shape Attribute result", "label": "class ManDist(Layer):\n    # initialize the layer, No need to include inputs parameter!\n    def __init__(self, **kwargs):\n        self.result = None\n        super(ManDist, self).__init__(**kwargs)\n\n    # input_shape will automatic collect input shapes to build layer\n    def build(self, input_shape):\n        super(ManDist, self).build(input_shape)\n\n    # This is where the layer's logic lives.\n    def call(self, x, **kwargs):\n        self.result = K.exp(-K.sum(K.abs(x[0] - x[1]), axis=1, keepdims=True))\n        return self.result\n\n    # return output shape\n    def compute_output_shape(self, input_shape):\n        return K.int_shape(self.result)"}
{"Repository": "LaTeXTools", "input": "Converts an dict into an object, such that every dict entry className objectview(object) Method __init__ Method __getattr__ Method __setattr__ Method copy Method __deepcopy__ Method __repr__", "label": "class objectview(object):\n    def __init__(self, d):\n        self.__dict__['_d'] = d\n\n    def __getattr__(self, attr):\n        return self._d[attr]\n\n    def __setattr__(self, attr, value):\n        raise TypeError('cannot set value on an objectview')\n\n    def copy(self, **add_or_replace):\n        new_dict = self._d.copy()\n        if add_or_replace:\n            new_dict.update(**add_or_replace)\n        return objectview(new_dict)\n\n    def __deepcopy__(self, memo):\n        cls = self.__class__\n        result = cls.__new__(cls)\n        memo[id(self)] = result\n        result.__dict__['_d'] = copy.deepcopy(self.__dict__['_d'], memo)\n        return result\n\n    def __repr__(self):\n        return repr(self._d)"}
{"Repository": "SegmenTron", "input": "Data parallelism Hide the difference of single/multiple GPUs to the user. className DataParallelModel(DataParallel) Method gather Method replicate", "label": "class DataParallelModel(DataParallel):\n    def gather(self, outputs, output_device):\n        return outputs\n\n    def replicate(self, module, device_ids):\n        modules = super(DataParallelModel, self).replicate(module, device_ids)\n        return modules"}
{"Repository": "Metis", "input": "The IsolationForest 'isolates' observations by randomly selecting a feature and then randomly selecting a split value between the maximum and minimum values of the selected feature. className IForest(object) Method predict", "label": "class IForest(object):\n    def __init__(self,\n                 n_estimators=3,\n                 max_samples=\"auto\",\n                 contamination=0.15,\n                 max_feature=1.,\n                 bootstrap=False,\n                 n_jobs=1,\n                 random_state=None,\n                 verbose=0):\n        self.n_estimators = n_estimators\n        self.max_samples = max_samples\n        self.contamination = contamination\n        self.max_feature = max_feature\n        self.bootstrap = bootstrap\n        self.n_jobs = n_jobs\n        self.random_state = random_state\n        self.verbose = verbose\n\n    def predict(self, X, window=DEFAULT_WINDOW):\n        x_train = list(range(0, 2 * window + 1)) + list(range(0, 2 * window + 1)) + list(range(0, window + 1))\n        sample_features = zip(x_train, X)\n        clf = IsolationForest(self.n_estimators, self.max_samples, self.contamination, self.max_feature, self.bootstrap, self.n_jobs, self.random_state, self.verbose)\n        clf.fit(sample_features)\n        predict_res = clf.predict(sample_features)\n        if predict_res[-1] == -1:\n            return 0\n        return 1"}
{"Repository": "django-fluent-comments", "input": "Manager to optimize SQL queries for comments. className FluentCommentManager(CommentManager) Method get_queryset", "label": "class FluentCommentManager(CommentManager):\n    def get_queryset(self):\n        return super().get_queryset().select_related(\"user\")"}
{"Repository": "contrastive-active-learning", "input": "Processor for the CoLA data set (GLUE version). className ColaProcessor(DataProcessor) Method get_example_from_tensor_dict Method get_train_examples Method get_dev_examples Method get_labels Method _create_examples", "label": "class ColaProcessor(DataProcessor):\n    def get_example_from_tensor_dict(self, tensor_dict):\n        return InputExample(\n            tensor_dict[\"idx\"].numpy(),\n            tensor_dict[\"sentence\"].numpy().decode(\"utf-8\"),\n            None,\n            str(tensor_dict[\"label\"].numpy()),\n        )\n\n    def get_train_examples(self, data_dir):\n        return self._create_examples(self._read_tsv(os.path.join(data_dir, \"train.tsv\")), \"train\")\n\n    def get_dev_examples(self, data_dir):\n        return self._create_examples(self._read_tsv(os.path.join(data_dir, \"dev.tsv\")), \"dev\")\n\n    def get_labels(self):\n        return [\"0\", \"1\"]\n\n    def _create_examples(self, lines, set_type):\n        examples = []\n        for (i, line) in enumerate(lines):\n            guid = \"%s-%s\" % (set_type, i)\n            text_a = line[3]\n            label = line[1]\n            examples.append(InputExample(guid=guid, text_a=text_a, text_b=None, label=label))\n        return examples"}
{"Repository": "pygogo", "input": "A logging class that logs events via different handlers depending on the severity http://stackoverflow. className Gogo(object) Method __init__ Method logger Method update_hdlr Method zip Method get_logger Method get_structured_logger Attribute levels Attribute loggers Attribute name Attribute handlers Attribute formatters Attribute monolog", "label": "class Gogo(object):\n    def __init__(self, name=\"root\", high_level=None, low_level=None, **kwargs):\n        verbose = kwargs.get(\"verbose\")\n        high_level = high_level or \"warning\"\n\n        if verbose is None:\n            low_level = low_level or \"debug\"\n        elif verbose:\n            low_level = \"debug\"\n        else:\n            low_level = \"info\"\n\n        self.levels = {\n            \"high\": getattr(logging, high_level.upper(), None),\n            \"low\": getattr(logging, low_level.upper(), None),\n        }\n\n        if not isinstance(self.levels[\"high\"], int):\n            raise ValueError(\"Invalid high_level: %s\" % self.levels[\"high\"])\n        elif not isinstance(self.levels[\"low\"], int):\n            raise ValueError(\"Invalid low_level: %s\" % self.levels[\"low\"])\n        elif self.levels[\"high\"] < self.levels[\"low\"]:\n            raise ValueError(\"high_level must be >= low_level\")\n\n        self.loggers = set()\n        self.name = name\n        self.handlers = {\n            \"high\": kwargs.get(\"high_hdlr\", handlers.stderr_hdlr()),\n            \"low\": kwargs.get(\"low_hdlr\", handlers.stdout_hdlr()),\n        }\n\n        self.formatters = {\n            \"high\": kwargs.get(\"high_formatter\", formatters.basic_formatter),\n            \"low\": kwargs.get(\"low_formatter\", formatters.basic_formatter),\n        }\n\n        self.monolog = kwargs.get(\"monolog\")\n\n    @property\n    def logger(self):\n        return self.get_logger()\n\n    def update_hdlr(self, hdlr, level, formatter=None, monolog=False, **kwargs):\n        hdlr.setLevel(level)\n\n        if monolog:\n            log_filter = utils.LogFilter(self.levels[\"high\"])\n            hdlr.addFilter(log_filter)\n\n        if kwargs:\n            structured_filter = utils.get_structured_filter(**kwargs)\n            hdlr.addFilter(structured_filter)\n\n        hdlr.setFormatter(formatter)\n\n    def zip(self, *fmtrs):\n        hdlrs = [self.handlers[\"high\"], self.handlers[\"low\"]]\n        levels = [self.levels[\"high\"], self.levels[\"low\"]]\n        monologs = [False, self.monolog]\n        return zip(hdlrs, levels, fmtrs, monologs)\n\n    def get_logger(self, name=\"base\", **kwargs):\n        lggr_name = \"%s.%s\" % (self.name, name)\n        logger = logging.getLogger(lggr_name)\n\n        if lggr_name not in self.loggers:\n            self.loggers.add(lggr_name)\n\n            if kwargs:\n                kwargs[\"name\"] = lggr_name\n\n            fmtrs = [self.formatters[\"high\"], self.formatters[\"low\"]]\n\n            for zipped in self.zip(*fmtrs):\n                hdlr, level, fmtr, monolog = zipped\n                copied_hdlr = copy_hdlr(hdlr)\n                self.update_hdlr(copied_hdlr, level, fmtr, monolog, **kwargs)\n                logger.addHandler(copied_hdlr)\n\n            logger.setLevel(self.levels[\"low\"])\n\n        return logger\n\n    def get_structured_logger(self, name=None, **kwargs):\n        # pylint: disable=dict-items-not-iterating\n        values = frozenset(kwargs.items())\n        name = name or hashlib.md5(str(values).encode(\"utf-8\")).hexdigest()\n        lggr_name = \"%s.structured.%s\" % (self.name, name)\n        logger = logging.getLogger(lggr_name)\n\n        if lggr_name not in self.loggers:\n            self.loggers.add(lggr_name)\n            formatter = formatters.basic_formatter\n\n            for zipped in self.zip(formatter, formatter):\n                hdlr, level, fmtr, monolog = zipped\n                copied_hdlr = copy_hdlr(hdlr)\n                self.update_hdlr(copied_hdlr, level, fmtr, monolog)\n                logger.addHandler(copied_hdlr)\n\n            logger.setLevel(self.levels[\"low\"])\n\n        return utils.StructuredAdapter(logger, kwargs)"}
{"Repository": "GOF_NeurIPS2021", "input": "Carla Dataset className Carla(Dataset) Method __init__ Method __len__ Method __getitem__ Attribute data Attribute transform", "label": "class Carla(Dataset):\n    def __init__(self, img_size, **kwargs):\n        super().__init__()\n        \n        dataset_path = './data/carla/*.png'\n        self.data = glob.glob(dataset_path)\n        assert len(self.data) > 0, \"Can't find data; make sure you specify the path to your dataset\"\n        self.transform = transforms.Compose(\n                    [transforms.Resize((img_size, img_size), interpolation=0), transforms.ToTensor(), transforms.Normalize([0.5], [0.5])])\n\n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, index):\n        X = PIL.Image.open(self.data[index])\n        X = self.transform(X)\n        \n        return X, 0"}
{"Repository": "SpanBERT", "input": "BERT model (\"Bidirectional Embedding Representations from a Transformer\"). className BertModel(BertPreTrainedModel) Method __init__ Method forward Attribute embeddings Attribute encoder Attribute pooler", "label": "class BertModel(BertPreTrainedModel):\n    def __init__(self, config):\n        super(BertModel, self).__init__(config)\n        self.embeddings = BertEmbeddings(config)\n        self.encoder = BertEncoder(config)\n        self.pooler = BertPooler(config)\n        self.apply(self.init_bert_weights)\n\n    def forward(self, input_ids, token_type_ids=None, attention_mask=None, output_all_encoded_layers=True):\n        if attention_mask is None:\n            attention_mask = torch.ones_like(input_ids)\n        if token_type_ids is None:\n            token_type_ids = torch.zeros_like(input_ids)\n\n        # We create a 3D attention mask from a 2D tensor mask.\n        # Sizes are [batch_size, 1, 1, to_seq_length]\n        # So we can broadcast to [batch_size, num_heads, from_seq_length, to_seq_length]\n        # this attention mask is more simple than the triangular masking of causal attention\n        # used in OpenAI GPT, we just need to prepare the broadcast dimension here.\n        extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)\n\n        # Since attention_mask is 1.0 for positions we want to attend and 0.0 for\n        # masked positions, this operation will create a tensor which is 0.0 for\n        # positions we want to attend and -10000.0 for masked positions.\n        # Since we are adding it to the raw scores before the softmax, this is\n        # effectively the same as removing these entirely.\n        extended_attention_mask = extended_attention_mask.to(dtype=next(self.parameters()).dtype) # fp16 compatibility\n        extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0\n\n        embedding_output = self.embeddings(input_ids, token_type_ids)\n        encoded_layers = self.encoder(embedding_output,\n                                      extended_attention_mask,\n                                      output_all_encoded_layers=output_all_encoded_layers)\n        sequence_output = encoded_layers[-1]\n        pooled_output = self.pooler(sequence_output)\n        if not output_all_encoded_layers:\n            encoded_layers = encoded_layers[-1]\n        return encoded_layers, pooled_output"}
{"Repository": "mail-security-tester", "input": "Dumps each test case into separate plain file className FileDelivery(DeliveryBase) Method deliver_testcase", "label": "class FileDelivery(DeliveryBase):\n    def deliver_testcase(self, testcase, recipient):\n        outname = \"{}/{}-{:04d}-{:02d}.msg\".format(\n                self.target,\n                self.testcases.identifier,\n                self.testcase_index,\n                self.recipient_index\n                )\n        try:\n            out = open(outname, \"wb\")\n            out.write(testcase.as_bytes())\n            out.close()\n        except ( IOError, OSError ) as e:\n            print(\"! Error while creation of test file {}: {}\".format(outname, str(e)))"}
{"Repository": "htag", "input": "Simple ASync Web Server (with starlette) with WebSocket interactions with HTag. className BrowserStarletteWS(Starlette) Method __init__ Method instanciate Attribute _hr_session Attribute hrenderer Attribute tagClass", "label": "class BrowserStarletteWS(Starlette):\n    def __init__(self,tagClass:Tag,file:\"str|None\"=None):\n        self._hr_session=commons.SessionFile(file) if file else None\n        assert issubclass(tagClass,Tag)\n        self.hrenderer = None\n        self.tagClass = tagClass\n\n        async def _sendactions(ws, actions:dict) -> bool:\n            try:\n                await ws.send_text( json.dumps(actions) )\n                return True\n            except Exception as e:\n                logger.error(\"Can't send to socket, error: %s\",e)\n                return False\n\n\n        class WsInteract(WebSocketEndpoint):\n            encoding = \"json\"\n\n            #=========================================================\n            async def on_connect(this, websocket):\n\n                # accept cnx\n                await websocket.accept()\n\n                # declare hr.sendactions (async method)\n                self.hrenderer.sendactions = lambda actions: _sendactions(websocket,actions)\n\n            #=========================================================\n\n            async def on_receive(this, websocket, data):\n                actions = await self.hrenderer.interact(data[\"id\"],data[\"method\"],data[\"args\"],data[\"kargs\"],data.get(\"event\"))\n                await _sendactions( websocket, actions )\n\n        Starlette.__init__(self,debug=True, routes=[\n            Route('/', self.GET, methods=[\"GET\"]),\n            WebSocketRoute(\"/ws\", WsInteract),\n        ])\n\n\n    def instanciate(self,url:str):\n        init = commons.url2ak(url)\n        if self.hrenderer and self.hrenderer.init == init:\n            return self.hrenderer\n\n        js = \"\"\""}
{"Repository": "matrixcli", "input": "The home server returned an error response. className MatrixRequestError(MatrixError) Method __init__ Attribute code Attribute content", "label": "class MatrixRequestError(MatrixError):\n    def __init__(self, code=0, content=\"\"):\n        super(MatrixRequestError, self).__init__(\"%d: %s\" % (code, content))\n        self.code = code\n        self.content = content"}
{"Repository": "Self-PU", "input": "Iterate two sets of indices An 'epoch' is one iteration through the primary indices. className TwoStreamBatchSampler(Sampler) Method __init__ Method __iter__ Method __len__ Attribute primary_indices Attribute secondary_indices Attribute secondary_batch_size Attribute primary_batch_size", "label": "class TwoStreamBatchSampler(Sampler):\n    def __init__(self, primary_indices, secondary_indices, batch_size, secondary_batch_size):\n        self.primary_indices = primary_indices\n        self.secondary_indices = secondary_indices\n        self.secondary_batch_size = secondary_batch_size\n        self.primary_batch_size = batch_size - secondary_batch_size\n\n        assert len(self.primary_indices) >= self.primary_batch_size > 0\n        assert len(self.secondary_indices) >= self.secondary_batch_size > 0\n\n    def __iter__(self):\n        primary_iter = iterate_once(self.primary_indices)\n        secondary_iter = iterate_eternally(self.secondary_indices)\n        return (\n            primary_batch + secondary_batch\n            for (primary_batch, secondary_batch)\n            in  zip(grouper(primary_iter, self.primary_batch_size),\n                    grouper(secondary_iter, self.secondary_batch_size))\n        )\n\n    def __len__(self):\n        return len(self.primary_indices) // self.primary_batch_size"}
{"Repository": "gym-doom", "input": "------------ Training Mission 1 - Basic ------------ This map is rectangular with gray walls, ceiling and floor. className DoomBasicEnv(DoomEnv) Method __init__", "label": "class DoomBasicEnv(DoomEnv):\n    def __init__(self):\n        super(DoomBasicEnv, self).__init__(0)"}
{"Repository": "peas", "input": "Represents a substrate, that is a configuration of nodes without connection weights. className Substrate(object) Method __init__ Method add_nodes Method add_connections Method get_connection_list Attribute nodes Attribute is_input Attribute num_nodes Attribute layers Attribute connections Attribute connection_ids Attribute linkexpression_ids", "label": "class Substrate(object):\n    def __init__(self, nodes_or_shape=None):\n        self.nodes = None\n        self.is_input = []\n        self.num_nodes = 0\n        self.layers = {}\n        self.connections = []\n        self.connection_ids = []\n        self.linkexpression_ids = []\n        # If a shape is passed, create a mesh grid of nodes.\n        if nodes_or_shape is not None:\n            self.add_nodes(nodes_or_shape, 'a')\n            self.add_connections('a', 'a')\n            \n                    \n    def add_nodes(self, nodes_or_shape, layer_id='a', is_input=False):\n        if type(nodes_or_shape) == list:\n            newnodes = np.array(nodes_or_shape)\n\n        elif type(nodes_or_shape) == tuple:\n            # Create coordinate grids\n            newnodes = np.mgrid[[slice(-1, 1, s*1j) for s in nodes_or_shape]]\n            # Move coordinates to last dimension\n            newnodes = newnodes.transpose(range(1,len(nodes_or_shape)+1) + [0])\n            # Reshape to a N x nD list.\n            newnodes = newnodes.reshape(-1, len(nodes_or_shape))\n            self.dimensions = len(nodes_or_shape)\n\n        elif type(nodes_or_shape) == np.ndarray:\n            pass # all is good\n\n        else:\n            raise Exception(\"nodes_or_shape must be a list of nodes or a shape tuple.\")\n            \n        if self.nodes is None:\n            self.dimensions = newnodes.shape[1]\n            self.nodes = np.zeros((0, self.dimensions))\n        \n        # keep a dictionary with the set of node IDs for each layer_id\n        ids = self.layers.get(layer_id, set())\n        ids |= set(range(len(self.nodes), len(self.nodes) + len(newnodes)))\n        self.layers[layer_id] = ids\n            \n        # append the new nodes\n        self.nodes = np.vstack((self.nodes, newnodes))\n        self.num_nodes += len(newnodes)\n        \n    def add_connections(self, from_layer='a', to_layer='a', connection_id=-1, max_length=inf, link_expression_id=None):\n        conns = product( self.layers[from_layer], self.layers[to_layer] )\n        conns = filter(lambda (fr, to): np.all(np.abs(self.nodes[fr] - self.nodes[to]) <=  max_length), conns)\n        self.connections.extend(conns)\n        self.connection_ids.extend([connection_id] * len(conns))\n        self.linkexpression_ids.extend([link_expression_id] * len(conns))\n        \n    def get_connection_list(self, add_deltas):\n        if not hasattr(self, '_connection_list'):\n            \n            self._connection_list = []\n            for ((i, j), conn_id, expr_id) in izip(self.connections, self.connection_ids, self.linkexpression_ids):\n                fr = self.nodes[i]\n                to = self.nodes[j]\n                if add_deltas:\n                    conn = np.hstack((fr, to, to-fr))\n                else:\n                    conn = np.hstack((fr, to))\n                self._connection_list.append(((i, j), conn, conn_id, expr_id))\n\n        return self._connection_list"}
{"Repository": "grammarinator", "input": "Tree node representing a parser rule. className UnparserRule(ParentRule) Method __getattr__ Method __deepcopy__", "label": "class UnparserRule(ParentRule):\n    def __getattr__(self, item):\n        # This check is needed to avoid infinite recursions when loading a tree\n        # with pickle. In such cases, the loaded instance is prepared by\n        # creating an empty object with the expected ``__class__`` and by\n        # restoring the saved attributes (without calling ``__init__``).\n        # During this operation, the ``__set_state__`` method of the target\n        # class is tried to be called, if it exists. Otherwise, ``__getattr__``\n        # throws an ``AttributeError``. However, if the instantiation of this\n        # error object tries to access any field that is not yet added by\n        # pickle, then it throws another ``AttributeError``, causing an\n        # infinite recursion. Filtering for the field names, that are used\n        # later in this method, eliminates the issue.\n        if item in ['name', 'children']:\n            raise AttributeError()\n\n        result = [child for child in self.children if child.name == item]\n\n        if not result:\n            raise AttributeError(f'[{self.name}] No child with name {item!r} {[child.name for child in self.children]}.')\n\n        return result[0] if len(result) == 1 else result\n\n    def __deepcopy__(self, memo):\n        return UnparserRule(name=deepcopy(self.name, memo), children=[deepcopy(child, memo) for child in self.children])"}
{"Repository": "sublime-phpcs", "input": "Concrete class for phpcbf className CodeBeautifier(ShellCommand) Method execute Method parse_report", "label": "class CodeBeautifier(ShellCommand):\n    def execute(self, path):\n        args = []\n\n        if pref.get(\n            \"phpcs_php_prefix_path\"\n        ) != \"\" and self.__class__.__name__ in pref.get(\"phpcs_commands_to_php_prefix\"):\n            args = [pref.get(\"phpcs_php_prefix_path\")]\n\n        if pref.get(\"phpcbf_executable_path\") != \"\":\n            if len(args) > 0:\n                args.append(pref.get(\"phpcbf_executable_path\"))\n            else:\n                args = [pref.get(\"phpcbf_executable_path\")]\n        else:\n            debug_message(\"phpcbf_executable_path is not set, therefore cannot execute\")\n            sublime.error_message(\n                'The \"phpcbf_executable_path\" is not set, therefore cannot execute this command'\n            )\n            return\n\n        args.append(os.path.normpath(path))\n\n        # Add the additional arguments from the settings file to the command\n        for key, value in pref.get(\"phpcbf_additional_args\").items():\n            arg = key\n            if value != \"\":\n                arg += \"=\" + value\n            args.append(arg)\n\n        self.parse_report(args)\n\n    def parse_report(self, args):\n        report = self.shell_out(args)\n        debug_message(report)\n        lines = re.finditer(\".*\\((?P<number>\\d+) fixable violations\\)\", report)\n\n        for line in lines:\n            error = CheckstyleError(0, line.group(\"number\") + \" fixed violations\")\n            self.error_list.append(error)"}
{"Repository": "ConceptWhitening", "input": "Computes and stores the average and current value className AverageMeter(object) Method __init__ Method reset Method update", "label": "class AverageMeter(object):\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count"}
{"Repository": "xcffib", "input": "This definitely isn't needed, but we keep it around for compatibility with xpyb. className ExtensionKey(object) Method __init__ Method __hash__ Method __eq__ Method __ne__ Method to_cffi Attribute name", "label": "class ExtensionKey(object):\n    def __init__(self, name):\n        self.name = name\n\n    def __hash__(self):\n        return hash(self.name)\n\n    def __eq__(self, o):\n        return self.name == o.name\n\n    def __ne__(self, o):\n        return self.name != o.name\n\n    def to_cffi(self):\n        c_key = ffi.new(\"struct xcb_extension_t *\")\n        c_key.name = name = ffi.new('char[]', self.name.encode())\n        cffi_explicit_lifetimes[c_key] = name\n        # xpyb doesn't ever set global_id, which seems wrong, but whatever.\n        c_key.global_id = 0\n\n        return c_key"}
{"Repository": "happybase", "input": "Batch mutation class. className Batch(object) Method _reset_mutations Method send Method put Method delete Method __enter__ Method __exit__", "label": "class Batch(object):\n    def __init__(self, table, timestamp=None, batch_size=None,\n                 transaction=False, wal=True):\n        if not (timestamp is None or isinstance(timestamp, Integral)):\n            raise TypeError(\"'timestamp' must be an integer or None\")\n\n        if batch_size is not None:\n            if transaction:\n                raise TypeError(\"'transaction' cannot be used when \"\n                                \"'batch_size' is specified\")\n            if not batch_size > 0:\n                raise ValueError(\"'batch_size' must be > 0\")\n\n        self._table = table\n        self._batch_size = batch_size\n        self._timestamp = timestamp\n        self._transaction = transaction\n        self._wal = wal\n        self._families = None\n        self._reset_mutations()\n\n    def _reset_mutations(self):\n        self._mutations = defaultdict(list)\n        self._mutation_count = 0\n\n    def send(self):\n        bms = [\n            BatchMutation(row, m)\n            for row, m in six.iteritems(self._mutations)\n        ]\n        if not bms:\n            return\n\n        logger.debug(\"Sending batch for '%s' (%d mutations on %d rows)\",\n                     self._table.name, self._mutation_count, len(bms))\n        if self._timestamp is None:\n            self._table.connection.client.mutateRows(self._table.name, bms, {})\n        else:\n            self._table.connection.client.mutateRowsTs(\n                self._table.name, bms, self._timestamp, {})\n\n        self._reset_mutations()\n\n    #\n    # Mutation methods\n    #\n\n    def put(self, row, data, wal=None):\n        if wal is None:\n            wal = self._wal\n\n        self._mutations[row].extend(\n            Mutation(\n                isDelete=False,\n                column=column,\n                value=value,\n                writeToWAL=wal)\n            for column, value in six.iteritems(data))\n\n        self._mutation_count += len(data)\n        if self._batch_size and self._mutation_count >= self._batch_size:\n            self.send()\n\n    def delete(self, row, columns=None, wal=None):\n        # Work-around Thrift API limitation: the mutation API can only\n        # delete specified columns, not complete rows, so just list the\n        # column families once and cache them for later use by the same\n        # batch instance.\n        if columns is None:\n            if self._families is None:\n                self._families = self._table._column_family_names()\n            columns = self._families\n\n        if wal is None:\n            wal = self._wal\n\n        self._mutations[row].extend(\n            Mutation(isDelete=True, column=column, writeToWAL=wal)\n            for column in columns)\n\n        self._mutation_count += len(columns)\n        if self._batch_size and self._mutation_count >= self._batch_size:\n            self.send()\n\n    #\n    # Context manager methods\n    #\n\n    def __enter__(self):\n        return self\n\n    def __exit__(self, exc_type, exc_value, traceback):\n        # If the 'with' block raises an exception, the batch will not be\n        # sent to the server.\n        if self._transaction and exc_type is not None:\n            return\n\n        self.send()"}
{"Repository": "redis-stack", "input": "A recipe, to build the redis-stack-server package className RedisStackServer(Recipe) Method __init__ Attribute OSNICK Attribute ARCH Attribute OSNAME Attribute __PATHS__ Attribute C", "label": "class RedisStackServer(Recipe):\n    PACKAGE_NAME = \"redis-stack-server\"\n\n    def __init__(self, osnick, arch=\"x86_64\", osname=\"Linux\"):\n        self.OSNICK = osnick\n        self.ARCH = arch\n        self.OSNAME = osname\n        self.__PATHS__ = Paths(self.PACKAGE_NAME, osnick, arch, osname)\n        self.C = Config()\n\n    def prepackage(\n        self,\n        binary_dir: Union[str, None],\n        ignore: bool = False,\n        version_override: str = None,\n    ):\n        for i in [\n            self.__PATHS__.EXTERNAL,\n            self.__PATHS__.DESTDIR,\n            self.__PATHS__.LIBDIR,\n            self.__PATHS__.BINDIR,\n            self.__PATHS__.SHAREDIR,\n            self.__PATHS__.BASEETCDIR,\n            self.__PATHS__.BASEVARDBDIR,\n        ]:\n            os.makedirs(i, exist_ok=True, mode=0o755)\n\n        m = Modules(self.PACKAGE_NAME, self.OSNICK, self.ARCH, self.OSNAME)\n        for i in [\n            m.redisearch,\n            m.redisgraph,\n            m.redistimeseries,\n            m.rejson,\n            m.redisgears,\n            m.redisbloom,\n        ]:\n            try:\n                if version_override is None:\n                    i()\n                else:\n                    i(version_override)\n            except requests.HTTPError:\n                if ignore:\n                    pass\n                else:\n                    raise\n\n        # per os\n        logger.debug(\"Copying redis-stack-server script\")\n        stackdest = os.path.join(self.__PATHS__.BINDIR, \"redis-stack-server\")\n        shutil.copyfile(\n            os.path.join(self.__PATHS__.SCRIPTDIR, \"scripts\", \"redis-stack-server\"),\n            stackdest,\n        )\n        os.chmod(stackdest, mode=0o755)\n\n        if binary_dir is not None:\n            r = Redis(self.PACKAGE_NAME, self.OSNICK, self.ARCH, self.OSNAME)\n            r.prepare()\n        else:\n            logger.debug(f\"Copying redis binaries from {binary_dir}\")\n            for i in [\n                \"redis-benchmark\",\n                \"redis-check-aof\",\n                \"redis-check-rdb\",\n                \"redis-cli\",\n                \"redis-sentinel\",\n                \"redis-server\",\n            ]:\n                dest = os.path.join(self.__PATHS__.BINDIR, i)\n                shutil.copy2(os.path.join(binary_dir, i), dest)\n                os.chmod(dest, mode=0o755)\n\n        # linux only - copy to /etc\n        if self.OSNAME == \"Linux\":\n            confdest = os.path.join(self.__PATHS__.BASEETCDIR, \"redis-stack.conf\")\n            shutil.copy(\n                os.path.join(\n                    self.__PATHS__.SCRIPTDIR, \"conf\", \"redis-stack-service.conf\"\n                ),\n                confdest,\n            )\n            os.chmod(confdest, mode=0o640)\n\n        # copy configuration files\n        shutil.copytree(\n            os.path.join(self.__PATHS__.SCRIPTDIR, \"conf\"),\n            self.__PATHS__.ETCDIR,\n            dirs_exist_ok=True,\n        )\n\n        # license files\n        shutil.copytree(\n            os.path.join(self.__PATHS__.SCRIPTDIR, \"licenses\"),\n            os.path.join(self.__PATHS__.SHAREDIR),\n            dirs_exist_ok=True,\n        )"}
{"Repository": "i3-py", "input": "Raised when a socket couldn't connect to the window manager. className ConnectionError(i3Exception) Method __init__", "label": "class ConnectionError(i3Exception):\n    def __init__(self, socket_path):\n        msg = \"Could not connect to socket at '%s'\" % socket_path\n        super(ConnectionError, self).__init__(msg)"}
{"Repository": "seed_rl", "input": "Context manager that allows to collect logging tensors from LoggingModules. className LoggingTape(object) Method __init__ Method __enter__ Method __exit__ Attribute _tracked_modules", "label": "class LoggingTape(object):\n  def __init__(self, tracked_modules):\n    if isinstance(tracked_modules, tf.Module):\n      tracked_modules = [tracked_modules] + list(tracked_modules.submodules)\n    self._tracked_modules = list(tracked_modules)\n\n  def __enter__(self):\n    logged_tensors = collections.OrderedDict()\n    for submodule in self._tracked_modules:\n      if not isinstance(submodule, LoggingModule):\n        continue\n      submodule.set_logging_dict(logged_tensors)\n    return logged_tensors\n\n  def __exit__(self, exc_type, exc_value, traceback):\n    del exc_type, exc_value, traceback\n    for submodule in self._tracked_modules:\n      if not isinstance(submodule, LoggingModule):\n        continue\n      submodule.unset_logging_dict()"}
{"Repository": "Beta-DARTS", "input": "3x3 max pool with no subsampling. className MaxPool3x3(BaseOp) Method build", "label": "class MaxPool3x3(BaseOp):\n  def build(self, inputs, channels):\n    del channels    # Unused\n    with tf.variable_scope('MaxPool3x3'):\n      net = tf.layers.max_pooling2d(\n          inputs=inputs,\n          pool_size=(3, 3),\n          strides=(1, 1),\n          padding='same',\n          data_format=self.data_format)\n\n    return net"}
{"Repository": "mmengine", "input": "OptimWrapper for ColossalAI. className ColossalAIOptimWrapper(OptimWrapper) Method optim_context Method backward", "label": "class ColossalAIOptimWrapper(OptimWrapper):\n    \"\"\"OptimWrapper for ColossalAI.\n\n    The available optimizers are:\n      - CPUAdam\n      - FusedAdam\n      - FusedLAMB\n      - FusedSGD\n      - HybridAdam\n      - Lamb\n      - Lars\n\n    You can find more details in the `colossalai tutorial`_\n\n    Args:\n        optimizer (dict or torch.optim.Optimizer): The optimizer to be\n            wrapped.\n        accumulative_counts (int): The number of iterations to accumulate\n            gradients. The parameters will be updated per\n            ``accumulative_counts``.\n\n    .. _colossalai tutorial: https://github.com/hpcaitech/ColossalAI/tree/main/colossalai/nn/optimizer\n    \"\"\"  # noqa: E501\n\n    def __init__(self,\n                 optimizer: torch.optim.Optimizer,\n                 booster: Optional[Booster] = None,\n                 accumulative_counts: int = 1):\n        super().__init__(optimizer, accumulative_counts=accumulative_counts)\n        self.booster = booster\n\n    @contextmanager\n    def optim_context(self, model: nn.Module):\n        assert isinstance(self.booster, Booster), \\\n            'Please set the booster attribute before using ' \\\n            '`ColossalAIOptimWrapper`.'\n        if self.booster.plugin.support_no_sync():\n            no_sync_context = self.booster.no_sync(model, self.optimizer)\n        else:\n            yield\n            return\n        if self.should_sync():\n            yield\n        else:\n            with no_sync_context:\n                yield\n\n    def backward(self, loss: torch.Tensor, **kwargs) -> None:\n        self._inner_count += 1\n        self.optimizer.backward(loss, **kwargs)"}
{"Repository": "Katalina", "input": "A variable-length unsigned integer using base128 encoding. className VlqBase128Le(KaitaiStruct) Method __init__ Method _read Method __init__ Method _read Method has_next Method value Method len Method value Attribute _io Attribute _parent Attribute _root", "label": "class VlqBase128Le(KaitaiStruct):\n    def __init__(self, _io, _parent=None, _root=None):\n        self._io = _io\n        self._parent = _parent\n        self._root = _root if _root else self\n        self._read()\n\n    def _read(self):\n        self.groups = []\n        i = 0\n        while True:\n            _ = VlqBase128Le.Group(self._io, self, self._root)\n            self.groups.append(_)\n            if not (_.has_next):\n                break\n            i += 1\n\n    class Group(KaitaiStruct):\n        def __init__(self, _io, _parent=None, _root=None):\n            self._io = _io\n            self._parent = _parent\n            self._root = _root if _root else self\n            self._read()\n\n        def _read(self):\n            self.b = self._io.read_u1()\n\n        @property\n        def has_next(self):\n            if hasattr(self, '_m_has_next'):\n                return self._m_has_next if hasattr(self, '_m_has_next') else None\n\n            self._m_has_next = (self.b & 128) != 0\n            return self._m_has_next if hasattr(self, '_m_has_next') else None\n\n        @property\n        def value(self):\n            if hasattr(self, '_m_value'):\n                return self._m_value if hasattr(self, '_m_value') else None\n\n            self._m_value = (self.b & 127)\n            return self._m_value if hasattr(self, '_m_value') else None\n\n\n    @property\n    def len(self):\n        if hasattr(self, '_m_len'):\n            return self._m_len if hasattr(self, '_m_len') else None\n\n        self._m_len = len(self.groups)\n        return self._m_len if hasattr(self, '_m_len') else None\n\n    @property\n    def value(self):\n        if hasattr(self, '_m_value'):\n            return self._m_value if hasattr(self, '_m_value') else None\n\n        self._m_value = (((((((self.groups[0].value + ((self.groups[1].value << 7) if self.len >= 2 else 0)) + ((self.groups[2].value << 14) if self.len >= 3 else 0)) + ((self.groups[3].value << 21) if self.len >= 4 else 0)) + ((self.groups[4].value << 28) if self.len >= 5 else 0)) + ((self.groups[5].value << 35) if self.len >= 6 else 0)) + ((self.groups[6].value << 42) if self.len >= 7 else 0)) + ((self.groups[7].value << 49) if self.len >= 8 else 0))\n        return self._m_value if hasattr(self, '_m_value') else None"}
{"Repository": "django-postgres-extra", "input": "Wraps the standard PostgreSQL database back-end. className DatabaseWrapper(Wrapper) Method __init__ Method prepare_database Attribute introspection Attribute ops", "label": "class DatabaseWrapper(Wrapper):\n    SchemaEditorClass = PostgresSchemaEditor  # type: ignore[assignment]\n    introspection_class = PostgresIntrospection\n    ops_class = PostgresOperations\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n\n        # Some base back-ends such as the PostGIS back-end don't properly\n        # set `ops_class` and `introspection_class` and initialize these\n        # classes themselves.\n        #\n        # This can lead to broken functionality. We fix this automatically.\n\n        if not isinstance(self.introspection, self.introspection_class):\n            self.introspection = self.introspection_class(self)\n\n        if not isinstance(self.ops, self.ops_class):\n            self.ops = self.ops_class(self)\n\n        for expected_compiler_class in self.ops.compiler_classes:\n            compiler_class = self.ops.compiler(expected_compiler_class.__name__)\n\n            if not issubclass(compiler_class, expected_compiler_class):\n                logger.warning(\n                    \"Compiler '%s.%s' is not properly deriving from '%s.%s'.\"\n                    % (\n                        compiler_class.__module__,\n                        compiler_class.__name__,\n                        expected_compiler_class.__module__,\n                        expected_compiler_class.__name__,\n                    )\n                )\n\n    def prepare_database(self):\n        super().prepare_database()\n\n        setup_ext = getattr(\n            settings, \"POSTGRES_EXTRA_AUTO_EXTENSION_SET_UP\", True\n        )\n        if not setup_ext:\n            return False\n\n        with self.cursor() as cursor:\n            try:\n                cursor.execute(\"CREATE EXTENSION IF NOT EXISTS hstore\")\n            except ProgrammingError:  # permission denied\n                logger.warning(\n                    'Failed to create \"hstore\" extension. '\n                    \"Tables with hstore columns may fail to migrate. \"\n                    \"If hstore is needed, make sure you are connected \"\n                    \"to the database as a superuser \"\n                    \"or add the extension manually.\",\n                    exc_info=True,\n                )"}
{"Repository": "seq2seq-keyphrase-pytorch", "input": "Data loader. className KeyphraseDataLoader(object) Method __iter__ Method __len__ Method one2one_number", "label": "class KeyphraseDataLoader(object):\n    def __init__(self, dataset, max_batch_example=5, max_batch_pair=1, shuffle=False, sampler=None, batch_sampler=None,\n                 num_workers=0, collate_fn=default_collate, pin_memory=False, drop_last=False):\n        self.dataset            = dataset\n        # used for generating one2many batches\n        self.num_trgs           = [len(e['trg']) for e in dataset.get_examples()]\n        self.batch_size         = max_batch_pair\n        self.max_example_number = max_batch_example\n        self.num_workers        = num_workers\n        self.collate_fn         = collate_fn\n        self.pin_memory         = pin_memory\n        self.drop_last          = drop_last\n\n        if batch_sampler is not None:\n            if max_batch_pair > 1 or shuffle or sampler is not None or drop_last:\n                raise ValueError('batch_sampler is mutually exclusive with '\n                                 'batch_size, shuffle, sampler, and drop_last')\n\n        if sampler is not None and shuffle:\n            raise ValueError('sampler is mutually exclusive with shuffle')\n\n        if batch_sampler is None:\n            if sampler is None:\n                if shuffle:\n                    sampler = RandomSampler(dataset)\n                else:\n                    sampler = SequentialSampler(dataset)\n\n        batch_sampler = One2ManyBatchSampler(sampler, self.num_trgs, max_batch_example=max_batch_example, max_batch_pair=max_batch_pair, drop_last=drop_last)\n\n        self.sampler = sampler\n        self.batch_sampler = batch_sampler\n\n    def __iter__(self):\n        return DataLoaderIter(self)\n\n    def __len__(self):\n        return len(self.batch_sampler)\n\n    def one2one_number(self):\n        return sum(self.num_trgs)"}
{"Repository": "tegaki", "input": "Class providing IO functionality to L{Character} and \\ L{CharacterCollection}. className _IOBase(object) Method __init__ Method read Method read_string Method write Method write_string Method save Attribute _path", "label": "class _IOBase(object):\n    def __init__(self, path=None):\n        self._path = path\n\n        if path is not None:\n            gzip = True if path.endswith(\".gz\") or path.endswith(\".gzip\") \\\n                        else False\n            bz2 = True if path.endswith(\".bz2\") or path.endswith(\".bzip2\") \\\n                       else False\n\n            self.read(path, gzip=gzip, bz2=bz2)\n\n    def read(self, file, gzip=False, bz2=False, compresslevel=9):\n        try:\n            if type(file) == str:\n                if gzip:\n                    file = gzipm.GzipFile(file, compresslevel=compresslevel)\n                elif bz2:\n                    try:\n                        file = bz2m.BZ2File(file, compresslevel=compresslevel)\n                    except NameError:\n                        raise NotImplementedError\n                else:\n                    file = open(file)\n\n                self._parse_file(file)\n                file.close()\n            else:\n                self._parse_file(file)\n        except (IOError, xml.parsers.expat.ExpatError):\n            raise ValueError\n\n    def read_string(self, string, gzip=False, bz2=False, compresslevel=9):\n        if gzip:\n            io = io.StringIO(string)\n            io = gzipm.GzipFile(fileobj=io, compresslevel=compresslevel)\n            string = io.read()\n        elif bz2:\n            try:\n                string = bz2m.decompress(string)\n            except NameError:\n                raise NotImplementedError\n\n        self._parse_str(string)\n\n    def write(self, file, gzip=False, bz2=False, compresslevel=9):\n        if type(file) == str:\n            if gzip:\n                file = gzipm.GzipFile(file, \"w\", compresslevel=compresslevel)\n            elif bz2:\n                try:\n                    file = bz2m.BZ2File(file, \"w\", compresslevel=compresslevel)\n                except NameError:\n                    raise NotImplementedError\n            else:\n                file = open(file, \"w\")\n\n            file.write(self.to_str())\n            file.close()\n        else:\n            file.write(self.to_str())\n\n    def write_string(self, gzip=False, bz2=False, compresslevel=9):\n        if bz2:\n            try:\n                return bz2m.compress(self.to_str(), compresslevel=compresslevel)\n            except NameError:\n                raise NotImplementedError\n        elif gzip:\n            io = io.StringIO()\n            f = gzipm.GzipFile(fileobj=io, mode=\"w\",\n                               compresslevel=compresslevel)\n            f.write(self.to_str())\n            f.close()\n            return io.getvalue()\n        else:\n            return self.to_str()\n\n    def save(self, path=None):\n        if [path, self._path] == [None, None]:\n            raise ValueError(\"A path must be specified\")\n        elif path is None:\n            path = self._path\n\n        gzip = True if path.endswith(\".gz\") or path.endswith(\".gzip\") \\\n                    else False\n        bz2 = True if path.endswith(\".bz2\") or path.endswith(\".bzip2\") \\\n                       else False\n\n        self.write(path, gzip=gzip, bz2=bz2)"}
{"Repository": "terraformpy", "input": "TypedObjectAttr is a wrapper returned by TypedObject for attributes accessed which don't exist. className TypedObjectAttr(str) Method __new__ Method _name_with_index Method __getitem__ Method __getattr__", "label": "class TypedObjectAttr(str):\n    def __new__(cls, terraform_name, name, item=None):\n        obj = super(TypedObjectAttr, cls).__new__(\n            cls, \"${{{0}.{1}}}\".format(terraform_name, cls._name_with_index(name, item))\n        )\n        obj._terraform_name = terraform_name\n        obj._name = name\n        obj._item = item\n        return obj\n\n    @staticmethod\n    def _name_with_index(name, item):\n        if item is None:\n            return name\n        else:\n            return \"{0}.{1}\".format(name, item)\n\n    def __getitem__(self, item):\n        return TypedObjectAttr(\n            self._terraform_name, self._name_with_index(self._name, self._item), item\n        )\n\n    def __getattr__(self, item):\n        return TypedObjectAttr(\n            self._terraform_name, self._name_with_index(self._name, self._item), item\n        )"}
{"Repository": "sublemacspro", "input": "Given a rectangle insert the text at points className SbpRectangleInsert(SbpTextCommand) Method run_cmd Method replace", "label": "class SbpRectangleInsert(SbpTextCommand):\n  def run_cmd(self, jove, **args):\n    self.jove = jove\n    self.view.window().show_input_panel(\"Content:\", \"\", self.replace, None, None)\n\n  def replace(self, content):\n    self.jove.view.run_command(\"sbp_rectangle_insert_handler\", {\"content\": content})"}
{"Repository": "skytools-legacy", "input": "BatchWalker that returns RetriableEvents className RetriableBatchWalker(BaseBatchWalker) Method __init__ Method _make_event Method tag_event_done Method tag_event_retry Method get_status Method iter_status Attribute status_map", "label": "class RetriableBatchWalker(BaseBatchWalker):\n    def __init__(self, curs, batch_id, queue_name, fetch_size = 300, consumer_filter = None):\n        super(RetriableBatchWalker, self).__init__(curs, batch_id, queue_name, fetch_size, consumer_filter)\n        self.status_map = {}\n\n    def _make_event(self, queue_name, row):\n        return RetriableWalkerEvent(self, queue_name, row)\n\n    def tag_event_done(self, event):\n        if event.id in self.status_map:\n            del self.status_map[event.id]\n\n    def tag_event_retry(self, event, retry_time):\n        self.status_map[event.id] = (EV_RETRY, retry_time)\n\n    def get_status(self, event):\n        return self.status_map.get(event.id, (EV_DONE, 0))[0]\n\n    def iter_status(self):\n        for res in self.status_map.iteritems():\n            yield res"}
{"Repository": "puzpy", "input": "Indicates a format error in the . className PuzzleFormatError(Exception) Method __init__ Attribute message", "label": "class PuzzleFormatError(Exception):\n    def __init__(self, message=''):\n        self.message = message"}
{"Repository": "weechat-notify-send", "input": "Tests for default_value_of(). className DefaultValueOfTests(TestsBase) Method test_returns_correct_value", "label": "class DefaultValueOfTests(TestsBase):\n    def test_returns_correct_value(self):\n        self.assertEqual(default_value_of('nick_separator'), ': ')"}
{"Repository": "plants_disease_detection", "input": "Computes and stores the average and current value className AverageMeter(object) Method __init__ Method reset Method update", "label": "class AverageMeter(object):\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count"}
{"Repository": "3D_CNN_tensorflow", "input": "representation an annotated object track Tracklets are created in function parseXML and can most conveniently used as follows: for trackletObj in parseXML(trackletFile): for translation, rotation, state, occlusion, truncation, amtOcclusion, amtBorders, absoluteFrameNumber in trackletObj: your code here #end: for all frames #end: for all tracklets absoluteFrameNumber is in range [firstFrame, firstFrame+nFrames[ amtOcclusion and amtBorders could be None You can of course also directly access the fields objType (string), size (len-3 ndarray), firstFrame/nFrames (int), trans/rots (nFrames x 3 float ndarrays), states/truncs (len-nFrames uint8 ndarrays), occs (nFrames x 2 uint8 ndarray), and for some tracklets amtOccs (nFrames x 2 float ndarray) and amtBorders (nFrames x 3 float ndarray). className Tracklet(object) Method __init__ Method __str__ Method __iter__ Attribute size", "label": "class Tracklet(object):\n    objectType = None\n    size = None  # len-3 float array: (height, width, length)\n    firstFrame = None\n    trans = None   # n x 3 float array (x,y,z)\n    rots = None    # n x 3 float array (x,y,z)\n    states = None  # len-n uint8 array of states\n    occs = None    # n x 2 uint8 array  (occlusion, occlusion_kf)\n    truncs = None  # len-n uint8 array of truncation\n    amtOccs = None    # None or (n x 2) float array  (amt_occlusion, amt_occlusion_kf)\n    amtBorders = None    # None (n x 3) float array  (amt_border_l / _r / _kf)\n    nFrames = None\n\n    def __init__(self):\n        self.size = np.nan*np.ones(3, dtype=float)\n\n    def __str__(self):\n        return '[Tracklet over {0} frames for {1}]'.format(self.nFrames, self.objectType)\n\n    def __iter__(self):\n        if self.amtOccs is None:\n          return itertools.izip(self.trans, self.rots, self.states, self.occs, self.truncs, \\\n              itertools.repeat(None), itertools.repeat(None), xrange(self.firstFrame, self.firstFrame+self.nFrames))\n        else:\n          return itertools.izip(self.trans, self.rots, self.states, self.occs, self.truncs, \\\n              self.amtOccs, self.amtBorders, xrange(self.firstFrame, self.firstFrame+self.nFrames))"}
{"Repository": "modrana", "input": "Base class for all objects in profile which can store events. className Object(object) Method __init__ Method __hash__ Method __eq__ Method __contains__ Method __getitem__ Method __setitem__ Attribute events Attribute events", "label": "class Object(object):\n    def __init__(self, events=None):\n        if events is None:\n            self.events = {}\n        else:\n            self.events = events\n\n    def __hash__(self):\n        return id(self)\n\n    def __eq__(self, other):\n        return self is other\n\n    def __contains__(self, event):\n        return event in self.events\n\n    def __getitem__(self, event):\n        try:\n            return self.events[event]\n        except KeyError:\n            raise UndefinedEvent(event)\n\n    def __setitem__(self, event, value):\n        if value is None:\n            if event in self.events:\n                del self.events[event]\n        else:\n            self.events[event] = value"}
{"Repository": "SSD_EfficientNet", "input": "Block Decoder for readability. className BlockDecoder(object) Method _decode_block_string Method _encode_block_string Method decode Method encode", "label": "class BlockDecoder(object):\n  def _decode_block_string(self, block_string):\n    assert isinstance(block_string, str)\n    ops = block_string.split('_')\n    options = {}\n    for op in ops:\n      splits = re.split(r'(\\d.*)', op)\n      if len(splits) >= 2:\n        key, value = splits[:2]\n        options[key] = value\n\n    if 's' not in options or len(options['s']) != 2:\n      raise ValueError('Strides options should be a pair of integers.')\n\n    return BlockArgs(\n        kernel_size=int(options['k']),\n        num_repeat=int(options['r']),\n        input_filters=int(options['i']),\n        output_filters=int(options['o']),\n        expand_ratio=int(options['e']),\n        id_skip=('noskip' not in block_string),\n        se_ratio=float(options['se']) if 'se' in options else None,\n        strides=[int(options['s'][0]), int(options['s'][1])])\n\n  def _encode_block_string(self, block):\n    args = [\n        'r%d' % block.num_repeat,\n        'k%d' % block.kernel_size,\n        's%d%d' % (block.strides[0], block.strides[1]),\n        'e%s' % block.expand_ratio,\n        'i%d' % block.input_filters,\n        'o%d' % block.output_filters\n    ]\n    if block.se_ratio > 0 and block.se_ratio <= 1:\n      args.append('se%s' % block.se_ratio)\n    if block.id_skip is False:\n      args.append('noskip')\n    return '_'.join(args)\n\n  def decode(self, string_list):\n    assert isinstance(string_list, list)\n    blocks_args = []\n    for block_string in string_list:\n      blocks_args.append(self._decode_block_string(block_string))\n    return blocks_args\n\n  def encode(self, blocks_args):\n    block_strings = []\n    for block in blocks_args:\n      block_strings.append(self._encode_block_string(block))\n    return block_strings"}
{"Repository": "aurman", "input": "Problem class for dependencies without at least one provider className DepAlgoNotProvided(DepAlgoFoundProblems) Method __init__ Method __repr__ Method __eq__ Method __hash__", "label": "class DepAlgoNotProvided(DepAlgoFoundProblems):\n    def __init__(self, dep_not_provided, package):\n        super().__init__()\n        self.dep_not_provided: str = dep_not_provided\n        self.package: 'Package' = package\n\n    def __repr__(self):\n        return \"Not provided: {} but needed by {}\".format(\n            Colors.BOLD(Colors.LIGHT_MAGENTA(self.dep_not_provided)), Colors.BOLD(Colors.LIGHT_MAGENTA(self.package))\n        )\n\n    def __eq__(self, other):\n        return isinstance(other, self.__class__) \\\n               and self.dep_not_provided == other.dep_not_provided \\\n               and self.package == other.package\n\n    def __hash__(self):\n        return hash((self.dep_not_provided, self.package))"}
{"Repository": "findimports", "input": "Node in a condenced module dependency graph. className ModuleCycle(object) Method __init__ Attribute modnames Attribute modname Attribute label Attribute imports", "label": "class ModuleCycle(object):\n    def __init__(self, modnames):\n        self.modnames = modnames\n        self.modname = modnames[0]\n        self.label = \"\\n\".join(modnames)\n        self.imports = set()"}
{"Repository": "django-constance", "input": "Decorator to modify constance setting for TestCase. className override_config(override_settings) Method __init__ Method __call__ Method inner Method modify_test_case Method _pre_setup Method _post_teardown Method enable Method disable Method unpack_values Attribute original_values", "label": "class override_config(override_settings):\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n        self.original_values = {}\n\n    def __call__(self, test_func):\n        if isinstance(test_func, type):\n            if not issubclass(test_func, SimpleTestCase):\n                raise Exception(\n                    \"Only subclasses of Django SimpleTestCase can be \"\n                    \"decorated with override_config\")\n            return self.modify_test_case(test_func)\n        else:\n            @wraps(test_func)\n            def inner(*args, **kwargs):\n                with self:\n                    return test_func(*args, **kwargs)\n        return inner\n\n    def modify_test_case(self, test_case):\n        original_pre_setup = test_case._pre_setup\n        original_post_teardown = test_case._post_teardown\n\n        def _pre_setup(inner_self):\n            self.enable()\n            original_pre_setup(inner_self)\n\n        def _post_teardown(inner_self):\n            original_post_teardown(inner_self)\n            self.disable()\n\n        test_case._pre_setup = _pre_setup\n        test_case._post_teardown = _post_teardown\n\n        return test_case\n\n    def enable(self):\n        # Store the original values to an instance variable\n        for config_key in self.options:\n            self.original_values[config_key] = getattr(config, config_key)\n\n        # Update config with the overriden values\n        self.unpack_values(self.options)\n\n    def disable(self):\n        self.unpack_values(self.original_values)\n\n    @staticmethod\n    def unpack_values(options):\n        for name, value in options.items():\n            setattr(config, name, value)"}
{"Repository": "python-bigquery-dataframes", "input": "A BigQuery DataFrames ML Model base class that can be used to predict outputs. className Predictor(BaseEstimator) Method __init__ Method predict Method register Method to_gbq", "label": "class Predictor(BaseEstimator):\n    def __init__(self):\n        self._bqml_model: Optional[core.BqmlModel] = None\n\n    @abc.abstractmethod\n    def predict(self, X):\n        pass\n\n    _T = TypeVar(\"_T\", bound=\"Predictor\")\n\n    def register(self: _T, vertex_ai_model_id: Optional[str] = None) -> _T:\n        if not self._bqml_model:\n            # TODO(garrettwu): find a more elegant way to do this.\n            try:\n                self._bqml_model = self._create_bqml_model()  # type: ignore\n            except AttributeError:\n                raise RuntimeError(\"A model must be trained before register.\")\n        self._bqml_model = cast(core.BqmlModel, self._bqml_model)\n\n        self._bqml_model.register(vertex_ai_model_id)\n        return self\n\n    @abc.abstractmethod\n    def to_gbq(self, model_name, replace):\n        pass"}
{"Repository": "License-Plate-Detector", "input": "Sampler that repeats forever className _RepeatSampler(object) Method __init__ Method __iter__ Attribute sampler", "label": "class _RepeatSampler(object):\n    def __init__(self, sampler):\n        self.sampler = sampler\n\n    def __iter__(self):\n        while True:\n            yield from iter(self.sampler)"}
{"Repository": "cms", "input": "The entity representing a submission. className Submission(Entity) Method __init__ Method validate Method set Method get Method consistent Attribute user Attribute task Attribute time", "label": "class Submission(Entity):\n    def __init__(self):\n        Entity.__init__(self)\n        self.user = None\n        self.task = None\n        self.time = None\n\n    @staticmethod\n    def validate(data):\n        try:\n            assert isinstance(data, dict), \\\n                \"Not a dictionary\"\n            assert isinstance(data['user'], str), \\\n                \"Field 'user' isn't a string\"\n            assert isinstance(data['task'], str), \\\n                \"Field 'task' isn't a string\"\n            assert isinstance(data['time'], int), \\\n                \"Field 'time' isn't an integer (unix timestamp)\"\n        except KeyError as exc:\n            raise InvalidData(\"Field %s is missing\" % exc)\n        except AssertionError as exc:\n            raise InvalidData(str(exc))\n\n    def set(self, data):\n        self.validate(data)\n        self.user = data['user']\n        self.task = data['task']\n        self.time = data['time']\n\n    def get(self):\n        result = self.__dict__.copy()\n        del result['key']\n        del result['score']\n        del result['token']\n        del result['extra']\n        return result\n\n    def consistent(self, stores):\n        return (\"task\" not in stores or self.task in stores[\"task\"]) \\\n               and (\"user\" not in stores or self.user in stores[\"user\"])"}
{"Repository": "PGDiff", "input": "A residual block that can optionally change the number of channels. className ResBlock(TimestepBlock) Method forward Method _forward", "label": "class ResBlock(TimestepBlock):\n    def __init__(\n        self,\n        channels,\n        emb_channels,\n        dropout,\n        out_channels=None,\n        use_conv=False,\n        use_scale_shift_norm=False,\n        dims=2,\n        use_checkpoint=False,\n        up=False,\n        down=False,\n    ):\n        super().__init__()\n        self.channels = channels\n        self.emb_channels = emb_channels\n        self.dropout = dropout\n        self.out_channels = out_channels or channels\n        self.use_conv = use_conv\n        self.use_checkpoint = use_checkpoint\n        self.use_scale_shift_norm = use_scale_shift_norm\n\n        self.in_layers = nn.Sequential(\n            normalization(channels),\n            nn.SiLU(),\n            conv_nd(dims, channels, self.out_channels, 3, padding=1),\n        )\n\n        self.updown = up or down\n\n        if up:\n            self.h_upd = Upsample(channels, False, dims)\n            self.x_upd = Upsample(channels, False, dims)\n        elif down:\n            self.h_upd = Downsample(channels, False, dims)\n            self.x_upd = Downsample(channels, False, dims)\n        else:\n            self.h_upd = self.x_upd = nn.Identity()\n\n        self.emb_layers = nn.Sequential(\n            nn.SiLU(),\n            linear(\n                emb_channels,\n                2 * self.out_channels if use_scale_shift_norm else self.out_channels,\n            ),\n        )\n        self.out_layers = nn.Sequential(\n            normalization(self.out_channels),\n            nn.SiLU(),\n            nn.Dropout(p=dropout),\n            zero_module(\n                conv_nd(dims, self.out_channels, self.out_channels, 3, padding=1)\n            ),\n        )\n\n        if self.out_channels == channels:\n            self.skip_connection = nn.Identity()\n        elif use_conv:\n            self.skip_connection = conv_nd(\n                dims, channels, self.out_channels, 3, padding=1\n            )\n        else:\n            self.skip_connection = conv_nd(dims, channels, self.out_channels, 1)\n\n    def forward(self, x, emb):\n        return checkpoint(\n            self._forward, (x, emb), self.parameters(), self.use_checkpoint\n        )\n\n    def _forward(self, x, emb):\n        if self.updown:\n            in_rest, in_conv = self.in_layers[:-1], self.in_layers[-1]\n            h = in_rest(x)\n            h = self.h_upd(h)\n            x = self.x_upd(x)\n            h = in_conv(h)\n        else:\n            h = self.in_layers(x)\n        emb_out = self.emb_layers(emb).type(h.dtype)\n        while len(emb_out.shape) < len(h.shape):\n            emb_out = emb_out[..., None]\n        if self.use_scale_shift_norm:\n            out_norm, out_rest = self.out_layers[0], self.out_layers[1:]\n            scale, shift = th.chunk(emb_out, 2, dim=1)\n            h = out_norm(h) * (1 + scale) + shift\n            h = out_rest(h)\n        else:\n            h = h + emb_out\n            h = self.out_layers(h)\n        return self.skip_connection(x) + h"}
{"Repository": "qiime", "input": "Coordinates list of AlphaDiversityCalc objects to apply to same data. className AlphaDiversityCalcs(FunctionWithParams) Method __init__ Method getResult Method formatResult Attribute Params Attribute Calcs", "label": "class AlphaDiversityCalcs(FunctionWithParams):\n    def __init__(self, alpha_calcs, params=None):\n        self.Params = params or {}\n        self.Calcs = alpha_calcs\n\n    def getResult(self, data_path, tree_path=None):\n        otu_table = self.getBiomData(data_path)\n\n        calc_names = []\n        for calc in self.Calcs:\n            # add either calc's multiple return value names, or fn name\n            calc_names.extend(getattr(calc.Metric, 'return_names',\n                                      (calc.Metric.__name__,)))\n        needs_tree = max([c.IsPhylogenetic for c in self.Calcs])\n        if needs_tree:\n            tree = self.getTree(tree_path)\n        else:\n            tree = None\n        # calculations\n        res = []\n        for c in self.Calcs:\n            # add either calc's multiple return value names, or fn name\n            metric_res = c(data_path=data_path,\n                           taxon_names=otu_table.ids(axis='observation'),\n                           tree_path=tree,\n                           sample_names=otu_table.ids())\n            if len(metric_res.shape) == 1:\n                res.append(metric_res)\n            elif len(metric_res.shape) == 2:\n                for met in metric_res.T:\n                    res.append(met)\n            else:\n                raise RuntimeError(\"alpha div shape not as expected\")\n        res_data = array(res).T\n\n        return res_data, otu_table.ids(), calc_names\n\n    def formatResult(self, result):\n        data, sample_names, calc_names = result\n        res = format_matrix(data, sample_names, calc_names)\n        return res"}
{"Repository": "MoEBERT", "input": "Processor for the RACE data set. className RaceProcessor(DataProcessor) Method get_train_examples Method get_dev_examples Method get_test_examples Method get_labels Method _read_txt Method _create_examples", "label": "class RaceProcessor(DataProcessor):\n    def get_train_examples(self, data_dir):\n        logger.info(\"LOOKING AT {} train\".format(data_dir))\n        high = os.path.join(data_dir, \"train/high\")\n        middle = os.path.join(data_dir, \"train/middle\")\n        high = self._read_txt(high)\n        middle = self._read_txt(middle)\n        return self._create_examples(high + middle, \"train\")\n\n    def get_dev_examples(self, data_dir):\n        logger.info(\"LOOKING AT {} dev\".format(data_dir))\n        high = os.path.join(data_dir, \"dev/high\")\n        middle = os.path.join(data_dir, \"dev/middle\")\n        high = self._read_txt(high)\n        middle = self._read_txt(middle)\n        return self._create_examples(high + middle, \"dev\")\n\n    def get_test_examples(self, data_dir):\n        logger.info(\"LOOKING AT {} test\".format(data_dir))\n        high = os.path.join(data_dir, \"test/high\")\n        middle = os.path.join(data_dir, \"test/middle\")\n        high = self._read_txt(high)\n        middle = self._read_txt(middle)\n        return self._create_examples(high + middle, \"test\")\n\n    def get_labels(self):\n        return [\"0\", \"1\", \"2\", \"3\"]\n\n    def _read_txt(self, input_dir):\n        lines = []\n        files = glob.glob(input_dir + \"/*txt\")\n        for file in tqdm.tqdm(files, desc=\"read files\"):\n            with open(file, \"r\", encoding=\"utf-8\") as fin:\n                data_raw = json.load(fin)\n                data_raw[\"race_id\"] = file\n                lines.append(data_raw)\n        return lines\n\n    def _create_examples(self, lines, set_type):\n        examples = []\n        for (_, data_raw) in enumerate(lines):\n            race_id = \"%s-%s\" % (set_type, data_raw[\"race_id\"])\n            article = data_raw[\"article\"]\n            for i in range(len(data_raw[\"answers\"])):\n                truth = str(ord(data_raw[\"answers\"][i]) - ord(\"A\"))\n                question = data_raw[\"questions\"][i]\n                options = data_raw[\"options\"][i]\n\n                examples.append(\n                    InputExample(\n                        example_id=race_id,\n                        question=question,\n                        contexts=[article, article, article, article],  # this is not efficient but convenient\n                        endings=[options[0], options[1], options[2], options[3]],\n                        label=truth,\n                    )\n                )\n        return examples"}
{"Repository": "hapi", "input": "Hold Alert information fetched from database and check for alerts. className Alert(object) Method __init__ Method __str__ Method update_alert Method check_alert Attribute alert_id Attribute lower_threshold Attribute upper_threshold Attribute message Attribute response_type Attribute value Attribute notify_enabled", "label": "class Alert(object):\n    def __init__(self, asset_id=\"\"):\n        self.alert_id = asset_id\n        self.lower_threshold = 0.0\n        self.upper_threshold = 0.0\n        self.message = \"\"\n        self.response_type = \"\"\n        self.value = 0.0\n        self.notify_enabled = False\n\n    def __str__(self):\n        return str({\"id\": self.alert_id,\n                    \"lower\": self.lower_threshold,\n                    \"upper\": self.upper_threshold,\n                    \"message\": self.message,\n                    \"response\": self.response_type,\n                    \"value\": self.value,\n                    \"notify_enabled\": self.notify_enabled})\n\n    def update_alert(self, asset_id):\n        self.alert_id = asset_id\n        try:\n            Log.info(\"Fetching alert parameters from database.\")\n            field_names = '''\n                lower_threshold\n                upper_threshold\n                message\n                response_type\n                notify_enabled\n            '''.split()\n            sql = \"SELECT {fields} FROM alert_params WHERE asset_id = '{asset}' LIMIT 1;\".format(\n                fields=', '.join(field_names), asset=str(asset_id))\n            database = sqlite3.connect(DB_CORE)\n            row = database.cursor().execute(sql).fetchone()\n            for key, value in zip(field_names, row):\n                setattr(self, key, value)\n            self.lower_threshold = float(self.lower_threshold)\n            self.upper_threshold = float(self.upper_threshold)\n        except Exception as excpt:\n            Log.exception(\"Error fetching alert parameters from database: %s.\", excpt)\n        finally:\n            database.close()\n            Log.info(\"Closing Alert database connection.\")\n\n    def check_alert(self, current_value):\n        self.value = current_value\n        if self.lower_threshold <= float(self.value) <= self.upper_threshold:\n            return False\n\n        Log.info(\"[!] ALERT DETECTED. Value: %s.\", self.value)\n        return True"}
{"Repository": "Variational-Ladder-Autoencoder", "input": "sess should be the session where the visualized network run, className SampleVisualizer(Visualizer) Method __init__ Method visualize Attribute dataset Attribute name", "label": "class SampleVisualizer(Visualizer):\n    def __init__(self, network, dataset):\n        Visualizer.__init__(self, network)\n        # self.fig.suptitle(\"Samples generated by \" + str(network.name))\n        self.dataset = dataset\n        self.name = \"samples\"\n\n    def visualize(self, num_rows=10, use_gui=False):\n        if use_gui and self.fig is None:\n            self.fig, self.ax = plt.subplots()\n        samples = self.network.generate_samples()\n        if samples is not None:\n            samples = self.dataset.display(samples)\n            width = samples.shape[1]\n            height = samples.shape[2]\n            channel = samples.shape[3]\n            canvas = np.zeros((width * num_rows, height * num_rows, channel))\n            for img_index1 in range(num_rows):\n                for img_index2 in range(num_rows):\n                    canvas[img_index1*width:(img_index1+1)*width, img_index2*height:(img_index2+1)*height, :] = \\\n                        samples[img_index1*num_rows+img_index2, :, :, :]\n            self.arr_to_file(canvas)\n\n            if use_gui:\n                self.ax.cla()\n                if channel == 1:\n                    self.ax.imshow(canvas[:, :, 0], cmap=plt.get_cmap('Greys'))\n                else:\n                    self.ax.imshow(canvas)\n                self.ax.xaxis.set_visible(False)\n                self.ax.yaxis.set_visible(False)\n\n                self.fig.suptitle('Samples for %s' % self.network.name)\n                plt.draw()\n                plt.pause(0.01)"}
{"Repository": "sympy", "input": "A printer to be used inside `plot_implicit` when `adaptive=True`, in which case the interval arithmetic module is going to be used, which requires the following edits. className IntervalMathPrinter(PythonCodePrinter) Method _print_And Method _print_Or", "label": "class IntervalMathPrinter(PythonCodePrinter):\n    def _print_And(self, expr):\n        PREC = precedence(expr)\n        return \" & \".join(self.parenthesize(a, PREC)\n                for a in sorted(expr.args, key=default_sort_key))\n\n    def _print_Or(self, expr):\n        PREC = precedence(expr)\n        return \" | \".join(self.parenthesize(a, PREC)\n                for a in sorted(expr.args, key=default_sort_key))"}
{"Repository": "instax_api", "input": "Model Name Command. className ModelNameCommand(Packet) Method __init__ Method encodeComPayload Method decodeComPayload Method encodeRespPayload Method decodeRespPayload Attribute payload Attribute mode Attribute byteArray Attribute header Attribute valid Attribute decodedCommandPayload Attribute payload Attribute mode Attribute modelName", "label": "class ModelNameCommand(Packet):\n    NAME = \"Model Name\"\n    TYPE = Packet.MESSAGE_TYPE_MODEL_NAME\n\n    def __init__(self, mode, byteArray=None, modelName=None):\n        super().__init__(mode)\n        self.payload = {}\n        self.mode = mode\n\n        if byteArray is not None:\n            self.byteArray = byteArray\n            self.header = super().decodeHeader(mode, byteArray)\n            self.valid = self.validatePacket(byteArray, self.header[\"packetLength\"])\n            if mode == self.MESSAGE_MODE_COMMAND:\n                self.decodedCommandPayload = self.decodeComPayload(byteArray)\n            elif mode == self.MESSAGE_MODE_RESPONSE:\n                self.payload = self.decodeRespPayload(byteArray)\n        else:\n            self.mode = mode\n            self.modelName = modelName\n\n    def encodeComPayload(self):\n        return {}\n\n    def decodeComPayload(self, byteArray):\n        return {}\n\n    def encodeRespPayload(self):\n        payload = bytearray()\n        payload = payload + self.encodeModelString(self.modelName)\n        return payload\n\n    def decodeRespPayload(self, byteArray):\n        self.modelName = self.getPrinterModelString(16, byteArray)\n        self.payload = {\"modelName\": self.modelName}\n        return self.payload"}
{"Repository": "moco-v3", "input": "Computes and stores the average and current value className AverageMeter(object) Method __init__ Method reset Method update Method __str__ Attribute name Attribute fmt", "label": "class AverageMeter(object):\n    def __init__(self, name, fmt=':f'):\n        self.name = name\n        self.fmt = fmt\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count\n\n    def __str__(self):\n        fmtstr = '{name} {val' + self.fmt + '} ({avg' + self.fmt + '})'\n        return fmtstr.format(**self.__dict__)"}
{"Repository": "pytorch-noise2void", "input": "Convert ndarrays in sample to Tensors. className ToNumpy(object) Method __call__", "label": "class ToNumpy(object):\n    def __call__(self, data):\n        # Swap color axis because numpy image: H x W x C\n        #                         torch image: C x H x W\n\n        # for key, value in data:\n        #     data[key] = value.transpose((2, 0, 1)).numpy()\n        #\n        # return data\n\n        return data.to('cpu').detach().numpy().transpose(0, 2, 3, 1)\n\n        # input, label = data['input'], data['label']\n        # input = input.transpose((2, 0, 1))\n        # label = label.transpose((2, 0, 1))\n        # return {'input': input.detach().numpy(), 'label': label.detach().numpy()}"}
{"Repository": "CiaoSR", "input": "LocalImplicitSR based on EDSR. className LocalImplicitSREDSR(LocalImplicitSRNet) Method gen_feature", "label": "class LocalImplicitSREDSR(LocalImplicitSRNet):\n    def __init__(self,\n                 encoder,\n                 imnet_q,\n                 imnet_k,\n                 imnet_v,\n                 query_mlp=None,\n                 key_mlp=None,\n                 value_mlp=None,\n                 local_size=2,\n                 feat_unfold=True,\n                 eval_bsize=None,\n                 non_local_attn=True,\n                 multi_scale=[2],\n                 softmax_scale=1,\n                 ):\n        super().__init__(\n            encoder=encoder,\n            imnet_q=imnet_q,\n            imnet_k=imnet_k,\n            imnet_v=imnet_v,\n            query_mlp=query_mlp,\n            key_mlp=key_mlp,\n            value_mlp=value_mlp,\n            local_size=local_size,\n            feat_unfold=feat_unfold,\n            eval_bsize=eval_bsize,\n            non_local_attn=non_local_attn,\n            multi_scale=multi_scale,\n            softmax_scale=softmax_scale,\n            )\n\n        self.conv_first = self.encoder.conv_first\n        self.body = self.encoder.body\n        self.conv_after_body = self.encoder.conv_after_body\n        del self.encoder\n\n    def gen_feature(self, x):\n        x = self.conv_first(x)\n        res = self.body(x)\n        res = self.conv_after_body(res)\n        res += x\n\n        return [res]"}
{"Repository": "asnrecon", "input": "Thread that uses ip queue and resolves ip to names. className IPResolverWorker(Thread) Method __init__ Method run Method resolve_name_for_ip Attribute generator Attribute lock", "label": "class IPResolverWorker(Thread):\n    _curl_param_str = '''curl -kvv --connect-timeout %(timeout)d --silent https://%(ip)s 2>&1 | awk 'BEGIN { FS = \"CN=\"} ; {print $2}' | awk 'NF' | awk 'FNR%%2' > %(out_file)s'''\n\n    _use_curl = USE_CURL\n\n\n\n    def __init__(self, generator, lock):\n\n        super(IPResolverWorker, self).__init__()\n\n        self.generator = generator\n\n        self.lock = lock\n\n\n\n    # noinspection PyBroadException\n\n    def run(self):\n\n        while True:\n\n            try:\n\n                ip_info = self.generator.get_next_ip()\n\n                if ip_info is not None:\n\n                    if ip_info['caption'] is not None:\n\n                        with self.lock:\n\n                            print('Testing %s...' % ip_info['caption'])\n\n\n\n                    ip = '.'.join(str(i) for i in ip_info['ip'])\n\n                    resolved = self.resolve_name_for_ip(ip)\n\n\n\n                    if resolved:\n\n                        with open(ip_info['filename'], 'a+') as f, self.lock:\n\n                            f.write('https://%s - %s\\n' % (ip, resolved))\n\n                            f.flush()\n\n\n\n                            print('[*] Domain found - https://%s - %s' % (ip, resolved))\n\n                else:\n\n                    return\n\n            except:\n\n                print(format_exc())\n\n\n\n    def resolve_name_for_ip(self, ip):\n        if self._use_curl:\n\n            # sick way to get server certificate\n\n            out_file_name = '%s/%d' % (TMP_DIR_NAME, self.ident)\n\n            system(self._curl_param_str % {'ip': ip,\n\n                                           'out_file': out_file_name,\n\n                                           'timeout': DEFAULT_SOCKET_TIMEOUT})\n\n\n\n            with open(out_file_name, 'r') as f:\n\n                content = f.read().strip()\n\n                if content:\n\n                    return content.split(';')[0]\n\n        else:\n\n            # usual way to get server certificate\n\n            try:\n\n                pem = get_server_certificate((ip, 443))\n\n            except socket_error:\n\n                pass\n\n            else:\n\n                if pem:\n\n                    for cid, val in load_certificate(FILETYPE_PEM, pem).get_subject().get_components():\n\n                        if cid == 'CN':\n\n                            return val"}
{"Repository": "flux_led", "input": "The newer LEDENET protocol with checksums that uses 9 bytes to set state. className ProtocolLEDENET9ByteDimmableEffects(ProtocolLEDENET9ByteAutoOn) Method dimmable_effects Method power_push_updates Method state_push_updates Method name", "label": "class ProtocolLEDENET9ByteDimmableEffects(ProtocolLEDENET9ByteAutoOn):\n    @property\n    def dimmable_effects(self) -> bool:\n        return True\n\n    @property\n    def power_push_updates(self) -> bool:\n        return True\n\n    @property\n    def state_push_updates(self) -> bool:\n        return True\n\n    @property\n    def name(self) -> str:\n        return PROTOCOL_LEDENET_9BYTE_DIMMABLE_EFFECTS\n\n    def construct_preset_pattern(\n        self, pattern: int, speed: int, brightness: int\n    ) -> bytearray:\n        delay = utils.speedToDelay(speed)\n        return self.construct_message(bytearray([0x38, pattern, delay, brightness]))"}
{"Repository": "python-gnupg", "input": "Metaclass for changing the :meth:GPG. className GPGMeta(type) Method __new__ Method _find_agent", "label": "class GPGMeta(type):\n    def __new__(cls, name, bases, attrs):\n        log.debug(\"Metaclass __new__ constructor called for %r\" % cls)\n        if cls._find_agent():\n            ## call the normal GPG.__init__() initialiser:\n            attrs['init'] = cls.__init__\n            attrs['_remove_agent'] = True\n        return super(GPGMeta, cls).__new__(cls, name, bases, attrs)\n\n    @classmethod\n    def _find_agent(cls):\n        if not psutil:\n            return False\n\n        this_process = psutil.Process(os.getpid())\n        ownership_match = False\n\n        if _util._running_windows:\n            identity = this_process.username()\n        else:\n            identity = this_process.uids\n\n        for proc in psutil.process_iter():\n            try:\n                # In my system proc.name & proc.is_running are methods\n                if (proc.name() == \"gpg-agent\") and proc.is_running():\n                    log.debug(\"Found gpg-agent process with pid %d\" % proc.pid)\n                    if _util._running_windows:\n                        if proc.username() == identity:\n                            ownership_match = True\n                    else:\n                        # proc.uids & identity are methods to\n                        if proc.uids() == identity():\n                            ownership_match = True\n            except psutil.Error as err:\n                # Exception when getting proc info, possibly because the\n                # process is zombie / process no longer exist. Just ignore it.\n                log.warn(\"Error while attempting to find gpg-agent process: %s\" % err)\n            # Next code must be inside for operator.\n            # Otherwise to _agent_proc will be saved not \"gpg-agent\" process buth an other.\n            if ownership_match:\n                log.debug(\"Effective UIDs of this process and gpg-agent match\")\n                setattr(cls, '_agent_proc', proc)\n                return True\n\n        return False"}
{"Repository": "differentiable_volumetric_rendering", "input": "Holds a binvox model. className Voxels(object) Method __init__ Method clone Method write Attribute data Attribute dims Attribute translate Attribute scale Attribute axis_order", "label": "class Voxels(object):\n    def __init__(self, data, dims, translate, scale, axis_order):\n        self.data = data\n        self.dims = dims\n        self.translate = translate\n        self.scale = scale\n        assert (axis_order in ('xzy', 'xyz'))\n        self.axis_order = axis_order\n\n    def clone(self):\n        data = self.data.copy()\n        dims = self.dims[:]\n        translate = self.translate[:]\n        return Voxels(data, dims, translate, self.scale, self.axis_order)\n\n    def write(self, fp):\n        write(self, fp)"}
{"Repository": "flask-restx", "input": "Show a single todo item and lets you delete them className Todo(Resource) Method get Method delete Method put", "label": "class Todo(Resource):\n    @api.doc(description=\"todo_id should be in {0}\".format(\", \".join(TODOS.keys())))\n    @api.marshal_with(todo)\n    def get(self, todo_id):\n        abort_if_todo_doesnt_exist(todo_id)\n        return TODOS[todo_id]\n\n    @api.doc(responses={204: \"Todo deleted\"})\n    def delete(self, todo_id):\n        abort_if_todo_doesnt_exist(todo_id)\n        del TODOS[todo_id]\n        return \"\", 204\n\n    @api.doc(parser=parser)\n    @api.marshal_with(todo)\n    def put(self, todo_id):\n        args = parser.parse_args()\n        task = {\"task\": args[\"task\"]}\n        TODOS[todo_id] = task\n        return task"}
{"Repository": "cmdx", "input": "Returned in place of an actual plug className CachedPlug(Plug) Method __init__ Method read Attribute _value", "label": "class CachedPlug(Plug):\n    def __init__(self, value):\n        self._value = value\n\n    def read(self):\n        return self._value"}
{"Repository": "Anime-Face-GAN-Keras", "input": "Sub-pixel convolutional upscaling layer based on the paper \"Real-Time Single Image and Video Super-Resolution Using an Efficient Sub-Pixel Convolutional Neural Network\" (https://arxiv. className SubPixelUpscaling(Layer) Method __init__ Method build Method call Method compute_output_shape Method get_config Attribute scale_factor Attribute data_format", "label": "class SubPixelUpscaling(Layer):\n    def __init__(self, scale_factor=2, data_format=None, **kwargs):\n\n        super(SubPixelUpscaling, self).__init__(**kwargs)\n\n\n\n        self.scale_factor = scale_factor\n\n        self.data_format = normalize_data_format(data_format)\n\n\n\n    def build(self, input_shape):\n\n        pass\n\n\n\n    def call(self, x, mask=None):\n\n        y = K.depth_to_space(x, self.scale_factor, self.data_format)\n\n        return y\n\n\n\n    def compute_output_shape(self, input_shape):\n\n        if self.data_format == 'channels_first':\n\n            b, k, r, c = input_shape\n\n            return (b, k // (self.scale_factor ** 2), r * self.scale_factor, c * self.scale_factor)\n\n        else:\n\n            b, r, c, k = input_shape\n\n            return (b, r * self.scale_factor, c * self.scale_factor, k // (self.scale_factor ** 2))\n\n\n\n    def get_config(self):\n\n        config = {'scale_factor': self.scale_factor,\n\n                  'data_format': self.data_format}\n\n        base_config = super(SubPixelUpscaling, self).get_config()\n\n        return dict(list(base_config.items()) + list(config.items()))"}
{"Repository": "IbPy", "input": "generated source for class ContractDetails className ContractDetails(object) Method __init__ Method __init___0 Attribute m_summary Attribute m_minTick Attribute m_underConId Attribute m_evMultiplier", "label": "class ContractDetails(object):\n    m_summary = None\n    m_marketName = \"\"\n    m_minTick = float()\n    m_priceMagnifier = 0\n    m_orderTypes = \"\"\n    m_validExchanges = \"\"\n    m_underConId = 0\n    m_longName = \"\"\n    m_contractMonth = \"\"\n    m_industry = \"\"\n    m_category = \"\"\n    m_subcategory = \"\"\n    m_timeZoneId = \"\"\n    m_tradingHours = \"\"\n    m_liquidHours = \"\"\n    m_evRule = \"\"\n    m_evMultiplier = float()\n    \n    m_secIdList = None  #  CUSIP/ISIN/etc.\n    \n    #  BOND values\n    m_cusip = \"\"\n    m_ratings = \"\"\n    m_descAppend = \"\"\n    m_bondType = \"\"\n    m_couponType = \"\"\n    m_callable = False\n    m_putable = False\n    m_coupon = 0\n    m_convertible = False\n    m_maturity = \"\"\n    m_issueDate = \"\"\n    m_nextOptionDate = \"\"\n    m_nextOptionType = \"\"\n    m_nextOptionPartial = False\n    m_notes = \"\"\n\n    @overloaded\n    def __init__(self):\n        self.m_summary = Contract()\n        self.m_minTick = 0\n        self.m_underConId = 0\n        self.m_evMultiplier = 0\n\n    @__init__.register(object, Contract, str, str, float, str, str, int, str, str, str, str, str, str, str, str, str, float)\n    def __init___0(self, p_summary, p_marketName, p_minTick, p_orderTypes, p_validExchanges, p_underConId, p_longName, p_contractMonth, p_industry, p_category, p_subcategory, p_timeZoneId, p_tradingHours, p_liquidHours, p_evRule, p_evMultiplier):\n        self.m_summary = p_summary\n        self.m_marketName = p_marketName\n        self.m_minTick = p_minTick\n        self.m_orderTypes = p_orderTypes\n        self.m_validExchanges = p_validExchanges\n        self.m_underConId = p_underConId\n        self.m_longName = p_longName\n        self.m_contractMonth = p_contractMonth\n        self.m_industry = p_industry\n        self.m_category = p_category\n        self.m_subcategory = p_subcategory\n        self.m_timeZoneId = p_timeZoneId\n        self.m_tradingHours = p_tradingHours\n        self.m_liquidHours = p_liquidHours\n        self.m_evRule = p_evRule\n        self.m_evMultiplier = p_evMultiplier"}
{"Repository": "psq", "input": "Similar to concurrent. className TaskResult(object) Method __init__ Method get_task Method result Attribute task_id Attribute storage", "label": "class TaskResult(object):\n    def __init__(self, task_id, queue=None):\n        self.task_id = task_id\n\n        if not queue:\n            queue = current_queue\n\n        self.storage = queue.storage\n\n    def get_task(self):\n        return self.storage.get_task(self.task_id)\n\n    def result(self, timeout=None):\n        start = time.time()\n        while True:\n            task = self.get_task()\n            if not task or task.status not in (FINISHED, FAILED):\n                if not timeout:\n                    continue\n                elif time.time() - start < timeout:\n                    continue\n                else:\n                    raise TimeoutError()\n\n            if task.status == FAILED:\n                raise task.result\n\n            return task.result"}
{"Repository": "sigver", "input": "Base class for Datasets. className IterableDataset(ABC) Method maxsize Method genuine_per_user Method skilled_per_user Method simple_per_user Method get_user_list Method iter_genuine Method iter_simple_forgery Method iter_forgery", "label": "class IterableDataset(ABC):\n    @property\n    def maxsize(self):\n        raise NotImplementedError\n\n    @property\n    def genuine_per_user(self):\n        raise NotImplementedError\n\n    @property\n    def skilled_per_user(self):\n        raise NotImplementedError\n\n    @property\n    def simple_per_user(self):\n        raise NotImplementedError\n\n    @abstractmethod\n    def get_user_list(self) -> List[int]:\n        pass\n\n    @abstractmethod\n    def iter_genuine(self, user: int) -> Iterable[Tuple[np.ndarray, str]]:\n        pass\n\n    @abstractmethod\n    def iter_simple_forgery(self, user: int) -> Iterable[Tuple[np.ndarray, str]]:\n        pass\n\n    @abstractmethod\n    def iter_forgery(self, user: int) -> Iterable[Tuple[np.ndarray, str]]:\n        pass"}
{"Repository": "M2m", "input": "Reference: https://gist.github.com/gyglim/1f8dfb1b5c82627ae3efcfbbadb9f514 className Logger(object) Method __init__ Method set_dir Method log Method log_dirname", "label": "class Logger(object):\n    def __init__(self, fn):\n        if not os.path.exists(\"./logs/\"):\n            os.mkdir(\"./logs/\")\n\n        logdir = 'logs/' + fn\n        if not os.path.exists(logdir):\n            os.mkdir(logdir)\n        if len(os.listdir(logdir)) != 0:\n            ans = input(\"log_dir is not empty. All data inside log_dir will be deleted. \"\n                            \"Will you proceed [y/N]? \")\n            if ans in ['y', 'Y']:\n                shutil.rmtree(logdir)\n            else:\n                exit(1)\n        self.set_dir(logdir)\n\n    def set_dir(self, logdir, log_fn='log.txt'):\n        self.logdir = logdir\n        if not os.path.exists(logdir):\n            os.mkdir(logdir)\n        self.log_file = open(os.path.join(logdir, log_fn), 'a')\n\n    def log(self, string):\n        self.log_file.write('[%s] %s' % (datetime.now(), string) + '\\n')\n        self.log_file.flush()\n\n        print('[%s] %s' % (datetime.now(), string))\n        sys.stdout.flush()\n\n    def log_dirname(self, string):\n        self.log_file.write('%s (%s)' % (string, self.logdir) + '\\n')\n        self.log_file.flush()\n\n        print('%s (%s)' % (string, self.logdir))\n        sys.stdout.flush()"}
{"Repository": "PerfKitBenchmarker", "input": "Queue to which multiple threads write but from which only one thread reads. className _SingleReaderQueue(object) Method __init__ Method Get Method Put Attribute _deque", "label": "class _SingleReaderQueue(object):\n  def __init__(self):\n    self._deque = collections.deque()\n\n  def Get(self, timeout=None):\n    if not _WaitForCondition(lambda: self._deque, timeout):\n      raise queue.Empty\n    return self._deque.popleft()\n\n  def Put(self, item):\n    self._deque.append(item)"}
{"Repository": "RRPN_plusplus", "input": "Wraps a BatchSampler, resampling from it until className IterationBasedBatchSampler(BatchSampler) Method __init__ Method __iter__ Method __len__ Attribute batch_sampler Attribute num_iterations Attribute start_iter", "label": "class IterationBasedBatchSampler(BatchSampler):\n    def __init__(self, batch_sampler, num_iterations, start_iter=0):\n        self.batch_sampler = batch_sampler\n        self.num_iterations = num_iterations\n        self.start_iter = start_iter\n\n    def __iter__(self):\n        iteration = self.start_iter\n        while iteration <= self.num_iterations:\n            # if the underlying sampler has a set_epoch method, like\n            # DistributedSampler, used for making each process see\n            # a different split of the dataset, then set it\n            if hasattr(self.batch_sampler.sampler, \"set_epoch\"):\n                self.batch_sampler.sampler.set_epoch(iteration)\n            for batch in self.batch_sampler:\n                iteration += 1\n                if iteration > self.num_iterations:\n                    break\n                yield batch\n\n    def __len__(self):\n        return self.num_iterations"}
{"Repository": "isanlp", "input": "The span with additional tag field. className TaggedSpan(Span) Method __init__ Attribute tag", "label": "class TaggedSpan(Span):\n    def __init__(self, tag, begin = -1, end = -1):\n        super().__init__(begin, end)\n        self.tag = tag"}
{"Repository": "GANWarping", "input": "A set of radio button choices. className Choice(Widget) Method widget_js Method widget_html", "label": "class Choice(Widget):\n    def __init__(self, choices=None, selection=None, horizontal=False,\n                 **kwargs):\n        super().__init__(**kwargs)\n        if choices is None:\n            choices = []\n        self.choices = Property(choices)\n        self.horizontal = Property(horizontal)\n        self.selection = Property(selection)\n\n    def widget_js(self):\n        # Note that the 'input' event would enable during-drag feedback,\n        # but this is pretty slow on google colab.\n        return minify('''\n          function esc(unsafe) {\n            return unsafe.replace(/&/g, \"&amp;\").replace(/</g, \"&lt;\")\n                   .replace(/>/g, \"&gt;\").replace(/\"/g, \"&quot;\");\n          }\n          function render() {\n            var lines = model.get('choices').map((c) => {\n              return '<label><input type=\"radio\" name=\"choice\" value=\"' +\n                 esc(c) + '\">' + esc(c) + '</label>'\n            });\n            element.innerHTML = lines.join(model.get('horizontal')?' ':'<br>');\n          }\n          model.on('choices horizontal', render);\n          model.on('selection', (ev) => {\n            [...element.querySelectorAll('input')].forEach((e) => {\n              e.checked = (e.value == ev.value);\n            })\n          });\n          element.addEventListener('change', (e) => {\n            model.set('selection', element.choice.value);\n          });\n        ''')\n\n    def widget_html(self):\n        radios = [\n            f\"\"\"<label><input name=\"choice\" type=\"radio\" {\n            'checked' if value == self.selection else ''\n            } value=\"{html.escape(value)}\">{html.escape(value)}</label>\"\"\"\n            for value in self.choices]\n        sep = \" \" if self.horizontal else \"<br>\"\n        return f'<form {self.std_attrs()}>{sep.join(radios)}</form>'"}
{"Repository": "txtai", "input": "Retrieval augmented generation (RAG) using txtai. className RAG(Embed) Method __init__ Method search Attribute extractor", "label": "class RAG(Embed):\n    def __init__(self, path, config, output, refresh):\n        # Parent logic\n        super().__init__(path, config, output, refresh)\n\n        # Read LLM configuration\n        llm = self.readconfig(\"llm\", {})\n\n        # Read Extractor configuration\n        extractor = self.readconfig(\"extractor\", {})\n\n        # Load Extractor\n        self.extractor = Extractor(self.backend, LLM(**llm), output=\"reference\", **extractor)\n\n    def search(self, queries, limit):\n        # Set context window size to limit and run\n        self.extractor.context = limit\n        return [[(x[\"reference\"], 1)] for x in self.extractor(queries, maxlength=4096)]"}
{"Repository": "Bert_OCR.pytorch", "input": "Compute average for torch.Tensor, used for loss average. className Averager(object) Method __init__ Method add Method reset Method val", "label": "class Averager(object):\n    def __init__(self):\n        self.reset()\n\n    def add(self, v):\n        count = v.data.numel()\n        v = v.data.sum()\n        self.n_count += count\n        self.sum += v\n\n    def reset(self):\n        self.n_count = 0\n        self.sum = 0\n\n    def val(self):\n        res = 0\n        if self.n_count != 0:\n            res = self.sum / float(self.n_count)\n        return res"}
{"Repository": "ganga", "input": "A custom setuptools command to run the Ganga test suite. className RunTestsCommand(Command) Method initialize_options Method finalize_options Method _get_test_env Method run", "label": "class RunTestsCommand(Command):\n    description = 'run the Ganga test suite'\n    all_types = ['unit', 'integration', 'all']\n    user_options = [\n        ('type=', 't', 'the type of tests: [{0}]'.format(', '.join(all_types))),\n        ('coverage', None, 'should coverage be generated'),\n        ('xunit', None, 'should xunit-compatible files be produced'),\n    ]\n\n    def initialize_options(self):\n        self.type = 'unit'\n        self.coverage = False\n        self.xunit = False\n\n    def finalize_options(self):\n        if self.type not in self.all_types:\n            raise Exception('Test type must be [{0}]'.format(', '.join(self.all_types)))\n\n    @staticmethod\n    def _get_test_env():\n        ganga_python_dir = os.path.join(file_path, 'ganga')\n\n        test_env = os.environ.copy()\n        path = ':'.join(s for s in [ganga_python_dir, test_env.get('PYTHONPATH', None)] if s)\n        test_env['PYTHONPATH'] = path\n\n        return test_env\n\n    def run(self):\n\n        cmd = ['py.test']\n\n        if self.type in ['unit', 'all']:\n            cmd.append('ganga/GangaCore/test/Unit')\n            cmd.append('ganga/GangaCore/Core')\n            cmd.append('ganga/GangaCore/Runtime')\n            cmd.append('ganga/GangaCore/Utility')\n        if self.type in ['integration', 'all']:\n            cmd.append('ganga/GangaCore/test/GPI')\n\n        if self.coverage:\n            cmd.append('--cov-report xml --cov .')\n        if self.xunit:\n            cmd.append('--junitxml tests.xml')\n\n        subprocess.check_call(' '.join(cmd), cwd=file_path, shell=True, env=self._get_test_env())"}
{"Repository": "hydra", "input": "contains state of a collection copy in a sqlite3 database, for ease of use in other code a separate state file should be used for each sharded collection being copied, to avoid deleting state should copy_collection. className CopyStateDB(object) Method __init__ Method drop_and_create Method add_source_and_dest Method select_by_state Method update_oplog_ts Method update_state Method get_oplog_ts Attribute _conn Attribute _path", "label": "class CopyStateDB(object):\n    STATE_TABLE = 'state'\n\n    STATE_INITIAL_COPY = 'initial copy'\n    STATE_WAITING_FOR_INDICES = 'waiting for indices'\n    STATE_APPLYING_OPLOG = 'applying oplog'\n\n    def __init__(self, path):\n        self._conn = sqlite3.connect(path)\n        self._path = path\n\n\n    def drop_and_create(self):\n        with self._conn:\n            cursor = self._conn.cursor()\n            cursor.execute(\"DROP TABLE IF EXISTS %s\" % self.STATE_TABLE)\n            cursor.execute(CREATE_STATE_SQL.format(table_name=self.STATE_TABLE))\n\n\n    def add_source_and_dest(self, source, dest):\n        source_str = _mongo_dict_to_str(source)\n        dest_str = _mongo_dict_to_str(dest)\n        with self._conn:\n            cursor = self._conn.cursor()\n            query  = \"INSERT OR IGNORE INTO \"+self.STATE_TABLE+\" \"\n            query += \"(source, dest, updated_at, state, oplog_ts) VALUES (?, ?, ?, ?, ?) \"\n            cursor.execute(query,\n                           (source_str, dest_str, time.time(), self.STATE_INITIAL_COPY, None))\n\n\n    def select_by_state(self, state):\n        cursor = self._conn.cursor()\n        query = \"SELECT * FROM \"+self.STATE_TABLE+\" WHERE state=?\"\n        cursor.execute(query, (state,))\n        return _results_as_dicts(cursor)\n\n\n    def update_oplog_ts(self, source, dest, oplog_ts):\n        assert isinstance(oplog_ts, Timestamp)\n        source_str = _mongo_dict_to_str(source)\n        dest_str = _mongo_dict_to_str(dest)\n        oplog_ts_json = json.dumps({'time': oplog_ts.time, 'inc': oplog_ts.inc})\n        query  = \"UPDATE \"+self.STATE_TABLE+\" \"\n        query += \"SET oplog_ts = ? \"\n        query += \"WHERE source = ? AND dest = ?\"\n        with self._conn:\n            cursor = self._conn.cursor()\n            cursor.execute(query, (oplog_ts_json, source_str, dest_str))\n\n\n    def update_state(self, source, dest, state):\n        source_str = _mongo_dict_to_str(source)\n        dest_str = _mongo_dict_to_str(dest)\n        query  = \"UPDATE \"+self.STATE_TABLE+\" \"\n        query += \"SET state = ? \"\n        query += \"WHERE source = ? AND dest = ?\"\n        with self._conn:\n            cursor = self._conn.cursor()\n            cursor.execute(query, (state, source_str, dest_str))\n\n\n    def get_oplog_ts(self, source, dest):\n        source_str = _mongo_dict_to_str(source)\n        dest_str = _mongo_dict_to_str(dest)\n        query  = \"SELECT oplog_ts \"\n        query += \"FROM %s \" % self.STATE_TABLE\n        query += \"WHERE source = ? AND dest = ?\"\n        with self._conn:\n            cursor = self._conn.cursor()\n            cursor.execute(query, (source_str, dest_str))\n            result = json.loads(cursor.fetchone()[0])\n            return Timestamp(time=result['time'], inc=result['inc'])"}
{"Repository": "lung-segmentation-2d", "input": "Abstract base class for image data iterators. className Iterator(object) Method __init__ Method reset Method _flow_index Method __iter__ Method __next__ Attribute n Attribute batch_size Attribute shuffle Attribute batch_index Attribute total_batches_seen Attribute lock Attribute index_generator", "label": "class Iterator(object):\n    def __init__(self, n, batch_size, shuffle, seed):\n        self.n = n\n        self.batch_size = batch_size\n        self.shuffle = shuffle\n        self.batch_index = 0\n        self.total_batches_seen = 0\n        self.lock = threading.Lock()\n        self.index_generator = self._flow_index(n, batch_size, shuffle, seed)\n\n    def reset(self):\n        self.batch_index = 0\n\n    def _flow_index(self, n, batch_size=32, shuffle=False, seed=None):\n        # Ensure self.batch_index is 0.\n        self.reset()\n        while 1:\n            if seed is not None:\n                np.random.seed(seed + self.total_batches_seen)\n            if self.batch_index == 0:\n                index_array = np.arange(n)\n                if shuffle:\n                    index_array = np.random.permutation(n)\n\n            current_index = (self.batch_index * batch_size) % n\n            if n > current_index + batch_size:\n                current_batch_size = batch_size\n                self.batch_index += 1\n            else:\n                current_batch_size = n - current_index\n                self.batch_index = 0\n            self.total_batches_seen += 1\n            yield (index_array[current_index: current_index + current_batch_size],\n                   current_index, current_batch_size)\n\n    def __iter__(self):\n        # Needed if we want to do something like:\n        # for x, y in data_gen.flow(...):\n        return self\n\n    def __next__(self, *args, **kwargs):\n        return self.next(*args, **kwargs)"}
{"Repository": "django-htk", "input": "Cache management object for static asset version for CSS and JavaScript className StaticAssetVersionCache(CustomCacheScheme) Method get_cache_duration", "label": "class StaticAssetVersionCache(CustomCacheScheme):\n    def get_cache_duration(self):\n        duration = TIMEOUT_30_DAYS\n        return duration"}
{"Repository": "screenplain", "input": "Handler for automatically opening and closing a tag. className tag(object) Method __init__ Method __enter__ Method __exit__ Attribute out Attribute tag Attribute classes", "label": "class tag(object):\n    def __init__(self, out, tag, classes=None):\n        self.out = out\n        self.tag = tag\n        self.classes = classes\n\n    def __enter__(self):\n        if self.classes:\n            self.out.write('<%s class=\"%s\">' % (\n                self.tag,\n                ' '.join(self.classes)\n            ))\n        else:\n            self.out.write('<%s>' % self.tag)\n\n    def __exit__(self, exception_type, value, traceback):\n        if not exception_type:\n            self.out.write('</%s>' % self.tag)\n        return False"}
{"Repository": "alternative-toolbar", "input": "RB ErrorsSource controller className AltErrorsController(AltGenericController) Method __init__ Method valid_source Method get_category Method get_gicon Attribute _gicon Attribute _source_types", "label": "class AltErrorsController(AltGenericController):\n    __gtype_name = 'AltErrorsController'\n\n    def __init__(self, header):\n        super(AltErrorsController, self).__init__(header)\n\n        self._gicon = Gio.ThemedIcon(name='dialog-error-symbolic')\n\n        self._source_types = [\"RBImportErrorsSource\",\n                              \"RBMissingFilesSource\"]\n\n    def valid_source(self, source):\n        print(type(source).__name__)\n        for source_type in self._source_types:\n            if source_type in type(source).__name__:\n                return True\n\n    def get_category(self):\n        return AltControllerCategory.LOCAL\n\n    def get_gicon(self, source):\n        return self._gicon"}
{"Repository": "ecg_pytorch", "input": "A generic data loader where the samples are arranged in this way: className ECGDataset(Dataset) Method __init__ Method __getitem__ Method __len__ Attribute train Attribute data Attribute idx2name Attribute file2idx Attribute wc", "label": "class ECGDataset(Dataset):\n    def __init__(self, data_path, train=True):\n        super(ECGDataset, self).__init__()\n        dd = torch.load(config.train_data)\n        self.train = train\n        self.data = dd['train'] if train else dd['val']\n        self.idx2name = dd['idx2name']\n        self.file2idx = dd['file2idx']\n        self.wc = 1. / np.log(dd['wc'])\n\n    def __getitem__(self, index):\n        fid = self.data[index]\n        file_path = os.path.join(config.train_dir, fid)\n        df = pd.read_csv(file_path, sep=' ').values\n        x = transform(df, self.train)\n        target = np.zeros(config.num_classes)\n        target[self.file2idx[fid]] = 1\n        target = torch.tensor(target, dtype=torch.float32)\n        return x, target\n\n    def __len__(self):\n        return len(self.data)"}
{"Repository": "elasticintel", "input": "Wraps a `httplib. className MockResponse(object) Method __init__ Method info Method getheaders Attribute _headers", "label": "class MockResponse(object):\n    def __init__(self, headers):\n        self._headers = headers\n\n    def info(self):\n        return self._headers\n\n    def getheaders(self, name):\n        self._headers.getheaders(name)"}
{"Repository": "vision_blender", "input": "Parent panel className RENDER_PT_gt_generator(GroundTruthGeneratorPanel) Method poll Method draw_header Method draw", "label": "class RENDER_PT_gt_generator(GroundTruthGeneratorPanel):\n    bl_label = \"VisionBlender UI\"\n    bl_idname = \"RENDER_PT_gt_generator\"\n    COMPAT_ENGINES = {'BLENDER_EEVEE', 'CYCLES'}\n    #bl_options = {'DEFAULT_CLOSED'} # makes panel closed by default\n    #bl_options = {'HIDE_HEADER'} # shows the panel on the top, not collapsable\n\n    @classmethod\n    def poll(cls, context):\n        return (context.engine in cls.COMPAT_ENGINES)\n\n    def draw_header(self, context):\n        self.layout.prop(context.scene.vision_blender, \"bool_save_gt_data\", text=\"\")\n\n    def draw(self, context):\n        scene = context.scene\n        rd = scene.render\n        layout = self.layout\n\n        vision_blender = scene.vision_blender\n        layout.active = vision_blender.bool_save_gt_data\n\n        layout.use_property_split = False\n        layout.use_property_decorate = False  # No animation.\n\n        # boolean flags to control what is being saved\n        #  reference: https://github.com/sobotka/blender/blob/662d94e020f36e75b9c6b4a258f31c1625573ee8/release/scripts/startup/bl_ui/properties_output.py\n        flow = layout.grid_flow(row_major=True, columns=0, even_columns=True, even_rows=False, align=False)\n        col = flow.column()\n        col.prop(vision_blender, \"bool_save_depth\", text=\"Depth / Disparity\")\n        col = flow.column()\n        col.enabled = context.engine == 'CYCLES' # ref: https://blenderartists.org/t/how-to-disable-a-checkbox-when-a-dropdown-option-is-picked/612801/2\n        col.prop(vision_blender, \"bool_save_segmentation_masks\", text=\"Segmentation Masks\")\n        col = flow.column()\n        col.prop(vision_blender, \"bool_save_normals\", text=\"Normals\")\n        col = flow.column()\n        col.enabled = context.engine == 'CYCLES'\n        col.prop(vision_blender, \"bool_save_opt_flow\", text=\"Optical Flow\")\n        col = flow.column()\n        col.prop(vision_blender, \"bool_save_obj_poses\", text=\"Objects' Pose\")\n        col = flow.column()\n        col.prop(vision_blender, \"bool_save_cam_param\", text=\"Camera Parameters\")\n\n        if context.engine != 'CYCLES':\n            col = layout.column(align=True)\n            col.label(text=\"Optical Flow and Segmentation Masks requires Cycles!\", icon='ERROR')\n\n        if vision_blender.bool_save_segmentation_masks and context.engine == 'CYCLES':\n            non_zero_obj_ind_found = check_any_obj_with_non_zero_index()\n            if not non_zero_obj_ind_found:\n                col = layout.column(align=True)\n                col.label(text=\"No object index found yet for Segmentation Masks...\", icon='ERROR')\n\n        # Get camera parameters\n        layout.label(text=\"Intrinsic parameters [pixels]:\")\n        f_x, f_y, c_x, c_y = get_camera_parameters_intrinsic(scene)\n\n        box_intr = self.layout.box()\n        col_intr = box_intr.column()\n\n        row_intr_0 = col_intr.split()\n        row_intr_0.label(text=str(f_x))# \"{}\".format(round(f_x, 3))\n        row_intr_0.label(text='0')\n        row_intr_0.label(text=str(c_x))\n\n        row_intr_1 = col_intr.split()\n        row_intr_1.label(text='0')\n        row_intr_1.label(text=str(f_y))\n        row_intr_1.label(text=str(c_y))\n\n        row_intr_2 = col_intr.split()\n        row_intr_2.label(text='0')\n        row_intr_2.label(text='0')\n        row_intr_2.label(text='1')\n        layout.label(text=\"Extrinsic parameters:\")\n\n        extr = get_camera_parameters_extrinsic(scene)\n\n        box_ext = self.layout.box()\n        col_ext = box_ext.column()\n\n        row_ext_0 = col_ext.split()\n        row_ext_0.label(text=str(extr[0, 0]))\n        row_ext_0.label(text=str(extr[0, 1]))\n        row_ext_0.label(text=str(extr[0, 2]))\n        row_ext_0.label(text=str(extr[0, 3]))\n\n        row_ext_1 = col_ext.split()\n        row_ext_1.label(text=str(extr[1, 0]))\n        row_ext_1.label(text=str(extr[1, 1]))\n        row_ext_1.label(text=str(extr[1, 2]))\n        row_ext_1.label(text=str(extr[1, 3]))\n\n        row_ext_2 = col_ext.split()\n        row_ext_2.label(text=str(extr[2, 0]))\n        row_ext_2.label(text=str(extr[2, 1]))\n        row_ext_2.label(text=str(extr[2, 2]))\n        row_ext_2.label(text=str(extr[2, 3]))"}
{"Repository": "GrowSP", "input": "DeepLab learning rate policy className PolyLR(LambdaStepLR) Method __init__", "label": "class PolyLR(LambdaStepLR):\n  def __init__(self, optimizer, max_iter=50000, power=0.9, last_step=-1):\n    super(PolyLR, self).__init__(optimizer, lambda s: (1 - s / (max_iter + 1))**power, last_step)"}
{"Repository": "iPokeMon-Server", "input": "A case-insensitive version of :class:`MultiDict` that defaults to replace the old value instead of appending it. className HeaderDict(MultiDict) Method __init__ Method __contains__ Method __delitem__ Method __getitem__ Method __setitem__ Method append Method replace Method getall Method get Method filter Attribute dict", "label": "class HeaderDict(MultiDict):\n    def __init__(self, *a, **ka):\n        self.dict = {}\n        if a or ka: self.update(*a, **ka)\n\n    def __contains__(self, key): return _hkey(key) in self.dict\n    def __delitem__(self, key): del self.dict[_hkey(key)]\n    def __getitem__(self, key): return self.dict[_hkey(key)][-1]\n    def __setitem__(self, key, value): self.dict[_hkey(key)] = [str(value)]\n    def append(self, key, value):\n        self.dict.setdefault(_hkey(key), []).append(str(value))\n    def replace(self, key, value): self.dict[_hkey(key)] = [str(value)]\n    def getall(self, key): return self.dict.get(_hkey(key)) or []\n    def get(self, key, default=None, index=-1):\n        return MultiDict.get(self, _hkey(key), default, index)\n    def filter(self, names):\n        for name in [_hkey(n) for n in names]:\n            if name in self.dict:\n                del self.dict[name]"}
{"Repository": "NetDissect", "input": "A dict-based CSV writer which will write rows to CSV file \"f\", which is encoded in the given encoding. className DictUnicodeWriter(object) Method writerow Method writerows Method writeheader", "label": "class DictUnicodeWriter(object):\n    def __init__(self, f, fieldnames, dialect=csv.excel,\n            encoding=\"utf-8\", **kwds):\n        # Redirect output to a queue\n        self.queue = cStringIO.StringIO()\n        self.writer = csv.DictWriter(self.queue, fieldnames, dialect=dialect, **kwds)\n        self.stream = f\n        self.encoder = codecs.getincrementalencoder(encoding)()\n\n    def writerow(self, D):\n        self.writer.writerow(\n                {k:unicode(v).encode(\"utf-8\") for k,v in D.items()})\n\n        # Fetch UTF-8 output from the queue ...\n        data = self.queue.getvalue()\n        data = data.decode(\"utf-8\")\n        # ... and reencode it into the target encoding\n        data = self.encoder.encode(data)\n        # write to the target stream\n        self.stream.write(data)\n        # empty queue\n        self.queue.truncate(0)\n\n    def writerows(self, rows):\n        for D in rows:\n            self.writerow(D)\n\n    def writeheader(self):\n        self.writer.writeheader()"}
{"Repository": "django-review", "input": "Represents a rating category. className RatingCategory(TranslatableModel) Method __str__ Method required Method get_choices Method get_rating_max_from_choices", "label": "class RatingCategory(TranslatableModel):\n    identifier = models.SlugField(\n        max_length=32,\n        verbose_name=_('Identifier'),\n        blank=True,\n    )\n\n    counts_for_average = models.BooleanField(\n        verbose_name=_('Counts for average rating'),\n        default=True,\n    )\n\n    translations = TranslatedFields(\n        name=models.CharField(max_length=256),\n        question=models.CharField(max_length=512, blank=True, null=True),\n    )\n\n    def __str__(self):\n        return self.lazy_translation_getter('name', 'Untranslated')\n\n    @property\n    def required(self):\n        if not hasattr(self, '_required'):\n            # get_choices sets _required\n            self.get_choices()\n        return self._required\n\n    def get_choices(self):\n        choices = ()\n        self._required = True\n        for choice in self.choices.all():\n            if choice.value is None or choice.value == '':\n                self._required = False\n            choices += (choice.value, choice.label),\n        if not choices:\n            return DEFAULT_CHOICES\n        return choices\n\n    def get_rating_max_from_choices(self):\n        return int(list(self.get_choices())[0][0])"}
{"Repository": "easystac", "input": "ImageCollection object for Radiant Earth ML Hub. className ImageCollection(BaseImageCollection) Method getInfo", "label": "class ImageCollection(BaseImageCollection):\n    def getInfo(self, **kwargs):\n        search = self._search(\n            url=\"https://api.radiant.earth/mlhub/v1/\",\n            parameters={\"key\": os.environ[\"MLHUB_API_KEY\"]},\n        )\n\n        items = [item.to_dict() for item in search.get_items()]\n\n        image_collection = stackstac.stack(items, **kwargs)\n\n        return image_collection"}
{"Repository": "normalizing-flows", "input": "Class conditional multivariate Gaussian distribution with diagonal covariance matrix className ClassCondDiagGaussian(BaseDistribution) Method __init__ Method forward Method log_prob Attribute shape Attribute n_dim Attribute perm Attribute d Attribute num_classes Attribute loc Attribute log_scale Attribute temperature", "label": "class ClassCondDiagGaussian(BaseDistribution):\n    def __init__(self, shape, num_classes):\n        super().__init__()\n        if isinstance(shape, int):\n            shape = (shape,)\n        if isinstance(shape, list):\n            shape = tuple(shape)\n        self.shape = shape\n        self.n_dim = len(shape)\n        self.perm = [self.n_dim] + list(range(self.n_dim))\n        self.d = np.prod(shape)\n        self.num_classes = num_classes\n        self.loc = nn.Parameter(torch.zeros(*self.shape, num_classes))\n        self.log_scale = nn.Parameter(torch.zeros(*self.shape, num_classes))\n        self.temperature = None  # Temperature parameter for annealed sampling\n\n    def forward(self, num_samples=1, y=None):\n        if y is not None:\n            num_samples = len(y)\n        else:\n            y = torch.randint(self.num_classes, (num_samples,), device=self.loc.device)\n        if y.dim() == 1:\n            y_onehot = torch.zeros(\n                (self.num_classes, num_samples),\n                dtype=self.loc.dtype,\n                device=self.loc.device,\n            )\n            y_onehot.scatter_(0, y[None], 1)\n            y = y_onehot\n        else:\n            y = y.t()\n        eps = torch.randn(\n            (num_samples,) + self.shape, dtype=self.loc.dtype, device=self.loc.device\n        )\n        loc = (self.loc @ y).permute(*self.perm)\n        log_scale = (self.log_scale @ y).permute(*self.perm)\n        if self.temperature is not None:\n            log_scale = np.log(self.temperature) + log_scale\n        z = loc + torch.exp(log_scale) * eps\n        log_p = -0.5 * self.d * np.log(2 * np.pi) - torch.sum(\n            log_scale + 0.5 * torch.pow(eps, 2), list(range(1, self.n_dim + 1))\n        )\n        return z, log_p\n\n    def log_prob(self, z, y):\n        if y.dim() == 1:\n            y_onehot = torch.zeros(\n                (self.num_classes, len(y)), dtype=self.loc.dtype, device=self.loc.device\n            )\n            y_onehot.scatter_(0, y[None], 1)\n            y = y_onehot\n        else:\n            y = y.t()\n        loc = (self.loc @ y).permute(*self.perm)\n        log_scale = (self.log_scale @ y).permute(*self.perm)\n        if self.temperature is not None:\n            log_scale = np.log(self.temperature) + log_scale\n        log_p = -0.5 * self.d * np.log(2 * np.pi) - torch.sum(\n            log_scale + 0.5 * torch.pow((z - loc) / torch.exp(log_scale), 2),\n            list(range(1, self.n_dim + 1)),\n        )\n        return log_p"}
{"Repository": "assignment1-2018", "input": "Op to element-wise multiply a nodes by a constant. className MulByConstOp(Op) Method __call__ Method compute Method gradient", "label": "class MulByConstOp(Op):\n    def __call__(self, node_A, const_val):\n        new_node = Op.__call__(self)\n        new_node.const_attr = const_val\n        new_node.inputs = [node_A]\n        new_node.name = \"(%s*%s)\" % (node_A.name, str(const_val))\n        return new_node\n\n    def compute(self, node, input_vals):\n        \"\"\"TODO: Your code here\"\"\"\n\n    def gradient(self, node, output_grad):\n        \"\"\"TODO: Your code here\"\"\""}
{"Repository": "lemon", "input": "Raised if the Astrometry.net timeout was reached className AstrometryNetTimeoutExpired(AstrometryNetUnsolvedField) Method __init__ Method __str__ Attribute path Attribute timeout", "label": "class AstrometryNetTimeoutExpired(AstrometryNetUnsolvedField):\n    def __init__(self, path, timeout):\n        self.path = path\n        self.timeout = timeout\n\n    def __str__(self):\n        msg = \"%s: could not solve field in less than %d seconds\"\n        return msg % (self.path, self.timeout)"}
{"Repository": "chromium-dashboard", "input": "Simple class that assigns all named args to attributes. className Blank(object) Method __init__ Method __repr__ Method __eq__", "label": "class Blank(object):\n  def __init__(self, **kwargs):\n    vars(self).update(kwargs)\n  def __repr__(self):\n    return '%s(%s)' % (self.__class__.__name__, str(vars(self)))\n  def __eq__(self, other):\n    if other is None:\n      return False\n    return vars(self) == vars(other)"}
{"Repository": "sphinxcontrib-versioning", "input": "Raised if git exits non-zero. className GitError(Exception) Method __init__ Attribute message Attribute output", "label": "class GitError(Exception):\n    def __init__(self, message, output):\n        self.message = message\n        self.output = output\n        super(GitError, self).__init__(message, output)"}
{"Repository": "vector-db-benchmark", "input": "A reader created specifically to read the format used in https://github. className AnnCompoundReader(JSONReader) Method read_vectors Method read_queries", "label": "class AnnCompoundReader(JSONReader):\n    VECTORS_FILE = \"vectors.npy\"\n    QUERIES_FILE = \"tests.jsonl\"\n\n    def read_vectors(self) -> Iterator[List[float]]:\n        vectors = np.load(self.path / self.VECTORS_FILE)\n        for vector in vectors:\n            if self.normalize:\n                vector = vector / np.linalg.norm(vector)\n            yield vector.tolist()\n\n    def read_queries(self) -> Iterator[Query]:\n        with open(self.path / self.QUERIES_FILE) as payloads_fp:\n            for idx, row in enumerate(payloads_fp):\n                row_json = json.loads(row)\n                vector = np.array(row_json[\"query\"])\n                if self.normalize:\n                    vector /= np.linalg.norm(vector)\n                yield Query(\n                    vector=vector.tolist(),\n                    sparse_vector=None,\n                    meta_conditions=row_json[\"conditions\"],\n                    expected_result=row_json[\"closest_ids\"],\n                    expected_scores=row_json[\"closest_scores\"],\n                )"}
{"Repository": "Deep-OC-SORT", "input": "A simple timer. className Timer(object) Method __init__ Method tic Method toc Method clear Attribute total_time Attribute calls Attribute start_time Attribute diff Attribute average_time Attribute duration", "label": "class Timer(object):\n    def __init__(self):\n        self.total_time = 0.0\n        self.calls = 0\n        self.start_time = 0.0\n        self.diff = 0.0\n        self.average_time = 0.0\n\n        self.duration = 0.0\n\n    def tic(self):\n        # using time.time instead of time.clock because time time.clock\n        # does not normalize for multithreading\n        self.start_time = time.time()\n\n    def toc(self, average=True):\n        self.diff = time.time() - self.start_time\n        self.total_time += self.diff\n        self.calls += 1\n        self.average_time = self.total_time / self.calls\n        if average:\n            self.duration = self.average_time\n        else:\n            self.duration = self.diff\n        return self.duration\n\n    def clear(self):\n        self.total_time = 0.0\n        self.calls = 0\n        self.start_time = 0.0\n        self.diff = 0.0\n        self.average_time = 0.0\n        self.duration = 0.0"}
{"Repository": "folders2flickr", "input": "A Flickr user. className User(object) Method _general_getattr Method _load_properties Method __str__ Method getPhotosets Method getPublicFavorites Method getFavorites Method getGalleries", "label": "class User(object):\n    def __init__(self, id, username=None, isadmin=None, ispro=None, \\\n                 realname=None, location=None, firstdate=None, count=None):\n        self.__loaded = False #so we don't keep loading data\n        self.__id = id\n        self.__username = username\n        self.__isadmin = isadmin\n        self.__ispro = ispro\n        self.__realname = realname\n        self.__location = location\n        self.__photos_firstdate = firstdate\n        self.__photos_count = count\n\n    #property fu\n    id = property(lambda self: self._general_getattr('id'))\n    username = property(lambda self: self._general_getattr('username'))\n    isadmin = property(lambda self: self._general_getattr('isadmin'))\n    ispro = property(lambda self: self._general_getattr('ispro'))\n    realname = property(lambda self: self._general_getattr('realname'))\n    location = property(lambda self: self._general_getattr('location'))\n    photos_firstdate = property(lambda self: \\\n                                self._general_getattr('photos_firstdate'))\n    photos_firstdatetaken = property(lambda self: \\\n                                     self._general_getattr\\\n                                     ('photos_firstdatetaken'))\n    photos_count = property(lambda self: \\\n                            self._general_getattr('photos_count'))\n    icon_server= property(lambda self: self._general_getattr('icon_server'))\n    icon_url= property(lambda self: self._general_getattr('icon_url'))\n\n    def _general_getattr(self, var):\n        if getattr(self, \"_%s__%s\" % (self.__class__.__name__, var)) is None \\\n           and not self.__loaded:\n            self._load_properties()\n        return getattr(self, \"_%s__%s\" % (self.__class__.__name__, var))\n\n    def _load_properties(self):\n        method = 'flickr.people.getInfo'\n        data = _doget(method, user_id=self.__id)\n\n        self.__loaded = True\n\n        person = data.rsp.person\n\n        self.__isadmin = person.isadmin\n        self.__ispro = person.ispro\n        self.__icon_server = person.iconserver\n        if int(person.iconserver) > 0:\n            self.__icon_url = 'http://photos%s.flickr.com/buddyicons/%s.jpg' \\\n                              % (person.iconserver, self.__id)\n        else:\n            self.__icon_url = 'http://www.flickr.com/images/buddyicon.jpg'\n\n        self.__username = person.username.text\n        self.__realname = getattr((getattr(person,  'realname',  u'')), 'text', u'')\n        self.__location = getattr((getattr(person,  'location',  u'')), 'text', u'')\n        self.__photos_count = getattr((getattr(getattr(person,  'photos',  None),  'count',  u'')), 'text', u'')\n        if self.__photos_count:\n            self.__photos_firstdate = person.photos.firstdate.text\n            self.__photos_firstdatetaken = person.photos.firstdatetaken.text\n        else:\n            self.__photos_firstdate = None\n            self.__photos_firstdatetaken = None\n\n    def __str__(self):\n        return '<Flickr User %s>' % self.id\n\n    def getPhotosets(self):\n        method = 'flickr.photosets.getList'\n        data = _doget(method, user_id=self.id)\n\n        sets = []\n        if not getattr(data.rsp.photosets,  'photoset',None):\n            return sets        #N.B. returns an empty set\n        if isinstance(data.rsp.photosets.photoset, list):\n            for photoset in data.rsp.photosets.photoset:\n                sets.append(Photoset(photoset.id, photoset.title.text,\\\n                                     Photo(photoset.primary),\\\n                                     secret=photoset.secret, \\\n                                     server=photoset.server, \\\n                                     description=photoset.description.text,\n                                     photos=photoset.photos))\n        else:\n            photoset = data.rsp.photosets.photoset\n            sets.append(Photoset(photoset.id, photoset.title.text,\\\n                                     Photo(photoset.primary),\\\n                                     secret=photoset.secret, \\\n                                     server=photoset.server, \\\n                                     description=photoset.description.text,\n                                     photos=photoset.photos))\n        return sets\n\n    def getPublicFavorites(self, per_page='', page=''):\n        return favorites_getPublicList(user_id=self.id, per_page=per_page, \\\n                                       page=page)\n\n    def getFavorites(self, per_page='', page=''):\n        return favorites_getList(user_id=self.id, per_page=per_page, \\\n                                 page=page)\n\n    def getGalleries(self, per_page='', page=''):\n        return galleries_getList(user_id=self.id, per_page=per_page, \\\n                                 page=page)"}
{"Repository": "Fatigue-Driven-Detection-Based-on-CNN", "input": "At test time, Detect is the final layer of SSD. className Detect(Function) Method __init__ Method forward Attribute num_classes Attribute background_label Attribute top_k Attribute nms_thresh Attribute conf_thresh Attribute variance", "label": "class Detect(Function):\n    def __init__(self, num_classes, bkg_label, top_k, conf_thresh, nms_thresh):\n        self.num_classes = num_classes\n        self.background_label = bkg_label\n        self.top_k = top_k\n        # Parameters used in nms.\n        self.nms_thresh = nms_thresh\n        if nms_thresh <= 0:\n            raise ValueError('nms_threshold must be non negative.')\n        self.conf_thresh = conf_thresh\n        self.variance = (0.1,0.2)\n\n    @staticmethod\n    def forward(self, loc_data, conf_data, prior_data, num_classes, top_k, conf_thresh, nms_thresh):\n        num = loc_data.size(0)  # batch size\n        num_priors = prior_data.size(0)\n        output = torch.zeros(num, num_classes, top_k, 5)\n        conf_preds = conf_data.view(num, num_priors,\n                                    num_classes).transpose(2, 1)\n\n        # Decode predictions into bboxes.\n        variance = (0.1, 0.2)\n        for i in range(num):\n            decoded_boxes = decode(loc_data[i], prior_data, variance)\n            # For each class, perform nms\n            conf_scores = conf_preds[i].clone()\n            for cl in range(1, num_classes):\n                c_mask = conf_scores[cl].gt(conf_thresh)\n                scores = conf_scores[cl][c_mask]\n                if scores.dim() == 0:\n                    continue\n                l_mask = c_mask.unsqueeze(1).expand_as(decoded_boxes)\n                boxes = decoded_boxes[l_mask].view(-1, 4)\n                # idx of highest scoring and non-overlapping boxes per class\n                ids, count = nms(boxes, scores, nms_thresh, top_k)\n\n                if count==0:\n                    continue\n                output[i, cl, :count] = \\\n                    torch.cat((scores[ids[:count]].unsqueeze(1),\n                               boxes[ids[:count]]), 1)\n        flt = output.contiguous().view(num, -1, 5)\n        _, idx = flt[:, :, 0].sort(1, descending=True)\n        _, rank = idx.sort(1)\n        flt[(rank < top_k).unsqueeze(-1).expand_as(flt)].fill_(0)\n        return output"}
{"Repository": "wikidata", "input": "No-op cache policy. className NullCachePolicy(CachePolicy) Method get Method set", "label": "class NullCachePolicy(CachePolicy):\n    def get(self, key: CacheKey) -> Optional[CacheValue]:\n        return None\n\n    def set(self, key: CacheKey, value: Optional[CacheValue]) -> None:\n        pass"}
{"Repository": "TCL", "input": "Implements Novograd algorithm. className NvNovoGrad(Optimizer) Method __setstate__ Method step", "label": "class NvNovoGrad(Optimizer):\n    def __init__(self, params, lr=1e-3, betas=(0.95, 0.98), eps=1e-8,\n                 weight_decay=0, grad_averaging=False, amsgrad=False):\n        if not 0.0 <= lr:\n            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n        if not 0.0 <= eps:\n            raise ValueError(\"Invalid epsilon value: {}\".format(eps))\n        if not 0.0 <= betas[0] < 1.0:\n            raise ValueError(\"Invalid beta parameter at index 0: {}\".format(betas[0]))\n        if not 0.0 <= betas[1] < 1.0:\n            raise ValueError(\"Invalid beta parameter at index 1: {}\".format(betas[1]))\n        defaults = dict(lr=lr, betas=betas, eps=eps,\n                        weight_decay=weight_decay,\n                        grad_averaging=grad_averaging,\n                        amsgrad=amsgrad)\n\n        super(NvNovoGrad, self).__init__(params, defaults)\n\n    def __setstate__(self, state):\n        super(NvNovoGrad, self).__setstate__(state)\n        for group in self.param_groups:\n            group.setdefault('amsgrad', False)\n\n    def step(self, closure=None):\n        loss = None\n        if closure is not None:\n            loss = closure()\n\n        for group in self.param_groups:\n            for p in group['params']:\n                if p.grad is None:\n                    continue\n                grad = p.grad.data\n                if grad.is_sparse:\n                    raise RuntimeError('Sparse gradients are not supported.')\n                amsgrad = group['amsgrad']\n\n                state = self.state[p]\n\n                # State initialization\n                if len(state) == 0:\n                    state['step'] = 0\n                    # Exponential moving average of gradient values\n                    state['exp_avg'] = torch.zeros_like(p.data)\n                    # Exponential moving average of squared gradient values\n                    state['exp_avg_sq'] = torch.zeros([]).to(state['exp_avg'].device)\n                    if amsgrad:\n                        # Maintains max of all exp. moving avg. of sq. grad. values\n                        state['max_exp_avg_sq'] = torch.zeros([]).to(state['exp_avg'].device)\n\n                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n                if amsgrad:\n                    max_exp_avg_sq = state['max_exp_avg_sq']\n                beta1, beta2 = group['betas']\n\n                state['step'] += 1\n\n                norm = torch.sum(torch.pow(grad, 2))\n\n                if exp_avg_sq == 0:\n                    exp_avg_sq.copy_(norm)\n                else:\n                    exp_avg_sq.mul_(beta2).add_(1 - beta2, norm)\n\n                if amsgrad:\n                    # Maintains the maximum of all 2nd moment running avg. till now\n                    torch.max(max_exp_avg_sq, exp_avg_sq, out=max_exp_avg_sq)\n                    # Use the max. for normalizing running avg. of gradient\n                    denom = max_exp_avg_sq.sqrt().add_(group['eps'])\n                else:\n                    denom = exp_avg_sq.sqrt().add_(group['eps'])\n\n                grad.div_(denom)\n                if group['weight_decay'] != 0:\n                    grad.add_(group['weight_decay'], p.data)\n                if group['grad_averaging']:\n                    grad.mul_(1 - beta1)\n                exp_avg.mul_(beta1).add_(grad)\n\n                p.data.add_(-group['lr'], exp_avg)\n\n        return loss"}
{"Repository": "Senta", "input": "FluidDataType data struct wrapper className FluidDataType(object) Method __init__ Attribute shape Attribute dtype Attribute lod_level", "label": "class FluidDataType(object):                                                                                                                                 \n    def __init__(self, shape, dtype, lod_level):                                                                                                             \n        self.shape = shape                                                                                                                                   \n        self.dtype = dtype                                                                                                                                   \n        self.lod_level = lod_level"}
{"Repository": "wasserstein-dist", "input": "Generates images from noise using a DCGAN-like network. className Generator(object) Method __init__ Method network Method get_batch Attribute bs Attribute noise_dim", "label": "class Generator(object):\n  def __init__(self, bs, noise_dim):\n    self.bs = bs  # batch size\n    self.noise_dim = noise_dim  # dimension of latent space\n\n  # minimal implementation of DCGAN-like network for 32x32x3 images\n  def network(self, inputs, reuse=True):\n    with tf.variable_scope('Generator', values=[inputs], reuse=reuse) as scope:\n      with slim.arg_scope([slim.conv2d_transpose], stride=2, kernel_size=4,\n                          normalizer_fn=slim.batch_norm):\n        net = tf.expand_dims(tf.expand_dims(inputs, 1), 1)\n        net = slim.conv2d_transpose(net, 512, stride=1, padding='VALID',\n                                    scope='deconv1')\n        net = slim.conv2d_transpose(net, 256, scope='deconv2')\n        net = slim.conv2d_transpose(net, 128, scope='deconv3')\n        net = slim.conv2d_transpose(net, 64, normalizer_fn=None,\n                                    activation_fn=None, scope='deconv4')\n        logits = slim.conv2d(net, 3, normalizer_fn=None, activation_fn=None,\n                             kernel_size=1, stride=1, padding='VALID',\n                             scope='logits')\n        images = tf.tanh(logits)\n        return images\n\n  # generate a batch of images from random noise\n  def get_batch(self, bs=None, reuse=True):\n    if not bs:\n      bs = self.bs\n    noise = tf.random_normal([bs, self.noise_dim], name='noise')\n    images = self.network(noise, reuse=reuse)\n    return images"}
{"Repository": "MC_OCR", "input": "Computes and stores the average and current value Imported from https://github. className AverageMeter(object) Method __init__ Method reset Method update", "label": "class AverageMeter(object):\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count"}
{"Repository": "unsupervised-volumetric-animation", "input": "Dataset of videos. className FramesDataset(Dataset) Method get_visualization_videos Method __len__ Method get_item_from_path Method __getitem__", "label": "class FramesDataset(Dataset):\n    def __init__(self,\n                 root_dir,\n                 resolution=(256, 256),\n                 is_train=True,\n                 max_frames=None,\n                 max_videos=None,\n                 repeat=1,\n                 visualization_videos=4,\n                 visualization_frames=None):\n        self.root_dir = root_dir\n        folder = 'train' if is_train else 'test'\n        folder = os.path.join(root_dir, folder)\n\n        # Select which videos will be used\n        videos = list(sorted(os.listdir(folder)))\n        if max_videos is not None:\n            np.random.seed(0)\n            videos = sorted(np.random.choice(videos, max_videos, replace=False))\n\n        # Select videos for visualization\n        self.visualization_videos = []\n        if isinstance(visualization_videos, int):\n            np.random.seed(0)\n            visualization_videos = min(len(videos), visualization_videos)\n            visualization_videos = np.random.choice(videos, visualization_videos, replace=False)\n\n        # Create a list of all frames (self.frames) and mapping from video_id to frames\n        np.random.seed(0)\n        self.frames = []\n        self.id_to_frames = defaultdict(list)\n        for video_id, video_name in tqdm(enumerate(videos)):\n            path = os.path.join(folder, video_name)\n            video_frames = []\n            frames = sorted(os.listdir(path))\n\n            if max_frames is not None and len(frames) > max_frames:\n                frames = np.random.choice(frames, max_frames, replace=False)\n            frames = sorted(frames)\n\n            for frame_index, frame_name in enumerate(frames):\n                info = {'video_id': video_id, 'frame_index': frame_index, 'path': os.path.join(path, frame_name)}\n\n                self.frames.append(info)\n                video_frames.append(info)\n                self.id_to_frames[video_id].append(info)\n\n            if video_name in visualization_videos:\n                if visualization_frames is None:\n                    self.visualization_videos.append(video_frames)\n                else:\n                    self.visualization_videos.append(video_frames[:visualization_frames])\n\n        print(\"Found \", len(self.id_to_frames), \" videos.\")\n        print(\"Found \", len(self.frames), \"frames.\")\n\n        self.is_train = is_train\n        self.resize_shape = resolution\n        self.resize_fn = partial(resize, output_shape=tuple(self.resize_shape))\n        self.max_videos = len(videos)\n        self.ids_list = list(self.id_to_frames.keys()) * repeat\n\n    def get_visualization_videos(self):\n        out_videos = []\n\n        for frames_data in self.visualization_videos:\n            video = defaultdict(list)\n            for frame_info in frames_data:\n                frame_dict = self.get_item_from_path(frame_info['video_id'], frame_info['path'])\n                video['frame'].append(np.array(frame_dict['frame'])[np.newaxis])\n                video['video_id'].append(np.array(frame_dict['video_id'])[np.newaxis])\n\n            frames = np.concatenate(video['frame'], axis=0)\n            frames = torch.Tensor(frames)\n            video_id = np.concatenate(video['video_id'], axis=0)\n            video_id = torch.LongTensor(video_id)\n            video = {'frame': frames, 'video_id': video_id}\n\n            out_videos.append(video)\n        return out_videos\n\n    def __len__(self):\n        return len(self.ids_list)\n\n    def get_item_from_path(self, video_id, path):\n        image = self.resize_fn(io.imread(path))\n        image = np.array(image, dtype='float32').transpose((2, 0, 1))\n        return {'frame': image, 'video_id': video_id}\n\n    def __getitem__(self, idx):\n        video_id = self.ids_list[idx]\n        frame_list = self.id_to_frames[video_id]\n        frame_info = frame_list[np.random.randint(0, len(frame_list))]\n        return self.get_item_from_path(frame_info['video_id'], frame_info['path'])"}
{"Repository": "ICIAR2018", "input": "Provides `update_to(n)` which uses `tqdm.update(delta_n)`. className TqdmUpTo(tqdm) Method update_to", "label": "class TqdmUpTo(tqdm):\n    def update_to(self, b=1, bsize=1, tsize=None):\n        if tsize is not None:\n            self.total = tsize\n        self.update(b * bsize - self.n)  # will also set self.n = b * bsize"}
{"Repository": "Teacher-free-Knowledge-Distillation", "input": "Tensorboard log utility className Board_Logger(object) Method __init__ Method scalar_summary Method image_summary Method histo_summary Attribute writer", "label": "class Board_Logger(object):\n    def __init__(self, log_dir):\n        self.writer = tf.summary.FileWriter(log_dir)\n\n    def scalar_summary(self, tag, value, step):\n        summary = tf.Summary(value=[tf.Summary.Value(tag=tag, simple_value=value)])\n        self.writer.add_summary(summary, step)\n\n    def image_summary(self, tag, images, step):\n        img_summaries = []\n        for i, img in enumerate(images):\n            # Write the image to a string\n            try:\n                s = StringIO()\n            except:\n                s = BytesIO()\n            scipy.misc.toimage(img).save(s, format=\"png\")\n\n            # Create an Image object\n            img_sum = tf.Summary.Image(encoded_image_string=s.getvalue(),\n                                       height=img.shape[0],\n                                       width=img.shape[1])\n            # Create a Summary value\n            img_summaries.append(tf.Summary.Value(tag='%s/%d' % (tag, i), image=img_sum))\n\n        # Create and write Summary\n        summary = tf.Summary(value=img_summaries)\n        self.writer.add_summary(summary, step)\n\n    def histo_summary(self, tag, values, step, bins=1000):\n        # Create a histogram using numpy\n        counts, bin_edges = np.histogram(values, bins=bins)\n\n        # Fill the fields of the histogram proto\n        hist = tf.HistogramProto()\n        hist.min = float(np.min(values))\n        hist.max = float(np.max(values))\n        hist.num = int(np.prod(values.shape))\n        hist.sum = float(np.sum(values))\n        hist.sum_squares = float(np.sum(values**2))\n\n        # Drop the start of the first bin\n        bin_edges = bin_edges[1:]\n\n        # Add bin edges and counts\n        for edge in bin_edges:\n            hist.bucket_limit.append(edge)\n        for c in counts:\n            hist.bucket.append(c)\n\n        # Create and write Summary\n        summary = tf.Summary(value=[tf.Summary.Value(tag=tag, histo=hist)])\n        self.writer.add_summary(summary, step)\n        self.writer.flush"}
{"Repository": "DiffPure", "input": "A residual block that can optionally change the number of channels. className ResBlock(TimestepBlock) Method forward Method _forward", "label": "class ResBlock(TimestepBlock):\n    def __init__(\n        self,\n        channels,\n        emb_channels,\n        dropout,\n        out_channels=None,\n        use_conv=False,\n        use_scale_shift_norm=False,\n        dims=2,\n        use_checkpoint=False,\n        up=False,\n        down=False,\n    ):\n        super().__init__()\n        self.channels = channels\n        self.emb_channels = emb_channels\n        self.dropout = dropout\n        self.out_channels = out_channels or channels\n        self.use_conv = use_conv\n        self.use_checkpoint = use_checkpoint\n        self.use_scale_shift_norm = use_scale_shift_norm\n\n        self.in_layers = nn.Sequential(\n            normalization(channels),\n            nn.SiLU(),\n            conv_nd(dims, channels, self.out_channels, 3, padding=1),\n        )\n\n        self.updown = up or down\n\n        if up:\n            self.h_upd = Upsample(channels, False, dims)\n            self.x_upd = Upsample(channels, False, dims)\n        elif down:\n            self.h_upd = Downsample(channels, False, dims)\n            self.x_upd = Downsample(channels, False, dims)\n        else:\n            self.h_upd = self.x_upd = nn.Identity()\n\n        self.emb_layers = nn.Sequential(\n            nn.SiLU(),\n            linear(\n                emb_channels,\n                2 * self.out_channels if use_scale_shift_norm else self.out_channels,\n            ),\n        )\n        self.out_layers = nn.Sequential(\n            normalization(self.out_channels),\n            nn.SiLU(),\n            nn.Dropout(p=dropout),\n            zero_module(\n                conv_nd(dims, self.out_channels, self.out_channels, 3, padding=1)\n            ),\n        )\n\n        if self.out_channels == channels:\n            self.skip_connection = nn.Identity()\n        elif use_conv:\n            self.skip_connection = conv_nd(\n                dims, channels, self.out_channels, 3, padding=1\n            )\n        else:\n            self.skip_connection = conv_nd(dims, channels, self.out_channels, 1)\n\n    def forward(self, x, emb):\n        return checkpoint(\n            self._forward, (x, emb), self.parameters(), self.use_checkpoint\n        )\n\n    def _forward(self, x, emb):\n        if self.updown:\n            in_rest, in_conv = self.in_layers[:-1], self.in_layers[-1]\n            h = in_rest(x)\n            h = self.h_upd(h)\n            x = self.x_upd(x)\n            h = in_conv(h)\n        else:\n            h = self.in_layers(x)\n        emb_out = self.emb_layers(emb).type(h.dtype)\n        while len(emb_out.shape) < len(h.shape):\n            emb_out = emb_out[..., None]\n        if self.use_scale_shift_norm:\n            out_norm, out_rest = self.out_layers[0], self.out_layers[1:]\n            scale, shift = th.chunk(emb_out, 2, dim=1)\n            h = out_norm(h) * (1 + scale) + shift\n            h = out_rest(h)\n        else:\n            h = h + emb_out\n            h = self.out_layers(h)\n        return self.skip_connection(x) + h"}
{"Repository": "amazon-scraper-python", "input": "Class of the products className Products(object) Method __init__ Method _add_product Attribute products Attribute last_html_page Attribute html_pages", "label": "class Products(object):\n    def __init__(self, product_dict_list=[]):\n        self.products = []\n        self.last_html_page = \"\"  # HTML content of the last scraped page\n        self.html_pages = []\n        for product_dict in product_dict_list:\n            self._add_product(product_dict)\n\n    def _add_product(self, product_dict):\n        \"\"\" Append a product to the object product list\n        >>> p = Products([{'title':'Book title', 'rating': '4.2',\\"}
{"Repository": "mdict-analysis", "input": "MDict resource file format (*. className MDD(MDict) Method __init__ Method items Method _decode_record_block", "label": "class MDD(MDict):\n    def __init__(self, fname, passcode=None):\n        MDict.__init__(self, fname, encoding='UTF-16', passcode=passcode)\n\n    def items(self):\n        return self._decode_record_block()\n\n    def _decode_record_block(self):\n        f = open(self._fname, 'rb')\n        f.seek(self._record_block_offset)\n\n        num_record_blocks = self._read_number(f)\n        num_entries = self._read_number(f)\n        assert(num_entries == self._num_entries)\n        record_block_info_size = self._read_number(f)\n        record_block_size = self._read_number(f)\n\n        # record block info section\n        record_block_info_list = []\n        size_counter = 0\n        for i in range(num_record_blocks):\n            compressed_size = self._read_number(f)\n            decompressed_size = self._read_number(f)\n            record_block_info_list += [(compressed_size, decompressed_size)]\n            size_counter += self._number_width * 2\n        assert(size_counter == record_block_info_size)\n\n        # actual record block\n        offset = 0\n        i = 0\n        size_counter = 0\n        for compressed_size, decompressed_size in record_block_info_list:\n            record_block_compressed = f.read(compressed_size)\n            # 4 bytes: compression type\n            record_block_type = record_block_compressed[:4]\n            # 4 bytes: adler32 checksum of decompressed record block\n            adler32 = unpack('>I', record_block_compressed[4:8])[0]\n            if record_block_type == b'\\x00\\x00\\x00\\x00':\n                record_block = record_block_compressed[8:]\n            elif record_block_type == b'\\x01\\x00\\x00\\x00':\n                if lzo is None:\n                    print(\"LZO compression is not supported\")\n                    break\n                # decompress\n                header = b'\\xf0' + pack('>I', decompressed_size)\n                record_block = lzo.decompress(header + record_block_compressed[8:])\n            elif record_block_type == b'\\x02\\x00\\x00\\x00':\n                # decompress\n                record_block = zlib.decompress(record_block_compressed[8:])\n\n            # notice that adler32 return signed value\n            assert(adler32 == zlib.adler32(record_block) & 0xffffffff)\n\n            assert(len(record_block) == decompressed_size)\n            # split record block according to the offset info from key block\n            while i < len(self._key_list):\n                record_start, key_text = self._key_list[i]\n                # reach the end of current record block\n                if record_start - offset >= len(record_block):\n                    break\n                # record end index\n                if i < len(self._key_list)-1:\n                    record_end = self._key_list[i+1][0]\n                else:\n                    record_end = len(record_block) + offset\n                i += 1\n                data = record_block[record_start-offset:record_end-offset]\n                yield key_text, data\n            offset += len(record_block)\n            size_counter += compressed_size\n        assert(size_counter == record_block_size)\n\n        f.close()"}
{"Repository": "DiCE", "input": "Class containing all required information about the data for DiCE. className Data(_BaseData) Method __init__ Method decide_implementation_type", "label": "class Data(_BaseData):\n    def __init__(self, **params):\n        self.decide_implementation_type(params)\n\n    def decide_implementation_type(self, params):\n        self.__class__ = decide(params)\n        self.__init__(params)"}
{"Repository": "wikidata", "input": "LRU (least recently used) cache in memory. className MemoryCachePolicy(CachePolicy) Method __init__ Method get Method set", "label": "class MemoryCachePolicy(CachePolicy):\n    def __init__(self, max_size: int = 128) -> None:\n        self.max_size = max_size  # type: int\n        self.values = \\\n            collections.OrderedDict()  # type: collections.OrderedDict\n\n    def get(self, key: CacheKey) -> Optional[CacheValue]:\n        try:\n            v = self.values[key]\n        except KeyError:\n            v = None\n        else:\n            self.values.move_to_end(key)\n        return v\n\n    def set(self, key: CacheKey, value: Optional[CacheValue]) -> None:\n        try:\n            del self.values[key]\n        except KeyError:\n            pass\n        if value is None:\n            return\n        self.values[key] = value\n        while len(self.values) > self.max_size:\n            self.values.popitem(last=False)"}
{"Repository": "VSR-Transformer", "input": "Video test dataset for DUF dataset. className VideoTestDUFDataset(VideoTestDataset) Method __getitem__", "label": "class VideoTestDUFDataset(VideoTestDataset):\n    def __getitem__(self, index):\n        folder = self.data_info['folder'][index]\n        idx, max_idx = self.data_info['idx'][index].split('/')\n        idx, max_idx = int(idx), int(max_idx)\n        border = self.data_info['border'][index]\n        lq_path = self.data_info['lq_path'][index]\n\n        select_idx = generate_frame_indices(\n            idx, max_idx, self.opt['num_frame'], padding=self.opt['padding'])\n\n        if self.cache_data:\n            if self.opt['use_duf_downsampling']:\n                # read imgs_gt to generate low-resolution frames\n                imgs_lq = self.imgs_gt[folder].index_select(\n                    0, torch.LongTensor(select_idx))\n                imgs_lq = duf_downsample(\n                    imgs_lq, kernel_size=13, scale=self.opt['scale'])\n            else:\n                imgs_lq = self.imgs_lq[folder].index_select(\n                    0, torch.LongTensor(select_idx))\n            img_gt = self.imgs_gt[folder][idx]\n        else:\n            if self.opt['use_duf_downsampling']:\n                img_paths_lq = [self.imgs_gt[folder][i] for i in select_idx]\n                # read imgs_gt to generate low-resolution frames\n                imgs_lq = read_img_seq(\n                    img_paths_lq,\n                    require_mod_crop=True,\n                    scale=self.opt['scale'])\n                imgs_lq = duf_downsample(\n                    imgs_lq, kernel_size=13, scale=self.opt['scale'])\n            else:\n                img_paths_lq = [self.imgs_lq[folder][i] for i in select_idx]\n                imgs_lq = read_img_seq(img_paths_lq)\n            img_gt = read_img_seq([self.imgs_gt[folder][idx]],\n                                  require_mod_crop=True,\n                                  scale=self.opt['scale'])\n            img_gt.squeeze_(0)\n\n        return {\n            'lq': imgs_lq,  # (t, c, h, w)\n            'gt': img_gt,  # (c, h, w)\n            'folder': folder,  # folder name\n            'idx': self.data_info['idx'][index],  # e.g., 0/99\n            'border': border,  # 1 for border, 0 for non-border\n            'lq_path': lq_path  # center frame\n        }"}
{"Repository": "DeepEventMine", "input": "BERT model for classification. className BertForSequenceClassification(BertPreTrainedModel) Method __init__ Method forward Attribute num_labels Attribute bert Attribute dropout Attribute classifier", "label": "class BertForSequenceClassification(BertPreTrainedModel):\n    def __init__(self, config, num_labels):\n        super(BertForSequenceClassification, self).__init__(config)\n        self.num_labels = num_labels\n        self.bert = BertModel(config)\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n        self.classifier = nn.Linear(config.hidden_size, num_labels)\n        self.apply(self.init_bert_weights)\n\n    def forward(self, input_ids, token_type_ids=None, attention_mask=None, labels=None):\n        _, pooled_output = self.bert(input_ids, token_type_ids, attention_mask, output_all_encoded_layers=False)\n        pooled_output = self.dropout(pooled_output)\n        logits = self.classifier(pooled_output)\n\n        if labels is not None:\n            loss_fct = CrossEntropyLoss()\n            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n            return loss\n        else:\n            return logits"}
{"Repository": "evalml", "input": "A collection of basic data checks that is used by AutoML by default. className DefaultDataChecks(DataChecks) Method __init__", "label": "class DefaultDataChecks(DataChecks):\n    _DEFAULT_DATA_CHECK_CLASSES = [\n        NullDataCheck,\n        IDColumnsDataCheck,\n        TargetLeakageDataCheck,\n        InvalidTargetDataCheck,\n        NoVarianceDataCheck,\n    ]\n\n    def __init__(self, problem_type, objective, n_splits=3, problem_configuration=None):\n        default_checks = self._DEFAULT_DATA_CHECK_CLASSES\n        data_check_params = {}\n\n        if is_time_series(problem_type):\n            if problem_configuration is None:\n                raise ValueError(\n                    \"problem_configuration cannot be None for time series problems!\",\n                )\n            if is_classification(problem_type):\n                default_checks = default_checks + [TimeSeriesSplittingDataCheck]\n                data_check_params.update(\n                    {\n                        \"TimeSeriesSplittingDataCheck\": {\n                            \"problem_type\": problem_type,\n                            \"n_splits\": n_splits,\n                        },\n                    },\n                )\n            default_checks = default_checks + [\n                DateTimeFormatDataCheck,\n                TimeSeriesParametersDataCheck,\n            ]\n            data_check_params.update(\n                {\n                    \"DateTimeFormatDataCheck\": {\n                        \"datetime_column\": problem_configuration[\"time_index\"],\n                    },\n                    \"TimeSeriesParametersDataCheck\": {\n                        \"problem_configuration\": problem_configuration,\n                        \"n_splits\": n_splits,\n                    },\n                },\n            )\n\n        if handle_problem_types(problem_type) in [\n            ProblemTypes.REGRESSION,\n            ProblemTypes.TIME_SERIES_REGRESSION,\n        ]:\n            default_checks = default_checks + [TargetDistributionDataCheck]\n            data_check_params.update(\n                {\n                    \"InvalidTargetDataCheck\": {\n                        \"problem_type\": problem_type,\n                        \"objective\": objective,\n                    },\n                },\n            )\n        else:\n            default_checks = default_checks + [ClassImbalanceDataCheck]\n            data_check_params.update(\n                {\n                    \"InvalidTargetDataCheck\": {\n                        \"problem_type\": problem_type,\n                        \"objective\": objective,\n                    },\n                    \"ClassImbalanceDataCheck\": {\"num_cv_folds\": n_splits},\n                },\n            )\n\n        super().__init__(\n            default_checks,\n            data_check_params=data_check_params,\n        )"}
{"Repository": "ddim", "input": "`LSUN <https://www. className LSUN(VisionDataset) Method __init__ Method _verify_classes Method __getitem__ Method __len__ Method extra_repr Attribute classes Attribute dbs Attribute indices Attribute length", "label": "class LSUN(VisionDataset):\n    def __init__(self, root, classes=\"train\", transform=None, target_transform=None):\n        super(LSUN, self).__init__(\n            root, transform=transform, target_transform=target_transform\n        )\n        self.classes = self._verify_classes(classes)\n\n        # for each class, create an LSUNClassDataset\n        self.dbs = []\n        for c in self.classes:\n            self.dbs.append(\n                LSUNClass(root=root + \"/\" + c + \"_lmdb\", transform=transform)\n            )\n\n        self.indices = []\n        count = 0\n        for db in self.dbs:\n            count += len(db)\n            self.indices.append(count)\n\n        self.length = count\n\n    def _verify_classes(self, classes):\n        categories = [\n            \"bedroom\",\n            \"bridge\",\n            \"church_outdoor\",\n            \"classroom\",\n            \"conference_room\",\n            \"dining_room\",\n            \"kitchen\",\n            \"living_room\",\n            \"restaurant\",\n            \"tower\",\n        ]\n        dset_opts = [\"train\", \"val\", \"test\"]\n\n        try:\n            verify_str_arg(classes, \"classes\", dset_opts)\n            if classes == \"test\":\n                classes = [classes]\n            else:\n                classes = [c + \"_\" + classes for c in categories]\n        except ValueError:\n            if not isinstance(classes, Iterable):\n                msg = (\n                    \"Expected type str or Iterable for argument classes, \"\n                    \"but got type {}.\"\n                )\n                raise ValueError(msg.format(type(classes)))\n\n            classes = list(classes)\n            msg_fmtstr = (\n                \"Expected type str for elements in argument classes, \"\n                \"but got type {}.\"\n            )\n            for c in classes:\n                verify_str_arg(c, custom_msg=msg_fmtstr.format(type(c)))\n                c_short = c.split(\"_\")\n                category, dset_opt = \"_\".join(c_short[:-1]), c_short[-1]\n\n                msg_fmtstr = \"Unknown value '{}' for {}. Valid values are {{{}}}.\"\n                msg = msg_fmtstr.format(\n                    category, \"LSUN class\", iterable_to_str(categories)\n                )\n                verify_str_arg(category, valid_values=categories, custom_msg=msg)\n\n                msg = msg_fmtstr.format(dset_opt, \"postfix\", iterable_to_str(dset_opts))\n                verify_str_arg(dset_opt, valid_values=dset_opts, custom_msg=msg)\n\n        return classes\n\n    def __getitem__(self, index):\n        target = 0\n        sub = 0\n        for ind in self.indices:\n            if index < ind:\n                break\n            target += 1\n            sub = ind\n\n        db = self.dbs[target]\n        index = index - sub\n\n        if self.target_transform is not None:\n            target = self.target_transform(target)\n\n        img, _ = db[index]\n        return img, target\n\n    def __len__(self):\n        return self.length\n\n    def extra_repr(self):\n        return \"Classes: {classes}\".format(**self.__dict__)"}
{"Repository": "tfserving-python-predict-client", "input": "PredictionService provides access to machine-learned models loaded by model_servers. className PredictionServiceStub(object) Method __init__ Attribute Predict", "label": "class PredictionServiceStub(object):\n  def __init__(self, channel):\n    self.Predict = channel.unary_unary(\n        '/tensorflow.serving.PredictionService/Predict',\n        request_serializer=predict__pb2.PredictRequest.SerializeToString,\n        response_deserializer=predict__pb2.PredictResponse.FromString,\n        )"}
{"Repository": "rec-attend-public", "input": "Analyze IOU for an entire dataset (not per image) className ForegroundIOUAnalyzer(AnalyzerBase) Method __init__ Method stage Method finalize Attribute inter Attribute union Attribute fname", "label": "class ForegroundIOUAnalyzer(AnalyzerBase):\n    def __init__(self, name='FG IOU ALL', fname=None):\n        self.inter = 0.0\n        self.union = 0.0\n        self.fname = fname\n        super(ForegroundIOUAnalyzer, self).__init__(name)\n        pass\n\n    def stage(self, results):\n        start = time.time()\n        y_out = results['y_out']\n        y_gt = results['y_gt']\n        for a, b in zip(y_out, y_gt):\n            if len(a.shape) == 3:\n                a = a.max(axis=0)\n                b = b.max(axis=0)\n            _inter = (a * b).sum()\n            _union = a.sum() + b.sum() - _inter\n            self.inter += _inter\n            self.union += _union\n            pass\n        end = time.time()\n        elapsed_time = (end - start) * 1000\n        with self.log.verbose_level(2):\n            self.log.info('{} finished in {:.2f}ms'.format(\n                self.name, elapsed_time))\n        pass\n\n    def finalize(self):\n        iou = self.inter / self.union\n        self.log.info('{:17s}{:7.4f}'.format(self.name, iou))\n        pass"}
{"Repository": "Marzban-node", "input": "Loads Xray config json className XRayConfig(dict) Method __init__ Method to_json Method _apply_api Attribute api_port Attribute ssl_cert Attribute ssl_key Attribute peer_ip", "label": "class XRayConfig(dict):\n    def __init__(self, config: str, peer_ip: str):\n        config = json.loads(config)\n\n        self.api_port = XRAY_API_PORT\n        self.ssl_cert = SSL_CERT_FILE\n        self.ssl_key = SSL_KEY_FILE\n        self.peer_ip = peer_ip\n\n        super().__init__(config)\n        self._apply_api()\n\n    def to_json(self, **json_kwargs):\n        return json.dumps(self, **json_kwargs)\n\n    def _apply_api(self):\n        for inbound in self.get('inbounds', []):\n            if inbound.get('protocol') == 'dokodemo-door':\n                self['inbounds'].remove(inbound)\n\n        for rule in self.get('routing', {}).get(\"rules\", []):\n            api_tag = self.get('api', {}).get('tag')\n            if api_tag and rule.get('outboundTag') == api_tag:\n                self['routing']['rules'].remove(rule)\n\n        self[\"api\"] = {\n            \"services\": [\n                \"HandlerService\",\n                \"StatsService\",\n                \"LoggerService\"\n            ],\n            \"tag\": \"API\"\n        }\n        self[\"stats\"] = {}\n        self[\"policy\"] = {\n            \"levels\": {\n                \"0\": {\n                    \"statsUserUplink\": True,\n                    \"statsUserDownlink\": True\n                }\n            },\n            \"system\": {\n                \"statsInboundDownlink\": False,\n                \"statsInboundUplink\": False,\n                \"statsOutboundDownlink\": True,\n                \"statsOutboundUplink\": True\n            }\n        }\n        inbound = {\n            \"listen\": \"0.0.0.0\",\n            \"port\": self.api_port,\n            \"protocol\": \"dokodemo-door\",\n            \"settings\": {\n                \"address\": \"127.0.0.1\"\n            },\n            \"streamSettings\": {\n                \"security\": \"tls\",\n                \"tlsSettings\": {\n                    \"certificates\": [\n                        {\n                            \"certificateFile\": self.ssl_cert,\n                            \"keyFile\": self.ssl_key\n                        }\n                    ]\n                }\n            },\n            \"tag\": \"API_INBOUND\"\n        }\n        try:\n            self[\"inbounds\"].insert(0, inbound)\n        except KeyError:\n            self[\"inbounds\"] = []\n            self[\"inbounds\"].insert(0, inbound)\n\n        rule = {\n            \"inboundTag\": [\n                \"API_INBOUND\"\n            ],\n            \"source\": [\n                \"127.0.0.1\",\n                self.peer_ip\n            ],\n            \"outboundTag\": \"API\",\n            \"type\": \"field\"\n        }\n        try:\n            self[\"routing\"][\"rules\"].insert(0, rule)\n        except KeyError:\n            self[\"routing\"] = {\"rules\": []}\n            self[\"routing\"][\"rules\"].insert(0, rule)"}
{"Repository": "pyMCR", "input": "Abstract class for constraints className Constraint(_ABC) Method __init__ Method transform Attribute copy", "label": "class Constraint(_ABC):\n    def __init__(self, copy=True):\n        self.copy = copy\n\n    @_abstractmethod\n    def transform(self, A):"}
{"Repository": "UniSumm", "input": "This wrapper class is a helper class to correctly load pretrained checkpoints when the causal language model is used in combination with the :class:`~transformers. className BartDecoderWrapper(BartPretrainedModel) Method __init__ Method forward Attribute decoder", "label": "class BartDecoderWrapper(BartPretrainedModel):\n    def __init__(self, config):\n        super().__init__(config)\n        self.decoder = BartDecoder(config)\n\n    def forward(self, *args, **kwargs):\n        return self.decoder(*args, **kwargs)"}
{"Repository": "jasmin", "input": "Config handler for 'dlr' section className DLRLookupConfig(ConfigFile) Method __init__ Attribute pid Attribute dlr_lookup_retry_delay Attribute dlr_lookup_max_retries Attribute smpp_receipt_on_success_submit_sm_resp Attribute log_level Attribute log_file Attribute log_rotate Attribute log_format Attribute log_date_format Attribute log_privacy", "label": "class DLRLookupConfig(ConfigFile):\n    def __init__(self, config_file=None):\n        ConfigFile.__init__(self, config_file)\n\n        self.pid = self._get('dlr', 'pid', 'main')\n\n        self.dlr_lookup_retry_delay = self._getint('dlr', 'dlr_lookup_retry_delay', 10)\n        self.dlr_lookup_max_retries = self._getint('dlr', 'dlr_lookup_max_retries', 2)\n\n        self.smpp_receipt_on_success_submit_sm_resp = self._getbool('dlr', 'smpp_receipt_on_success_submit_sm_resp',\n                                                                    False)\n\n        self.log_level = logging.getLevelName(self._get('dlr', 'log_level', 'INFO'))\n        self.log_file = self._get('dlr', 'log_file', '%s/messages.log' % LOG_PATH)\n        self.log_rotate = self._get('dlr', 'log_rotate', 'midnight')\n        self.log_format = self._get('dlr', 'log_format', DEFAULT_LOGFORMAT)\n        self.log_date_format = self._get('dlr', 'log_date_format', '%Y-%m-%d %H:%M:%S')\n        self.log_privacy = self._getbool('dlr', 'log_privacy', False)"}
{"Repository": "changelog-ci", "input": "Generates, commits and/or comments changelog for other events such as `workflow_dispatch` className ChangelogCICustomEvent(ChangelogCIBase) Method _commit_branch_name Method _comment_changelog Method _create_new_branch Method _commit_changelog Method _get_release_version", "label": "class ChangelogCICustomEvent(ChangelogCIBase):\n    @property\n    def _commit_branch_name(self) -> str:\n        return self._create_new_branch()\n\n    def _comment_changelog(self, changelog_string: str) -> None:\n        gha_utils.error(\n            \"`comment_changelog` can only be used \"\n            \"if Changelog CI is triggered on a pull request event. \"\n            \"Please Check the Documentation for more details.\"\n        )\n\n    def _create_new_branch(self) -> str:\n        # Use timestamp to ensure uniqueness of the new branch\n        new_branch = f\"changelog-ci-{self.release_version}-{int(time.time())}\"\n        create_new_git_branch(self.action_env.base_branch, new_branch)\n        return new_branch\n\n    def _commit_changelog(self, commit_branch_name: str) -> None:\n        super()._commit_changelog(commit_branch_name)\n\n        if self.config.changelog_file_type == RESTRUCTUREDTEXT_FILE:\n            markdown_changelog_string = self.builder.parse_changelog(MARKDOWN_FILE)\n        else:\n            markdown_changelog_string = self.builder.changelog_string\n\n        with gha_utils.group(\"Create Pull Request\"):\n            self._create_pull_request(commit_branch_name, markdown_changelog_string)\n\n    def _get_release_version(self) -> str:\n        release_version = self.config.release_version\n\n        if not release_version:\n            gha_utils.error(\n                \"`release_version` input must be provided to generate Changelog. \"\n                \"Please Check the Documentation for more details. \"\n                \"Aborting Changelog Generation\"\n            )\n            raise SystemExit(1)\n\n        return release_version"}
{"Repository": "Self-PU", "input": "Computes and stores the average and current value className AverageMeter(object) Method __init__ Method reset Method update", "label": "class AverageMeter(object):\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        #print(val, n)\n        if self.count == 0:\n            self.avg = 0\n        else:\n            self.avg = self.sum / self.count"}
{"Repository": "FD-GAN", "input": "Computes and stores the average and current value className AverageMeter(object) Method __init__ Method reset Method update", "label": "class AverageMeter(object):\n  def __init__(self):\n      self.reset()\n\n  def reset(self):\n    self.val = 0\n    self.avg = 0\n    self.sum = 0\n    self.count = 0\n\n  def update(self, val, n=1):\n    self.val = val\n    self.sum += val * n\n    self.count += n\n    self.avg = self.sum / self.count"}
{"Repository": "simple_ocr", "input": "Crops the given PIL Image at the center. className CenterCrop(object) Method __init__ Method __call__ Attribute size", "label": "class CenterCrop(object):\n    def __init__(self, size):\n        self.size = size\n\n    def __call__(self, img):\n        return center_crop(img, self.size)"}
{"Repository": "DPC", "input": "Randomly change the brightness, contrast and saturation of an image. className ColorJitter(object) Method __init__ Method _check_input Method get_params Method __call__ Method __repr__ Attribute brightness Attribute contrast Attribute saturation Attribute hue Attribute consistent Attribute threshold", "label": "class ColorJitter(object):\n    def __init__(self, brightness=0, contrast=0, saturation=0, hue=0, consistent=False, p=1.0):\n        self.brightness = self._check_input(brightness, 'brightness')\n        self.contrast = self._check_input(contrast, 'contrast')\n        self.saturation = self._check_input(saturation, 'saturation')\n        self.hue = self._check_input(hue, 'hue', center=0, bound=(-0.5, 0.5),\n                                     clip_first_on_zero=False)\n        self.consistent = consistent\n        self.threshold = p \n\n    def _check_input(self, value, name, center=1, bound=(0, float('inf')), clip_first_on_zero=True):\n        if isinstance(value, numbers.Number):\n            if value < 0:\n                raise ValueError(\"If {} is a single number, it must be non negative.\".format(name))\n            value = [center - value, center + value]\n            if clip_first_on_zero:\n                value[0] = max(value[0], 0)\n        elif isinstance(value, (tuple, list)) and len(value) == 2:\n            if not bound[0] <= value[0] <= value[1] <= bound[1]:\n                raise ValueError(\"{} values should be between {}\".format(name, bound))\n        else:\n            raise TypeError(\"{} should be a single number or a list/tuple with lenght 2.\".format(name))\n\n        # if value is 0 or (1., 1.) for brightness/contrast/saturation\n        # or (0., 0.) for hue, do nothing\n        if value[0] == value[1] == center:\n            value = None\n        return value\n\n    @staticmethod\n    def get_params(brightness, contrast, saturation, hue):\n        transforms = []\n\n        if brightness is not None:\n            brightness_factor = random.uniform(brightness[0], brightness[1])\n            transforms.append(torchvision.transforms.Lambda(lambda img: F.adjust_brightness(img, brightness_factor)))\n\n        if contrast is not None:\n            contrast_factor = random.uniform(contrast[0], contrast[1])\n            transforms.append(torchvision.transforms.Lambda(lambda img: F.adjust_contrast(img, contrast_factor)))\n\n        if saturation is not None:\n            saturation_factor = random.uniform(saturation[0], saturation[1])\n            transforms.append(torchvision.transforms.Lambda(lambda img: F.adjust_saturation(img, saturation_factor)))\n\n        if hue is not None:\n            hue_factor = random.uniform(hue[0], hue[1])\n            transforms.append(torchvision.transforms.Lambda(lambda img: F.adjust_hue(img, hue_factor)))\n\n        random.shuffle(transforms)\n        transform = torchvision.transforms.Compose(transforms)\n\n        return transform\n\n    def __call__(self, imgmap):\n        if random.random() < self.threshold: # do ColorJitter\n            if self.consistent:\n                transform = self.get_params(self.brightness, self.contrast,\n                                            self.saturation, self.hue)\n                return [transform(i) for i in imgmap]\n            else:\n                result = []\n                for img in imgmap:\n                    transform = self.get_params(self.brightness, self.contrast,\n                                                self.saturation, self.hue)\n                    result.append(transform(img))\n                return result\n        else: # don't do ColorJitter, do nothing\n            return imgmap \n\n    def __repr__(self):\n        format_string = self.__class__.__name__ + '('\n        format_string += 'brightness={0}'.format(self.brightness)\n        format_string += ', contrast={0}'.format(self.contrast)\n        format_string += ', saturation={0}'.format(self.saturation)\n        format_string += ', hue={0})'.format(self.hue)\n        return format_string"}
{"Repository": "json2csv", "input": "Process a JSON object to a CSV file className Json2Csv(object) Method __init__ Method load Method process_each Method process_row Method make_strings Method make_string Method write_csv Attribute rows Attribute key_map Attribute collection", "label": "class Json2Csv(object):\n    collection = None\n\n    # Better for single-nested dictionaries\n    SEP_CHAR = ', '\n    KEY_VAL_CHAR = ': '\n    DICT_SEP_CHAR = '\\r'\n    DICT_OPEN = ''\n    DICT_CLOSE = ''\n\n    # Better for deep-nested dictionaries\n    # SEP_CHAR = ', '\n    # KEY_VAL_CHAR = ': '\n    # DICT_SEP_CHAR = '; '\n    # DICT_OPEN = '{ '\n    # DICT_CLOSE = '} '\n\n    def __init__(self, outline):\n        self.rows = []\n\n        if not isinstance(outline, dict):\n            raise ValueError('You must pass in an outline for JSON2CSV to follow')\n        elif 'map' not in outline or len(outline['map']) < 1:\n            raise ValueError('You must specify at least one value for \"map\"')\n\n        key_map = OrderedDict()\n        for header, key in outline['map']:\n            splits = key.split('.')\n            splits = [int(s) if s.isdigit() else s for s in splits]\n            key_map[header] = splits\n\n        self.key_map = key_map\n        if 'collection' in outline:\n            self.collection = outline['collection']\n\n    def load(self, json_file):\n        self.process_each(json.load(json_file))\n\n    def process_each(self, data):\n        if self.collection and self.collection in data:\n            data = data[self.collection]\n\n        for d in data:\n            logging.info(d)\n            self.rows.append(self.process_row(d))\n\n    def process_row(self, item):\n        row = {}\n\n        for header, keys in self.key_map.items():\n            try:\n                row[header] = reduce(operator.getitem, keys, item)\n            except (KeyError, IndexError, TypeError):\n                row[header] = None\n\n        return row\n\n    def make_strings(self):\n        str_rows = []\n        for row in self.rows:\n            str_rows.append({k: self.make_string(val)\n                             for k, val in row.items()})\n        return str_rows\n\n    def make_string(self, item):\n        if isinstance(item, list) or isinstance(item, set) or isinstance(item, tuple):\n            return self.SEP_CHAR.join([self.make_string(subitem) for subitem in item])\n        elif isinstance(item, dict):\n            return self.DICT_OPEN + self.DICT_SEP_CHAR.join([self.KEY_VAL_CHAR.join([k, self.make_string(val)]) for k, val in item.items()]) + self.DICT_CLOSE\n        else:\n            return unicode(item)\n\n    def write_csv(self, filename='output.csv', make_strings=False):\n        if (len(self.rows) <= 0):\n            raise AttributeError('No rows were loaded')\n        if make_strings:\n            out = self.make_strings()\n        else:\n            out = self.rows\n        with open(filename, 'wb+') as f:\n            writer = csv.DictWriter(f, self.key_map.keys())\n            writer.writeheader()\n            writer.writerows(out)"}
{"Repository": "asyncmongo", "input": "singleton to keep track of named connection pools className ConnectionPools(object) Method get_connection_pool Method close_idle_connections", "label": "class ConnectionPools(object):\n    @classmethod\n    def get_connection_pool(self, pool_id, *args, **kwargs):\n        assert isinstance(pool_id, (str, unicode))\n        if not hasattr(self, '_pools'):\n            self._pools = {}\n        if pool_id not in self._pools:\n            self._pools[pool_id] = ConnectionPool(*args, **kwargs)\n        # logging.debug(\"%s: _connections = %d\", pool_id, self._pools[pool_id]._connections)\n        return self._pools[pool_id]\n    \n    @classmethod\n    def close_idle_connections(self, pool_id=None):\n        if not hasattr(self, '_pools'):\n            return\n\n        if pool_id:\n            if pool_id not in self._pools:\n                raise ProgrammingError(\"pool %r does not exist\" % pool_id)\n            else:\n                pool = self._pools[pool_id]\n                pool.close()\n        else:\n            for pool_id, pool in self._pools.items():\n                pool.close()"}
{"Repository": "ViTAE-VSA", "input": "Sampler that repeats forever. className _RepeatSampler(object) Method __init__ Method __iter__ Attribute sampler", "label": "class _RepeatSampler(object):\n    def __init__(self, sampler):\n        self.sampler = sampler\n\n    def __iter__(self):\n        while True:\n            yield from iter(self.sampler)"}
{"Repository": "katna", "input": "Exception raised when mediapipe autoflip build file not found. className MediapipeAutoflipBuildNotFound(KatnaExceptions) Method __init__ Attribute message", "label": "class MediapipeAutoflipBuildNotFound(KatnaExceptions):\n    _DEFAULT_AUTOFLIP_BUILD_LOCATION = \"/path/to/mediapipe/repo/bazel-bin/examples/desktop/autoflip\"\n    _ERROR = \"Mediapipe autflip build dir not found. Autoflip build dir can be located here : %s\" % _DEFAULT_AUTOFLIP_BUILD_LOCATION\n\n    def __init__(self):\n        self.message = self._ERROR\n        super().__init__(self.message)"}
{"Repository": "guietta", "input": "Build and upload conda package. className CondaCommand(Command) Method using_conda Method status Method initialize_options Method finalize_options Method run", "label": "class CondaCommand(Command):\n    description = 'Build and publish the package on conda.'\n    user_options = []\n    \n    @staticmethod    \n    def using_conda():\n        if sys.platform == \"win32\":\n            env_variable = \"%CONDA_PREFIX%\"\n            \n        elif sys.platform in [\"linux\", \"darwin\"]:\n            env_variable = \"$CONDA_PREFIX\"\n            \n        command = \"echo \" + env_variable\n        out = subprocess.check_output(command, shell=True).decode('utf-8')\n        \n        if env_variable in out:\n            return False\n        else:\n            return True\n\n    @staticmethod\n    def status(s):\n        print('\\033[1m{0}\\033[0m'.format(s))\n\n    def initialize_options(self):\n        pass\n\n    def finalize_options(self):\n        pass\n\n    def run(self):\n        if not self.using_conda():\n            self.status('Checking conda environment status')\n            print(\"Not inside conda environment.\")\n            sys.exit()\n        \n        try:\n            self.status('Removing previous builds')\n            rmtree(os.path.join(here, 'dist_conda'))\n        except OSError:\n            pass\n\n        self.status('Building conda distribution')\n        os.system('conda build . --output-folder dist_conda/ -c conda-forge')\n\n        self.status('Uploading the package to conda')\n        os.system('anaconda upload dist_conda/noarch/guietta-{}-py_0.tar.bz2'.format(about['__version__']))\n\n        sys.exit()"}
{"Repository": "attention-sampling", "input": "MultiResolutionBatch defines an interface to access data with multiple resolutions in TF. className MultiResolutionBatch(object) Method targets Method data Method inputs", "label": "class MultiResolutionBatch(object):\n    def targets(self):\n        raise NotImplementedError()\n\n    def patches(self, samples, offsets, sample_space, previous_patch_size,\n                patch_size, fromlevel, tolevel):\n        raise NotImplementedError()\n\n    def data(self, level):\n        raise NotImplementedError()\n\n    def inputs(self):\n        raise NotImplementedError()"}
{"Repository": "ControlLoRA", "input": "A dataset to prepare the instance and class images with the prompts for fine-tuning the model. className DreamBoothDataset(Dataset) Method __len__ Method __getitem__", "label": "class DreamBoothDataset(Dataset):\n    def __init__(\n        self,\n        instance_data_root,\n        instance_prompt,\n        tokenizer,\n        class_data_root=None,\n        class_prompt=None,\n        size=512,\n        center_crop=False,\n    ):\n        self.size = size\n        self.center_crop = center_crop\n        self.tokenizer = tokenizer\n\n        self.instance_data_root = Path(instance_data_root)\n        if not self.instance_data_root.exists():\n            raise ValueError(\"Instance images root doesn't exists.\")\n\n        self.instance_images_path = list(Path(instance_data_root).iterdir())\n        self.num_instance_images = len(self.instance_images_path)\n        self.instance_prompt = instance_prompt\n        self._length = self.num_instance_images\n\n        if class_data_root is not None:\n            self.class_data_root = Path(class_data_root)\n            self.class_data_root.mkdir(parents=True, exist_ok=True)\n            self.class_images_path = list(self.class_data_root.iterdir())\n            self.num_class_images = len(self.class_images_path)\n            self._length = max(self.num_class_images, self.num_instance_images)\n            self.class_prompt = class_prompt\n        else:\n            self.class_data_root = None\n\n        self.image_transforms = transforms.Compose(\n            [\n                transforms.Resize(size, interpolation=transforms.InterpolationMode.BILINEAR),\n                transforms.CenterCrop(size) if center_crop else transforms.RandomCrop(size),\n                transforms.ToTensor(),\n                transforms.Normalize([0.5], [0.5]),\n            ]\n        )\n\n    def __len__(self):\n        return self._length\n\n    def __getitem__(self, index):\n        example = {}\n        instance_image = Image.open(self.instance_images_path[index % self.num_instance_images])\n        if not instance_image.mode == \"RGB\":\n            instance_image = instance_image.convert(\"RGB\")\n        example[\"instance_images\"] = self.image_transforms(instance_image)\n        example[\"instance_prompt_ids\"] = self.tokenizer(\n            self.instance_prompt,\n            truncation=True,\n            padding=\"max_length\",\n            max_length=self.tokenizer.model_max_length,\n            return_tensors=\"pt\",\n        ).input_ids\n\n        if self.class_data_root:\n            class_image = Image.open(self.class_images_path[index % self.num_class_images])\n            if not class_image.mode == \"RGB\":\n                class_image = class_image.convert(\"RGB\")\n            example[\"class_images\"] = self.image_transforms(class_image)\n            example[\"class_prompt_ids\"] = self.tokenizer(\n                self.class_prompt,\n                truncation=True,\n                padding=\"max_length\",\n                max_length=self.tokenizer.model_max_length,\n                return_tensors=\"pt\",\n            ).input_ids\n\n        return example"}
{"Repository": "mlforecast", "input": "Expanding statistic className _ExpandingBase(_BaseLagTransform) Method __init__", "label": "class _ExpandingBase(_BaseLagTransform):\n    def __init__(self): ..."}
{"Repository": "DangerousDrivingDetector", "input": "At test time, Detect is the final layer of SSD. className Detect(Function) Method __init__ Method forward Attribute num_classes Attribute background_label Attribute top_k Attribute nms_thresh Attribute conf_thresh Attribute variance", "label": "class Detect(Function):\n    def __init__(self, num_classes, bkg_label, top_k, conf_thresh, nms_thresh):\n        self.num_classes = num_classes\n        self.background_label = bkg_label\n        self.top_k = top_k\n        # Parameters used in nms.\n        self.nms_thresh = nms_thresh\n        if nms_thresh <= 0:\n            raise ValueError('nms_threshold must be non negative.')\n        self.conf_thresh = conf_thresh\n        self.variance = (0.1,0.2)\n\n    def forward(self, loc_data, conf_data, prior_data):\n        num = loc_data.size(0)  # batch size\n        num_priors = prior_data.size(0)\n        output = torch.zeros(num, self.num_classes, self.top_k, 5)\n        conf_preds = conf_data.view(num, num_priors,\n                                    self.num_classes).transpose(2, 1)\n\n        # Decode predictions into bboxes.\n        for i in range(num):\n            decoded_boxes = decode(loc_data[i], prior_data, self.variance)\n            # For each class, perform nms\n            conf_scores = conf_preds[i].clone()\n            for cl in range(1, self.num_classes):\n                c_mask = conf_scores[cl].gt(self.conf_thresh)\n                scores = conf_scores[cl][c_mask]\n                if scores.dim() == 0:\n                    continue\n                l_mask = c_mask.unsqueeze(1).expand_as(decoded_boxes)\n                boxes = decoded_boxes[l_mask].view(-1, 4)\n                # idx of highest scoring and non-overlapping boxes per class\n                ids, count = nms(boxes, scores, self.nms_thresh, self.top_k)\n\n                if count==0:\n                    continue\n                output[i, cl, :count] = \\\n                    torch.cat((scores[ids[:count]].unsqueeze(1),\n                               boxes[ids[:count]]), 1)\n        flt = output.contiguous().view(num, -1, 5)\n        _, idx = flt[:, :, 0].sort(1, descending=True)\n        _, rank = idx.sort(1)\n        flt[(rank < self.top_k).unsqueeze(-1).expand_as(flt)].fill_(0)\n        return output"}
{"Repository": "pyftpsync", "input": "A RuntimeError that will suppress a stacktrace in CLI mode if verbosity <= x. className CliSilentRuntimeError(RuntimeError) Method __init__", "label": "class CliSilentRuntimeError(RuntimeError):\n    def __init__(self, msg, *, min_verbosity, exit_code=1) -> None:\n        self.min_verbosity = min_verbosity\n        self.exit_code = exit_code\n        super().__init__(msg)"}
{"Repository": "python-lambda", "input": "Support setup.py publish. className UploadCommand(Command) Method status Method initialize_options Method finalize_options Method run", "label": "class UploadCommand(Command):\n    description = \"Build and publish the package.\"\n    user_options = []\n\n    @staticmethod\n    def status(s):\n        print(f\"\\033[1m{s}\\033[0m\")\n\n    def initialize_options(self):\n        pass\n\n    def finalize_options(self):\n        pass\n\n    def run(self):\n        try:\n            self.status(\"Removing previous builds ...\")\n            rmtree(os.path.join(THIS_DIR, \"dist\"))\n        except Exception:\n            pass\n        self.status(\"Building Source distribution ...\")\n        os.system(f\"{sys.executable} setup.py sdist\")\n        self.status(\"Uploading the package to PyPI via Twine ...\")\n        os.system(\"twine upload dist/*\")\n        sys.exit()"}
{"Repository": "kipoiseq", "input": "Reads a tsv file in the following format: ``` chr  start  stop  task1  task2 . className BedDataset(object) Method __getitem__ Method __len__ Method get_targets", "label": "class BedDataset(object):\n    # bed types accorging to\n    # https://www.ensembl.org/info/website/upload/bed.html\n    bed_types = [str,  # chrom\n                 int,  # chromStart\n                 int,  # chromEnd\n                 str,  # name\n                 str,  # score, as str to prevent issues, also its useless\n                 str,  # strand\n                 int,  # thickStart\n                 int,  # thickEnd\n                 str,  # itemRbg\n                 int,  # blockCount\n                 int,  # blockSizes\n                 int]  # blockStarts\n\n    def __init__(self, tsv_file,\n                 label_dtype=None,\n                 bed_columns=3,\n                 num_chr=False,\n                 ambiguous_mask=None,\n                 incl_chromosomes=None,\n                 excl_chromosomes=None,\n                 ignore_targets=False):\n        # TODO - `chrom` column: use pd.Categorical for memory efficiency\n        self.tsv_file = tsv_file\n        self.bed_columns = bed_columns\n        self.num_chr = num_chr\n        self.label_dtype = label_dtype\n        self.ambiguous_mask = ambiguous_mask\n        self.incl_chromosomes = incl_chromosomes\n        self.excl_chromosomes = excl_chromosomes\n        self.ignore_targets = ignore_targets\n\n        df_peek = pd.read_table(self.tsv_file,\n                                header=None,\n                                nrows=1,\n                                sep='\\t')\n        found_columns = df_peek.shape[1]\n        self.n_tasks = found_columns - self.bed_columns\n        if self.n_tasks < 0:\n            raise ValueError(\"BedDataset requires at least {} valid bed columns. Found only {} columns\".\n                             format(self.bed_columns, found_columns))\n\n        self.df = pd.read_table(self.tsv_file,\n                                header=None,\n                                dtype={i: d\n                                       for i, d in enumerate(self.bed_types[:self.bed_columns] +\n                                                             [self.label_dtype] * self.n_tasks)},\n                                sep='\\t')\n        if self.num_chr and self.df.iloc[0][0].startswith(\"chr\"):\n            self.df[0] = self.df[0].str.replace(\"^chr\", \"\")\n        if not self.num_chr and not self.df.iloc[0][0].startswith(\"chr\"):\n            self.df[0] = \"chr\" + self.df[0]\n\n        if ambiguous_mask is not None:\n            # exclude regions where only ambigous labels are present\n            self.df = self.df[~np.all(\n                self.df.iloc[:, self.bed_columns:] == ambiguous_mask, axis=1)]\n\n            # omit data outside chromosomes\n        if incl_chromosomes is not None:\n            self.df = self.df[self.df[0].isin(incl_chromosomes)]\n        if excl_chromosomes is not None:\n            self.df = self.df[~self.df[0].isin(excl_chromosomes)]\n\n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n\n        # TODO: use kipoiseq.dataclasses.interval instead of pybedtools\n        import pybedtools\n        interval = pybedtools.create_interval_from_list(\n                [to_scalar(x) for x in row.iloc[:self.bed_columns]])\n\n        if self.ignore_targets or self.n_tasks == 0:\n            labels = {}\n        else:\n            labels = row.iloc[self.bed_columns:].values.astype(\n                self.label_dtype)\n        return interval, labels\n\n    def __len__(self):\n        return len(self.df)\n\n    def get_targets(self):\n        return self.df.iloc[:, self.bed_columns:].values.astype(self.label_dtype)"}
{"Repository": "array_tools", "input": "Raised when all reference objects have been deleted or renamed className ReferenceMissing(Exception) Method __init__", "label": "class ReferenceMissing(Exception):\n    def __init__(self, message):\n        cancel_at(message)"}
{"Repository": "pycnic", "input": "Capture any stdout and stderr from importing the files. className nostdout(object) Method __enter__ Method __exit__", "label": "class nostdout(object):\n    def __enter__(self):\n        self.old_stdout = sys.stdout\n        self.old_stderr = sys.stderr\n        sys.stdout = DummyFile()\n        sys.stderr = DummyFile()\n        return self\n\n    def __exit__(self, exc_type, exc_val, traceback):\n        sys.stdout = self.old_stdout\n        sys.stderr = self.old_stderr"}
{"Repository": "darts", "input": "Parent class for all LikelihoodScorer className NLLScorer(NonFittableAnomalyScorer) Method __init__ Method _window_adjustment_series Method is_probabilistic Method _score_core_nllikelihood", "label": "class NLLScorer(NonFittableAnomalyScorer):\n    def __init__(self, window) -> None:\n        super().__init__(univariate_scorer=False, window=window)\n\n    def _score_core_from_prediction(\n        self,\n        actual_series: TimeSeries,\n        pred_series: TimeSeries,\n    ) -> TimeSeries:\n        actual_series = self._assert_deterministic(actual_series, \"actual_series\")\n        self._assert_stochastic(pred_series, \"pred_series\")\n\n        np_actual_series = actual_series.all_values(copy=False)\n        np_pred_series = pred_series.all_values(copy=False)\n\n        np_anomaly_scores = []\n        for component_idx in range(pred_series.width):\n            np_anomaly_scores.append(\n                self._score_core_nllikelihood(\n                    # shape actual: (time_steps, )\n                    # shape pred: (time_steps, samples)\n                    np_actual_series[:, component_idx].squeeze(-1),\n                    np_pred_series[:, component_idx],\n                )\n            )\n\n        anomaly_scores = TimeSeries.from_times_and_values(\n            pred_series.time_index, list(zip(*np_anomaly_scores))\n        )\n\n        def _window_adjustment_series(series: TimeSeries) -> TimeSeries:\n            if self.window == 1:\n                # the process results in replacing every value by itself -> return directly the series\n                return series\n            else:\n                return series.window_transform(\n                    transforms={\n                        \"window\": self.window,\n                        \"function\": \"mean\",\n                        \"mode\": \"rolling\",\n                        \"min_periods\": self.window,\n                    },\n                    treat_na=\"dropna\",\n                )\n\n        return _window_adjustment_series(anomaly_scores)\n\n    @property\n    def is_probabilistic(self) -> bool:\n        return True\n\n    @abstractmethod\n    def _score_core_nllikelihood(self, input_1: Any, input_2: Any) -> Any:\n        pass"}
{"Repository": "sngan_projection", "input": "Linear layer with Spectral Normalization. className SNLinear(Linear) Method W_bar Method _initialize_params Method __call__", "label": "class SNLinear(Linear):\n    def __init__(self, in_size, out_size, use_gamma=False, nobias=False,\n                 initialW=None, initial_bias=None, Ip=1, factor=None):\n        self.Ip = Ip\n        self.use_gamma = use_gamma\n        self.factor = factor\n        super(SNLinear, self).__init__(\n            in_size, out_size, nobias, initialW, initial_bias\n        )\n        self.u = np.random.normal(size=(1, out_size)).astype(dtype=\"f\")\n        self.register_persistent('u')\n\n    @property\n    def W_bar(self):\n        sigma, _u, _ = max_singular_value(self.W, self.u, self.Ip)\n        if self.factor:\n            sigma = sigma / self.factor\n        sigma = broadcast_to(sigma.reshape((1, 1)), self.W.shape)\n        self.u[:] = _u\n        if hasattr(self, 'gamma'):\n            return broadcast_to(self.gamma, self.W.shape) * self.W / sigma\n        else:\n            return self.W / sigma\n\n    def _initialize_params(self, in_size):\n        super(SNLinear, self)._initialize_params(in_size)\n        if self.use_gamma:\n            _, s, _ = np.linalg.svd(self.W.data)\n            with self.init_scope():\n                self.gamma = chainer.Parameter(s[0], (1, 1))\n\n    def __call__(self, x):\n        if self.W.data is None:\n            self._initialize_params(x.size // x.shape[0])\n        return linear.linear(x, self.W_bar, self.b)"}
{"Repository": "bitnodes", "input": "MaxMind databases. className GeoIp(object) Method __init__ Method city Method country Method asn Attribute geoip_city Attribute geoip_country Attribute geoip_asn", "label": "class GeoIp(object):\n    def __init__(self):\n        # Retry on InvalidDatabaseError due to geoip/update.sh updating\n        # *.mmdb that may cause this exception temporarily.\n        for i in range(10):\n            try:\n                self.geoip_city = Reader('geoip/GeoLite2-City.mmdb')\n                self.geoip_country = Reader('geoip/GeoLite2-Country.mmdb')\n                self.geoip_asn = Reader('geoip/GeoLite2-ASN.mmdb')\n            except (InvalidDatabaseError, IOError) as err:\n                logging.warning(err)\n                time.sleep(0.1)\n                continue\n            else:\n                break\n\n    def city(self, address):\n        return self.geoip_city.city(address)\n\n    def country(self, address):\n        return self.geoip_country.country(address)\n\n    def asn(self, address):\n        return self.geoip_asn.asn(address)"}
{"Repository": "YOLOX_OBB", "input": "Resolve certain links in markdown files to github source. className GithubURLDomain(Domain) Method resolve_any_xref", "label": "class GithubURLDomain(Domain):\n    name = \"githuburl\"\n    ROOT = \"https://github.com/Megvii-BaseDetection/YOLOX\"\n    # LINKED_DOC = [\"tutorials/install\", \"tutorials/getting_started\"]\n    LINKED_DOC = [\"tutorials/install\",]\n\n    def resolve_any_xref(self, env, fromdocname, builder, target, node, contnode):\n        github_url = None\n        if not target.endswith(\"html\") and target.startswith(\"../../\"):\n            url = target.replace(\"../\", \"\")\n            github_url = url\n        if fromdocname in self.LINKED_DOC:\n            # unresolved links in these docs are all github links\n            github_url = target\n\n        if github_url is not None:\n            if github_url.endswith(\"MODEL_ZOO\") or github_url.endswith(\"README\"):\n                # bug of recommonmark.\n                # https://github.com/readthedocs/recommonmark/blob/ddd56e7717e9745f11300059e4268e204138a6b1/recommonmark/parser.py#L152-L155\n                github_url += \".md\"\n            print(\"Ref {} resolved to github:{}\".format(target, github_url))\n            contnode[\"refuri\"] = self.ROOT + github_url\n            return [(\"githuburl:any\", contnode)]\n        else:\n            return []"}
{"Repository": "adidnsdump", "input": "DNS_RPC_RECORD_TS [MS-DNSP] section 2. className DNS_RPC_RECORD_TS(Structure) Method toDatetime", "label": "class DNS_RPC_RECORD_TS(Structure):\n    structure = (\n        ('entombedTime', '<Q'),\n    )\n    def toDatetime(self):\n        microseconds = int(self['entombedTime'] / 10)\n        try:\n            return datetime.datetime(1601,1,1) + datetime.timedelta(microseconds=microseconds)\n        except OverflowError:\n            return None"}
{"Repository": "PyMultiNest", "input": "This class can be used to plot marginal and conditional likelihoods. className PlotMarginal(object) Method __init__ Method plot_marginal Method plot_conditional Attribute analyser", "label": "class PlotMarginal(object):\n\tdef __init__(self, analyser):\n\t\tself.analyser = analyser\n\n\tdef plot_marginal(self, dim1, **kwargs):\n\t\tposterior = self.analyser.get_data()\n\t\t\n\t\tb = list(zip(posterior[:,0], posterior[:,2+dim1]))\n\t\tb.sort(key=lambda x: x[1])\n\t\tb = numpy.array(b)\n\t\tb[:,0] = b[:,0].cumsum()\n\t\tprobdiff = (b[1:,0] + b[:-1,0]) / 2.\n\t\tbincenter = (b[1:,1] + b[:-1,1]) / 2.\n\t\treturn plt.plot(bincenter, probdiff, **kwargs)\n\t\n\tdef plot_conditional(self, dim1, dim2, grid_points = 40, **kwargs):\n\t\tvalues = self.analyser.get_equal_weighted_posterior()\n\t\t\n\t\treturn plt.hexbin(x=values[:,dim1], y=values[:,dim2], \n\t\t\tgridsize=grid_points, reduce_C_function=numpy.nansum,\n\t\t\t**kwargs)"}
{"Repository": "mitogen", "input": ":class:`Policy` for machines where the only control method available is fixed CPU placement. className FixedPolicy(Policy) Method __init__ Method _set_affinity Method _balance Method _set_cpu Method _clear Method assign_controller Method assign_muxprocess Method assign_worker Method assign_subprocess Attribute cpu_count Attribute mem Attribute state Attribute _reserve_mux Attribute _reserve_controller Attribute _reserve_mask Attribute _reserve_shift Attribute _reserve_mux Attribute _reserve_controller Attribute _reserve_mask Attribute _reserve_shift Attribute _reserve_mux Attribute _reserve_controller Attribute _reserve_mask Attribute _reserve_shift", "label": "class FixedPolicy(Policy):\n    def __init__(self, cpu_count=None):\n        #: For tests.\n        self.cpu_count = cpu_count or multiprocessing.cpu_count()\n        self.mem = mmap.mmap(-1, 4096)\n        self.state = State.from_buffer(self.mem)\n        self.state.lock.init()\n\n        if self.cpu_count < 2:\n            # uniprocessor\n            self._reserve_mux = False\n            self._reserve_controller = False\n            self._reserve_mask = 0\n            self._reserve_shift = 0\n        elif self.cpu_count < 4:\n            # small SMP\n            self._reserve_mux = True\n            self._reserve_controller = False\n            self._reserve_mask = 1\n            self._reserve_shift = 1\n        else:\n            # big SMP\n            self._reserve_mux = True\n            self._reserve_controller = True\n            self._reserve_mask = 3\n            self._reserve_shift = 2\n\n    def _set_affinity(self, descr, mask):\n        if descr:\n            LOG.debug('CPU mask for %s: %#08x', descr, mask)\n        mitogen.parent._preexec_hook = self._clear\n        self._set_cpu_mask(mask)\n\n    def _balance(self, descr):\n        self.state.lock.acquire()\n        try:\n            n = self.state.counter\n            self.state.counter += 1\n        finally:\n            self.state.lock.release()\n\n        self._set_cpu(descr, self._reserve_shift + (\n            (n % (self.cpu_count - self._reserve_shift))\n        ))\n\n    def _set_cpu(self, descr, cpu):\n        self._set_affinity(descr, 1 << (cpu % self.cpu_count))\n\n    def _clear(self):\n        all_cpus = (1 << self.cpu_count) - 1\n        self._set_affinity(None, all_cpus & ~self._reserve_mask)\n\n    def assign_controller(self):\n        if self._reserve_controller:\n            self._set_cpu('Ansible top-level process', 1)\n        else:\n            self._balance('Ansible top-level process')\n\n    def assign_muxprocess(self, index):\n        self._set_cpu('MuxProcess %d' % (index,), index)\n\n    def assign_worker(self):\n        self._balance('WorkerProcess')\n\n    def assign_subprocess(self):\n        self._clear()"}
{"Repository": "sqlservice", "input": "Base class for mapper event decorators. className MapperEventDecorator(EventDecorator) Method _make_listener Method _listener", "label": "class MapperEventDecorator(EventDecorator):\n    def _make_listener(self, func):\n        func = super()._make_listener(func)\n\n        @wraps(func)\n        def _listener(mapper, connection, target):\n            return func(target, connection, mapper)\n\n        return _listener"}
{"Repository": "pyfive", "input": "An HDF5 Group which may hold attributes, datasets, or other groups. className Group(Mapping) Method __init__ Method __repr__ Method __len__ Method _dereference Method __getitem__ Method __iter__ Method visit Method visititems Method attrs Attribute parent Attribute file Attribute name Attribute _links Attribute _dataobjects Attribute _attrs", "label": "class Group(Mapping):\n    def __init__(self, name, dataobjects, parent):\n        self.parent = parent\n        self.file = parent.file\n        self.name = name\n\n        self._links = dataobjects.get_links()\n        self._dataobjects = dataobjects\n        self._attrs = None  # cached property\n\n    def __repr__(self):\n        return '<HDF5 group \"%s\" (%d members)>' % (self.name, len(self))\n\n    def __len__(self):\n        return len(self._links)\n\n    def _dereference(self, ref):\n        if not ref:\n            raise ValueError('cannot deference null reference')\n        obj = self.file._get_object_by_address(ref.address_of_reference)\n        if obj is None:\n            raise ValueError('reference not found in file')\n        return obj\n\n    def __getitem__(self, y):\n        if isinstance(y, Reference):\n            return self._dereference(y)\n\n        path = posixpath.normpath(y)\n        if path == '.':\n            return self\n        if path.startswith('/'):\n            return self.file[path[1:]]\n\n        if posixpath.dirname(path) != '':\n            next_obj, additional_obj = path.split('/', 1)\n        else:\n            next_obj = path\n            additional_obj = '.'\n\n        if next_obj not in self._links:\n            raise KeyError('%s not found in group' % (next_obj))\n\n        obj_name = posixpath.join(self.name, next_obj)\n        link_target = self._links[next_obj]\n\n        if isinstance(link_target, str):\n            try:\n                return self.__getitem__(link_target)\n            except KeyError:\n                return None\n\n        dataobjs = DataObjects(self.file._fh, link_target)\n        if dataobjs.is_dataset:\n            if additional_obj != '.':\n                raise KeyError('%s is a dataset, not a group' % (obj_name))\n            return Dataset(obj_name, dataobjs, self)\n        return Group(obj_name, dataobjs, self)[additional_obj]\n\n    def __iter__(self):\n        for k in self._links.keys():\n            yield k\n\n    def visit(self, func):\n        return self.visititems(lambda name, obj: func(name))\n\n    def visititems(self, func):\n        root_name_length = len(self.name)\n        if not self.name.endswith('/'):\n            root_name_length += 1\n        queue = deque(self.values())\n        while queue:\n            obj = queue.popleft()\n            name = obj.name[root_name_length:]\n            ret = func(name, obj)\n            if ret is not None:\n                return ret\n            if isinstance(obj, Group):\n                queue.extend(obj.values())\n        return None\n\n    @property\n    def attrs(self):\n        if self._attrs is None:\n            self._attrs = self._dataobjects.get_attributes()\n        return self._attrs"}
{"Repository": "motpy", "input": "A single object tracker using Kalman filter with specified motion model specification className KalmanTracker(SingleObjectTracker) Method _predict Method _update_box Method box Method is_invalid", "label": "class KalmanTracker(SingleObjectTracker):\n    def __init__(self,\n                 model_kwargs: dict = DEFAULT_MODEL_SPEC,\n                 x0: Optional[Vector] = None,\n                 box0: Optional[Box] = None,\n                 **kwargs) -> None:\n\n        super(KalmanTracker, self).__init__(**kwargs)\n\n        self.model_kwargs: dict = model_kwargs\n        self.model = Model(**self.model_kwargs)\n\n        if x0 is None:\n            x0 = self.model.box_to_x(box0)\n\n        self._tracker: KalmanFilter = get_kalman_object_tracker(model=self.model, x0=x0)\n\n    def _predict(self) -> None:\n        self._tracker.predict()\n\n    def _update_box(self, detection: Detection) -> None:\n        z = self.model.box_to_z(detection.box)\n        self._tracker.update(z)\n\n    def box(self) -> Box:\n        return self.model.x_to_box(self._tracker.x)\n\n    def is_invalid(self) -> bool:\n        try:\n            has_nans = any(np.isnan(self._tracker.x))\n            return has_nans\n        except Exception as e:\n            logger.warning(f'invalid tracker - exception: {e}')\n            return True"}
{"Repository": "terminedia", "input": "Cached ChainMap className PixelDict(ChainMap) Method __init__ Method __setitem__ Method __getitem__ Method clear Method pop Method push Attribute maps Attribute cache Attribute _sentinel", "label": "class PixelDict(ChainMap):\n    def __init__(self, maps=None):\n        # Chainmap uses a plain list, still, maps are pilld top-most on the left-side.\n        # go figure!\n        super().__init__(maps or {})\n        self.maps = deque(self.maps)\n        self.cache = {}\n        self._sentinel = object()\n\n    def __setitem__(self, key, value):\n        self.cache[key] = value\n        super().__setitem__(key, value)\n\n    def __getitem__(self, key):\n        value = self.cache.get(key, self._sentinel)\n        if value is self._sentinel:\n            value = super().__getitem__(key)\n            self.cache[key] = value\n        return value\n\n    def clear(self):\n        self.maps = [{}]\n        self.cache.clear()\n\n    def pop(self):\n        self.cache.clear()\n        return self.maps.popleft()\n\n    def push(self, map=None):\n        if map:\n            self.cache.clear()\n        self.maps.appendleft(map or {})"}
{"Repository": "deepcorrect", "input": "Support setup.py upload. className UploadCommand(Command) Method status Method initialize_options Method finalize_options Method run", "label": "class UploadCommand(Command):\n    description = 'Build and publish the package.'\n    user_options = []\n\n    @staticmethod\n    def status(s):\n        print('\\033[1m{0}\\033[0m'.format(s))\n\n    def initialize_options(self):\n        pass\n\n    def finalize_options(self):\n        pass\n\n    def run(self):\n        try:\n            self.status('Removing previous builds')\n            rmtree(os.path.join(here, 'dist'))\n        except OSError:\n            pass\n\n        self.status('Building Source and Wheel (universal) distribution')\n        os.system('{0} setup.py sdist bdist_wheel --universal'.format(sys.executable))\n\n        self.status('Uploading the package to PyPI via Twine')\n        os.system('twine upload dist/*')\n\n        self.status('Pushing git tags')\n        os.system('git tag v{0}'.format(about['__version__']))\n        os.system('git push --tags')\n        \n        sys.exit()"}
{"Repository": "Flask-SuperAdmin", "input": "Default administrative interface index page when visiting the ``/admin/`` URL. className AdminIndexView(BaseView) Method index Method __init__ Method index", "label": "class AdminIndexView(BaseView):\n    def __init__(self, name=None, category=None, endpoint=None, url=None):\n        super(AdminIndexView, self).__init__(name or babel.lazy_gettext('Home'),\n                                             category,\n                                             endpoint or 'admin',\n                                             url or '/admin',\n                                             'static')\n\n    @expose('/')\n    def index(self):\n        return self.render('admin/index.html')"}
{"Repository": "ro.py", "input": "Represents a Roblox gamepass ID. className BaseGamePass(BaseItem) Method __init__", "label": "class BaseGamePass(BaseItem):\n    def __init__(self, client: Client, gamepass_id: int):\n        self._client: Client = client\n        self.id: int = gamepass_id"}
{"Repository": "cotcha", "input": "Access class for ESP8266 ROM bootloader className ESP8266ROM(ESPLoader) Method get_chip_description Method flash_spi_attach Method flash_set_parameters Method chip_id Method read_mac Method get_erase_size", "label": "class ESP8266ROM(ESPLoader):\n    CHIP_NAME = \"ESP8266\"\n    IS_STUB = False\n\n    DATE_REG_VALUE = 0x00062000\n\n    # OTP ROM addresses\n    ESP_OTP_MAC0    = 0x3ff00050\n    ESP_OTP_MAC1    = 0x3ff00054\n    ESP_OTP_MAC3    = 0x3ff0005c\n\n    SPI_REG_BASE    = 0x60000200\n    SPI_W0_OFFS     = 0x40\n    SPI_HAS_MOSI_DLEN_REG = False\n\n    FLASH_SIZES = {\n        '512KB':0x00,\n        '256KB':0x10,\n        '1MB':0x20,\n        '2MB':0x30,\n        '4MB':0x40,\n        '2MB-c1': 0x50,\n        '4MB-c1':0x60,\n        '8MB':0x80,\n        '16MB':0x90,\n    }\n\n    BOOTLOADER_FLASH_OFFSET = 0\n\n    def get_chip_description(self):\n        return \"ESP8266\"\n\n    def flash_spi_attach(self, hspi_arg):\n        if self.IS_STUB:\n            super(ESP8266ROM, self).flash_spi_attach(hspi_arg)\n        else:\n            # ESP8266 ROM has no flash_spi_attach command in serial protocol,\n            # but flash_begin will do it\n            self.flash_begin(0, 0)\n\n    def flash_set_parameters(self, size):\n        # not implemented in ROM, but OK to silently skip for ROM\n        if self.IS_STUB:\n            super(ESP8266ROM, self).flash_set_parameters(size)\n\n    def chip_id(self):\n        id0 = self.read_reg(self.ESP_OTP_MAC0)\n        id1 = self.read_reg(self.ESP_OTP_MAC1)\n        return (id0 >> 24) | ((id1 & MAX_UINT24) << 8)\n\n    def read_mac(self):\n        mac0 = self.read_reg(self.ESP_OTP_MAC0)\n        mac1 = self.read_reg(self.ESP_OTP_MAC1)\n        mac3 = self.read_reg(self.ESP_OTP_MAC3)\n        if (mac3 != 0):\n            oui = ((mac3 >> 16) & 0xff, (mac3 >> 8) & 0xff, mac3 & 0xff)\n        elif ((mac1 >> 16) & 0xff) == 0:\n            oui = (0x18, 0xfe, 0x34)\n        elif ((mac1 >> 16) & 0xff) == 1:\n            oui = (0xac, 0xd0, 0x74)\n        else:\n            raise FatalError(\"Unknown OUI\")\n        return oui + ((mac1 >> 8) & 0xff, mac1 & 0xff, (mac0 >> 24) & 0xff)\n\n    def get_erase_size(self, offset, size):\n        sectors_per_block = 16\n        sector_size = self.FLASH_SECTOR_SIZE\n        num_sectors = (size + sector_size - 1) // sector_size\n        start_sector = offset // sector_size\n\n        head_sectors = sectors_per_block - (start_sector % sectors_per_block)\n        if num_sectors < head_sectors:\n            head_sectors = num_sectors\n\n        if num_sectors < 2 * head_sectors:\n            return (num_sectors + 1) // 2 * sector_size\n        else:\n            return (num_sectors - head_sectors) * sector_size"}
{"Repository": "TwinLiteNet", "input": "Tversky loss for image segmentation task. className TverskyLoss(DiceLoss) Method aggregate_loss Method compute_score", "label": "class TverskyLoss(DiceLoss):\n    def __init__(\n        self,\n        mode: str,\n        classes: List[int] = None,\n        log_loss: bool = False,\n        from_logits: bool = True,\n        smooth: float = 0.0,\n        ignore_index: Optional[int] = None,\n        eps: float = 1e-7,\n        alpha: float = 0.5,\n        beta: float = 0.5,\n        gamma: float = 1.0\n    ):\n\n        assert mode in {BINARY_MODE, MULTILABEL_MODE, MULTICLASS_MODE}\n        super().__init__(mode, classes, log_loss, from_logits, smooth, ignore_index, eps)\n        self.alpha = alpha\n        self.beta = beta\n        self.gamma = gamma\n\n    def aggregate_loss(self, loss):\n        return loss.mean() ** self.gamma\n\n    def compute_score(self, output, target, smooth=0.0, eps=1e-7, dims=None) -> torch.Tensor:\n        return soft_tversky_score(output, target, self.alpha, self.beta, smooth, eps, dims)"}
{"Repository": "IDArling", "input": "The \"Open from server...\" action installed in the \"File\" menu. className OpenAction(Action) Method __init__", "label": "class OpenAction(Action):\n    _ACTION_ID = \"idarling:open\"\n\n    def __init__(self, plugin):\n        super(OpenAction, self).__init__(\n            plugin,\n            \"File/Open\",\n            \"Open from server...\",\n            \"Load a database from server\",\n            plugin.plugin_resource(\"download.png\"),\n            OpenActionHandler(plugin),\n        )"}
{"Repository": "AbletonOSC", "input": "Asynchronous OSC Server An asynchronous OSC Server using UDP. className AsyncIOOSCUDPServer() Method __init__ Method __init__ Method datagram_received Method serve Method create_serve_endpoint Method dispatcher", "label": "class AsyncIOOSCUDPServer():\n    def __init__(self, server_address: Tuple[str, int], dispatcher: Dispatcher, loop: BaseEventLoop) -> None:\n        self._server_address = server_address\n        self._dispatcher = dispatcher\n        self._loop = loop\n\n    class _OSCProtocolFactory(asyncio.DatagramProtocol):\n        def __init__(self, dispatcher: Dispatcher) -> None:\n            self.dispatcher = dispatcher\n\n        def datagram_received(self, data: bytes, client_address: Tuple[str, int]) -> None:\n            self.dispatcher.call_handlers_for_packet(data, client_address)\n\n    def serve(self) -> None:\n        self._loop.run_until_complete(self.create_serve_endpoint())\n\n    def create_serve_endpoint(self) -> Coroutine[Any, Any, Tuple[asyncio.transports.BaseTransport, asyncio.DatagramProtocol]]:\n        return self._loop.create_datagram_endpoint(\n            lambda: self._OSCProtocolFactory(self.dispatcher),\n            local_addr=self._server_address)\n\n    @property\n    def dispatcher(self) -> Dispatcher:\n        return self._dispatcher"}
{"Repository": "Augmentation-PyTorch-Transforms", "input": "Resize the input PIL Image to the given size. className Resize(object) Method __init__ Method __call__ Method __repr__ Attribute size Attribute interpolation", "label": "class Resize(object):\n    def __init__(self, size, interpolation=Image.BILINEAR):\n        assert isinstance(size, int) or (isinstance(size, Iterable) and len(size) == 2)\n        self.size = size\n        self.interpolation = interpolation\n\n    def __call__(self, img):\n        return F.resize(img, self.size, self.interpolation)\n\n    def __repr__(self):\n        interpolate_str = _pil_interpolation_to_str[self.interpolation]\n        return self.__class__.__name__ + '(size={0}, interpolation={1})'.format(self.size, interpolate_str)"}
{"Repository": "gae_bingo", "input": "Gets and sets BingoCache/BingoIdentityCaches in multiple cache layers. className CacheLayers(object) Method fill_request_cache Method compress Method decompress Method set Method get", "label": "class CacheLayers(object):\n    INSTANCE_SECONDS = 60  # number of secs BingoCache stays in instance cache\n\n    @staticmethod\n    def fill_request_cache():\n        if not request_cache.cache.get(\"bingo_request_cache_filled\"):\n\n            # Assume that we're going to grab both BingoCache and\n            # BingoIdentityCache from memcache\n            memcache_keys = [\n                BingoCache.CACHE_KEY,\n                BingoIdentityCache.key_for_identity(identity())\n            ]\n\n            # Try to grab BingoCache from instance cache\n            bingo_instance = instance_cache.get(BingoCache.CACHE_KEY)\n            if bingo_instance:\n                # If successful, use instance cached version...\n                request_cache.cache[BingoCache.CACHE_KEY] = bingo_instance\n                # ...and don't load BingoCache from memcache\n                memcache_keys.remove(BingoCache.CACHE_KEY)\n\n            # Load necessary caches from memcache\n            dict_memcache = memcache.get_multi(memcache_keys)\n\n            # Decompress BingoCache if we loaded it from memcache\n            if BingoCache.CACHE_KEY in dict_memcache:\n                dict_memcache[BingoCache.CACHE_KEY] = CacheLayers.decompress(\n                        dict_memcache[BingoCache.CACHE_KEY])\n\n            # Update request cache with values loaded from memcache\n            request_cache.cache.update(dict_memcache)\n\n            if not bingo_instance:\n                # And if BingoCache wasn't in the instance cache already, store\n                # it with a 1-minute expiry\n                instance_cache.set(BingoCache.CACHE_KEY,\n                        request_cache.cache.get(BingoCache.CACHE_KEY),\n                        expiry=CacheLayers.INSTANCE_SECONDS)\n\n            request_cache.cache[\"bingo_request_cache_filled\"] = True\n\n    @staticmethod\n    def compress(value):\n        pickled = pickle_util.dump(value)\n        return zlib.compress(pickled)\n\n    @staticmethod\n    def decompress(data):\n        pickled = zlib.decompress(data)\n        return pickle_util.load(pickled)\n\n    @staticmethod\n    def set(key, value):\n        instance_cache.set(key, value, expiry=CacheLayers.INSTANCE_SECONDS)\n        memcache.set(key, CacheLayers.compress(value))\n\n        logging.info(\"Set BingoCache in instance cache and memcache\")\n\n    @staticmethod\n    def get(key, fxn_load):\n        CacheLayers.fill_request_cache()\n\n        if not request_cache.cache.get(key):\n            request_cache.cache[key] = fxn_load()\n\n        return request_cache.cache[key]"}
{"Repository": "yanmtt", "input": "Configuration class to store the configuration of a `GPT2Model`. className GPT2Config(PretrainedConfig) Method max_position_embeddings Method hidden_size Method num_attention_heads Method num_hidden_layers", "label": "class GPT2Config(PretrainedConfig):\n    pretrained_config_archive_map = GPT2_PRETRAINED_CONFIG_ARCHIVE_MAP\n\n    def __init__(\n        self,\n        vocab_size_or_config_json_file=50257,\n        n_positions=1024,\n        n_ctx=1024,\n        n_embd=768,\n        n_layer=12,\n        n_head=12,\n        resid_pdrop=0.1,\n        embd_pdrop=0.1,\n        attn_pdrop=0.1,\n        layer_norm_epsilon=1e-5,\n        initializer_range=0.02,\n\n        num_labels=1,\n        summary_type='cls_index',\n        summary_use_proj=True,\n        summary_activation=None,\n        summary_proj_to_labels=True,\n        summary_first_dropout=0.1,\n        **kwargs\n    ):\n        super(GPT2Config, self).__init__(**kwargs)\n\n        if isinstance(vocab_size_or_config_json_file, str) or (sys.version_info[0] == 2\n                        and isinstance(vocab_size_or_config_json_file, unicode)):\n            with open(vocab_size_or_config_json_file, \"r\", encoding=\"utf-8\") as reader:\n                json_config = json.loads(reader.read())\n            for key, value in json_config.items():\n                self.__dict__[key] = value\n        elif isinstance(vocab_size_or_config_json_file, int):\n            self.vocab_size = vocab_size_or_config_json_file\n            self.n_ctx = n_ctx\n            self.n_positions = n_positions\n            self.n_embd = n_embd\n            self.n_layer = n_layer\n            self.n_head = n_head\n            self.resid_pdrop = resid_pdrop\n            self.embd_pdrop = embd_pdrop\n            self.attn_pdrop = attn_pdrop\n            self.layer_norm_epsilon = layer_norm_epsilon\n            self.initializer_range = initializer_range\n\n            self.num_labels = num_labels\n            self.summary_type = summary_type\n            self.summary_use_proj = summary_use_proj\n            self.summary_activation = summary_activation\n            self.summary_first_dropout = summary_first_dropout\n            self.summary_proj_to_labels = summary_proj_to_labels\n        else:\n            raise ValueError(\n                \"First argument must be either a vocabulary size (int)\"\n                \"or the path to a pretrained model config file (str)\"\n            )\n\n    @property\n    def max_position_embeddings(self):\n        return self.n_positions\n\n    @property\n    def hidden_size(self):\n        return self.n_embd\n\n    @property\n    def num_attention_heads(self):\n        return self.n_head\n\n    @property\n    def num_hidden_layers(self):\n        return self.n_layer"}
{"Repository": "riffusion", "input": "Parameters for the conversion from audio to spectrograms to images and back. className SpectrogramParams Method n_fft Method win_length Method hop_length Method to_exif Method from_exif", "label": "class SpectrogramParams:\n    # Whether the audio is stereo or mono\n    stereo: bool = False\n\n    # FFT parameters\n    sample_rate: int = 44100\n    step_size_ms: int = 10\n    window_duration_ms: int = 100\n    padded_duration_ms: int = 400\n\n    # Mel scale parameters\n    num_frequencies: int = 512\n    # TODO(hayk): Set these to [20, 20000] for newer models\n    min_frequency: int = 0\n    max_frequency: int = 10000\n    mel_scale_norm: T.Optional[str] = None\n    mel_scale_type: str = \"htk\"\n    max_mel_iters: int = 200\n\n    # Griffin Lim parameters\n    num_griffin_lim_iters: int = 32\n\n    # Image parameterization\n    power_for_image: float = 0.25\n\n    class ExifTags(Enum):\n        SAMPLE_RATE = 11000\n        STEREO = 11005\n        STEP_SIZE_MS = 11010\n        WINDOW_DURATION_MS = 11020\n        PADDED_DURATION_MS = 11030\n\n        NUM_FREQUENCIES = 11040\n        MIN_FREQUENCY = 11050\n        MAX_FREQUENCY = 11060\n\n        POWER_FOR_IMAGE = 11070\n        MAX_VALUE = 11080\n\n    @property\n    def n_fft(self) -> int:\n        return int(self.padded_duration_ms / 1000.0 * self.sample_rate)\n\n    @property\n    def win_length(self) -> int:\n        return int(self.window_duration_ms / 1000.0 * self.sample_rate)\n\n    @property\n    def hop_length(self) -> int:\n        return int(self.step_size_ms / 1000.0 * self.sample_rate)\n\n    def to_exif(self) -> T.Dict[int, T.Any]:\n        return {\n            self.ExifTags.SAMPLE_RATE.value: self.sample_rate,\n            self.ExifTags.STEREO.value: self.stereo,\n            self.ExifTags.STEP_SIZE_MS.value: self.step_size_ms,\n            self.ExifTags.WINDOW_DURATION_MS.value: self.window_duration_ms,\n            self.ExifTags.PADDED_DURATION_MS.value: self.padded_duration_ms,\n            self.ExifTags.NUM_FREQUENCIES.value: self.num_frequencies,\n            self.ExifTags.MIN_FREQUENCY.value: self.min_frequency,\n            self.ExifTags.MAX_FREQUENCY.value: self.max_frequency,\n            self.ExifTags.POWER_FOR_IMAGE.value: float(self.power_for_image),\n        }\n\n    @classmethod\n    def from_exif(cls, exif: T.Mapping[int, T.Any]) -> SpectrogramParams:\n        # TODO(hayk): Handle missing tags\n        return cls(\n            sample_rate=exif[cls.ExifTags.SAMPLE_RATE.value],\n            stereo=bool(exif[cls.ExifTags.STEREO.value]),\n            step_size_ms=exif[cls.ExifTags.STEP_SIZE_MS.value],\n            window_duration_ms=exif[cls.ExifTags.WINDOW_DURATION_MS.value],\n            padded_duration_ms=exif[cls.ExifTags.PADDED_DURATION_MS.value],\n            num_frequencies=exif[cls.ExifTags.NUM_FREQUENCIES.value],\n            min_frequency=exif[cls.ExifTags.MIN_FREQUENCY.value],\n            max_frequency=exif[cls.ExifTags.MAX_FREQUENCY.value],\n            power_for_image=exif[cls.ExifTags.POWER_FOR_IMAGE.value],\n        )"}
{"Repository": "CAN", "input": "Graph convolution layer for sparse inputs. className GraphConvolutionSparse(Layer) Method __init__ Method _call Attribute dropout Attribute adj Attribute act Attribute issparse Attribute features_nonzero", "label": "class GraphConvolutionSparse(Layer):\n    def __init__(self, input_dim, output_dim, adj, features_nonzero, dropout=0., act=tf.nn.relu, **kwargs):\n        super(GraphConvolutionSparse, self).__init__(**kwargs)\n        with tf.variable_scope(self.name + '_vars'):\n            self.vars['weights'] = weight_variable_glorot(input_dim, output_dim, name=\"weights\")\n        self.dropout = dropout\n        self.adj = adj\n        self.act = act\n        self.issparse = True\n        self.features_nonzero = features_nonzero\n\n    def _call(self, inputs):\n        x = inputs\n        x = dropout_sparse(x, 1 - self.dropout, self.features_nonzero)\n        x = tf.sparse_tensor_dense_matmul(x, self.vars['weights'])\n        x = tf.sparse_tensor_dense_matmul(self.adj, x)\n        outputs = self.act(x)\n        return outputs"}
{"Repository": "av_hubert", "input": "Crop the given image at the center className RandomCrop(object) Method __init__ Method __call__ Method __repr__ Attribute size", "label": "class RandomCrop(object):\n    def __init__(self, size):\n        self.size = size\n\n    def __call__(self, frames):\n        t, h, w = frames.shape\n        th, tw = self.size\n        delta_w = random.randint(0, w-tw)\n        delta_h = random.randint(0, h-th)\n        frames = frames[:, delta_h:delta_h+th, delta_w:delta_w+tw]\n        return frames\n\n    def __repr__(self):\n        return self.__class__.__name__ + '(size={0})'.format(self.size)"}
{"Repository": "FENeRF", "input": "Redirect stderr to stdout, optionally print stdout to a file, and optionally force flushing on both stdout and the file. className Logger(object) Method __init__ Method __enter__ Method __exit__ Method write Method flush Method close Attribute file Attribute file Attribute should_flush Attribute stdout Attribute stderr", "label": "class Logger(object):\n    def __init__(self, file_name: str = None, file_mode: str = \"w\", should_flush: bool = True):\n        self.file = None\n\n        if file_name is not None:\n            self.file = open(file_name, file_mode)\n\n        self.should_flush = should_flush\n        self.stdout = sys.stdout\n        self.stderr = sys.stderr\n\n        sys.stdout = self\n        sys.stderr = self\n\n    def __enter__(self) -> \"Logger\":\n        return self\n\n    def __exit__(self, exc_type: Any, exc_value: Any, traceback: Any) -> None:\n        self.close()\n\n    def write(self, text: Union[str, bytes]) -> None:\n        if isinstance(text, bytes):\n            text = text.decode()\n        if len(text) == 0: # workaround for a bug in VSCode debugger: sys.stdout.write(''); sys.stdout.flush() => crash\n            return\n\n        if self.file is not None:\n            self.file.write(text)\n\n        self.stdout.write(text)\n\n        if self.should_flush:\n            self.flush()\n\n    def flush(self) -> None:\n        if self.file is not None:\n            self.file.flush()\n\n        self.stdout.flush()\n\n    def close(self) -> None:\n        self.flush()\n\n        # if using multiple loggers, prevent closing in wrong order\n        if sys.stdout is self:\n            sys.stdout = self.stdout\n        if sys.stderr is self:\n            sys.stderr = self.stderr\n\n        if self.file is not None:\n            self.file.close()\n            self.file = None"}
{"Repository": "openseg.pytorch", "input": "Rotate the input numpy. className RandomRotate(_BaseTransform) Method __init__ Method _warp Method _process_img Method _process_labelmap Method _process_maskmap Method __call__ Attribute max_degree Attribute ratio Attribute mean", "label": "class RandomRotate(_BaseTransform):\n    def __init__(self, max_degree, rotate_ratio=0.5, mean=(104, 117, 123)):\n        assert isinstance(max_degree, int)\n        self.max_degree = max_degree\n        self.ratio = rotate_ratio\n        self.mean = mean\n        Log.warn(\n            'Currently `RandomRotate` is only implemented for `img`, `labelmap` and `maskmap`.')\n\n    def _warp(self, x, border_value, rotate_mat, new_width, new_height):\n        return cv2.warpAffine(x, rotate_mat, (new_width, new_height), borderValue=border_value)\n\n    def _process_img(self, x, *args):\n        return self._warp(x, self.mean, *args).astype(np.uint8)\n\n    def _process_labelmap(self, x, *args):\n        return self._warp(x, (255, 255, 255), *args).astype(np.uint8)\n\n    def _process_maskmap(self, x, *args):\n        return self._warp(x, (1, 1, 1), *args).astype(np.uint8)\n\n    def __call__(self, img, **kwargs):\n        img, data_dict = super().__call__(img, **kwargs)\n\n        rotate_degree = random.uniform(-self.max_degree, self.max_degree)\n        height, width, _ = img.shape\n        img_center = (width / 2.0, height / 2.0)\n        rotate_mat = cv2.getRotationMatrix2D(img_center, rotate_degree, 1.0)\n        cos_val = np.abs(rotate_mat[0, 0])\n        sin_val = np.abs(rotate_mat[0, 1])\n        new_width = int(height * sin_val + width * cos_val)\n        new_height = int(height * cos_val + width * sin_val)\n        rotate_mat[0, 2] += (new_width / 2.) - img_center[0]\n        rotate_mat[1, 2] += (new_height / 2.) - img_center[1]\n\n        return self._process(\n            img, data_dict,\n            random.random() > self.ratio,\n            rotate_mat, new_width, new_height\n        )"}
{"Repository": "PCL", "input": "Computes and stores the average and current value className AverageMeter(object) Method __init__ Method reset Method update Method __str__ Attribute name Attribute fmt", "label": "class AverageMeter(object):\n    def __init__(self, name, fmt=':f'):\n        self.name = name\n        self.fmt = fmt\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count\n\n    def __str__(self):\n        fmtstr = '{name} {val' + self.fmt + '} ({avg' + self.fmt + '})'\n        return fmtstr.format(**self.__dict__)"}
{"Repository": "flickrapi", "input": "Builds a method name for FlickrAPI calls. className CallBuilder(object) Method __init__ Method __getattr__ Method __repr__ Method __call__ Attribute flickrapi_object Attribute method_name Attribute __name__", "label": "class CallBuilder(object):\n    def __init__(self, flickrapi_object, method_name='flickr'):\n        self.flickrapi_object = flickrapi_object\n        self.method_name = method_name\n        self.__name__ = method_name.split('.')[-1]\n\n    def __getattr__(self, name):\n        # Refuse to act as a proxy for unimplemented special methods\n        if name.startswith('_'):\n            raise AttributeError(\"No such attribute '%s'\" % name)\n\n        return self.__class__(self.flickrapi_object,\n                              self.method_name + '.' + name)\n\n    def __repr__(self):\n        return '%s(%r)' % (self.__class__.__name__, self.method_name)\n\n    def __call__(self, **kwargs):\n        return self.flickrapi_object.do_flickr_call(self.method_name, **kwargs)"}
{"Repository": "rietveld", "input": "Implementation of the VersionControlSystem interface for CVS. className CVSVCS(VersionControlSystem) Method __init__ Method GetGUID Method GetOriginalContent_ Method GetBaseFile Method GenerateDiff Method GetUnknownFiles", "label": "class CVSVCS(VersionControlSystem):\n  def __init__(self, options):\n    super(CVSVCS, self).__init__(options)\n\n  def GetGUID(self):\n    return\n\n  def GetOriginalContent_(self, filename):\n    RunShell([\"cvs\", \"up\", filename], silent_ok=True)\n    # TODO need detect file content encoding\n    content = open(filename).read()\n    return content.replace(\"\\r\\n\", \"\\n\")\n\n  def GetBaseFile(self, filename):\n    base_content = None\n    new_content = None\n    status = \"A\"\n\n    output, retcode = RunShellWithReturnCode([\"cvs\", \"status\", filename])\n    if retcode:\n      ErrorExit(\"Got error status from 'cvs status %s'\" % filename)\n\n    if output.find(\"Status: Locally Modified\") != -1:\n      status = \"M\"\n      temp_filename = \"%s.tmp123\" % filename\n      os.rename(filename, temp_filename)\n      base_content = self.GetOriginalContent_(filename)\n      os.rename(temp_filename, filename)\n    elif output.find(\"Status: Locally Added\"):\n      status = \"A\"\n      base_content = \"\"\n    elif output.find(\"Status: Needs Checkout\"):\n      status = \"D\"\n      base_content = self.GetOriginalContent_(filename)\n\n    return (base_content, new_content, self.IsBinaryData(base_content), status)\n\n  def GenerateDiff(self, extra_args):\n    cmd = [\"cvs\", \"diff\", \"-u\", \"-N\"]\n    if self.options.revision:\n      cmd += [\"-r\", self.options.revision]\n\n    cmd.extend(extra_args)\n    data, retcode = RunShellWithReturnCode(cmd)\n    count = 0\n    if retcode in [0, 1]:\n      for line in data.splitlines():\n        if line.startswith(\"Index:\"):\n          count += 1\n          LOGGER.info(line)\n\n    if not count:\n      ErrorExit(\"No valid patches found in output from cvs diff\")\n\n    return data\n\n  def GetUnknownFiles(self):\n    data, retcode = RunShellWithReturnCode([\"cvs\", \"diff\"])\n    if retcode not in [0, 1]:\n      ErrorExit(\"Got error status from 'cvs diff':\\n%s\" % (data,))\n    unknown_files = []\n    for line in data.split(\"\\n\"):\n      if line and line[0] == \"?\":\n        unknown_files.append(line)\n    return unknown_files"}
{"Repository": "SliderYolo", "input": "normalize image className Normalize(object) Method __init__ Method __call__ Attribute mean Attribute std Attribute is_scale Attribute is_channel_first", "label": "class Normalize(object):\n    def __init__(self, mean, std, is_scale=True, is_channel_first=False):\n        self.mean = mean\n        self.std = std\n        self.is_scale = is_scale\n        self.is_channel_first = is_channel_first\n\n    def __call__(self, im, im_info):\n        im = im.astype(np.float32, copy=False)\n        if self.is_channel_first:\n            mean = np.array(self.mean)[:, np.newaxis, np.newaxis]\n            std = np.array(self.std)[:, np.newaxis, np.newaxis]\n        else:\n            mean = np.array(self.mean)[np.newaxis, np.newaxis, :]\n            std = np.array(self.std)[np.newaxis, np.newaxis, :]\n        if self.is_scale:\n            im = im / 255.0\n        im -= mean\n        im /= std\n        return im, im_info"}
{"Repository": "djangosaml2idp", "input": "Default error view used when a 'known' error occurs in the saml2 authentication views. className SamlIDPErrorView(TemplateView) Method handle_error Method get_context_data", "label": "class SamlIDPErrorView(TemplateView):\n    template_name = 'djangosaml2idp/error.html'\n\n    @classmethod\n    def handle_error(cls, request: HttpRequest, exception: Exception, status_code: int = 500, **kwargs) -> HttpResponse:\n        logger.error(kwargs, exc_info=exception)\n\n        # Render an http response and return it\n        response = cls.as_view()(request, exception=exception, **kwargs)\n        response.status_code = status_code\n        return response\n\n    def get_context_data(self, **kwargs) -> dict:\n        context = super().get_context_data(**kwargs)\n        exception = kwargs.get(\"exception\")\n\n        context.update({\n            \"exception\": exception,\n            \"exception_type\": exception.__class__.__name__ if exception else None,\n            \"exception_msg\": exception.message if exception and hasattr(exception, 'message') else str(exception) if exception else None,\n            \"extra_message\": kwargs.get(\"extra_message\"),\n        })\n        return context"}
{"Repository": "injector", "input": "Requirement could not be satisfied. className UnsatisfiedRequirement(Error) Method __init__ Method __str__", "label": "class UnsatisfiedRequirement(Error):\n    def __init__(self, owner: Optional[object], interface: type) -> None:\n        super().__init__(owner, interface)\n        self.owner = owner\n        self.interface = interface\n\n    def __str__(self) -> str:\n        on = '%s has an ' % _describe(self.owner) if self.owner else ''\n        return '%sunsatisfied requirement on %s' % (on, _describe(self.interface))"}
{"Repository": "structural-probes", "input": "A class for simple contextualization of word-level embeddings. className ProjectionModel(Model) Method __init__ Method forward Attribute lstm", "label": "class ProjectionModel(Model):\n  def __init__(self, args):\n    super(ProjectionModel, self).__init__(args)\n    input_dim = args['model']['hidden_dim']\n    self.lstm = nn.LSTM(input_size=input_dim, hidden_size=int(input_dim/2),\n        num_layers=1, batch_first=True, bidirectional=True)\n    for param in self.lstm.parameters():\n      param.requires_grad = False\n    self.lstm.to(args['device'])\n\n  def forward(self, batch):\n    with torch.no_grad():\n      projected, _ = self.lstm(batch)\n    return projected"}
{"Repository": "DPSNet", "input": "Create an object with a write method that writes to a specific place on the screen, defined at instantiation. className Writer(object) Method __init__ Method write Method flush Attribute location Attribute t", "label": "class Writer(object):\n    def __init__(self, t, location):\n        self.location = location\n        self.t = t\n\n    def write(self, string):\n        with self.t.location(*self.location):\n            sys.stdout.write(\"\\033[K\")\n            print(string)\n\n    def flush(self):\n        return"}
{"Repository": "Q-ViT", "input": "Track a series of values and provide access to smoothed values over a window or the global series average. className SmoothedValue(object) Method __init__ Method update Method synchronize_between_processes Method median Method avg Method global_avg Method max Method value Method __str__ Attribute deque Attribute total Attribute count Attribute fmt", "label": "class SmoothedValue(object):\n    def __init__(self, window_size=20, fmt=None):\n        if fmt is None:\n            fmt = \"{median:.4f} ({global_avg:.4f})\"\n        self.deque = deque(maxlen=window_size)\n        self.total = 0.0\n        self.count = 0\n        self.fmt = fmt\n\n    def update(self, value, n=1):\n        self.deque.append(value)\n        self.count += n\n        self.total += value * n\n\n    def synchronize_between_processes(self):\n        if not is_dist_avail_and_initialized():\n            return\n        t = torch.tensor([self.count, self.total], dtype=torch.float64, device='cuda')\n        dist.barrier()\n        dist.all_reduce(t)\n        t = t.tolist()\n        self.count = int(t[0])\n        self.total = t[1]\n\n    @property\n    def median(self):\n        d = torch.tensor(list(self.deque))\n        return d.median().item()\n\n    @property\n    def avg(self):\n        d = torch.tensor(list(self.deque), dtype=torch.float32)\n        return d.mean().item()\n\n    @property\n    def global_avg(self):\n        return self.total / self.count\n\n    @property\n    def max(self):\n        return max(self.deque)\n\n    @property\n    def value(self):\n        return self.deque[-1]\n\n    def __str__(self):\n        return self.fmt.format(\n            median=self.median,\n            avg=self.avg,\n            global_avg=self.global_avg,\n            max=self.max,\n            value=self.value)"}
{"Repository": "gcnn-survey-paper", "input": "Gets config parameters from flags to train the GNN models. className Config(object) Method __init__ Method set_num_nodes_edges Method get_filename_suffix Attribute n_hidden_node Attribute n_att_node Attribute n_hidden_edge Attribute n_att_edge Attribute topk Attribute att_mechanism Attribute edge_loss Attribute cheby_k_loc Attribute semi_emb_k Attribute sparse_features Attribute lr Attribute epochs Attribute patience Attribute node_l2_reg Attribute edge_l2_reg Attribute edge_reg Attribute p_drop_node Attribute p_drop_edge", "label": "class Config(object):\n  def __init__(self):\n    # Model parameters\n    self.n_hidden_node = list(map(int, FLAGS.n_hidden_node.split('-')))\n    self.n_att_node = list(map(int, FLAGS.n_att_node.split('-')))\n    self.n_hidden_edge = list(map(int, FLAGS.n_hidden_edge.split('-')))\n    self.n_att_edge = list(map(int, FLAGS.n_att_edge.split('-')))\n    self.topk = FLAGS.topk\n    self.att_mechanism = FLAGS.att_mechanism\n    self.edge_loss = FLAGS.edge_loss\n    self.cheby_k_loc = FLAGS.cheby_k_loc\n    self.semi_emb_k = FLAGS.semi_emb_k\n\n    # Dataset parameters\n    self.sparse_features = FLAGS.sparse_features\n\n    # Training parameters\n    self.lr = FLAGS.lr\n    self.epochs = FLAGS.epochs\n    self.patience = FLAGS.patience\n    self.node_l2_reg = FLAGS.node_l2_reg\n    self.edge_l2_reg = FLAGS.edge_l2_reg\n    self.edge_reg = FLAGS.edge_reg\n    self.p_drop_node = FLAGS.p_drop_node\n    self.p_drop_edge = FLAGS.p_drop_edge\n\n  def set_num_nodes_edges(self, data):\n    if self.sparse_features:\n      self.nb_nodes, self.input_dim = data['features'][-1]\n    else:\n      self.nb_nodes, self.input_dim = data['features'].shape\n    self.nb_classes = data['node_labels'].shape[-1]\n    self.n_hidden_node += [int(self.nb_classes)]\n    self.nb_edges = np.sum(data['adj_train'] > 0) - self.nb_nodes\n    self.multilabel = np.max(np.sum(data['node_labels'], 1)) > 1\n\n  def get_filename_suffix(self, run_id):\n    all_params = [\n        self.lr, self.epochs, self.patience, self.node_l2_reg, self.edge_l2_reg,\n        self.edge_reg, self.p_drop_node, self.p_drop_edge, '.'.join([\n            str(x) for x in self.n_hidden_node\n        ]), '.'.join([str(x) for x in self.n_att_node]),\n        '.'.join([str(x) for x in self.n_hidden_edge]), '.'.join(\n            [str(x) for x in self.n_att_edge]), self.topk, self.att_mechanism,\n        self.edge_loss, self.cheby_k_loc, self.semi_emb_k, run_id\n    ]\n    file_suffix = '-'.join([str(x) for x in all_params])\n    return file_suffix"}
{"Repository": "friture", "input": "A function. className Function(Object) Method __init__ Method add_call Method __repr__ Attribute id Attribute name Attribute calls Attribute cycle", "label": "class Function(Object):\n    def __init__(self, id, name):\n        Object.__init__(self)\n        self.id = id\n        self.name = name\n        self.calls = {}\n        self.cycle = None\n    \n    def add_call(self, call):\n        if call.callee_id in self.calls:\n            sys.stderr.write('warning: overwriting call from function %s to %s\\n' % (str(self.id), str(call.callee_id)))\n        self.calls[call.callee_id] = call\n\n    # TODO: write utility functions\n\n    def __repr__(self):\n        return self.name"}
{"Repository": "DeathSleep", "input": "Holds relocation information. className RelocationData(DataContainer) Method __setattr__", "label": "class RelocationData(DataContainer):\n    def __setattr__(self, name, val):\n\n        # If the instance doesn't yet have a struct attribute\n        # it's not fully initialized so can't do any of the\n        # following\n        #\n        if hasattr(self, \"struct\"):\n            # Get the word containing the type and data\n            #\n            word = self.struct.Data\n\n            if name == \"type\":\n                word = (val << 12) | (word & 0xFFF)\n            elif name == \"rva\":\n                offset = max(val - self.base_rva, 0)\n                word = (word & 0xF000) | (offset & 0xFFF)\n\n            # Store the modified data\n            #\n            self.struct.Data = word\n\n        self.__dict__[name] = val"}
{"Repository": "YouTubeDownLoader", "input": "Title bar with icon and title className CustomTitleBar(TitleBar) Method __init__ Method setTitle Method setIcon Attribute iconLabel Attribute titleLabel Attribute vBoxLayout Attribute buttonLayout", "label": "class CustomTitleBar(TitleBar):\n    def __init__(self, parent):\n        super().__init__(parent)\n        self.setFixedHeight(48)\n        self.hBoxLayout.removeWidget(self.minBtn)\n        self.hBoxLayout.removeWidget(self.maxBtn)\n        self.hBoxLayout.removeWidget(self.closeBtn)\n\n        # add window icon\n        self.iconLabel = QLabel(self)\n        self.iconLabel.setFixedSize(18, 18)\n        self.hBoxLayout.insertSpacing(0, 10)\n        self.hBoxLayout.insertWidget(1, self.iconLabel, 0, Qt.AlignLeft | Qt.AlignVCenter)\n        self.window().windowIconChanged.connect(self.setIcon)\n\n        # add title label\n        self.titleLabel = QLabel(self)\n        self.hBoxLayout.insertWidget(2, self.titleLabel, 0, Qt.AlignLeft | Qt.AlignVCenter)\n        self.titleLabel.setObjectName('titleLabel')\n        self.window().windowTitleChanged.connect(self.setTitle)\n\n        self.vBoxLayout = QVBoxLayout()\n        self.buttonLayout = QHBoxLayout()\n        self.buttonLayout.setSpacing(0)\n        self.buttonLayout.setContentsMargins(0, 0, 0, 0)\n        self.buttonLayout.setAlignment(Qt.AlignTop)\n        self.buttonLayout.addWidget(self.minBtn)\n        self.buttonLayout.addWidget(self.maxBtn)\n        self.buttonLayout.addWidget(self.closeBtn)\n        self.vBoxLayout.addLayout(self.buttonLayout)\n        self.vBoxLayout.addStretch(1)\n        self.hBoxLayout.addLayout(self.vBoxLayout, 0)\n\n    def setTitle(self, title):\n        self.titleLabel.setText(title)\n        self.titleLabel.adjustSize()\n\n    def setIcon(self, icon):\n        self.iconLabel.setPixmap(QIcon(icon).pixmap(18, 18))"}
{"Repository": "tesla_dashcam", "input": "Support setup.py upload. className TestUploadCommand(Command) Method status Method initialize_options Method finalize_options Method run", "label": "class TestUploadCommand(Command):\n    description = \"Build and publish the package to TestPyPi.\"\n    user_options = []  # type: ignore\n\n    @staticmethod\n    def status(string):\n        print(\"\\033[1m{0}\\033[0m\".format(string))\n\n    def initialize_options(self):\n        pass\n\n    def finalize_options(self):\n        pass\n\n    def run(self):\n        try:\n            self.status(\"Removing previous builds\")\n            rmtree(os.path.join(HERE, \"dist\"))\n        except OSError:\n            pass\n\n        self.status(\"Building Source and Wheel (universal) distribution\")\n        os.system(\"{0} setup.py sdist bdist_wheel --universal\".format(sys.executable))\n\n        self.status(\"Uploading the package to TestPyPi via Twine\")\n        os.system(\n            \"twine upload --repository-url \" \"https://test.pypi.org/legacy/ dist/*\"\n        )\n\n        self.status(\"Pushing git tags\")\n        os.system(\"git tag v{0}\".format(ABOUT[\"__version__\"]))\n        os.system(\"git push --tags\")\n\n        sys.exit()"}
{"Repository": "ChineseAiDungeon", "input": "Configuration class to store the configuration of a `BertModel`. className BertConfig(object) Method from_dict Method from_json_file Method __repr__ Method to_dict Method to_json_string", "label": "class BertConfig(object):\n    def __init__(self,\n                 vocab_size_or_config_json_file,\n                 hidden_size=768,\n                 num_hidden_layers=12,\n                 num_attention_heads=12,\n                 intermediate_size=3072,\n                 hidden_act=\"gelu\",\n                 hidden_dropout_prob=0.1,\n                 attention_probs_dropout_prob=0.1,\n                 max_position_embeddings=512,\n                 type_vocab_size=2,\n                 initializer_range=0.02,\n                 deep_init=False,\n                 fp32_layernorm=False,\n                 fp32_embedding=False,\n                 fp32_tokentypes=False,\n                 layernorm_epsilon=1e-12):\n        if isinstance(vocab_size_or_config_json_file, str):\n            with open(vocab_size_or_config_json_file, \"r\", encoding='utf-8') as reader:\n                json_config = json.loads(reader.read())\n            for key, value in json_config.items():\n                self.__dict__[key] = value\n        elif isinstance(vocab_size_or_config_json_file, int):\n            self.vocab_size = vocab_size_or_config_json_file\n            self.hidden_size = hidden_size\n            self.num_hidden_layers = num_hidden_layers\n            self.num_attention_heads = num_attention_heads\n            self.hidden_act = hidden_act\n            self.intermediate_size = intermediate_size\n            self.hidden_dropout_prob = hidden_dropout_prob\n            self.attention_probs_dropout_prob = attention_probs_dropout_prob\n            self.max_position_embeddings = max_position_embeddings\n            self.type_vocab_size = type_vocab_size\n            self.initializer_range = initializer_range\n            self.deep_init = deep_init\n            self.fp32_layernorm = fp32_layernorm\n            self.fp32_embedding = fp32_embedding\n            self.layernorm_epsilon = layernorm_epsilon\n            self.fp32_tokentypes = fp32_tokentypes\n        else:\n            raise ValueError(\"First argument must be either a vocabulary size (int)\"\n                             \"or the path to a pretrained model config file (str)\")\n\n    @classmethod\n    def from_dict(cls, json_object):\n        config = BertConfig(vocab_size_or_config_json_file=-1)\n        for key, value in json_object.items():\n            config.__dict__[key] = value\n        return config\n\n    @classmethod\n    def from_json_file(cls, json_file):\n        with open(json_file, \"r\", encoding='utf-8') as reader:\n            text = reader.read()\n        return cls.from_dict(json.loads(text))\n\n    def __repr__(self):\n        return str(self.to_json_string())\n\n    def to_dict(self):\n        output = copy.deepcopy(self.__dict__)\n        return output\n\n    def to_json_string(self):\n        return json.dumps(self.to_dict(), indent=2, sort_keys=True) + \"\\n\""}
{"Repository": "cmGAN", "input": "Image Person ReID Dataset className Image_dataset(Dataset) Method __init__ Method __len__ Method __getitem__ Attribute dataset Attribute transform", "label": "class Image_dataset(Dataset):\n    def __init__(self, dataset, transform=None):\n        self.dataset = dataset\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.dataset)\n\n    def __getitem__(self, index):\n        img_path, pid, camid = self.dataset[index]\n        img = Image.open(img_path)\n        if self.transform is not None:\n            img = self.transform(img)\n        return img, pid, camid"}
{"Repository": "flowattack", "input": "Define Loss based on Geman-Mcclure className AdaptiveGemanMcclureLoss(Function) Method forward Method backward", "label": "class AdaptiveGemanMcclureLoss(Function):\n    @staticmethod\n    def forward(ctx, input, target):\n        _mad = lambda x : (x - x.median()).abs().median()\n        x = input-target\n        sigma = 1.4826 * _mad(x)\n        ctx.saved_variable = (x,sigma)\n        # ipdb.set_trace()\n        return input.new([(x**2 / (x**2 + sigma**2)).sum() / x.nelement()])\n        # return (x**2 / (x**2 + sigma**2)).sum() / x.nelement()\n\n    @staticmethod\n    def backward(ctx, grad_output=None):\n        x,sigma = ctx.saved_variable\n\n        # import ipdb;ipdb.set_trace()\n        grad = Variable(2*x*sigma**2 / ((x**2 + sigma**2)**2) / x.nelement())\n\n        return grad*grad_output,None"}
{"Repository": "crackq", "input": "Session Logout className Logout(MethodView) Method get", "label": "class Logout(MethodView):\n    @login_required\n    def get(self):\n        logger.debug('User logged out: {}'.format(current_user.username))\n        user = User.query.filter_by(username=current_user.username).first()\n        crackq.app.session_interface.destroy(session)\n        user.active = False\n        db.session.commit()\n        logout_user()\n        return 'Logged Out', 200"}
{"Repository": "Chat-UniVi", "input": "Collate examples for supervised fine-tuning. className DataCollatorForSupervisedDataset(object) Method __call__", "label": "class DataCollatorForSupervisedDataset(object):\n    tokenizer: transformers.PreTrainedTokenizer\n\n    def __call__(self, instances: Sequence[Dict]) -> Dict[str, torch.Tensor]:\n        input_ids, labels = tuple([instance[key] for instance in instances]\n                                  for key in (\"input_ids\", \"labels\"))\n        input_ids = torch.nn.utils.rnn.pad_sequence(\n            input_ids,\n            batch_first=True,\n            padding_value=self.tokenizer.pad_token_id)\n        labels = torch.nn.utils.rnn.pad_sequence(labels,\n                                                 batch_first=True,\n                                                 padding_value=IGNORE_INDEX)\n        input_ids = input_ids[:, :self.tokenizer.model_max_length]\n        labels = labels[:, :self.tokenizer.model_max_length]\n        batch = dict(\n            input_ids=input_ids,\n            labels=labels,\n            attention_mask=input_ids.ne(self.tokenizer.pad_token_id),\n        )\n\n        if 'image' in instances[0]:\n            images = [instance['image'] for instance in instances]\n\n            new_images = []\n            for image in images:\n                if type(image) is list:\n                    for i in image:\n                        new_images.append(i)\n                else:\n                    new_images.append(image)\n            images = new_images\n\n            if all(x is not None and x.shape == images[0].shape for x in images):\n                batch['images'] = torch.stack(images)\n            else:\n                batch['images'] = images\n\n        return batch"}
{"Repository": "SearchAnything", "input": "Parser for Word files (.docx) className WordParser(BaseParser) Method __init__ Method parse Method _to_sentences Method metadata Method _check_format", "label": "class WordParser(BaseParser):  \n    type = 'docx'  \n      \n    def __init__(self, file_path: str=None, model=None, out_path: str=None) -> None:  \n        super().__init__(file_path, model, out_path)  \n  \n    def parse(self) -> List[Dict]:  \n        page_sents = self._to_sentences()  \n        if not page_sents:  \n            return None  \n  \n        self.parse_output = []  \n        for pageno, sents in page_sents:  \n            for sent in sents:  \n                file_dict = {}  \n                file_dict['title'] = None\n                file_dict['author'] = None\n                file_dict['page'] = pageno  \n                file_dict['content'] = sent  \n                file_dict['embedding'] = encode_text(self.model, sent)  \n                file_dict['file_path'] = self.file_path  \n                file_dict['subject'] = None\n  \n                self.parse_output.append(file_dict)  \n  \n        return self.parse_output  \n  \n    def _to_sentences(self) -> List[Tuple[int, str]]:  \n        if not self._check_format():  \n            self.parse_output = None  \n            return []  \n  \n        word_doc = Document(self.file_path)  \n  \n        raw_text = \"\"  \n        page_text = []  \n        pageno = 1  \n        for para in word_doc.paragraphs:  \n            raw_text = para.text  \n            raw_text = raw_text.replace(\"\\n\", \" \")  \n            page_text.append((pageno, raw_text))  \n  \n        page_sents = map(lambda x: (x[0], sent_tokenize(x[1])), page_text)  \n        return page_sents  \n  \n    @property  \n    def metadata(self) -> defaultdict:  \n        if not self._metadata:  \n            # Word files don't have built-in metadata like PDFs, so we set it to empty strings\n            metadata = defaultdict(str)  \n            self._metadata = metadata  \n  \n        return self._metadata  \n  \n    def _check_format(self) -> bool:  \n        f_path = Path(self.file_path)  \n        return f_path.exists() and f_path.suffix == '.docx'"}
{"Repository": "graalpython", "input": "Test usage of __complex__() with a __new__() method className complex1(complex) Method __new__ Method __complex__ Method __complex__", "label": "class complex1(complex):\n            def __new__(self, value=0j):\n                return complex.__new__(self, 2*value)\n            def __complex__(self):\n                return self\n\n        class complex2(complex):\n            def __complex__(self):\n                return None\n\n        self.assertEqual(complex(complex0(1j)), 42j)\n        # TODO we are not able to throw warning now."}
{"Repository": "pygran", "input": "An install class that enables the tracking of installation/compilation progress className Track(install) Method execute Method print_progress Method run Method do_pre_install_stuff", "label": "class Track(install):\n    def execute(self, cmd, cwd=\".\"):\n        popen = subprocess.Popen(\n            cmd, stdout=subprocess.PIPE, universal_newlines=True, cwd=cwd, shell=True\n        )\n\n        for stdout_line in iter(popen.stdout.readline, \"\"):\n            yield stdout_line\n\n        popen.stdout.close()\n        return_code = popen.wait()\n\n        if return_code:\n            raise subprocess.CalledProcessError(return_code, cmd)\n\n    def print_progress(self, iteration, total, prefix=\"\", suffix=\"\", decimals=1):\n        str_format = \"{0:.\" + str(decimals) + \"f}\"\n        percents = str_format.format(100 * iteration / total)\n        sys.stdout.write(f\"\\r{prefix} {percents}% {suffix}\")\n        sys.stdout.flush()\n\n    def run(self):\n        self.do_pre_install_stuff()\n\n    def do_pre_install_stuff(self):\n        raise NotImplementedError"}
{"Repository": "Semantic-Segmentation-of-Remote-Sensing-Images", "input": "Just a simple bilinear upsampling layer. className BilinearUpsampling(Layer) Method __init__ Method compute_output_shape Method call Method get_config Attribute data_format Attribute input_spec Attribute output_size Attribute upsampling Attribute output_size Attribute upsampling", "label": "class BilinearUpsampling(Layer):\n    def __init__(self, upsampling=(2, 2), output_size=None, data_format=None, **kwargs):\n\n        super(BilinearUpsampling, self).__init__(**kwargs)\n\n        self.data_format = conv_utils.normalize_data_format(data_format)\n        self.input_spec = InputSpec(ndim=4)\n        if output_size:\n            self.output_size = conv_utils.normalize_tuple(output_size, 2, 'output_size')\n            self.upsampling = None\n        else:\n            self.output_size = None\n            self.upsampling = conv_utils.normalize_tuple(upsampling, 2, 'upsampling')\n\n    def compute_output_shape(self, input_shape):\n        if self.upsampling:\n            height = self.upsampling[0] * input_shape[1] if input_shape[1] is not None else None\n            width = self.upsampling[1] * input_shape[2] if input_shape[2] is not None else None\n        else:\n            height = self.output_size[0]\n            width = self.output_size[1]\n        return (input_shape[0],height,width,input_shape[3])\n\n    def call(self, inputs):\n        if self.upsampling:\n            return K.tf.image.resize_bilinear(inputs, (inputs.shape[1] * self.upsampling[0],inputs.shape[2] * self.upsampling[1]),align_corners=True)\n        else:\n            return K.tf.image.resize_bilinear(inputs, (self.output_size[0],self.output_size[1]),align_corners=True)\n\n    def get_config(self):\n        config = {'upsampling': self.upsampling,'output_size': self.output_size,'data_format': self.data_format}\n        base_config = super(BilinearUpsampling, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))"}
{"Repository": "adapter-bert", "input": "Configuration for `BertModel`. className BertConfig(object) Method from_dict Method from_json_file Method to_dict Method to_json_string", "label": "class BertConfig(object):\n  def __init__(self,\n               vocab_size,\n               hidden_size=768,\n               num_hidden_layers=12,\n               num_attention_heads=12,\n               intermediate_size=3072,\n               hidden_act=\"gelu\",\n               hidden_dropout_prob=0.1,\n               attention_probs_dropout_prob=0.1,\n               max_position_embeddings=512,\n               type_vocab_size=16,\n               initializer_range=0.02):\n    self.vocab_size = vocab_size\n    self.hidden_size = hidden_size\n    self.num_hidden_layers = num_hidden_layers\n    self.num_attention_heads = num_attention_heads\n    self.hidden_act = hidden_act\n    self.intermediate_size = intermediate_size\n    self.hidden_dropout_prob = hidden_dropout_prob\n    self.attention_probs_dropout_prob = attention_probs_dropout_prob\n    self.max_position_embeddings = max_position_embeddings\n    self.type_vocab_size = type_vocab_size\n    self.initializer_range = initializer_range\n\n  @classmethod\n  def from_dict(cls, json_object):\n    config = BertConfig(vocab_size=None)\n    for (key, value) in six.iteritems(json_object):\n      config.__dict__[key] = value\n    return config\n\n  @classmethod\n  def from_json_file(cls, json_file):\n    with tf.gfile.GFile(json_file, \"r\") as reader:\n      text = reader.read()\n    return cls.from_dict(json.loads(text))\n\n  def to_dict(self):\n    output = copy.deepcopy(self.__dict__)\n    return output\n\n  def to_json_string(self):\n    return json.dumps(self.to_dict(), indent=2, sort_keys=True) + \"\\n\""}
{"Repository": "Precision-Drawing-Tools", "input": "Use Directional, or Distance @ Angle Placement className PDT_OT_PlacementDis(Operator) Method execute", "label": "class PDT_OT_PlacementDis(Operator):\n    bl_idname = \"pdt.distance\"\n    bl_label = \"Distance@Angle Mode\"\n    bl_options = {\"REGISTER\", \"UNDO\"}\n\n    def execute(self, context):\n        pg = context.scene.pdt_pg\n        operation = pg.operation\n        decimal_places = context.preferences.addons[__package__].preferences.pdt_input_round\n\n        if operation == \"CU\":\n            # Cursor\n            pg.command = (\n                f\"ci{str(round(pg.distance, decimal_places))}\"\n                f\",{str(round(pg.angle, decimal_places))}\"\n            )\n        elif operation == \"PP\":\n            # Pivot Point\n            pg.command = (\n                f\"pi{str(round(pg.distance, decimal_places))}\"\n                f\",{str(round(pg.angle, decimal_places))}\"\n            )\n        elif operation == \"MV\":\n            # Move Entities\n            pg.command = (\n                f\"gi{str(round(pg.distance, decimal_places))}\"\n                f\",{str(round(pg.angle, decimal_places))}\"\n            )\n        elif operation == \"SE\":\n            # Split Edges\n            pg.command = (\n                f\"si{str(round(pg.distance, decimal_places))}\"\n                f\",{str(round(pg.angle, decimal_places))}\"\n                )\n        elif operation == \"NV\":\n            # New Vertex\n            pg.command = (\n                f\"ni{str(round(pg.distance, decimal_places))}\"\n                f\",{str(round(pg.angle, decimal_places))}\"\n            )\n        elif operation == \"EV\":\n            # Extrude Vertices\n            pg.command = (\n                f\"vi{str(round(pg.distance, decimal_places))}\"\n                f\",{str(round(pg.angle, decimal_places))}\"\n            )\n        elif operation == \"DG\":\n            # Duplicate Geometry\n            pg.command = (\n                f\"di{str(round(pg.distance, decimal_places))}\"\n                f\",{str(round(pg.angle, decimal_places))}\"\n            )\n        elif operation == \"EG\":\n            # Extrude Geometry\n            pg.command = (\n                f\"ei{str(round(pg.distance, decimal_places))}\"\n                f\",{str(round(pg.angle, decimal_places))}\"\n            )\n        else:\n            error_message = f\"{operation} {PDT_ERR_NON_VALID} {PDT_LAB_DIR}\"\n            self.report({\"ERROR\"}, error_message)\n        return {\"FINISHED\"}"}
{"Repository": "SceneClassify", "input": "Iterator yielding data from a Numpy array. className NumpyArrayIterator(Iterator) Method _get_batches_of_transformed_samples Method next", "label": "class NumpyArrayIterator(Iterator):\n    def __init__(self, x, y, image_data_generator,\n                 batch_size=32, shuffle=False, seed=None,\n                 data_format=None,\n                 save_to_dir=None, save_prefix='', save_format='png'):\n        if y is not None and len(x) != len(y):\n            raise ValueError('X (images tensor) and y (labels) '\n                             'should have the same length. '\n                             'Found: X.shape = %s, y.shape = %s' %\n                             (np.asarray(x).shape, np.asarray(y).shape))\n\n        if data_format is None:\n            data_format = K.image_data_format()\n        self.x = np.asarray(x, dtype=K.floatx())\n\n        if self.x.ndim != 4:\n            raise ValueError('Input data in `NumpyArrayIterator` '\n                             'should have rank 4. You passed an array '\n                             'with shape', self.x.shape)\n        channels_axis = 3 if data_format == 'channels_last' else 1\n        if self.x.shape[channels_axis] not in {1, 3, 4}:\n            warnings.warn('NumpyArrayIterator is set to use the '\n                          'data format convention \"' + data_format + '\" '\n                                                                     '(channels on axis ' + str(\n                channels_axis) + '), i.e. expected '\n                                 'either 1, 3 or 4 channels on axis ' + str(channels_axis) + '. '\n                                                                                             'However, it was passed an array with shape ' + str(\n                self.x.shape) +\n                          ' (' + str(self.x.shape[channels_axis]) + ' channels).')\n        if y is not None:\n            self.y = np.asarray(y)\n        else:\n            self.y = None\n        self.image_data_generator = image_data_generator\n        self.data_format = data_format\n        self.save_to_dir = save_to_dir\n        self.save_prefix = save_prefix\n        self.save_format = save_format\n        super(NumpyArrayIterator, self).__init__(x.shape[0], batch_size, shuffle, seed)\n\n    def _get_batches_of_transformed_samples(self, index_array):\n        batch_x = np.zeros(tuple([len(index_array)] + list(self.x.shape)[1:]),\n                           dtype=K.floatx())\n        for i, j in enumerate(index_array):\n            x = self.x[j]\n            x = self.image_data_generator.random_transform(x.astype(K.floatx()))\n            x = self.image_data_generator.standardize(x)\n            batch_x[i] = x\n        if self.save_to_dir:\n            for i, j in enumerate(index_array):\n                img = array_to_img(batch_x[i], self.data_format, scale=True)\n                fname = '{prefix}_{index}_{hash}.{format}'.format(prefix=self.save_prefix,\n                                                                  index=j,\n                                                                  hash=np.random.randint(1e4),\n                                                                  format=self.save_format)\n                img.save(os.path.join(self.save_to_dir, fname))\n        if self.y is None:\n            return batch_x\n        batch_y = self.y[index_array]\n        return batch_x, batch_y\n\n    def next(self):\n        # Keeps under lock only the mechanism which advances\n        # the indexing of each batch.\n        with self.lock:\n            index_array = next(self.index_generator)\n        # The transformation of images is not under thread lock\n        # so it can be done in parallel\n        return self._get_batches_of_transformed_samples(index_array)"}
{"Repository": "ziggurat_foundations", "input": "For tree manager className NOOP(object) Method __nonzero__", "label": "class NOOP(object):\n    def __nonzero__(self):\n        return False\n\n        # py3 compat\n\n    __bool__ = __nonzero__"}
{"Repository": "quora-python", "input": "Decorator that caches a function's return value each time it is called. className _memoized(object) Method __init__ Method __call__ Method __repr__ Attribute func Attribute cache", "label": "class _memoized(object):\n   def __init__(self, func):\n      self.func = func\n      self.cache = {}\n   def __call__(self, *args):\n      try:\n         return self.cache[args]\n      except KeyError:\n         self.cache[args] = value = self.func(*args)\n         return value\n      except TypeError:\n         # uncachable -- for instance, passing a list as an argument.\n         # Better to not cache than to blow up entirely.\n         return self.func(*args)\n   def __repr__(self):\n      return self.func.__doc__"}
{"Repository": "GraphTR", "input": "Aggregates via mean followed by matmul and non-linearity. className MeanAggregator(Layer) Method _call", "label": "class MeanAggregator(Layer):\n    def __init__(self, input_dim, output_dim, neigh_input_dim=None,\n            dropout=0., bias=False, act=tf.nn.relu,\n            name=None, concat=False, **kwargs):\n        super(MeanAggregator, self).__init__(**kwargs)\n\n        self.dropout = dropout\n        self.bias = bias\n        self.act = act\n        self.concat = concat\n\n        if neigh_input_dim is None:\n            neigh_input_dim = input_dim\n\n        if name is not None:\n            name = '/' + name\n        else:\n            name = ''\n\n        with tf.variable_scope(self.name + name + '_vars'):\n            self.vars['neigh_weights'] = glorot([neigh_input_dim, output_dim],\n                                                        name='neigh_weights')\n            self.vars['self_weights'] = glorot([input_dim, output_dim],\n                                                        name='self_weights')\n            if self.bias:\n                self.vars['bias'] = zeros([self.output_dim], name='bias')\n\n        if self.logging:\n            self._log_vars()\n\n        self.input_dim = input_dim\n        self.output_dim = output_dim\n\n    def _call(self, inputs):\n\n        self_vecs, neigh_vecs = inputs\n\n        neigh_vecs = tf.nn.dropout(neigh_vecs, 1-self.dropout)\n        self_vecs = tf.nn.dropout(self_vecs, 1-self.dropout)\n        neigh_means = tf.reduce_mean(neigh_vecs, axis=1)\n\n        # [nodes] x [out_dim]\n        from_neighs = tf.matmul(neigh_means, self.vars['neigh_weights'])\n\n        from_self = tf.matmul(self_vecs, self.vars[\"self_weights\"])\n\n        if not self.concat:\n            output = tf.add_n([from_self, from_neighs])\n        else:\n            output = tf.concat([from_self, from_neighs], axis=1)\n\n        # bias\n        if self.bias:\n            output += self.vars['bias']\n\n        return self.act(output)"}
{"Repository": "spriteworld", "input": "Class for visualising the environment based on Matplotlib. className MatplotlibUI(object) Method __init__ Method ax_image Method _setup_callbacks Method _onkeypress Method _draw_observation Method _draw_rewards Method register_callback Method update Attribute rewards Attribute rewards_bounds Attribute last_success Attribute _fig Attribute _ax_image Attribute _ax_scalar", "label": "class MatplotlibUI(object):\n  def __init__(self):\n    self.rewards = 10 * [np.nan]\n    self.rewards_bounds = [-10, 10]\n    self.last_success = None\n\n    plt.ion()\n    self._fig = plt.figure(\n        figsize=(9, 12), num='Spriteworld', facecolor='white')\n    gs = gridspec.GridSpec(2, 1, height_ratios=[3, 1])\n    self._ax_image = plt.subplot(gs[0])\n    self._ax_image.axis('off')\n\n    self._ax_scalar = plt.subplot(gs[1])\n    self._ax_scalar.spines['right'].set_visible(False)\n    self._ax_scalar.spines['top'].set_visible(False)\n    self._ax_scalar.xaxis.set_ticks_position('bottom')\n    self._ax_scalar.yaxis.set_ticks_position('left')\n    self._setup_callbacks()\n\n  @property\n  def ax_image(self):\n    return self._ax_image\n\n  def _setup_callbacks(self):\n    # Pressing escape should stop the UI\n    def _onkeypress(event):\n      if event.key == 'escape':\n        # Stop UI\n        logging.info('Pressed escape, stopping UI.')\n        plt.close(self._fig)\n        sys.exit()\n\n    self._fig.canvas.mpl_connect('key_release_event', _onkeypress)\n\n    # Disable default keyboard shortcuts\n    for key in ('keymap.fullscreen', 'keymap.home', 'keymap.back',\n                'keymap.forward', 'keymap.pan', 'keymap.zoom', 'keymap.save',\n                'keymap.quit', 'keymap.grid', 'keymap.yscale', 'keymap.xscale',\n                'keymap.all_axes'):\n      plt.rcParams[key] = ''\n\n    # Disable logging of some matplotlib events\n    log.getLogger('matplotlib').setLevel('WARNING')\n\n  def _draw_observation(self, image, action):\n    self._ax_image.clear()\n    self._ax_image.imshow(image, interpolation='none')\n    self._ax_image.set_xticks([])\n    self._ax_image.set_yticks([])\n    if action is not None:\n      self._ax_image.annotate(\n          '',\n          xycoords='axes fraction',\n          xy=action[:2],  # Start of arrow\n          xytext=action[2:],  # End of arrow\n          arrowprops={\n              'arrowstyle': '<|-',\n              'color': 'red',\n              'lw': 4,\n          })\n\n    # Indicate success\n    linewidth = 1\n    color = 'black'\n    if np.isnan(self.rewards[-1]):\n      linewidth = 8\n      color = 'green' if self.last_success else 'red'\n\n    for sp in self._ax_image.spines.values():\n      sp.set_color(color)\n      sp.set_linewidth(linewidth)\n\n  def _draw_rewards(self):\n    self._ax_scalar.clear()\n    self._ax_scalar.set_ylabel('Rewards')\n    self._ax_scalar.set_xlabel('Timestep')\n    xs = np.arange(-len(self.rewards), 0)\n    self._ax_scalar.set_xticks(xs)\n    self._ax_scalar.axhline(y=0.0, color='lightgrey', linestyle='--')\n    self._ax_scalar.stem(xs, self.rewards, basefmt=' ')\n\n    self._ax_scalar.set_xlim((xs[0] - 1.0, xs[-1] + 1.0))\n    self._ax_scalar.set_ylim(\n        (self.rewards_bounds[0] - 1.0, self.rewards_bounds[1] + 1.0))\n\n  def register_callback(self, event_name, callback):\n    self._fig.canvas.mpl_connect(event_name, callback)\n\n  def update(self, timestep, action):\n    reward = timestep.reward\n    if reward is None:\n      reward = np.nan\n    self.rewards = self.rewards[1:] + [reward]\n    self.rewards_bounds[0] = np.nanmin(\n        [np.nanmin(self.rewards), self.rewards_bounds[0]])\n    self.rewards_bounds[1] = np.nanmax(\n        [np.nanmax(self.rewards), self.rewards_bounds[1]])\n    self._draw_observation(timestep.observation['image'], action)\n    self._draw_rewards()\n    plt.show(block=False)\n\n    self.last_success = timestep.observation['success']"}
{"Repository": "pybedtools", "input": "Custom distutils command to clean the various files created by cython. className CleanCommand(Command) Method initialize_options Method finalize_options Method run", "label": "class CleanCommand(Command):\n    user_options = []\n\n    def initialize_options(self):\n        self._clean_me = []\n        self._clean_trees = []\n\n        # Add files to be protected here\n        self._clean_exclude = ['bedFile.cpp', 'fileType.cpp', 'gzstream.cpp']\n\n        for root, dirs, files in list(os.walk('pybedtools')):\n            for f in files:\n                if f in self._clean_exclude:\n                    continue\n                if os.path.splitext(f)[-1] in ('.pyc', '.so', '.o', '.pyo',\n                                               '.pyd', '.c', '.cpp', '.cxx',\n                                               '.orig'):\n                    self._clean_me.append(os.path.join(root, f))\n            for d in dirs:\n                if d == '__pycache__':\n                    self._clean_trees.append(os.path.join(root, d))\n\n        for d in ('build',):\n            if os.path.exists(d):\n                self._clean_trees.append(d)\n\n    def finalize_options(self):\n        pass\n\n    def run(self):\n        for clean_me in self._clean_me:\n            try:\n                print('removing', clean_me)\n                os.unlink(clean_me)\n            except Exception:\n                pass\n        for clean_tree in self._clean_trees:\n            try:\n                import shutil\n                print('removing directory', clean_tree)\n                shutil.rmtree(clean_tree)\n            except Exception:\n                pass"}
{"Repository": "MONet-pytorch", "input": "A template dataset class for you to implement custom datasets. className TemplateDataset(BaseDataset) Method modify_commandline_options Method __init__ Method __getitem__ Method __len__ Attribute image_paths Attribute transform", "label": "class TemplateDataset(BaseDataset):\n    @staticmethod\n    def modify_commandline_options(parser, is_train):\n        parser.add_argument('--new_dataset_option', type=float, default=1.0, help='new dataset option')\n        parser.set_defaults(max_dataset_size=10, new_dataset_option=2.0)  # specify dataset-specific default values\n        return parser\n\n    def __init__(self, opt):\n        # save the option and dataset root\n        BaseDataset.__init__(self, opt)\n        # get the image paths of your dataset;\n        self.image_paths = []  # You can call sorted(make_dataset(self.root, opt.max_dataset_size)) to get all the image paths under the directory self.root\n        # define the default transform function. You can use <base_dataset.get_transform>; You can also define your custom transform function\n        self.transform = get_transform(opt)\n\n    def __getitem__(self, index):\n        path = 'temp'    # needs to be a string\n        data_A = None    # needs to be a tensor\n        data_B = None    # needs to be a tensor\n        return {'data_A': data_A, 'data_B': data_B, 'path': path}\n\n    def __len__(self):\n        return len(self.image_paths)"}
{"Repository": "deep-cross-modal-hashing", "input": "Triplet: return one sample(s), one positive(p), one negative(n) className CrossModalTripletTrain(CrossModalTrainBase) Method __get_triplet_indexes Method __get_positive_index Method re_random_item Method get_random_item Method _get_random_triplet_index Method __getitem__", "label": "class CrossModalTripletTrain(CrossModalTrainBase):\n    def __init__(self, img_dir: str, img_names: np.ndarray, txt_matrix: np.ndarray, label_matrix: np.ndarray, batch_size,\n                 img_transform, txt_transform):\n        super(CrossModalTripletTrain, self).__init__(img_dir, img_names, txt_matrix, label_matrix, img_transform, txt_transform)\n        self.batch_size = batch_size\n        self.sim = calc_neighbor(self.get_all_label(), self.get_all_label())\n        self.triplet_indexes = self.__get_triplet_indexes()\n        self.re_random_item()\n\n    def __get_triplet_indexes(self):\n        indexes = []\n        for ind in range(self.length):\n            pos_ind = self.__get_positive_index(ind)\n            neg_ind = np.setdiff1d(np.arange(self.length), pos_ind)\n            neg_ind = np.setdiff1d(neg_ind, ind)\n            index = [pos_ind, neg_ind]\n            indexes.append(index)\n        return indexes\n\n    def __get_positive_index(self, ind):\n        current_sim = self.sim[ind]\n        index = torch.nonzero(current_sim)\n        if len(index.shape) > 1:\n            index = index.reshape(-1)\n        index = np.setdiff1d(index.numpy(), ind)\n        return index\n\n    def re_random_item(self):\n        self.random_item = []\n        for _ in range(self.length // self.batch_size):\n            random_ind = np.random.permutation(range(self.length))\n            self.random_item.append(random_ind[:self.batch_size])\n\n    def get_random_item(self, item):\n        return self.random_item[item // self.batch_size][item % self.batch_size]\n\n    def _get_random_triplet_index(self, query_ind):\n        pos_indexes = self.triplet_indexes[query_ind][0]\n        neg_indexes = self.triplet_indexes[query_ind][1]\n        pos_ind = np.random.choice(pos_indexes)\n        neg_ind = np.random.choice(neg_indexes)\n        return pos_ind, neg_ind\n\n    def __getitem__(self, item):\n        query_ind = self.get_random_item(item)\n        positive_ind, negative_ind = self._get_random_triplet_index(query_ind)\n        if self.img_read:\n            img = self.read_img(query_ind)\n            pos_img = self.read_img(positive_ind)\n            neg_img = self.read_img(negative_ind)\n        if self.txt_read:\n            txt = self.read_txt(query_ind)\n            pos_txt = self.read_txt(positive_ind)\n            neg_txt = self.read_txt(negative_ind)\n        label = torch.Tensor(self.label[query_ind])\n        label_pos = torch.Tensor(self.label[positive_ind])\n        label_neg = torch.Tensor(self.label[negative_ind])\n        query_ind = torch.from_numpy(np.array(query_ind))\n        positive_ind = torch.from_numpy(np.array(positive_ind))\n        negative_ind = torch.from_numpy(np.array(negative_ind))\n        back_dict = {'index': query_ind, 'pos_index': positive_ind, 'neg_index': negative_ind,\n                     'label': label, 'pos_label': label_pos, 'neg_label': label_neg}\n        if self.txt_read:\n            back_dict['txt'] = txt\n            back_dict['pos_txt'] = pos_txt\n            back_dict['neg_txt'] = neg_txt\n        if self.img_read:\n            back_dict['img'] = img\n            back_dict['pos_img'] = pos_img\n            back_dict['neg_img'] = neg_img\n        return back_dict"}
{"Repository": "genielibs", "input": "This stage will clean APIC controllers. className FabricClean(BaseStage) Method fabric_clean", "label": "class FabricClean(BaseStage):\n    # =================\n    # Argument Defaults\n    # =================\n    CLEANING_TIMEOUT = 90\n    RELOAD_TIMEOUT = 800\n    SLEEP_AFTER_RELOAD = 60\n\n    # ============\n    # Stage Schema\n    # ============\n    schema = {\n        Optional(\"cleaning_timeout\"): int,\n        Optional(\"reload_timeout\"): int,\n        Optional(\"sleep_after_reload\"): int,\n    }\n\n    # ==============================\n    # Execution order of Stage steps\n    # ==============================\n    exec_order = [\n        'fabric_clean',\n        'reload'\n    ]\n\n    def fabric_clean(self, steps, device, cleaning_timeout=CLEANING_TIMEOUT):\n\n        with steps.start(\"Cleaning the device\") as step:\n            result = device.api.execute_clean_controller_fabric(\n                max_time=cleaning_timeout)\n\n            if not result:\n                step.failed(\"Failed to clean the device\")\n            else:\n                step.passed(\"Successfully cleaned the device\")\n\n    def reload(self, steps, device, reload_timeout=RELOAD_TIMEOUT,\n               sleep_after_reload=SLEEP_AFTER_RELOAD):\n\n        with steps.start(\"Reloading '{dev}'\".format(dev=device.name)) as step:\n\n            reload_dialog = Dialog([\n                Statement(\n                    pattern=r\".*This command will restart this device\\, Proceed\\? \\[y\\/N\\].*\",\n                    action='sendline(y)'\n                )\n            ])\n\n            device.sendline('acidiag reboot')\n            reload_dialog.process(device.spawn)\n\n            timeout = Timeout(reload_timeout, 60)\n            while timeout.iterate():\n                timeout.sleep()\n                device.destroy()\n\n                try:\n                    device.connect(learn_hostname=True)\n                except Exception:\n                    log.info(\"{dev} is not reloaded\".format(dev=device.hostname))\n                else:\n                    log.info(\"Sleeping for '{}' seconds for '{}' to stabilize.\"\n                             .format(sleep_after_reload, device.name))\n                    time.sleep(sleep_after_reload)\n\n                    step.passed(\"{dev} has successfully reloaded\".format(dev=device.hostname))\n\n            step.failed(\"{dev} failed to reboot\".format(dev=device.hostname))"}
{"Repository": "Kiwi", "input": "Clone one case or multiple case into other plan or plans className CloneTestCaseView(View) Method post Method get Method _is_request_data_valid", "label": "class CloneTestCaseView(View):\n    template_name = \"testcases/clone.html\"\n    http_method_names = [\"get\", \"post\"]\n\n    def post(self, request):\n        if not self._is_request_data_valid(request):\n            return HttpResponseRedirect(request.META.get(\"HTTP_REFERER\", \"/\"))\n\n        # Do the clone action\n        clone_form = CloneCaseForm(request.POST)\n        clone_form.populate(case_ids=request.POST.getlist(\"case\"))\n\n        if clone_form.is_valid():\n            for tc_src in clone_form.cleaned_data[\"case\"]:\n                tc_dest = tc_src.clone(request.user, clone_form.cleaned_data[\"plan\"])\n\n            # Detect the number of items and redirect to correct one\n            if len(clone_form.cleaned_data[\"case\"]) == 1:\n                return HttpResponseRedirect(\n                    reverse(\n                        \"testcases-get\",\n                        args=[\n                            tc_dest.pk,\n                        ],\n                    )\n                )\n\n            if len(clone_form.cleaned_data[\"plan\"]) == 1:\n                test_plan = clone_form.cleaned_data[\"plan\"][0]\n                return HttpResponseRedirect(\n                    reverse(\"test_plan_url_short\", args=[test_plan.pk])\n                )\n\n            # Otherwise tell the user the clone action is successful\n            messages.add_message(\n                request, messages.SUCCESS, _(\"TestCase cloning was successful\")\n            )\n            return HttpResponseRedirect(reverse(\"plans-search\"))\n\n        # invalid form\n        messages.add_message(request, messages.ERROR, clone_form.errors)\n        return HttpResponseRedirect(request.META.get(\"HTTP_REFERER\", \"/\"))\n\n    def get(self, request):\n        if not self._is_request_data_valid(request, \"c\"):\n            return HttpResponseRedirect(request.META.get(\"HTTP_REFERER\", \"/\"))\n\n        # account for short param names in URI\n        get_params = request.GET.copy()\n        get_params.setlist(\"case\", request.GET.getlist(\"c\"))\n        del get_params[\"c\"]\n\n        clone_form = CloneCaseForm(get_params)\n        clone_form.populate(case_ids=get_params.getlist(\"case\"))\n\n        context = {\n            \"form\": clone_form,\n        }\n        return render(request, self.template_name, context)\n\n    @staticmethod\n    def _is_request_data_valid(request, field_name=\"case\"):\n        request_data = getattr(request, request.method)\n\n        if field_name not in request_data:\n            messages.add_message(\n                request, messages.ERROR, _(\"At least one TestCase is required\")\n            )\n            return False\n\n        return True"}
{"Repository": "FALCON", "input": "A FastaRecord object models a named sequence in a FASTA file. className FastaRecord(object) Method __init__ Method name Method id Method metadata Method sequence Method length Method md5 Method fromString Method __eq__ Method __ne__ Method __str__ Attribute _name Attribute _sequence Attribute _md5 Attribute _metadata", "label": "class FastaRecord(object):\n    DELIMITER = \">\"\n    COLUMNS = 60\n\n    def __init__(self, name, sequence):\n        try:\n            assert \"\\n\" not in name\n            assert \"\\n\" not in sequence\n            assert self.DELIMITER not in sequence\n            self._name = name\n            self._sequence = sequence\n            self._md5 = md5.md5(self.sequence).hexdigest()\n            self._id, self._metadata = splitFastaHeader(name)\n        except AssertionError:\n            raise ValueError(\"Invalid FASTA record data\")\n\n    @property\n    def name(self):\n        return self._name\n\n    @property\n    def id(self):\n        return self._id\n\n    @property\n    def metadata(self):\n        return self._metadata\n\n    @property\n    def sequence(self):\n        return self._sequence\n\n    @property\n    def length(self):\n        return len(self._sequence)\n\n    @property\n    def md5(self):\n        return self._md5\n\n    @classmethod\n    def fromString(cls, s):\n        try:\n            lines = s.splitlines()\n            assert len(lines) > 1\n            assert lines[0][0] == cls.DELIMITER\n            name = lines[0][1:]\n            sequence = \"\".join(lines[1:])\n            return FastaRecord(name, sequence)\n        except AssertionError:\n            raise ValueError(\"String not recognized as a valid FASTA record\")\n\n    def __eq__(self, other):\n        if isinstance(other, self.__class__):\n            return (self.name == other.name and\n                    self.sequence == other.sequence)\n        else:\n            return False\n\n    def __ne__(self, other):\n        return not self.__eq__(other)\n\n    def __str__(self):\n        return (\">%s\\n\" % self.name) + \\\n            wrap(self.sequence, self.COLUMNS)"}
{"Repository": "bert_hae", "input": "Configuration for `BertModel`. className BertConfig(object) Method from_dict Method from_json_file Method to_dict Method to_json_string", "label": "class BertConfig(object):\n  def __init__(self,\n               vocab_size,\n               hidden_size=768,\n               num_hidden_layers=12,\n               num_attention_heads=12,\n               intermediate_size=3072,\n               hidden_act=\"gelu\",\n               hidden_dropout_prob=0.1,\n               attention_probs_dropout_prob=0.1,\n               max_position_embeddings=512,\n               type_vocab_size=16,\n               initializer_range=0.02):\n    self.vocab_size = vocab_size\n    self.hidden_size = hidden_size\n    self.num_hidden_layers = num_hidden_layers\n    self.num_attention_heads = num_attention_heads\n    self.hidden_act = hidden_act\n    self.intermediate_size = intermediate_size\n    self.hidden_dropout_prob = hidden_dropout_prob\n    self.attention_probs_dropout_prob = attention_probs_dropout_prob\n    self.max_position_embeddings = max_position_embeddings\n    self.type_vocab_size = type_vocab_size\n    self.initializer_range = initializer_range\n\n  @classmethod\n  def from_dict(cls, json_object):\n    config = BertConfig(vocab_size=None)\n    for (key, value) in six.iteritems(json_object):\n      config.__dict__[key] = value\n    return config\n\n  @classmethod\n  def from_json_file(cls, json_file):\n    with tf.gfile.GFile(json_file, \"r\") as reader:\n      text = reader.read()\n    return cls.from_dict(json.loads(text))\n\n  def to_dict(self):\n    output = copy.deepcopy(self.__dict__)\n    return output\n\n  def to_json_string(self):\n    return json.dumps(self.to_dict(), indent=2, sort_keys=True) + \"\\n\""}
{"Repository": "io_pdx_mesh", "input": "Handles connection to Githubs API to get some data on releases for this repository. className Github_API(object) Method __init__ Method get_data Method refresh Attribute LATEST_VERSION Attribute LATEST_URL Attribute AT_LATEST Attribute CURRENT_VERSION Attribute api Attribute owner Attribute repo Attribute args", "label": "class Github_API(object):\n    def __init__(self):\n        self.LATEST_VERSION = None\n        self.LATEST_URL = None\n        self.AT_LATEST = None\n        self.CURRENT_VERSION = float(\".\".join(map(str, bl_info[\"version\"])))\n\n        self.api = API_URL\n        self.owner = bl_info[\"author\"]\n        self.repo = bl_info[\"project_name\"]\n        self.args = {\"owner\": self.owner, \"repo\": self.repo, \"api\": self.api}\n        self.refresh()\n\n    @staticmethod\n    def get_data(url, t):\n        req = Request(url)\n        result = urlopen(req, timeout=t)\n        result_str = result.read()\n        result.close()\n\n        return json.JSONDecoder().decode(result_str.decode())\n\n    def refresh(self, force=False):\n        recheck = True\n\n        # only check for updates once per day\n        last_check_date = IO_PDX_SETTINGS.last_update_check\n        if last_check_date is not None:\n            recheck = date.today() > datetime.strptime(last_check_date, \"%Y-%m-%d\").date()\n\n        if recheck or force:\n            start = time.time()\n\n            # get latest release data\n            releases_url = \"{api}/repos/{owner}/{repo}/releases\".format(**self.args)\n            try:\n                release_list = self.get_data(releases_url, TIMEOUT)\n            except URLError as err:\n                UPDATER_LOG.warning(\"Unable to check for update. ({})\".format(err.reason))\n                return\n            except Exception as err:\n                UPDATER_LOG.error(\"Failed on check for update. ({})\".format(err))\n                return\n            self.LATEST_RELEASE = release_list[0]\n\n            latest = release_list[0]\n\n            # store data\n            self.LATEST_VERSION = float(latest[\"tag_name\"])\n            self.LATEST_URL = latest[\"assets\"][0][\"browser_download_url\"]\n            self.LATEST_NOTES = \"{0}\\r\\nRelease version: {1}\\r\\n{2}\".format(\n                latest[\"published_at\"].split(\"T\")[0], latest[\"tag_name\"], latest[\"body\"]\n            )\n\n            # cache data to settings\n            IO_PDX_SETTINGS.github_latest_version = self.LATEST_VERSION\n            IO_PDX_SETTINGS.github_latest_url = self.LATEST_URL\n            IO_PDX_SETTINGS.github_latest_notes = self.LATEST_NOTES\n\n            IO_PDX_SETTINGS.last_update_check = str(date.today())\n            UPDATER_LOG.info(\"Checked for update. ({0:.4f} sec)\".format(time.time() - start))\n\n        else:\n            # used cached release data in settings\n            self.LATEST_VERSION = IO_PDX_SETTINGS.github_latest_version\n            self.LATEST_URL = IO_PDX_SETTINGS.github_latest_url\n            self.LATEST_NOTES = IO_PDX_SETTINGS.github_latest_notes\n\n            UPDATER_LOG.info(\"Skipped update check. (already ran today)\")\n\n        self.AT_LATEST = self.CURRENT_VERSION == self.LATEST_VERSION"}
{"Repository": "covidAsk", "input": "A single training/test example for simple sequence classification. className SquadExample(object) Method __str__ Method __repr__", "label": "class SquadExample(object):\n    def __init__(self,\n                 qas_id=None,\n                 question_text=None,\n                 paragraph_text=None,\n                 doc_words=None,\n                 orig_answer_text=None,\n                 start_position=None,\n                 end_position=None,\n                 title=\"\",\n                 doc_idx=0,\n                 par_idx=0,\n                 metadata=None):\n        self.qas_id = qas_id\n        self.question_text = question_text\n        self.paragraph_text = paragraph_text\n        self.doc_words = doc_words\n        self.orig_answer_text = orig_answer_text\n        self.start_position = start_position\n        self.end_position = end_position\n        self.title = title\n        self.doc_idx = doc_idx\n        self.par_idx = par_idx\n        self.metadata = metadata\n\n    def __str__(self):\n        return self.__repr__()\n\n    '''\n    def __repr__(self):\n        s = \"\"\n        s += \"qas_id: %s\" % (tokenization.printable_text(self.qas_id))\n        s += \", question_text: %s\" % (tokenization.printable_text(self.question_text))\n        s += \", paragraph_text: %s\" % (tokenization.printable_text(self.paragraph_text))\n        if self.start_position:\n            s += \", start_position: %d\" % (self.start_position)\n        if self.start_position:\n            s += \", end_position: %d\" % (self.end_position)\n        return s\n    '''"}
{"Repository": "XTConsistency", "input": "Output space for image-class segmentation tasks className ImageClassTask(ImageTask) Method __init__ Method norm Method plot_func Method file_loader Attribute classes", "label": "class ImageClassTask(ImageTask):\n    def __init__(self, *args, **kwargs):\n\n        self.classes = kwargs.pop(\"classes\", (3, 256, 256))\n        super().__init__(*args, **kwargs)\n\n    def norm(self, pred, target):\n        loss = F.kl_div(F.log_softmax(pred, dim=1), F.softmax(target, dim=1))\n        return loss, (loss.detach(),)\n\n    def plot_func(self, data, name, logger, resize=None):\n        _, idx = torch.max(data, dim=1)\n        idx = idx.float()/16.0\n        idx = idx.unsqueeze(1).expand(-1, 3, -1, -1)\n        logger.images(idx.clamp(min=0, max=1), name, nrow=2, resize=resize or self.resize)\n\n    def file_loader(self, path, resize=None):\n\n        data = (self.image_transform(Image.open(open(path, 'rb')))*255.0).long()\n        one_hot = torch.zeros((self.classes, data.shape[1], data.shape[2]))\n        one_hot = one_hot.scatter_(0, data, 1)\n        return one_hot"}
{"Repository": "PCRLv2", "input": "Computes and stores the average and current value className AverageMeter(object) Method __init__ Method reset Method update Attribute val Attribute avg Attribute sum Attribute count", "label": "class AverageMeter(object):\n    def __init__(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count"}
{"Repository": "drn", "input": "Computes and stores the average and current value className AverageMeter(object) Method __init__ Method reset Method update", "label": "class AverageMeter(object):\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count"}
{"Repository": "pugsql", "input": "Exception raised when syntax errors are encountered parsing PugSQL files. className ParserError(ValueError) Method __init__ Attribute token", "label": "class ParserError(ValueError):\n    token = None\n\n    def __init__(self, message, token):\n        super(ParserError, self).__init__(\n            'Error in %s:%s:%s - %s' % (\n                token.context.sqlfile,\n                token.context.line,\n                token.context.col,\n                message))\n        self.token = token"}
{"Repository": "biobear", "input": "An Indexed BCF File Reader. className BCFIndexedReader(Reader) Method __init__ Method inner Method query Attribute _bcf_reader", "label": "class BCFIndexedReader(Reader):\n    def __init__(self, path: os.PathLike):\n        self._bcf_reader = _BCFIndexedReader(str(path))\n\n    @property\n    def inner(self):\n        return self._bcf_reader\n\n    def query(self, region: str) -> pa.RecordBatchReader:\n        return self._bcf_reader.query(region)"}
{"Repository": "ceph-deploy", "input": "Platform is not supported className UnsupportedPlatform(DeployError) Method __init__ Method __str__ Attribute distro Attribute codename Attribute release", "label": "class UnsupportedPlatform(DeployError):\n    def __init__(self, distro, codename, release):\n        self.distro = distro\n        self.codename = codename\n        self.release = release\n\n    def __str__(self):\n        return '{doc}: {distro} {codename} {release}'.format(\n            doc=self.__doc__.strip(),\n            distro=self.distro,\n            codename=self.codename,\n            release=self.release,\n        )"}
{"Repository": "PyODM", "input": "Information about a node Args: version (str): Current API version task_queue_count (int): Number of tasks currently being processed or waiting to be processed total_memory (int): Amount of total RAM in the system in bytes available_memory (int): Amount of RAM available in bytes cpu_cores (int): Number of virtual CPU cores max_images (int): Maximum number of images allowed for new tasks or None if there's no limit. className NodeInfo(JsonResponse) Method __init__ Attribute version Attribute task_queue_count Attribute total_memory Attribute available_memory Attribute cpu_cores Attribute max_images Attribute max_parallel_tasks Attribute engine Attribute engine_version Attribute odm_version Attribute engine_version Attribute engine Attribute engine_version", "label": "class NodeInfo(JsonResponse):\n    def __init__(self, json):\n        self.version = json.get('version', '?')\n        self.task_queue_count = json.get('taskQueueCount')\n        self.total_memory = json.get('totalMemory')\n        self.available_memory = json.get('availableMemory')\n        self.cpu_cores = json.get('cpuCores')\n        self.max_images = json.get('maxImages')\n        self.max_parallel_tasks = json.get('maxParallelTasks')\n        self.engine = json.get('engine', '?')\n        self.engine_version = json.get('engineVersion', '?')\n        \n        # Deprecated\n        self.odm_version = json.get('odmVersion', '?')\n\n        # Guess\n        if self.engine_version == '?' and self.odm_version != '?':\n            self.engine = 'odm'\n            self.engine_version = self.odm_version"}
{"Repository": "reclist", "input": "This is a simple RecList over the EvalRS2023 dataset. className DFSessionRecList(RecList) Method hit_rate_at_100 Method mrr_at_100 Method mred_country Method being_less_wrong Method cosine_sim", "label": "class DFSessionRecList(RecList):\n    def __init__(\n        self,\n        dataset,\n        predictions,\n        model_name,\n        logger: LOGGER,\n        metadata_store: METADATA_STORE,\n        **kwargs\n    ):\n        super().__init__(\n            model_name,\n            logger,\n            metadata_store,\n            **kwargs\n        )\n        self.dataset = dataset\n        self._y_preds = predictions\n        self._y_test = kwargs.get(\"y_test\", None)\n        self._user_metadata = kwargs.get(\"user_metadata\", None)\n        if not isinstance(self._user_metadata, type(None)):\n            self._user_metadata = self._user_metadata.set_index(\"user_id\")\n        self.similarity_model = kwargs.get(\"similarity_model\", None)\n\n\n    @rec_test(test_type='HIT_RATE')\n    def hit_rate_at_100(self):\n        from reclist.metrics.standard_metrics import hit_rate_at_k\n        hr = hit_rate_at_k(self._y_preds, self._y_test, k=100)\n        return hr\n\n    @rec_test(test_type='MRR')\n    def mrr_at_100(self):\n        from reclist.metrics.standard_metrics import mrr_at_k\n\n        return mrr_at_k(self._y_preds, self._y_test, k=100)\n\n    @rec_test(test_type='MRED_COUNTRY', display_type=CHART_TYPE.BARS)\n    def mred_country(self):\n        country_list = [\"US\", \"RU\", \"DE\", \"UK\", \"PL\", \"BR\", \"FI\", \"NL\", \"ES\", \"SE\", \"UA\", \"CA\", \"FR\", \"NaN\"]\n        \n        user_countries = self._user_metadata.loc[self._y_test.index, ['country']].fillna('NaN')\n        valid_country_mask = user_countries['country'].isin(country_list)\n        y_pred_valid = self._y_preds[valid_country_mask]\n        y_test_valid = self._y_test[valid_country_mask]\n        user_countries = user_countries[valid_country_mask]\n\n        return self.miss_rate_equality_difference(y_pred_valid, y_test_valid, user_countries, 'country')\n\n    @rec_test(test_type='BEING_LESS_WRONG')\n    def being_less_wrong(self):\n        from reclist.metrics.standard_metrics import hits_at_k\n\n        hits = hits_at_k(self._y_preds, self._y_test, k=100).max(axis=2)\n        misses = (hits == False)\n        miss_gt_vectors = self.similarity_model[self._y_test.loc[misses, 'track_id'].values.reshape(-1)]\n        # we calculate the score w.r.t to the first prediction\n        miss_pred_vectors = self.similarity_model[self._y_preds.loc[misses, '0'].values.reshape(-1)]\n\n        return float(self.cosine_sim(miss_gt_vectors, miss_pred_vectors).mean())\n    \n    def cosine_sim(self, u: np.array, v: np.array) -> np.array:\n        return np.sum(u * v, axis=-1) / (np.linalg.norm(u, axis=-1) * np.linalg.norm(v, axis=-1))\n\n    def miss_rate_at_k_slice(self,\n                                   y_preds: pd.DataFrame,\n                                   y_test: pd.DataFrame,\n                                   slice_info: pd.DataFrame,\n                                   slice_key: str):\n        from reclist.metrics.standard_metrics import misses_at_k\n        # get false positives\n        m = misses_at_k(y_preds, y_test, k=100).min(axis=2)\n        # convert to dataframe\n        m = pd.DataFrame(m, columns=['mr'], index=y_test.index)\n        # grab slice info\n        m[slice_key] = slice_info[slice_key].values\n        # group-by slice and get per-slice mrr\n        return m.groupby(slice_key)['mr'].agg('mean')\n\n    def miss_rate_equality_difference(self,\n                                      y_preds: pd.DataFrame,\n                                      y_test: pd.DataFrame,\n                                      slice_info: pd.DataFrame,\n                                      slice_key: str):\n        from reclist.metrics.standard_metrics import misses_at_k\n\n        mr_per_slice = self.miss_rate_at_k_slice(y_preds, y_test, slice_info, slice_key)\n        mr = misses_at_k(y_preds, y_test, k=100).min(axis=2).mean()\n        # take negation so that higher values => better fairness\n        mred = -(mr_per_slice-mr).abs().mean()\n        res = mr_per_slice.to_dict()\n        return {'mred': mred, 'mr': mr, **res}"}
{"Repository": "NebulaSolarDash", "input": "A case-insensitive version of :class:`MultiDict` that defaults to replace the old value instead of appending it. className HeaderDict(MultiDict) Method __init__ Method __contains__ Method __delitem__ Method __getitem__ Method __setitem__ Method append Method replace Method getall Method get Method filter Attribute dict", "label": "class HeaderDict(MultiDict):\n    def __init__(self, *a, **ka):\n        self.dict = {}\n        if a or ka: self.update(*a, **ka)\n\n    def __contains__(self, key):\n        return _hkey(key) in self.dict\n\n    def __delitem__(self, key):\n        del self.dict[_hkey(key)]\n\n    def __getitem__(self, key):\n        return self.dict[_hkey(key)][-1]\n\n    def __setitem__(self, key, value):\n        self.dict[_hkey(key)] = [value if isinstance(value, unicode) else\n                                 str(value)]\n\n    def append(self, key, value):\n        self.dict.setdefault(_hkey(key), []).append(\n            value if isinstance(value, unicode) else str(value))\n\n    def replace(self, key, value):\n        self.dict[_hkey(key)] = [value if isinstance(value, unicode) else\n                                 str(value)]\n\n    def getall(self, key):\n        return self.dict.get(_hkey(key)) or []\n\n    def get(self, key, default=None, index=-1):\n        return MultiDict.get(self, _hkey(key), default, index)\n\n    def filter(self, names):\n        for name in [_hkey(n) for n in names]:\n            if name in self.dict:\n                del self.dict[name]"}
{"Repository": "addons", "input": "This class is needed in order to create OS specific wheels. className BinaryDistribution(Distribution) Method has_ext_modules", "label": "class BinaryDistribution(Distribution):\n    def has_ext_modules(self):\n        return True"}
{"Repository": "BingAds-Python-SDK", "input": "Represents a legacy Bing Ads authentication method using user name and password. className PasswordAuthentication(Authentication) Method __init__ Method user_name Method password Method enrich_headers Attribute _user_name Attribute _password", "label": "class PasswordAuthentication(Authentication):\n    def __init__(self, user_name, password):\n        self._user_name = user_name\n        self._password = password\n\n    @property\n    def user_name(self):\n        return self._user_name\n\n    @property\n    def password(self):\n        return self._password\n\n    def enrich_headers(self, headers):\n        headers['UserName'] = self.user_name\n        headers['Password'] = self.password"}
{"Repository": "ThorCryptocurrencyQuant", "input": "ECDSA signature. className Signature(object) Method __init__ Method recover_public_keys Attribute r Attribute s Attribute recovery_param", "label": "class Signature(object):\n  def __init__(self, r, s, recovery_param):\n    self.r = r\n    self.s = s\n    self.recovery_param = recovery_param\n\n  def recover_public_keys(self, hash, generator):\n    curve = generator.curve()\n    n = generator.order()\n    r = self.r\n    s = self.s\n    e = hash\n    x = r\n\n    # Compute the curve point with x as x-coordinate\n    alpha = (pow(x, 3, curve.p()) + (curve.a() * x) + curve.b()) % curve.p()\n    beta = numbertheory.square_root_mod_prime(alpha, curve.p())\n    y = beta if beta % 2 == 0 else curve.p() - beta\n\n    # Compute the public key\n    R1 = ellipticcurve.Point(curve, x, y, n)\n    Q1 = numbertheory.inverse_mod(r, n) * (s * R1 + (-e % n) * generator)\n    Pk1 = Public_key(generator, Q1)\n\n    # And the second solution\n    R2 = ellipticcurve.Point(curve, x, -y, n)\n    Q2 = numbertheory.inverse_mod(r, n) * (s * R2 + (-e % n) * generator)\n    Pk2 = Public_key(generator, Q2)\n\n    return [Pk1, Pk2]"}
{"Repository": "HDR", "input": "A wrapper of repeated dataset. className RepeatDataset(object) Method __init__ Method __getitem__ Method __len__ Attribute dataset Attribute times Attribute CLASSES Attribute PALETTE Attribute _ori_len", "label": "class RepeatDataset(object):\n    def __init__(self, dataset, times):\n        self.dataset = dataset\n        self.times = times\n        self.CLASSES = dataset.CLASSES\n        self.PALETTE = dataset.PALETTE\n        self._ori_len = len(self.dataset)\n\n    def __getitem__(self, idx):\n        return self.dataset[idx % self._ori_len]\n\n    def __len__(self):\n        return self.times * self._ori_len"}
{"Repository": "nbcelltests", "input": "Tests that state in notebook is built up correctly. className TestCumulativeRun(_TestCellTests) Method test_state", "label": "class TestCumulativeRun(_TestCellTests):\n    NBNAME = CUMULATIVE_RUN\n\n    def test_state(self):\n        # this is one long method rather individual ones because of\n        # building up state\n\n        t = self.generated_tests.TestNotebook()\n        t.setUpClass()\n\n        # check cell did not run\n        # (deliberate no cell in test, no code in test)\n        # TODO: should this be no method, as there's only empty ast in the test\n        t.setUp()\n        _assert_undefined(t, \"x\")\n        t.test_code_cell_1()\n        _assert_undefined(t, \"x\")\n        t.tearDown()\n\n        # check cell ran\n        # (%cell in test)\n        t.setUp()\n        t.test_code_cell_2()\n        t._run(\n        )\n        t.tearDown()\n\n        # check cumulative cells ran\n        t.setUp()\n        t.test_code_cell_3()\n        t._run(\n        )\n        t.tearDown()\n\n        # check cumulative cells ran (but not multiple times!)\n        t.setUp()\n        t.test_code_cell_4()\n        t._run(\n        )\n        t.tearDown()\n\n        # check test affects state\n        t.setUp()\n        t.test_code_cell_5()\n        t._run(\n        )\n        t.tearDown()\n\n        # test defaults to %cell\n        t.setUp()\n        t.test_code_cell_6()\n        t._run(\n        )\n        t.tearDown()\n\n        # deliberate no %cell; cell 8 will check it's also not subsequently run\n        # i.e. a will never be defined\n        t.setUp()\n        t.test_code_cell_7()\n        _assert_undefined(t, \"a\")\n        t.tearDown()\n\n        # check cell 7 above did not run\n        t.setUp()\n        t.test_code_cell_8()\n        t._run(\n        )\n        _assert_undefined(t, \"a\")\n        t.tearDown()\n\n        t.tearDownClass()"}
{"Repository": "GrailQA", "input": "Predictor for the :class:`~allennlp.models.semantic_parsing.atis.AtisSemanticParser` model. className AtisParserPredictor(Predictor) Method _json_to_instance", "label": "class AtisParserPredictor(Predictor):\n    @overrides\n    def _json_to_instance(self, json_dict: JsonDict) -> Instance:\n        utterance = json_dict[\"utterance\"]\n        return self._dataset_reader.text_to_instance([utterance])"}
{"Repository": "s3prl", "input": "Linearly increases learning rate from 0 to 1 over `warmup` fraction of training steps. className WarmupConstantSchedule(_LRSchedule) Method get_lr_", "label": "class WarmupConstantSchedule(_LRSchedule):\n    def get_lr_(self, progress):\n        if progress < self.warmup:\n            return progress / self.warmup\n        return 1."}
{"Repository": "yolov3-tensorrt", "input": "Helper class used to store the names of ONNX output names, corresponding to the output of a DarkNet layer and its output channels. className MajorNodeSpecs(object) Method __init__ Attribute name Attribute channels Attribute created_onnx_node Attribute created_onnx_node", "label": "class MajorNodeSpecs(object):\n    def __init__(self, name, channels):\n        self.name = name\n        self.channels = channels\n        self.created_onnx_node = False\n        if name is not None and isinstance(channels, int) and channels > 0:\n            self.created_onnx_node = True"}
{"Repository": "cotcha", "input": "Access class for ESP32 ROM bootloader className ESP32ROM(ESPLoader) Method get_chip_description Method read_efuse Method chip_id Method read_mac Method get_erase_size", "label": "class ESP32ROM(ESPLoader):\n    CHIP_NAME = \"ESP32\"\n    IS_STUB = False\n\n    DATE_REG_VALUE = 0x15122500\n\n    IROM_MAP_START = 0x400d0000\n    IROM_MAP_END   = 0x40400000\n    DROM_MAP_START = 0x3F400000\n    DROM_MAP_END   = 0x3F700000\n\n    # ESP32 uses a 4 byte status reply\n    STATUS_BYTES_LENGTH = 4\n\n    SPI_REG_BASE   = 0x60002000\n    EFUSE_REG_BASE = 0x6001a000\n\n    SPI_W0_OFFS = 0x80\n    SPI_HAS_MOSI_DLEN_REG = True\n\n    FLASH_SIZES = {\n        '1MB':0x00,\n        '2MB':0x10,\n        '4MB':0x20,\n        '8MB':0x30,\n        '16MB':0x40\n    }\n\n    BOOTLOADER_FLASH_OFFSET = 0x1000\n\n    def get_chip_description(self):\n        blk3 = self.read_efuse(3)\n        chip_version = (blk3 >> 12) & 0xF\n        pkg_version = (blk3 >> 9) & 0x07\n\n        silicon_rev = {\n            0: \"0\",\n            8: \"1\"\n        }.get(chip_version, \"(unknown 0x%x)\" % chip_version)\n\n        chip_name = {\n            0: \"ESP32D0WDQ6\",\n            1: \"ESP32D0WDQ5\",\n            2: \"ESP32D2WDQ5\",\n        }.get(pkg_version, \"unknown ESP32\")\n\n        return \"%s (revision %s)\" % (chip_name, silicon_rev)\n\n    def read_efuse(self, n):\n        return self.read_reg(self.EFUSE_REG_BASE + (4 * n))\n\n    def chip_id(self):\n        word16 = self.read_efuse(1)\n        word17 = self.read_efuse(2)\n        return ((word17 & MAX_UINT24) << 24) | (word16 >> 8) & MAX_UINT24\n\n    def read_mac(self):\n        words = [self.read_efuse(2), self.read_efuse(1)]\n        bitstring = struct.pack(\">II\", *words)\n        bitstring = bitstring[2:8]  # trim the 2 byte CRC\n        try:\n            return tuple(ord(b) for b in bitstring)\n        except TypeError:  # Python 3, bitstring elements are already bytes\n            return tuple(bitstring)\n\n    def get_erase_size(self, offset, size):\n        return size"}
{"Repository": "loxigen", "input": "Exception thrown when parse expressions don't match class; supported attributes by name are: - lineno - returns the line number of the exception text - col - returns the column number of the exception text - line - returns the line containing the exception text Example:: try: Word(nums). className ParseException(ParseBaseException) Method explain", "label": "class ParseException(ParseBaseException):\n    @staticmethod\n    def explain(exc, depth=16):\n        import inspect\n\n        if depth is None:\n            depth = sys.getrecursionlimit()\n        ret = []\n        if isinstance(exc, ParseBaseException):\n            ret.append(exc.line)\n            ret.append(' ' * (exc.col - 1) + '^')\n        ret.append(\"{0}: {1}\".format(type(exc).__name__, exc))\n\n        if depth > 0:\n            callers = inspect.getinnerframes(exc.__traceback__, context=depth)\n            seen = set()\n            for i, ff in enumerate(callers[-depth:]):\n                frm = ff[0]\n\n                f_self = frm.f_locals.get('self', None)\n                if isinstance(f_self, ParserElement):\n                    if frm.f_code.co_name not in ('parseImpl', '_parseNoCache'):\n                        continue\n                    if f_self in seen:\n                        continue\n                    seen.add(f_self)\n\n                    self_type = type(f_self)\n                    ret.append(\"{0}.{1} - {2}\".format(self_type.__module__,\n                                                      self_type.__name__,\n                                                      f_self))\n                elif f_self is not None:\n                    self_type = type(f_self)\n                    ret.append(\"{0}.{1}\".format(self_type.__module__,\n                                                self_type.__name__))\n                else:\n                    code = frm.f_code\n                    if code.co_name in ('wrapper', '<module>'):\n                        continue\n\n                    ret.append(\"{0}\".format(code.co_name))\n\n                depth -= 1\n                if not depth:\n                    break\n\n        return '\\n'.join(ret)"}
{"Repository": "fly", "input": "Monitor the scales of weights and gradients. className NormMonitor(Callback) Method __init__ Method on_before_optimizer_step Attribute layer_norm_only", "label": "class NormMonitor(Callback):\n    def __init__(self, layer_norm_only: bool = False):\n        super().__init__()\n        self.layer_norm_only = layer_norm_only\n\n    # Use on_before_optimizer_step instead of on_train_batch_start since there might be\n    # gradient accumulation and we only care about  scale when it could change (i.e., optimizer.step).\n    @rank_zero_only\n    def on_before_optimizer_step(self, trainer: Trainer, pl_module, *args: Any, **kwargs: Any) -> None:\n        if not trainer._logger_connector.should_update_logs:\n            return\n        model = pl_module.model\n        named_parameters = {}\n        if self.layer_norm_only:\n            ln_modules = (nn.LayerNorm, nn.Embedding)\n            if FastLayerNorm is not None:\n                ln_modules += (FastLayerNorm,)\n            for mn, m in model.named_modules():\n                if isinstance(m, ln_modules):\n                    for pn, p in m.named_parameters():\n                        fpn = '%s.%s' % (mn, pn) if mn else pn # full param name\n                        named_parameters[fpn] = p\n        else:\n            named_parameters = dict(model.named_parameters())\n\n        stats = {}\n        param_l1_norm, grad_l1_norm = [], []\n        for param_name, param in named_parameters.items():\n            param_abs = param.abs()\n            param_abs_mean = param_abs.mean()\n            stats[f'stats/{param_name}_max'] = param_abs.max()\n            stats[f'stats/{param_name}_mean'] = param_abs_mean\n            param_l1_norm.append(param_abs_mean * param.numel())\n            if param.grad is not None:\n                # Gradient is already unscaled by the AMP loss scaler at this point\n                # https://github.com/Lightning-AI/lightning/pull/9606\n                param_grad_abs = param.grad.abs()\n                param_grad_abs_mean = param_grad_abs.mean()\n                stats[f'stats/{param_name}_grad_max'] = param_grad_abs.max()\n                stats[f'stats/{param_name}_grad_mean'] = param_grad_abs_mean\n                grad_l1_norm.append(param_grad_abs_mean * param.grad.numel())\n        stats['total_param_l1_norm'] = torch.stack(param_l1_norm).sum()\n        if grad_l1_norm:\n            stats['total_grad_l1_norm'] = torch.stack(grad_l1_norm).sum()\n        # Sort by params name\n        stats = OrderedDict(sorted(stats.items()))\n        if trainer.loggers is not None:\n            for logger in trainer.loggers:\n                logger.log_metrics(stats, step=trainer.fit_loop.epoch_loop._batches_that_stepped)"}
{"Repository": "exam", "input": "Helper class that is itself callable, whose return values when called are configured via the tuples passed in to the constructor. className effect(list) Method __init__ Method __call__", "label": "class effect(list):\n    call_class = call\n\n    def __init__(self, *calls):\n        super(effect, self).__init__(calls)\n\n    def __call__(self, *args, **kwargs):\n        this_call = self.call_class(*args, **kwargs)\n\n        for call_obj, return_value in self:\n            if call_obj == this_call:\n                return return_value\n\n        raise TypeError('Unknown effect for: %r, %r' % (args, kwargs))"}
{"Repository": "hard-prompts-made-easy", "input": "@inproceedings{peng2015ner, title={Named Entity Recognition for Chinese Social Media with Jointly Trained Embeddings}, author={Peng, Nanyun and Dredze, Mark}, booktitle={Processings of the Conference on Empirical Methods in Natural Language Processing (EMNLP)}, pages={548-554}, year={2015}, File={https://www. className Weibo_NER(CLSProcessor) Method __init__ Method get_examples Method get_templates", "label": "class Weibo_NER(CLSProcessor):\n    def __init__(self):\n        super().__init__(\n            labels_origin = [ # TODO  NAM  NOM \n                \"PER.NAM\",  \"PER.NOM\", \"LOC.NAM\", \"LOC.NOM\", \"ORG.NAM\", \"ORG.NOM\", \"GPE.NAM\", \"GPE.NOM\", \n            ],\n            labels_mapped = [\n                \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \n            ]\n        )\n\n    def get_examples(self, data_dir, split):\n        path = os.path.join(data_dir, f\"{split}.jsonline\")\n        \n        with open(path, encoding='utf8') as f:\n            for line in f:\n                example_json = json.loads(line)\n                for span in example_json[\"span_list\"]:\n                    example = InputExample(\n                        meta = {\n                            \"context\": \"\".join(example_json[\"tokens\"]),\n                            \"entity\": \"\".join(example_json[\"tokens\"][span[\"start\"]: span[\"end\"]+1]),\n                            \"options\": self.labels_mapped,\n                        },\n                        tgt_text = self.get_label(span[\"type\"]),\n                    )\n                    examples.append(example)\n                \n        \n\n    def get_templates(self):\n        return [\n            '{context} {entity}? {options}',\n        ]"}
{"Repository": "stratis-cli", "input": "Generic Yes or No enum, for toggling modes in CI. className YesOrNo(Enum) Method __bool__ Method __str__", "label": "class YesOrNo(Enum):\n    YES = \"yes\"\n    NO = \"no\"\n\n    def __bool__(self):\n        return self is YesOrNo.YES\n\n    def __str__(self):\n        return self.value"}
{"Repository": "sensegram", "input": "A graph based on the CSR sparse matrix data structure. className CRSGraph(object) Method __init__ Method _get_or_add Method _load Method get_neighbors Method get_weight Attribute index", "label": "class CRSGraph(object):\n    def __init__(self, neighbors_fpath):\n        self._graph, self.index = self._load(neighbors_fpath) \n      \n    \n    def _get_or_add(self, dictionary, value):\n        if value not in dictionary:\n            value_idx = len(dictionary)\n            dictionary[value] = len(dictionary)\n        else:\n            value_idx = dictionary[value]\n\n        return value_idx\n\n    \n    def _load(self, neighbors_fpath):   \n        tic = time()\n        with codecs.open(neighbors_fpath, \"r\", \"utf-8\") as graph:\n            src_lst = []\n            dst_lst = []\n            data_lst = []\n            index = bidict()\n            word_dict = {}\n            for i, line in enumerate(graph):                \n                if i % 10000000 == 0 and i != 0: print(i)\n                try:\n                    src, dst, weight = line.split(\"\\t\")\n                    src = src.strip()\n                    dst = dst.strip()\n                    src_idx = self._get_or_add(index, src)\n                    dst_idx = self._get_or_add(index, dst) \n\n                    src_lst.append(int(src_idx))\n                    dst_lst.append(int(dst_idx))\n                    data_lst.append(np.int16(floor(float(weight) * WEIGHT_COEF)))\n                except:\n                    print(format_exc())\n                    print(\"Bad line:\", line)\n\n        rows = np.array(src_lst)\n        cols = np.array(dst_lst)\n        data = np.array(data_lst, dtype=np.int16)\n        graph = csr_matrix( (data, (rows, cols)), shape=(len(index),len(index)), dtype=np.int16 )       \n        print(\"Loaded in {:f} sec.\".format(time() - tic))\n\n        return graph, index \n\n    def get_neighbors(self, word):\n        idx_i = self.index[word]\n        data_i = self._graph[idx_i].data\n        nns = {self.index.inv[idx_j]: data_i[j] \n               for j, idx_j in enumerate(self._graph[idx_i].indices)}\n        \n        return nns\n       \n    def get_weight(self, word_i, word_j):\n        idx_i = self.index[word_i]\n        idx_j = self.index[word_j]\n        slice_i = self._graph[idx_i]\n        r = np.where(slice_i.indices == idx_j)[0]\n        if r.size > 0:\n            return slice_i.data[r[0]]\n        else:\n            return 0.0"}
{"Repository": "DL-Art-School", "input": "Sampler that restricts data loading to a subset of the dataset. className DistIterSampler(Sampler) Method __init__ Method __iter__ Method __len__ Method set_epoch Attribute dataset Attribute num_replicas Attribute rank Attribute epoch Attribute num_samples Attribute total_size", "label": "class DistIterSampler(Sampler):\n    def __init__(self, dataset, num_replicas=None, rank=None, ratio=100):\n        if num_replicas is None:\n            if not dist.is_available():\n                raise RuntimeError(\"Requires distributed package to be available\")\n            num_replicas = dist.get_world_size()\n        if rank is None:\n            if not dist.is_available():\n                raise RuntimeError(\"Requires distributed package to be available\")\n            rank = dist.get_rank()\n        self.dataset = dataset\n        self.num_replicas = num_replicas\n        self.rank = rank\n        self.epoch = 0\n        self.num_samples = int(math.ceil(len(self.dataset) * ratio / self.num_replicas))\n        self.total_size = self.num_samples * self.num_replicas\n\n    def __iter__(self):\n        # deterministically shuffle based on epoch\n        g = torch.Generator()\n        g.manual_seed(self.epoch)\n        indices = torch.randperm(self.total_size, generator=g).tolist()\n\n        dsize = len(self.dataset)\n        indices = [v % dsize for v in indices]\n\n        # subsample\n        indices = indices[self.rank:self.total_size:self.num_replicas]\n        assert len(indices) == self.num_samples\n\n        return iter(indices)\n\n    def __len__(self):\n        return self.num_samples\n\n    def set_epoch(self, epoch):\n        self.epoch = epoch"}
{"Repository": "flask-restapi-recipe", "input": "Cake class that defines how cake object are kept in the database. className Cake(BaseModel) Method __init__ Method as_dict Method __repr__ Attribute cakename Attribute bakername Attribute price", "label": "class Cake(BaseModel):\n    id = db.Column(db.Integer, primary_key=True)\n    cakename = db.Column(db.String(80), unique=True)\n    bakername = db.Column(db.String(120), unique=False)\n    price = db.Column(db.Float)\n\n    def __init__(self, cakename, bakername, price):\n        self.cakename = cakename\n        self.bakername = bakername\n        self.price = price\n\n    def as_dict(self):\n        #return {c.name: getattr(self, c.name) for c in self.__table__.columns} # Python 2.6 does not support this.\n        cake_dict = {}\n        for c in self.__table__.columns:\n            cake_dict[c.name] = getattr(self, c.name)\n        return cake_dict\n\n    def __repr__(self):\n        return '<Cake %r>' % self.cakename"}
{"Repository": "hdrnet", "input": "Replaces the pointwise curves in the guide by a pointwise neural net. className HDRNetPointwiseNNGuide(HDRNetCurves) Method _guide", "label": "class HDRNetPointwiseNNGuide(HDRNetCurves):\n  @classmethod\n  def _guide(cls, input_tensor, params, is_training):\n    n_guide_feats = params['guide_complexity']\n    guidemap = conv(input_tensor, n_guide_feats, 1, \n                    batch_norm=True, is_training=is_training,\n                    scope='conv1')\n    guidemap = conv(guidemap, 1, 1, activation_fn=tf.nn.sigmoid, scope='conv2')\n    guidemap = tf.squeeze(guidemap, squeeze_dims=[3,])\n    return guidemap"}
{"Repository": "pointer-generator", "input": "Vocabulary class for mapping between words and ids (integers) className Vocab(object) Method __init__ Method word2id Method id2word Method size Method write_metadata Attribute _word_to_id Attribute _id_to_word Attribute _count", "label": "class Vocab(object):\n  def __init__(self, vocab_file, max_size):\n    self._word_to_id = {}\n    self._id_to_word = {}\n    self._count = 0 # keeps track of total number of words in the Vocab\n\n    # [UNK], [PAD], [START] and [STOP] get the ids 0,1,2,3.\n    for w in [UNKNOWN_TOKEN, PAD_TOKEN, START_DECODING, STOP_DECODING]:\n      self._word_to_id[w] = self._count\n      self._id_to_word[self._count] = w\n      self._count += 1\n\n    # Read the vocab file and add words up to max_size\n    with open(vocab_file, 'r') as vocab_f:\n      for line in vocab_f:\n        pieces = line.split()\n        if len(pieces) != 2:\n          print 'Warning: incorrectly formatted line in vocabulary file: %s\\n' % line\n          continue\n        w = pieces[0]\n        if w in [SENTENCE_START, SENTENCE_END, UNKNOWN_TOKEN, PAD_TOKEN, START_DECODING, STOP_DECODING]:\n          raise Exception('<s>, </s>, [UNK], [PAD], [START] and [STOP] shouldn\\'t be in the vocab file, but %s is' % w)\n        if w in self._word_to_id:\n          raise Exception('Duplicated word in vocabulary file: %s' % w)\n        self._word_to_id[w] = self._count\n        self._id_to_word[self._count] = w\n        self._count += 1\n        if max_size != 0 and self._count >= max_size:\n          print \"max_size of vocab was specified as %i; we now have %i words. Stopping reading.\" % (max_size, self._count)\n          break\n\n    print \"Finished constructing vocabulary of %i total words. Last word added: %s\" % (self._count, self._id_to_word[self._count-1])\n\n  def word2id(self, word):\n    if word not in self._word_to_id:\n      return self._word_to_id[UNKNOWN_TOKEN]\n    return self._word_to_id[word]\n\n  def id2word(self, word_id):\n    if word_id not in self._id_to_word:\n      raise ValueError('Id not found in vocab: %d' % word_id)\n    return self._id_to_word[word_id]\n\n  def size(self):\n    return self._count\n\n  def write_metadata(self, fpath):\n    print \"Writing word embedding metadata file to %s...\" % (fpath)\n    with open(fpath, \"w\") as f:\n      fieldnames = ['word']\n      writer = csv.DictWriter(f, delimiter=\"\\t\", fieldnames=fieldnames)\n      for i in xrange(self.size()):\n        writer.writerow({\"word\": self._id_to_word[i]})"}
{"Repository": "pyphysim", "input": "Concrete class derived from IASolverBaseClass for testing purposes. className IASolverBaseClassConcret(IASolverBaseClass) Method solve", "label": "class IASolverBaseClassConcret(IASolverBaseClass):\n    def solve(self, Ns, P=None):\n        pass  # pragma: nocover"}
{"Repository": "python-miio", "input": "Describes a property exposed by the device. className PropertyDescriptor(Descriptor) Method __cli_output__", "label": "class PropertyDescriptor(Descriptor):\n    #: Name of the attribute in the status container that contains the value.\n    status_attribute: str\n    #: Sensors are read-only and settings are (usually) read-write.\n    access: AccessFlags = attr.ib(default=AccessFlags.Read)\n\n    #: Constraint type defining the allowed values for an integer property.\n    constraint: PropertyConstraint = attr.ib(default=PropertyConstraint.Unset)\n    #: Callable to set the value of the property.\n    setter: Optional[Callable] = attr.ib(default=None, repr=False)\n    #: Name of the method in the device class that can be used to set the value.\n    #: If set, the callable with this name will override the `setter` attribute.\n    setter_name: Optional[str] = attr.ib(default=None, repr=False)\n\n    @property\n    def __cli_output__(self) -> str:\n        s = super().__cli_output__\n\n        if self.setter:\n            s += f\"\\tSetter: {self.setter}\\n\"\n        if self.setter_name:\n            s += f\"\\tSetter Name: {self.setter_name}\\n\"\n        if self.constraint:\n            s += f\"\\tConstraint: {self.constraint}\\n\"\n\n        return s"}
{"Repository": "score_sde_fast_sampling", "input": "The ancestral sampling predictor. Currently only supports VE/VP SDEs. className AncestralSamplingPredictor(Predictor) Method vesde_update_fn Method vpsde_update_fn Method update_fn", "label": "class AncestralSamplingPredictor(Predictor):\n  def __init__(self, sde, score_fn, shape=None, probability_flow=False, eps=1e-3, abstol = 1e-2, reltol = 1e-2, \n    error_use_prev=True, norm = \"L2_scaled\", safety = .9, sde_improved_euler=True, extrapolation = True, exp=0.9):\n    super().__init__(sde, score_fn, probability_flow)\n    if not isinstance(sde, sde_lib.VPSDE) and not isinstance(sde, sde_lib.VESDE):\n      raise NotImplementedError(f\"SDE class {sde.__class__.__name__} not yet supported.\")\n    assert not probability_flow, \"Probability flow not supported by ancestral sampling\"\n\n  def vesde_update_fn(self, rng, x, t, h=None, x_prev=None):\n    sde = self.sde\n    timestep = (t * (sde.N - 1) / sde.T).astype(jnp.int32)\n    sigma = sde.discrete_sigmas[timestep]\n    adjacent_sigma = jnp.where(timestep == 0, jnp.zeros(t.shape), sde.discrete_sigmas[timestep - 1])\n    score = self.score_fn(x, t)\n    x_mean = x + batch_mul(score, sigma ** 2 - adjacent_sigma ** 2)\n    std = jnp.sqrt((adjacent_sigma ** 2 * (sigma ** 2 - adjacent_sigma ** 2)) / (sigma ** 2))\n    noise = random.normal(rng, x.shape)\n    x = x_mean + batch_mul(std, noise)\n    return x, x_mean\n\n  def vpsde_update_fn(self, rng, x, t, h=None, x_prev=None):\n    sde = self.sde\n    timestep = (t * (sde.N - 1) / sde.T).astype(jnp.int32)\n    beta = sde.discrete_betas[timestep]\n    score = self.score_fn(x, t)\n    x_mean = batch_mul((x + batch_mul(beta, score)), 1. / jnp.sqrt(1. - beta))\n    noise = random.normal(rng, x.shape)\n    x = x_mean + batch_mul(jnp.sqrt(beta), noise)\n    return x, x_mean\n\n  def update_fn(self, rng, x, t, h=None, x_prev=None):\n    if isinstance(self.sde, sde_lib.VESDE):\n      return self.vesde_update_fn(rng, x, t, h, x_prev)\n    elif isinstance(self.sde, sde_lib.VPSDE):\n      return self.vpsde_update_fn(rng, x, t, h, x_prev)"}
{"Repository": "meta-transfer-learning", "input": "The class to generate episodes for pre-train phase. className PreDataGenerator(object) Method __init__ Method make_data_tensor Attribute num_classes Attribute img_size Attribute dim_input Attribute pretrain_class_num Attribute pretrain_batch_size Attribute pretrain_character_folders", "label": "class PreDataGenerator(object):\n    def __init__(self):\n        self.num_classes = FLAGS.way_num\n        self.img_size = (FLAGS.img_size, FLAGS.img_size)\n        self.dim_input = np.prod(self.img_size)*3\n        self.pretrain_class_num = FLAGS.pretrain_class_num\n        self.pretrain_batch_size = FLAGS.pretrain_batch_size\n        pretrain_folder = FLAGS.pretrain_folders\n\n        pretrain_folders = [os.path.join(pretrain_folder, label) for label in os.listdir(pretrain_folder) if os.path.isdir(os.path.join(pretrain_folder, label))]\n        self.pretrain_character_folders = pretrain_folders\n    \n    def make_data_tensor(self):\n        print('Generating pre-training data')\n        all_filenames_and_labels = []\n        folders = self.pretrain_character_folders\n\n        for idx, path in enumerate(folders):\n            all_filenames_and_labels += get_pretrain_images(path, idx)\n        random.shuffle(all_filenames_and_labels)\n        all_labels = [li[0] for li in all_filenames_and_labels]\n        all_filenames = [li[1] for li in all_filenames_and_labels]\n        filename_queue = tf.train.string_input_producer(tf.convert_to_tensor(all_filenames), shuffle=False)\n        label_queue = tf.train.slice_input_producer([tf.convert_to_tensor(all_labels)], shuffle=False)\n        image_reader = tf.WholeFileReader()\n        _, image_file = image_reader.read(filename_queue)\n\n        image = tf.image.decode_jpeg(image_file, channels=3)\n        image.set_shape((self.img_size[0],self.img_size[1],3))\n        image = tf.reshape(image, [self.dim_input])\n        image = tf.cast(image, tf.float32) / 255.0\n\n        num_preprocess_threads = 1\n        min_queue_examples = 256\n        batch_image_size = self.pretrain_batch_size\n        image_batch, label_batch = tf.train.batch([image, label_queue], batch_size = batch_image_size, num_threads=num_preprocess_threads,capacity=min_queue_examples + 3 * batch_image_size)\n        label_batch = tf.one_hot(tf.reshape(label_batch, [-1]), self.pretrain_class_num)\n        return image_batch, label_batch"}
{"Repository": "bosnobot", "input": "Output twisted messages to Python standard library L{logging} module. className PythonLoggingObserver(object) Method __init__ Method emit Method start Method stop Attribute logger", "label": "class PythonLoggingObserver(object):\n    def __init__(self, loggerName=\"twisted\"):\n        self.logger = logging.getLogger(loggerName)\n\n    def emit(self, eventDict):\n        if 'logLevel' in eventDict:\n            level = eventDict['logLevel']\n        elif eventDict['isError']:\n            level = logging.ERROR\n        else:\n            level = logging.INFO\n        text = textFromEventDict(eventDict)\n        if text is None:\n            return\n        self.logger.log(level, text)\n\n    def start(self):\n        addObserver(self.emit)\n\n    def stop(self):\n        removeObserver(self.emit)"}
{"Repository": "Read_Bert_Code", "input": "Processor for the MultiNLI Mismatched data set (GLUE version). className MnliMismatchedProcessor(MnliProcessor) Method get_dev_examples", "label": "class MnliMismatchedProcessor(MnliProcessor):\n    def get_dev_examples(self, data_dir):\n        return self._create_examples(\n            self._read_tsv(os.path.join(data_dir, \"dev_mismatched.tsv\")),\n            \"dev_matched\")"}
{"Repository": "asyncio-redis", "input": "Run all the tests from `RedisProtocolTest` again without a global event loop className RedisProtocolWithoutGlobalEventloopTest(RedisProtocolTest) Method setUp Method tearDown", "label": "class RedisProtocolWithoutGlobalEventloopTest(RedisProtocolTest):\n    def setUp(self):\n        super().setUp()\n\n        # Remove global loop and create a new one.\n        self._old_loop = asyncio.get_event_loop()\n        asyncio.set_event_loop(None)\n        self.loop = asyncio.new_event_loop()\n\n    def tearDown(self):\n        self.loop.close()\n        asyncio.set_event_loop(self._old_loop)\n        super().tearDown()"}
{"Repository": "pytorch-dpn-pretrained", "input": "Normalize to -1..1 in Google Inception style className LeNormalize(object) Method __call__", "label": "class LeNormalize(object):\n    def __call__(self, tensor):\n        for t in tensor:\n            t.sub_(0.5).mul_(2.0)\n        return tensor"}
{"Repository": "python-decouple", "input": "Produces a csv parser that return a list of transformed elements. className Csv(object) Method __init__ Method __call__ Attribute cast Attribute delimiter Attribute strip Attribute post_process", "label": "class Csv(object):\n    def __init__(self, cast=text_type, delimiter=',', strip=string.whitespace, post_process=list):\n        self.cast = cast\n        self.delimiter = delimiter\n        self.strip = strip\n        self.post_process = post_process\n\n    def __call__(self, value):\n        if value is None:\n            return self.post_process()\n\n        transform = lambda s: self.cast(s.strip(self.strip))\n\n        splitter = shlex(value, posix=True)\n        splitter.whitespace = self.delimiter\n        splitter.whitespace_split = True\n\n        return self.post_process(transform(s) for s in splitter)"}
{"Repository": "Faster-Mean-Shift", "input": "The dataset that processes files from the celltracking challenge. className Dataset(object) Method get_channel_axis Method image_postprocessing Method datasources_single_frame Method data_generators_single_frame Method spatial_transformation Method dataset_val_single_frame", "label": "class Dataset(object):\n    def __init__(self,\n                 image_size,\n                 base_folder='',\n                 debug_image_folder=None,\n                 data_format='channels_first',\n                 image_gaussian_blur_sigma=2.0,\n                 image_interpolator='linear',\n                 normalization_consideration_factors=(0.2, 0.1),\n                 debug_folder_prefix='',\n                 additional_scale=None):\n        self.image_size = image_size\n        self.output_size = self.image_size\n        self.base_folder = base_folder\n        self.image_base_folder = base_folder\n        self.debug_image_folder = debug_image_folder\n        self.data_format = data_format\n        self.image_gaussian_blur_sigma = image_gaussian_blur_sigma\n        self.image_interpolator = image_interpolator\n        self.additional_scale = additional_scale\n        if self.additional_scale is None:\n            self.additional_scale = [1, 1]\n        self.normalization_consideration_factors = normalization_consideration_factors\n        self.debug_folder_prefix = debug_folder_prefix\n        self.dim = 2\n\n    def get_channel_axis(self, image, data_format):\n        if len(image.shape) == 3:\n            return 0 if data_format == 'channels_first' else 2\n        if len(image.shape) == 4:\n            return 0 if data_format == 'channels_first' else 3\n\n    def image_postprocessing(self, image):\n        if self.image_gaussian_blur_sigma > 0:\n            image = gaussian(image, [self.image_gaussian_blur_sigma, self.image_gaussian_blur_sigma])\n        image_float = sitk.Cast(image, sitk.sitkFloat32)\n        image_float = normalize_robust_sitk(image_float, (-1, 1), consideration_factors=self.normalization_consideration_factors)\n        return image_float\n\n    def datasources_single_frame(self, iterator):\n        datasources_dict = {}\n        # image data source loads input image.\n        datasources_dict['image'] = CachedImageDataSource(self.image_base_folder, 't', '', '.tif',\n                                                          set_identity_spacing=True,\n                                                          preprocessing=self.image_postprocessing,\n                                                          sitk_pixel_type=sitk.sitkUInt16,\n                                                          cache_maxsize=512,\n                                                          name='image',\n                                                          parents=[iterator])\n\n        return datasources_dict\n\n    def data_generators_single_frame(self, dim, datasources, image_transformation, image_post_processing):\n        image_size = self.image_size\n        data_generators_dict = {}\n        data_generators_dict['image'] = ImageGenerator(dim, image_size,\n                                                       interpolator=self.image_interpolator,\n                                                       post_processing_np=image_post_processing,\n                                                       data_format=self.data_format,\n                                                       resample_default_pixel_value=-1,\n                                                       name='image',\n                                                       parents=[datasources['image'], image_transformation])\n\n        return data_generators_dict\n\n\n    def spatial_transformation(self, image):\n        return composite.Composite(self.dim,\n                                   [translation.InputCenterToOrigin(self.dim),\n                                    scale.Fit(self.dim, self.image_size),\n                                    scale.Fixed(self.dim, self.additional_scale),\n                                    translation.OriginToOutputCenter(self.dim, self.image_size)],\n                                   name='image',\n                                   kwparents={'image': image})\n\n    def dataset_val_single_frame(self):\n        sources = self.datasources_single_frame('id_dict')\n        image_key = 'image'\n        image_transformation = self.spatial_transformation(sources[image_key])\n        generators = self.data_generators_single_frame(2, sources, image_transformation, None)\n\n        return GraphDataset(data_generators=list(generators.values()),\n                            data_sources=list(sources.values()),\n                            transformations=[image_transformation],\n                            iterator='id_dict',\n                            debug_image_folder=self.debug_image_folder)"}
{"Repository": "pyroomacoustics", "input": "Helper class to determine the pybind11 include path The purpose of this class is to postpone importing pybind11 until it is actually installed, so that the ``get_include()`` method can be invoked. className get_pybind_include(object) Method __init__ Method __str__ Attribute user", "label": "class get_pybind_include(object):\n    def __init__(self, user=False):\n        self.user = user\n\n    def __str__(self):\n        import pybind11\n\n        return pybind11.get_include(self.user)"}
{"Repository": "cola", "input": "Each combiner is a ``list``. className MergeAggregator(Aggregator) Method create_combiner Method merge_combiner Method merge_val", "label": "class MergeAggregator(Aggregator):\n    def create_combiner(self, val):\n        return [val, ]\n    \n    def merge_combiner(self, combiner1, combiner2):\n        combiner1.extend(combiner2)\n        return combiner1\n            \n    def merge_val(self, combiner, val):\n        combiner.append(val)\n        return combiner"}
{"Repository": "malort", "input": "Mapping of types/statistics to Redshift Column Types className RedshiftMapper(AbstractMapper) Method booleans Method strings Method ints Method floats Method dates", "label": "class RedshiftMapper(AbstractMapper):\n    @staticmethod\n    def booleans(stat):\n        return \"BOOLEAN\"\n\n    @staticmethod\n    def strings(stat):\n        trues = ['TRUE', 't', 'true', 'y', 'yes']\n        falses = ['FALSE', 'f', 'false', 'n', 'no']\n        matcher_bool = []\n        for entry in stat['sample']:\n            if entry in trues or entry in falses:\n                matcher_bool.append(True)\n            else:\n                matcher_bool.append(False)\n        if all(matcher_bool):\n            return \"BOOLEAN\"\n        else:\n            if stat['min'] == stat['max'] == int(stat['mean']):\n                return 'char({})'.format(stat['max'])\n            elif stat['max'] > 65535:\n                return 'Too large for any char column!'\n            else:\n                return 'varchar({})'.format(stat['max'])\n\n    @staticmethod\n    def ints(stat):\n        if stat['min'] > -32768 and stat['max'] < 32767:\n            return 'SMALLINT'\n        elif stat['min'] > -2147483648 and stat['max'] < 2147483647:\n            return 'INTEGER'\n        else:\n            return 'BIGINT'\n\n    @staticmethod\n    def floats(stat):\n        if (stat['fixed_length'] and stat['max_precision'] <= 38\n            and stat['max_scale'] <= 37):\n            return 'decimal({}, {})'.format(stat['max_precision'],\n                                            stat['max_scale'])\n        else:\n            if stat['max_precision'] <= 6:\n                return 'REAL'\n            else:\n                return 'FLOAT'\n\n    @staticmethod\n    def dates(stat):\n        return \"TIMESTAMP\""}
{"Repository": "esphome-docs", "input": "a MyDict is like a dict except that when you set an item, before className SetObservable(dict) Method __setitem__", "label": "class SetObservable(dict):\n    def __init__(\n        self,\n        value,\n        setitem_callback=None,\n        inner_key=None,\n        original_dict=None,\n        *args,\n        **kwargs,\n    ):\n        super(SetObservable, self).__init__(value, *args, **kwargs)\n        self._setitem_callback = setitem_callback\n        self.inner_key = inner_key\n        self.original_dict = original_dict\n\n    def __setitem__(self, key, value):\n        if self._setitem_callback:\n            self._setitem_callback(self.inner_key, self.original_dict, key, value)\n        super(SetObservable, self).__setitem__(key, value)"}
{"Repository": "ShapeKeyTransferBlender", "input": "Move items up and down, add and remove className SKT_OT_actions(Operator) Method invoke", "label": "class SKT_OT_actions(Operator):\n    bl_idname = \"customshapekeylist.list_action\"\n    bl_label = \"List Actions\"\n    bl_description = \"Move items up and down, add and remove\"\n    bl_options = {'REGISTER'}\n\n    action : bpy.props.EnumProperty(\n        items=(\n            ('UP', \"Up\", \"\"),\n            ('DOWN', \"Down\", \"\"),\n            ('REMOVE', \"Remove\", \"\"),\n            ('ADD', \"Add\", \"\"),\n            ('DEFAULT', \"Default\", \"\")))    \n\n    def invoke(self, context, event):\n        scn = context.scene\n        idx = scn.shapekeytransfer_list_index\n\n        try:\n            item = scn.customshapekeylist[idx]\n        except IndexError:\n            pass\n        else:\n            if self.action == 'DOWN' and idx < len(scn.customshapekeylist) - 1:\n                item_next = scn.customshapekeylist[idx+1].name\n                scn.customshapekeylist.move(idx, idx+1)\n                scn.shapekeytransfer_list_index += 1\n                info = 'Item \"%s\" moved to position %d' % (item.name, scn.shapekeytransfer_list_index + 1)\n                self.report({'INFO'}, info)\n\n            elif self.action == 'UP' and idx >= 1:\n                item_prev = scn.customshapekeylist[idx-1].name\n                scn.customshapekeylist.move(idx, idx-1)\n                scn.shapekeytransfer_list_index -= 1\n                info = 'Item \"%s\" moved to position %d' % (item.name, scn.shapekeytransfer_list_index + 1)\n                self.report({'INFO'}, info)\n\n            elif self.action == 'REMOVE':\n                info = 'Item \"%s\" removed from list' % (scn.customshapekeylist[idx].name)\n                scn.shapekeytransfer_list_index -= 1\n                scn.customshapekeylist.remove(idx)\n                self.report({'INFO'}, info)\n            \n        if self.action == 'ADD':                               \n            scn = context.scene\n            item = scn.customshapekeylist.add()\n            item.name = \"key\"\n            item.obj_type = \"STRING\"\n            item.obj_id = len(scn.customshapekeylist)\n            scn.shapekeytransfer_list_index = len(scn.customshapekeylist)-1             \n            info = '\"%s\" added to list' % (item.name)\n            self.report({'INFO'}, info)\n        \n        elif self.action == 'DEFAULT':\n            for key in SKT.get_default_excluded_keys():                    \n                item = scn.customshapekeylist.add()\n                item.name = key\n                item.obj_type = \"STRING\"\n                item.obj_id = len(scn.customshapekeylist)\n                scn.shapekeytransfer_list_index = len(scn.customshapekeylist)-1\n                info = '\"%s\" added to list' % (item.name)\n                self.report({'INFO'}, info)\n        \n        return {\"FINISHED\"}"}
{"Repository": "BERT_Chinese_Classification", "input": "A single set of features of data. className InputFeatures(object) Method __init__ Attribute unique_id Attribute tokens Attribute input_ids Attribute input_mask Attribute input_type_ids", "label": "class InputFeatures(object):\n  def __init__(self, unique_id, tokens, input_ids, input_mask, input_type_ids):\n    self.unique_id = unique_id\n    self.tokens = tokens\n    self.input_ids = input_ids\n    self.input_mask = input_mask\n    self.input_type_ids = input_type_ids"}
{"Repository": "pytorch-graphsage", "input": "Samples from \"sparse 2D edgelist\", which looks like [ [0, 0, 0, 0, . className SparseUniformNeighborSampler(object) Method __init__ Method __call__ Attribute adj Attribute degrees", "label": "class SparseUniformNeighborSampler(object):\n    def __init__(self, adj,):\n        assert sparse.issparse(adj), \"SparseUniformNeighborSampler: not sparse.issparse(adj)\"\n        self.adj = adj\n        \n        idx, partial_degrees = np.unique(adj.nonzero()[0], return_counts=True)\n        self.degrees = np.zeros(adj.shape[0]).astype(int)\n        self.degrees[idx] = partial_degrees\n        \n    def __call__(self, ids, n_samples=128):\n        assert n_samples > 0, 'SparseUniformNeighborSampler: n_samples must be set explicitly'\n        is_cuda = ids.is_cuda\n        \n        ids = to_numpy(ids)\n        \n        tmp = self.adj[ids]\n        \n        sel = np.random.choice(self.adj.shape[1], (ids.shape[0], n_samples))\n        sel = sel % self.degrees[ids].reshape(-1, 1)\n        tmp = tmp[\n            np.arange(ids.shape[0]).repeat(n_samples).reshape(-1),\n            np.array(sel).reshape(-1)\n        ]\n        tmp = np.asarray(tmp).squeeze() \n        \n        tmp = Variable(torch.LongTensor(tmp))\n        \n        if is_cuda:\n            tmp = tmp.cuda()\n        \n        return tmp"}
{"Repository": "bert_cn_finetune", "input": "Configuration class to store the configuration of a `BertModel`. className ALBertConfig(object) Method from_dict Method from_json_file Method __repr__ Method to_dict Method to_json_string", "label": "class ALBertConfig(object):\n    def __init__(self,\n                 vocab_size_or_config_json_file,\n                 hidden_size=768,\n                 num_hidden_layers=12,\n                 num_attention_heads=12,\n                 intermediate_size=3072,\n                 embedding_size=128,\n                 ln_type=\"postln\",\n                 hidden_act=\"gelu\",\n                 hidden_dropout_prob=0.1,\n                 attention_probs_dropout_prob=0.1,\n                 max_position_embeddings=512,\n                 type_vocab_size=2,\n                 initializer_range=0.02):\n        if isinstance(vocab_size_or_config_json_file, str):\n            with open(vocab_size_or_config_json_file, \"r\", encoding='utf-8') as reader:\n                json_config = json.loads(reader.read())\n            for key, value in json_config.items():\n                self.__dict__[key] = value\n        elif isinstance(vocab_size_or_config_json_file, int):\n            self.vocab_size = vocab_size_or_config_json_file\n            self.hidden_size = hidden_size\n            self.num_hidden_layers = num_hidden_layers\n            self.num_attention_heads = num_attention_heads\n            self.embedding_size = embedding_size\n            self.ln_type = ln_type\n            self.hidden_act = hidden_act\n            self.intermediate_size = intermediate_size\n            self.hidden_dropout_prob = hidden_dropout_prob\n            self.attention_probs_dropout_prob = attention_probs_dropout_prob\n            self.max_position_embeddings = max_position_embeddings\n            self.type_vocab_size = type_vocab_size\n            self.initializer_range = initializer_range\n        else:\n            raise ValueError(\"First argument must be either a vocabulary size (int)\"\n                             \"or the path to a pretrained model config file (str)\")\n\n    @classmethod\n    def from_dict(cls, json_object):\n        config = BertConfig(vocab_size_or_config_json_file=-1)\n        for key, value in json_object.items():\n            config.__dict__[key] = value\n        return config\n\n    @classmethod\n    def from_json_file(cls, json_file):\n        with open(json_file, \"r\", encoding='utf-8') as reader:\n            text = reader.read()\n        return cls.from_dict(json.loads(text))\n\n    def __repr__(self):\n        return str(self.to_json_string())\n\n    def to_dict(self):\n        output = copy.deepcopy(self.__dict__)\n        return output\n\n    def to_json_string(self):\n        return json.dumps(self.to_dict(), indent=2, sort_keys=True) + \"\\n\""}
{"Repository": "DistillBERT", "input": "A single training/test example for simple sequence classification. className InputExample(object) Method __init__ Attribute guid Attribute text_a Attribute text_b Attribute label", "label": "class InputExample(object):\n  def __init__(self, guid, text_a, text_b=None, label=None):\n    self.guid = guid\n    self.text_a = text_a\n    self.text_b = text_b\n    self.label = label"}
{"Repository": "BMaskR-CNN", "input": "In training, we only care about the \"infinite stream\" of training data. className TrainingSampler(Sampler) Method __init__ Method __iter__ Method _infinite_indices Attribute _size Attribute _shuffle Attribute _seed Attribute _rank Attribute _world_size", "label": "class TrainingSampler(Sampler):\n    def __init__(self, size: int, shuffle: bool = True, seed: Optional[int] = None):\n        self._size = size\n        assert size > 0\n        self._shuffle = shuffle\n        if seed is None:\n            seed = comm.shared_random_seed()\n        self._seed = int(seed)\n\n        self._rank = comm.get_rank()\n        self._world_size = comm.get_world_size()\n\n    def __iter__(self):\n        start = self._rank\n        yield from itertools.islice(self._infinite_indices(), start, None, self._world_size)\n\n    def _infinite_indices(self):\n        g = torch.Generator()\n        g.manual_seed(self._seed)\n        while True:\n            if self._shuffle:\n                yield from torch.randperm(self._size, generator=g)\n            else:\n                yield from torch.arange(self._size)"}
{"Repository": "peewee-async", "input": "Mixin for `peewee. className AsyncPostgresqlMixin(AsyncDatabase) Method connect_params_async", "label": "class AsyncPostgresqlMixin(AsyncDatabase):\n    if psycopg2:\n        Error = psycopg2.Error\n\n    def init_async(self, conn_cls=AioPostgresqlPool,\n                   enable_json=False, enable_hstore=False):\n        if not aiopg:\n            raise Exception(\"Error, aiopg is not installed!\")\n        self._async_conn_cls = conn_cls\n        self._enable_json = enable_json\n        self._enable_hstore = enable_hstore\n\n    @property\n    def connect_params_async(self):\n        kwargs = self.connect_params.copy()\n        kwargs.update({\n            'minsize': self.min_connections,\n            'maxsize': self.max_connections,\n            'enable_json': self._enable_json,\n            'enable_hstore': self._enable_hstore,\n        })\n        return kwargs\n\n    async def last_insert_id_async(self, cursor):\n        # try:\n        #     return cursor if query_type else cursor[0][0]\n        # except (IndexError, KeyError, TypeError):\n        #     pass\n        return cursor.lastrowid"}
{"Repository": "BasicVSR_IconVSR_PyTorch", "input": "Video test dataset for DUF dataset. className VideoTestDUFDataset(VideoTestDataset) Method __getitem__", "label": "class VideoTestDUFDataset(VideoTestDataset):\n    def __getitem__(self, index):\n        folder = self.data_info['folder'][index]\n        idx, max_idx = self.data_info['idx'][index].split('/')\n        idx, max_idx = int(idx), int(max_idx)\n        border = self.data_info['border'][index]\n        lq_path = self.data_info['lq_path'][index]\n\n        select_idx = generate_frame_indices(\n            idx, max_idx, self.opt['num_frame'], padding=self.opt['padding'])\n\n        if self.cache_data:\n            if self.opt['use_duf_downsampling']:\n                # read imgs_gt to generate low-resolution frames\n                imgs_lq = self.imgs_gt[folder].index_select(\n                    0, torch.LongTensor(select_idx))\n                imgs_lq = duf_downsample(\n                    imgs_lq, kernel_size=13, scale=self.opt['scale'])\n            else:\n                imgs_lq = self.imgs_lq[folder].index_select(\n                    0, torch.LongTensor(select_idx))\n            img_gt = self.imgs_gt[folder][idx]\n        else:\n            if self.opt['use_duf_downsampling']:\n                img_paths_lq = [self.imgs_gt[folder][i] for i in select_idx]\n                # read imgs_gt to generate low-resolution frames\n                imgs_lq = read_img_seq(\n                    img_paths_lq,\n                    require_mod_crop=True,\n                    scale=self.opt['scale'])\n                imgs_lq = duf_downsample(\n                    imgs_lq, kernel_size=13, scale=self.opt['scale'])\n            else:\n                img_paths_lq = [self.imgs_lq[folder][i] for i in select_idx]\n                imgs_lq = read_img_seq(img_paths_lq)\n            img_gt = read_img_seq([self.imgs_gt[folder][idx]],\n                                  require_mod_crop=True,\n                                  scale=self.opt['scale'])\n            img_gt.squeeze_(0)\n\n        return {\n            'lq': imgs_lq,  # (t, c, h, w)\n            'gt': img_gt,  # (c, h, w)\n            'folder': folder,  # folder name\n            'idx': self.data_info['idx'][index],  # e.g., 0/99\n            'border': border,  # 1 for border, 0 for non-border\n            'lq_path': lq_path  # center frame\n        }"}
{"Repository": "deep-active-learning", "input": "Computes and stores the minimum loss value and its epoch index className RecorderMeter(object) Method __init__ Method reset Method update Method max_accuracy Method plot_curve", "label": "class RecorderMeter(object):\n    def __init__(self, total_epoch):\n        self.reset(total_epoch)\n\n    def reset(self, total_epoch):\n        assert total_epoch > 0\n        self.total_epoch = total_epoch\n        self.current_epoch = 0\n        self.epoch_losses = np.zeros((self.total_epoch, 2),\n                                     dtype=np.float32)  # [epoch, train/val]\n        self.epoch_losses = self.epoch_losses - 1\n\n        self.epoch_accuracy = np.zeros((self.total_epoch, 2),\n                                       dtype=np.float32)  # [epoch, train/val]\n        self.epoch_accuracy = self.epoch_accuracy\n\n    def update(self, idx, train_loss, train_acc, val_loss, val_acc):\n        assert idx >= 0 and idx < self.total_epoch, 'total_epoch : {} , but update with the {} index'.format(\n            self.total_epoch, idx)\n        self.epoch_losses[idx, 0] = train_loss\n        self.epoch_losses[idx, 1] = val_loss\n        self.epoch_accuracy[idx, 0] = train_acc\n        self.epoch_accuracy[idx, 1] = val_acc\n        self.current_epoch = idx + 1\n        # return self.max_accuracy(False) == val_acc\n\n    def max_accuracy(self, istrain):\n        if self.current_epoch <= 0: return 0\n        if istrain: return self.epoch_accuracy[:self.current_epoch, 0].max()\n        else: return self.epoch_accuracy[:self.current_epoch, 1].max()\n\n    def plot_curve(self, save_path):\n        title = 'the accuracy/loss curve of train/val'\n        dpi = 80\n        width, height = 1200, 800\n        legend_fontsize = 10\n        scale_distance = 48.8\n        figsize = width / float(dpi), height / float(dpi)\n\n        fig = plt.figure(figsize=figsize)\n        x_axis = np.array([i for i in range(self.total_epoch)])  # epochs\n        y_axis = np.zeros(self.total_epoch)\n\n        plt.xlim(0, self.total_epoch)\n        plt.ylim(0, 1)\n        interval_y = 0.05\n        interval_x = 5\n        plt.xticks(np.arange(0, self.total_epoch + interval_x, interval_x))\n        plt.yticks(np.arange(0, 1 + interval_y, interval_y))\n        plt.grid()\n        plt.title(title, fontsize=20)\n        plt.xlabel('the training epoch', fontsize=16)\n        plt.ylabel('accuracy', fontsize=16)\n\n        y_axis[:] = self.epoch_accuracy[:, 0]\n        plt.plot(x_axis,\n                 y_axis,\n                 color='g',\n                 linestyle='-',\n                 label='train-accuracy',\n                 lw=2)\n        plt.legend(loc=4, fontsize=legend_fontsize)\n\n        y_axis[:] = self.epoch_accuracy[:, 1]\n        plt.plot(x_axis,\n                 y_axis,\n                 color='y',\n                 linestyle='-',\n                 label='valid-accuracy',\n                 lw=2)\n        plt.legend(loc=4, fontsize=legend_fontsize)\n\n        y_axis[:] = self.epoch_losses[:, 0]\n        plt.plot(x_axis,\n                 y_axis * 50,\n                 color='g',\n                 linestyle=':',\n                 label='train-loss-x50',\n                 lw=2)\n        plt.legend(loc=4, fontsize=legend_fontsize)\n\n        y_axis[:] = self.epoch_losses[:, 1]\n        plt.plot(x_axis,\n                 y_axis * 50,\n                 color='y',\n                 linestyle=':',\n                 label='valid-loss-x50',\n                 lw=2)\n        plt.legend(loc=4, fontsize=legend_fontsize)\n\n\n        if save_path is not None:\n            fig.savefig(save_path, dpi=dpi, bbox_inches='tight')\n            print('---- save figure {} into {}'.format(title, save_path))\n        plt.close(fig)"}
{"Repository": "prjxray", "input": "Object that represents an overlay. className Overlay(object) Method __init__ Method tile_in_roi Attribute region_dict", "label": "class Overlay(object):\n    def __init__(self, region_dict):\n        self.region_dict = region_dict\n\n    def tile_in_roi(self, grid_loc):\n        x = grid_loc.grid_x\n        y = grid_loc.grid_y\n        for _, bounds in self.region_dict.items():\n            x1, x2, y1, y2 = bounds\n            if x1 <= x and x <= x2 and y1 <= y and y <= y2:\n                return False\n        return True"}
{"Repository": "pass-git-helper", "input": "Extracts a specific line number from an entry. className SpecificLineExtractor(SkippingDataExtractor) Method __init__ Method configure", "label": "class SpecificLineExtractor(SkippingDataExtractor):\n    def __init__(self, line: int, prefix_length: int, option_suffix: Text = \"\") -> None:\n        super().__init__(prefix_length, option_suffix)\n        self._line = line\n\n    def configure(self, config: configparser.SectionProxy) -> None:\n        super().configure(config)\n        self._line = config.getint(\n            \"line{suffix}\".format(suffix=self._option_suffix), fallback=self._line\n        )\n\n    def _get_raw(\n        self, entry_name: Text, entry_lines: Sequence[Text]  # noqa: ARG002\n    ) -> Optional[Text]:\n        if len(entry_lines) > self._line:\n            return entry_lines[self._line]\n        else:\n            return None"}
{"Repository": "OctConv", "input": "ResNetV2 BottleneckV2 className _BottleneckV2(HybridBlock) Method _sum Method hybrid_forward", "label": "class _BottleneckV2(HybridBlock):\n    # pylint: disable=unused-argument\n    def __init__(self, in_planes, mid_planes, out_planes, groups=1, strides=1,\n                 norm_kwargs=None, name_prefix=None,\n                 down_pos=0, use_se=False, se_planes=-1, **kwargs):\n        super(_BottleneckV2, self).__init__(prefix=name_prefix)\n        assert down_pos in [0, 1, 2], \\\n            \"down_pos value({}) is unknown.\".format(down_pos)\n        strides1 = strides if down_pos == 0 else 1\n        strides2 = strides if down_pos == 1 else 1\n        strides3 = strides if down_pos == 2 else 1\n        with self.name_scope():\n            # extract information\n            self.bn1 = nn.BatchNorm(in_channels=in_planes, prefix='bn1',\n                                  **({} if norm_kwargs is None else norm_kwargs))\n            self.relu1 = nn.Activation('relu')\n            self.conv1 = nn.Conv2D(channels=mid_planes, in_channels=in_planes,\n                                  kernel_size=1, use_bias=False, strides=strides1,\n                                  prefix='conv1')\n            # capture spatial relations\n            self.bn2 = nn.BatchNorm(in_channels=mid_planes, prefix='bn2',\n                                  **({} if norm_kwargs is None else norm_kwargs))\n            self.relu2 = nn.Activation('relu')\n            self.conv2 = nn.Conv2D(channels=mid_planes, in_channels=mid_planes,\n                                  kernel_size=3, padding=1, groups=groups,\n                                  strides=strides2, use_bias=False, prefix='conv2')\n            # embeding back to information highway\n            self.bn3 = nn.BatchNorm(in_channels=mid_planes, prefix='bn3',\n                                  **({} if norm_kwargs is None else norm_kwargs))\n            self.relu3 = nn.Activation('relu')\n            self.conv3 = nn.Conv2D(channels=out_planes, in_channels=mid_planes,\n                                  kernel_size=1, use_bias=False, strides=strides3,\n                                  prefix='conv3')\n\n            self.se_block = nn.SE(in_channels=out_planes, channels=se_planes,\n                                  prefix='se') if use_se else None\n\n            if strides != 1 or in_planes != out_planes:\n                self.bn4 = nn.BatchNorm(in_channels=in_planes, prefix='bn4',\n                                  **({} if norm_kwargs is None else norm_kwargs))\n                self.relu4 = nn.Activation('relu')\n                self.conv4 = nn.Conv2D(channels=out_planes, in_channels=in_planes,\n                                  kernel_size=1, strides=strides,\n                                  prefix='conv4')\n\n    def _sum(self, x1, x2):\n        if (x1 is not None) and (x2 is not None):\n            return x1 + x2\n        else:\n            return x1 if x2 is None else x2\n\n    def hybrid_forward(self, F, x1, x2=None):\n        x = (x1, x2)\n        shortcut = x\n\n        out = self.conv1(*self.relu1(*self.bn1(*x)))\n        out = self.conv2(*self.relu2(*self.bn2(*out)))\n        out = self.conv3(*self.relu3(*self.bn3(*out)))\n\n        out = out if self.se_block is None else self.se_block(*out)\n\n        if hasattr(self, 'conv4'):\n            shortcut = self.conv4(*self.relu4(*self.bn4(*x)))\n\n        out = (self._sum(out[0], shortcut[0]), self._sum(out[1], shortcut[1]))\n        return out"}
{"Repository": "python-mysql-replication", "input": "Represent the values that you may report when connecting as a slave to a master. className ReportSlave(object) Method __init__ Method __repr__ Method encoded Attribute hostname Attribute username Attribute password Attribute port Attribute hostname", "label": "class ReportSlave(object):\n    hostname = \"\"\n    username = \"\"\n    password = \"\"\n    port = 0\n\n    def __init__(self, value):\n        if isinstance(value, (tuple, list)):\n            try:\n                self.hostname = value[0]\n                self.username = value[1]\n                self.password = value[2]\n                self.port = int(value[3])\n            except IndexError:\n                pass\n        elif isinstance(value, dict):\n            for key in [\"hostname\", \"username\", \"password\", \"port\"]:\n                try:\n                    setattr(self, key, value[key])\n                except KeyError:\n                    pass\n        else:\n            self.hostname = value\n\n    def __repr__(self):\n        return \"<ReportSlave hostname=%s username=%s password=%s port=%d>\" % (\n            self.hostname,\n            self.username,\n            self.password,\n            self.port,\n        )\n\n    def encoded(self, server_id, master_id=0):\n        # 1              [15] COM_REGISTER_SLAVE\n        # 4              server-id\n        # 1              slaves hostname length\n        # string[$len]   slaves hostname\n        # 1              slaves user len\n        # string[$len]   slaves user\n        # 1              slaves password len\n        # string[$len]   slaves password\n        # 2              slaves mysql-port\n        # 4              replication rank\n        # 4              master-id\n\n        lhostname = len(self.hostname.encode())\n        lusername = len(self.username.encode())\n        lpassword = len(self.password.encode())\n\n        packet_len = (\n            1\n            + 4  # command\n            + 1  # server-id\n            + lhostname  # hostname length\n            + 1\n            + lusername  # username length\n            + 1\n            + lpassword  # password length\n            + 2\n            + 4  # slave mysql port\n            + 4  # replication rank\n        )  # master-id\n\n        MAX_STRING_LEN = 257  # one byte for length + 256 chars\n\n        return (\n            struct.pack(\"<i\", packet_len)\n            + bytes(bytearray([COM_REGISTER_SLAVE]))\n            + struct.pack(\"<L\", server_id)\n            + struct.pack(\n                f\"<{min(MAX_STRING_LEN, lhostname + 1)}p\", self.hostname.encode()\n            )\n            + struct.pack(\n                f\"<{min(MAX_STRING_LEN, lusername + 1)}p\", self.username.encode()\n            )\n            + struct.pack(\n                f\"<{min(MAX_STRING_LEN, lpassword + 1)}p\", self.password.encode()\n            )\n            + struct.pack(\"<H\", self.port)\n            + struct.pack(\"<l\", 0)\n            + struct.pack(\"<l\", master_id)\n        )"}
{"Repository": "avionix", "input": ":param open_apiv3_schema: openAPIV3Schema is the OpenAPI v3 schema to use for \\ validation and pruning. className CustomResourceValidation(HelmYaml) Method __init__ Attribute openAPIV3Schema", "label": "class CustomResourceValidation(HelmYaml):\n    def __init__(self, open_apiv3_schema: JSONSchemaProps):\n        self.openAPIV3Schema = open_apiv3_schema"}
{"Repository": "batter", "input": "Middleware that requires a user to be authenticated to view any page other than LOGIN_URL. className LoginRequiredMiddleware(object) Method process_request", "label": "class LoginRequiredMiddleware(object):\n    def process_request(self, request):\n        assert hasattr(request, 'user')\n        if not request.user.is_authenticated():\n            path = request.path_info.lstrip('/')\n            if not any(m.match(path) for m in EXEMPT_URLS):\n                return HttpResponseRedirect(settings.LOGIN_URL)"}
{"Repository": "HAP-python", "input": "Fake garage door. className GarageDoor(Accessory) Method __init__ Method change_state", "label": "class GarageDoor(Accessory):\n    category = CATEGORY_GARAGE_DOOR_OPENER\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n\n        self.add_preload_service('GarageDoorOpener')\\\n            .configure_char(\n                'TargetDoorState', setter_callback=self.change_state)\n\n    def change_state(self, value):\n        logging.info(\"Bulb value: %s\", value)\n        self.get_service('GarageDoorOpener')\\\n            .get_characteristic('CurrentDoorState')\\\n            .set_value(value)"}
{"Repository": "tf-quant-finance", "input": "This class is needed in order to create OS specific wheels. className BinaryDistribution(Distribution) Method has_ext_modules", "label": "class BinaryDistribution(Distribution):\n  def has_ext_modules(self):\n    return False"}
{"Repository": "benchmark_VAE", "input": "A proposed fully Convolutional decoder for VQVAE. className Fully_Conv_Decoder_Conv_AE_MNIST(BaseDecoder) Method __init__ Method forward Attribute input_dim Attribute n_channels Attribute layers Attribute depth", "label": "class Fully_Conv_Decoder_Conv_AE_MNIST(BaseDecoder):\n    def __init__(self, args: dict):\n        BaseDecoder.__init__(self)\n        self.input_dim = (1, 28, 28)\n        self.n_channels = 1\n\n        layers = nn.ModuleList()\n\n        layers.append(\n            nn.Sequential(\n                nn.ConvTranspose2d(1, 1, 3, 2, padding=1),\n                # nn.BatchNorm2d(512),\n                # nn.ReLU(),\n            )\n        )\n\n        layers.append(\n            nn.Sequential(\n                nn.ConvTranspose2d(1, 1, 3, 2, padding=1, output_padding=1),\n                # nn.BatchNorm2d(256),\n                # nn.ReLU(),\n            )\n        )\n\n        layers.append(\n            nn.Sequential(\n                nn.ConvTranspose2d(\n                    1, self.n_channels, 3, 2, padding=1, output_padding=1\n                ),\n                nn.Sigmoid(),\n            )\n        )\n\n        self.layers = layers\n        self.depth = len(layers)\n\n    def forward(self, z: torch.Tensor, output_layer_levels: List[int] = None):\n        output = ModelOutput()\n\n        max_depth = self.depth\n\n        z = z.reshape(-1, 1, 4, 4)\n\n        if output_layer_levels is not None:\n\n            assert all(\n                self.depth >= levels > 0 or levels == -1\n                for levels in output_layer_levels\n            ), (\n                f\"Cannot output layer deeper than depth ({self.depth}).\"\n                f\"Got ({output_layer_levels})\"\n            )\n\n            if -1 in output_layer_levels:\n                max_depth = self.depth\n            else:\n                max_depth = max(output_layer_levels)\n\n        out = z\n\n        for i in range(max_depth):\n            out = self.layers[i](out)\n\n            if output_layer_levels is not None:\n                if i + 1 in output_layer_levels:\n                    output[f\"reconstruction_layer_{i+1}\"] = out\n\n            if i + 1 == self.depth:\n                output[\"reconstruction\"] = out\n\n        return output"}
{"Repository": "pyannotate", "input": "Internal representation of Tuple type. className TupleType(object) Method __init__ Method __repr__ Method __hash__ Method __eq__ Method __ne__ Attribute val_types", "label": "class TupleType(object):\n    def __init__(self, val_types):\n        #  type: (List[InternalType]) -> None\n        self.val_types = val_types\n\n    def __repr__(self):\n        # type: () -> str\n        return 'Tuple[%s]' % ', '.join([name_from_type(vt) for vt in self.val_types])\n\n    def __hash__(self):\n        # type: () -> int\n        return _my_hash(self.val_types)\n\n    def __eq__(self, other):\n        # type: (object) -> bool\n        if not isinstance(other, TupleType):\n            return False\n\n        if len(self.val_types) != len(other.val_types):\n            return False\n\n        for i, v in enumerate(self.val_types):\n            if v != other.val_types[i]:\n                return False\n\n        return True\n\n    def __ne__(self, other):\n        # type: (object) -> bool\n        return not self.__eq__(other)"}
{"Repository": "TDAN-VSR-CVPR-2020", "input": "convert ndarrays in sample to Tensors. className ToTensor(object) Method __call__", "label": "class ToTensor(object):\n    def __call__(self, sample):\n        lr, hr, name = sample['lr']/255.0 - 0.5, sample['hr']/255.0 - 0.5, sample['im_name']\n        lr = torch.from_numpy(lr).float()\n        hr = torch.from_numpy(hr).float()\n        return {'lr': lr.permute(0, 3, 1, 2), 'hr': hr.permute(2, 0, 1), 'im_name':name}"}
{"Repository": "python-documentcloud", "input": "A document returned by the API. className Document(BaseAPIObject) Method __init__ Method put Method save Method delete Method _lazy_load Method get_contributor Method get_contributor_organization Method set_data Method get_data Method get_annotations Method get_sections Method get_entities Method _get_url Attribute __dict__ Attribute resources Attribute mentions Attribute created_at Attribute updated_at", "label": "class Document(BaseAPIObject):\n    def __init__(self, d):\n        self.__dict__ = d\n        self.resources = Resource(d.get(\"resources\"))\n        self.mentions = [Mention(i) for i in d.get(\"mentions\", [])] or None\n        self.created_at = dateparser(d.get(\"created_at\"))\n        self.updated_at = dateparser(d.get(\"updated_at\"))\n\n    #\n    # Updates and such\n    #\n\n    def put(self):\n        params = dict(\n            title=self.title or '',\n            source=self.source or '',\n            description=self.description or '',\n            related_article=self.resources.related_article or '',\n            published_url=self.resources.published_url or '',\n            access=self.access,\n            data=self.data,\n        )\n        self._connection.put('documents/%s.json' % self.id, params)\n\n    def save(self):\n        self.put()\n\n    def delete(self):\n        self._connection.documents.delete(self.id)\n\n    #\n    # Lazy loaded attributes\n    #\n\n    def _lazy_load(self):\n        obj = self._connection.documents.get(id=self.id)\n        self.__dict__['contributor'] = obj.contributor\n        self.__dict__['contributor_organization'] = \\\n            obj.contributor_organization\n        self.__dict__['data'] = obj.data\n        self.__dict__['annotations'] = obj.__dict__['annotations']\n        self.__dict__['sections'] = obj.__dict__['sections']\n\n    def get_contributor(self):\n        try:\n            return self.__dict__['contributor']\n        except KeyError:\n            self._lazy_load()\n            return self.__dict__['contributor']\n    contributor = property(get_contributor)\n\n    def get_contributor_organization(self):\n        try:\n            return self.__dict__['contributor_organization']\n        except KeyError:\n            self._lazy_load()\n            return self.__dict__['contributor_organization']\n    contributor_organization = property(get_contributor_organization)\n\n    def set_data(self, data):\n        # Make sure a dict got passed it\n        if not isinstance(data, type({})):\n            raise TypeError(\"This attribute must be a dictionary.\")\n        # Set the attribute\n        self.__dict__['data'] = DocumentDataDict(data)\n\n    def get_data(self):\n        try:\n            return DocumentDataDict(self.__dict__['data'])\n        except KeyError:\n            self._lazy_load()\n            return DocumentDataDict(self.__dict__['data'])\n    data = property(get_data, set_data)\n\n    def get_annotations(self):\n        try:\n            obj_list = self.__dict__['annotations']\n            return [Annotation(i) for i in obj_list]\n        except KeyError:\n            self._lazy_load()\n            obj_list = self.__dict__['annotations']\n            return [Annotation(i) for i in obj_list]\n    annotations = property(get_annotations)\n\n    def get_sections(self):\n        try:\n            obj_list = self.__dict__['sections']\n            return [Section(i) for i in obj_list]\n        except KeyError:\n            self._lazy_load()\n            obj_list = self.__dict__['sections']\n            return [Section(i) for i in obj_list]\n    sections = property(get_sections)\n\n    def get_entities(self):\n        try:\n            return self.__dict__['entities']\n        except KeyError:\n            entities = self._connection.fetch(\n                \"documents/%s/entities.json\" % self.id\n            ).get(\"entities\")\n            obj_list = []\n            for type, entity_list in list(entities.items()):\n                for entity in entity_list:\n                    entity['type'] = type\n                    obj = Entity(entity)\n                    obj_list.append(obj)\n            self.__dict__['entities'] = obj_list\n            return self.__dict__['entities']\n    entities = property(get_entities)\n\n    #\n    # Text\n    #\n\n    def _get_url(self, url):\n        if self.access == 'public':\n            req = urllib.request.Request(\n                url,\n                headers={'User-Agent': \"python-documentcloud\"}\n            )\n            return urllib.request.urlopen(req).read()\n        else:\n            raise NotImplementedError(\n                \"Currently, DocumentCloud only allows you to access this \\"}
{"Repository": "bbopt", "input": "Multi-thread implementation of map. className concurrent_map(_coconut_base_parallel_concurrent_map) Method make_pool", "label": "class concurrent_map(_coconut_base_parallel_concurrent_map):\n    __slots__ = ()\n    threadlocal_ns = _coconut.threading.local()\n    @staticmethod\n    def make_pool(max_workers=None):\n        return _coconut.multiprocessing_dummy.Pool(_coconut.multiprocessing.cpu_count() * 5 if max_workers is None else max_workers)"}
{"Repository": "Dwarf", "input": "DwarfDialog className DwarfDialog(QDialog) Method __init__ Method title Method title Method modal Method modal Method showEvent Attribute _title", "label": "class DwarfDialog(QDialog):\n    def __init__(self, parent=None):\n        super().__init__(parent=parent)\n        self._title = \"Dwarf\"\n        self.setWindowFlag(Qt.WindowContextHelpButtonHint, False)\n        self.setWindowFlag(Qt.WindowCloseButtonHint, True)\n\n    # ************************************************************************\n    # **************************** Properties ********************************\n    # ************************************************************************\n\n    @property\n    def title(self):\n        return self._title\n\n    @title.setter\n    def title(self, value):\n        if isinstance(value, str):\n            self._title = \"Dwarf - \" + value\n\n    @property\n    def modal(self):\n        return self.isModal()\n\n    @modal.setter\n    def modal(self, value):\n        if isinstance(value, bool):\n            self.setModal(value)\n\n    # override show\n    def showEvent(self, QShowEvent):  # pylint: disable=invalid-name\n        self.setWindowTitle(self.title)\n        self.setGeometry(\n            QStyle.alignedRect(Qt.LeftToRight, Qt.AlignCenter, self.size(),\n                               qApp.desktop().availableGeometry()))\n        return super().showEvent(QShowEvent)"}
{"Repository": "google-cloud-python-expenses-demo", "input": "Downloa a given expense receipt. className DownloadReceipt(object) Method __init__ Method __call__ Attribute receipter Attribute filename", "label": "class DownloadReceipt(object):\n    def __init__(self, receipter, *args):\n        self.receipter = receipter\n        args = list(args)\n        parser = optparse.OptionParser(\n            usage=\"%prog [OPTIONS] EMPLOYEE_ID REPORT_ID FILENAME\")\n\n        options, args = parser.parse_args(args)\n        try:\n            self.employee_id, self.report_id, self.filename = args\n        except:\n            raise InvalidCommandLine('Specify employee ID, report ID, filename')\n\n    def __call__(self):\n        try:\n            download_receipt(self.employee_id, self.report_id, self.filename)\n        except NoSuchReport:\n            self.receipter.blather(\"No such report: %s/%s\"\n                                   % (self.employee_id, self.report_id))\n        except NoSuchReceipt:\n            self.receipter.blather(\"No such report: %s/%s\"\n                                   % (self.employee_id, self.report_id))\n        else:\n            self.receipter.blather(\"Employee-ID: %s\" % self.employee_id)\n            self.receipter.blather(\"Report-ID: %s\" % self.report_id)\n            self.receipter.blather(\"\")\n            self.receipter.blather(\"Downloaded: %s\" % self.filename)"}
{"Repository": "django-comments-xtd", "input": "Encapsulates comment-moderation options for a given-model. className XtdCommentModerator(CommentModerator) Method notify_removal_suggestion", "label": "class XtdCommentModerator(CommentModerator):\n    removal_suggestion_notification = None\n\n    def notify_removal_suggestion(self, comment, content_object, request):\n        if not self.removal_suggestion_notification:\n            return\n        recipient_list = [manager_tuple[1]\n                          for manager_tuple in settings.MANAGERS]\n        t = loader.get_template('django_comments_xtd/'\n                                'removal_notification_email.txt')\n        c = {'comment': comment,\n             'content_object': content_object,\n             'current_site': get_current_site(request),\n             'request': request}\n        subject = ('[%s] Comment removal suggestion on \"%s\"' %\n                   (c['current_site'].name, content_object))\n        message = t.render(c)\n        send_mail(subject, message, settings.COMMENTS_XTD_FROM_EMAIL,\n                  recipient_list, fail_silently=True)"}
{"Repository": "ddsp", "input": "Parameterize impulse response as a simple exponential decay. className ExpDecayReverb(Reverb) Method _get_ir Method build Method get_controls", "label": "class ExpDecayReverb(Reverb):\n  def __init__(self,\n               trainable=False,\n               reverb_length=48000,\n               scale_fn=core.exp_sigmoid,\n               add_dry=True,\n               name='exp_decay_reverb'):\n    super().__init__(name=name, add_dry=add_dry, trainable=trainable)\n    self._reverb_length = reverb_length\n    self._scale_fn = scale_fn\n\n  def _get_ir(self, gain, decay):\n    gain = self._scale_fn(gain)\n    decay_exponent = 2.0 + tf.exp(decay)\n    time = tf.linspace(0.0, 1.0, self._reverb_length)[tf.newaxis, :]\n    noise = tf.random.uniform([1, self._reverb_length], minval=-1.0, maxval=1.0)\n    ir = gain * tf.exp(-decay_exponent * time) * noise\n    return ir\n\n  def build(self, unused_input_shape):\n    if self.trainable:\n      self._gain = self.add_weight(\n          name='gain',\n          shape=[1],\n          dtype=tf.float32,\n          initializer=tf.constant_initializer(2.0))\n      self._decay = self.add_weight(\n          name='decay',\n          shape=[1],\n          dtype=tf.float32,\n          initializer=tf.constant_initializer(4.0))\n    self.built = True\n\n  def get_controls(self, audio, gain=None, decay=None):\n    if self.trainable:\n      gain, decay = self._gain[tf.newaxis, :], self._decay[tf.newaxis, :]\n    else:\n      if gain is None or decay is None:\n        raise ValueError('Must provide \"gain\" and \"decay\" tensors if '\n                         'ExpDecayReverb trainable=False.')\n\n    ir = self._get_ir(gain, decay)\n\n    if self.trainable:\n      ir = self._match_dimensions(audio, ir)\n\n    return {'audio': audio, 'ir': ir}"}
{"Repository": "OpenKomodoIDE", "input": "Set svn:eol-style=native for text files under the current dir that don't already have it set. className svneolstylenative(lsnosvneolstyle) Method make", "label": "class svneolstylenative(lsnosvneolstyle):\n    def make(self):\n        for path in self._no_svn_eol_style_files(os.curdir):\n            os.system('svn propset svn:eol-style native \"%s\"' % path)"}
{"Repository": "StyleFlow", "input": "Computes and stores the average and current value className AverageValueMeter(object) Method __init__ Method reset Method update Attribute val Attribute avg Attribute sum Attribute count", "label": "class AverageValueMeter(object):\n    def __init__(self):\n\n        self.val = 0\n\n        self.avg = 0\n\n        self.sum = 0\n\n        self.count = 0.0\n\n\n\n    def reset(self):\n\n        self.val = 0\n\n        self.avg = 0\n\n        self.sum = 0\n\n        self.count = 0.0\n\n\n\n    def update(self, val, n=1):\n\n        self.val = val\n\n        self.sum += val * n\n\n        self.count += n\n\n        self.avg = self.sum / self.count"}
{"Repository": "sdc", "input": "Model for DataFrameRollingType type. className DataFrameRollingTypeModel(RollingTypeModel) Method __init__", "label": "class DataFrameRollingTypeModel(RollingTypeModel):\n    def __init__(self, dmm, fe_type):\n        super(DataFrameRollingTypeModel, self).__init__(dmm, fe_type)"}
{"Repository": "pyinfra", "input": "The default/base configuration options for a pyinfra deploy. className Config(ConfigDefaults) Method __init__ Method __setattr__ Method get_current_state Method set_current_state Method lock_current_state Method reset_locked_state Method copy Attribute ENV", "label": "class Config(ConfigDefaults):\n    def __init__(self, **kwargs):\n        # Always apply some env\n        env = kwargs.pop(\"ENV\", {})\n        self.ENV = env\n\n        config = config_defaults.copy()\n        config.update(kwargs)\n\n        for key, value in config.items():\n            setattr(self, key, value)\n\n    def __setattr__(self, key, value):\n        super().__setattr__(key, value)\n\n        checker = config_checkers.get(key)\n        if checker:\n            checker(value)\n\n    def get_current_state(self):\n        return [(key, getattr(self, key)) for key in config_defaults.keys()]\n\n    def set_current_state(self, config_state):\n        for key, value in config_state:\n            setattr(self, key, value)\n\n    def lock_current_state(self):\n        self._locked_config = self.get_current_state()\n\n    def reset_locked_state(self):\n        self.set_current_state(self._locked_config)\n\n    def copy(self) -> \"Config\":\n        return Config(**dict(self.get_current_state()))"}
{"Repository": "Urban-Region-Function-Classification", "input": "Computes and stores the average and current value className AverageMeter(object) Method __init__ Method reset Method update", "label": "class AverageMeter(object):\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count"}
{"Repository": "tabbyAPI", "input": "Represents a common tokenization request. className CommonTokenRequest(BaseModel) Method get_params", "label": "class CommonTokenRequest(BaseModel):\n    add_bos_token: bool = True\n    encode_special_tokens: bool = True\n    decode_special_tokens: bool = True\n\n    def get_params(self):\n        return {\n            \"add_bos_token\": self.add_bos_token,\n            \"encode_special_tokens\": self.encode_special_tokens,\n            \"decode_special_tokens\": self.decode_special_tokens,\n        }"}
{"Repository": "soupy", "input": "A binary operation className BinaryOp(Expression) Method __init__ Method eval_ Method __str__ Attribute op Attribute left Attribute right Attribute symbol", "label": "class BinaryOp(Expression):\n    def __init__(self, op, symbol, left, right):\n        self.op = op\n        self.left = left\n        self.right = right\n        self.symbol = symbol\n\n    @_helpful_failure\n    def eval_(self, val):\n        left = self.left\n        right = self.right\n        if isinstance(left, Expression):\n            left = left.eval_(val)\n\n        if isinstance(right, Expression):\n            right = right.eval_(val)\n\n        return self.op(left, right)\n\n    def __str__(self):\n        l, r = self.left, self.right\n        if isinstance(l, BinaryOp):\n            l = '(%s)' % str(l)\n        if isinstance(r, BinaryOp):\n            r = '(%s)' % str(r)\n\n        return \"%s %s %s\" % (l, self.symbol, r)"}
{"Repository": "migrate", "input": "Abstract base class for checkpoint read and write. className AbstractCheckpointKeyMap(ABC) Method write Method contains", "label": "class AbstractCheckpointKeyMap(ABC):\n    @abstractmethod\n    def write(self, key):\n        pass\n\n    @abstractmethod\n    def contains(self, key):\n        pass"}
{"Repository": "genomeview", "input": "Visualizes quantitative data as a line across coordinates within the current genomic view. className GraphTrack(Track) Method __init__ Method add_series Method ytopixels Method render Attribute series Attribute min_y Attribute max_y Attribute height Attribute ymargin", "label": "class GraphTrack(Track):\n    def __init__(self, name=None, x=None, y=None):\n        super().__init__(name)\n\n        self.series = collections.OrderedDict()\n        \n        self.min_y = 0\n        self.max_y = 0\n        \n        if x is not None:\n            self.add_series(x, y)\n\n        self.height = 100\n        self.ymargin = 5\n        \n    def add_series(self, x, y, color=None, label=None):\n        if label is None:\n            label = \"series_{}\".format(len(self.series))\n            \n        assert label not in self.series\n\n        x = numpy.asarray(x)\n        y = numpy.asarray(y)\n\n        if color is None:\n            color = COLORS[len(self.series) % len(COLORS)]\n            \n        self.series[label] = Series(x, y, color, label)\n\n        self.min_y = min(self.min_y, y[numpy.isfinite(y)].min())\n        self.max_y = max(self.max_y, y[numpy.isfinite(y)].max())\n\n    def ytopixels(self, yval):\n        height = self.max_y - self.min_y\n        return self.height - ((yval - self.min_y) / height * (self.height-2*self.ymargin) + self.ymargin)\n        \n    def render(self, renderer):\n        for label, series in self.series.items():\n            for i in range(len(series.x)-1):\n                if any(numpy.isnan(series.x[i:i+2])) or any(numpy.isnan(series.y[i:i+2])):\n                    continue\n                x1 = self.scale.topixels(series.x[i])\n                x2 = self.scale.topixels(series.x[i+1])\n                y1 = self.ytopixels(series.y[i])\n                y2 = self.ytopixels(series.y[i+1])\n                \n                yield from renderer.line(x1, y1, x2, y2, \n                    **{\"stroke-width\":1, \"stroke\":series.color, \"stroke-linecap\":\"square\"})\n\n        # since the labels are drawn at the top of the ticks, let's make sure the top tick/label is \n        # more than 12 pixels from the top of the track so it doesn't get clipped\n        # TODO: this ignores the margin, as of now\n        axis_max_y = self.min_y + (self.max_y - self.min_y) * (1-7/self.height)\n\n        # ticks = get_ticks(self.min_y, axis_max_y, 4)\n        ticks = numpy.linspace(self.min_y, axis_max_y, 4)\n\n        yield from renderer.line(1, self.ytopixels(ticks[0]), 1, self.ytopixels(ticks[-1]), \n                                 **{\"stroke-width\":2, \"stroke\":\"gray\", \"stroke-linecap\":\"square\"})\n        for tick in ticks:\n            if self.max_y > 1_000:\n                label = \"{:.1g}\".format(tick)\n            elif self.max_y < 1:\n                label = \"{:.1f}\".format(tick)\n            else:\n                label = \"{:,.0f}\".format(tick)\n\n            y = self.ytopixels(tick)\n            yield from renderer.line(1, y, 10, y, \n                                     **{\"stroke-width\":2, \"stroke\":\"gray\", \"stroke-linecap\":\"square\"})\n            yield from renderer.text(14, y, label, anchor=\"start\", fill=\"gray\")\n            \n        for x in self.render_label(renderer):\n            yield x"}
{"Repository": "nanoemoji", "input": "A rule to reorder something in a font to match the fonts glyph order. className ReorderRule(ABC) Method apply", "label": "class ReorderRule(ABC):\n    @abstractmethod\n    def apply(self, font: ttLib.TTFont, value: otBase.BaseTable) -> None:\n        ..."}
{"Repository": "V3n0M-Scanner", "input": "socket_err contains original socket.error exception. className ProxyError(IOError) Method __init__ Method __str__ Attribute msg Attribute socket_err", "label": "class ProxyError(IOError):\n    def __init__(self, msg, socket_err=None):\n\n        self.msg = msg\n\n        self.socket_err = socket_err\n\n\n\n        if socket_err:\n\n            self.msg += \": {0}\".format(socket_err)\n\n\n\n    def __str__(self):\n\n        return self.msg"}
{"Repository": "USI3D", "input": "An adaptor aim to balance loss via the std of the loss className LossAdaptor(object) Method __init__ Method __call__ Attribute size Attribute history Attribute param_only", "label": "class LossAdaptor(object):\n    def __init__(self, queue_size=100, param_only=True):\n        self.size = queue_size\n        self.history = Queue(maxsize=self.size)\n        self.param_only = param_only\n\n    def __call__(self, loss_var):\n        if self.history.qsize() < self.size:\n            param = 1.\n            self.history.put(loss_var)\n        else:\n            self.history.put(loss_var)\n            param = np.mean(self.history.queue)\n\n        if self.param_only:\n            return param\n        else:\n            return param * loss_var"}
{"Repository": "flsim", "input": "Generate federated learning training and testing data. className Generator(object) Method read Method group Method generate", "label": "class Generator(object):\n    # Abstract read function\n    def read(self, path):\n        # Read the dataset, set: trainset, testset, labels\n        raise NotImplementedError\n\n    # Group the data by label\n    def group(self):\n        # Create empty dict of labels\n        grouped_data = {label: []\n                        for label in self.labels}  # pylint: disable=no-member\n\n        # Populate grouped data dict\n        for datapoint in self.trainset:  # pylint: disable=all\n            _, label = datapoint  # Extract label\n            label = self.labels[label]\n\n            grouped_data[label].append(  # pylint: disable=no-member\n                datapoint)\n\n        self.trainset = grouped_data  # Overwrite trainset with grouped data\n\n    # Run data generation\n    def generate(self, path):\n        self.read(path)\n        self.trainset_size = len(self.trainset)  # Extract trainset size\n        self.group()\n\n        return self.trainset"}
{"Repository": "liveusb-creator", "input": "Wrapper for the iso download process. className ReleaseDownload(QObject) Method __init__ Method reset Method start Method update Method end Method childFinished Method childError Method run Method cancel Method maxProgress Method progress Method running Method path Method path Attribute release Attribute _grabber Attribute _live", "label": "class ReleaseDownload(QObject):\n    runningChanged = pyqtSignal()\n    currentChanged = pyqtSignal()\n    maximumChanged = pyqtSignal()\n    pathChanged = pyqtSignal()\n\n    _running = False\n    _current = -1.0\n    _maximum = -1.0\n    _path = ''\n\n    def __init__(self, parent):\n        QObject.__init__(self, parent)\n        self.release = parent\n        self._grabber = ReleaseDownloadThread(self, parent.live.get_proxies())\n        self._live = parent.live\n\n    def reset(self):\n        self._running = False\n        self._current = -1.0\n        self._maximum = -1.0\n        self.path = ''\n        self.runningChanged.emit()\n        self.currentChanged.emit()\n        self.maximumChanged.emit()\n\n    def start(self, size=None):\n        self._maximum = size\n        self._running = True\n        self.maximumChanged.emit()\n        self.runningChanged.emit()\n\n    def update(self, amount_read):\n        if self._current < amount_read:\n            self._current = amount_read\n            self.currentChanged.emit()\n\n    def end(self):\n        self._current = self._maximum\n        self.currentChanged.emit()\n        self._running = False\n        self.runningChanged.emit()\n\n    @pyqtSlot(str)\n    def childFinished(self, iso):\n        self.reset()\n        self.path = iso\n        self._running = False\n        self.runningChanged.emit()\n\n    @pyqtSlot(str)\n    def childError(self, err):\n        self.reset()\n        self.release.addError(err)\n\n    @pyqtSlot(str)\n    def run(self):\n        if len(self.parent().path) <= 0:\n            self._grabber.downloadFinished.connect(self.childFinished)\n            self._grabber.downloadError.connect(self.childError)\n            self._grabber.start()\n\n    @pyqtSlot()\n    def cancel(self):\n        self._grabber.cancelDownload()\n        self.reset()\n\n    @pyqtProperty(float, notify=maximumChanged)\n    def maxProgress(self):\n        return self._maximum\n\n    @pyqtProperty(float, notify=currentChanged)\n    def progress(self):\n        return self._current\n\n    @pyqtProperty(bool, notify=runningChanged)\n    def running(self):\n        return self._running\n\n    @pyqtProperty(str, notify=pathChanged)\n    def path(self):\n        return self._path\n\n    @path.setter\n    def path(self, value):\n        if self._path != value:\n            self._path = value\n            self._live.set_iso(value)\n            self.pathChanged.emit()"}
{"Repository": "a-plus", "input": "JSON Web Key used by LTI tool to validate JSON web tokens sent in LTI messages. className LTI1p3JwksView(BaseView) Method get", "label": "class LTI1p3JwksView(BaseView):\n    access_mode = ACCESS.ANONYMOUS\n\n    def get(self, request, *args, **kwargs) -> HttpResponse:\n        pem = settings.APLUS_AUTH_LOCAL['PRIVATE_KEY']\n        key = jwk.JWK.from_pem(pem.encode(\"utf8\"))\n        data = {\"alg\": \"RS256\", \"use\": \"sig\", \"kid\": key.thumbprint()}\n        data.update(json.loads(key.export_public()))\n        response = JsonResponse({\"keys\": [data]})\n        return response"}
{"Repository": "LateTemporalModeling3DCNN", "input": "Computes and stores the average and current value className AverageMeter(object) Method __init__ Method reset Method update", "label": "class AverageMeter(object):\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count"}
{"Repository": "DeepMimic_mujoco", "input": "Raised when an asynchronous step is running while step_async() is called again. className AlreadySteppingError(Exception) Method __init__", "label": "class AlreadySteppingError(Exception):\n    def __init__(self):\n        msg = 'already running an async step'\n        Exception.__init__(self, msg)"}
{"Repository": "bcbb", "input": "Handle obtaining a reference genome from Ensembl className EnsemblFtpRetrieval(_BaseRetrieval) Method __init__ Method _initialize Method _files_at_url Method retrieve_db Method _get_ftp_url Attribute _main_ftp Attribute _genome_ftp Attribute _genome_dbs Attribute _initialized Attribute _config", "label": "class EnsemblFtpRetrieval(_BaseRetrieval):\n    def __init__(self, config):\n        self._main_ftp = \"ftp://ftp.ensembl.org/pub/current_fasta/\"\n        self._genome_ftp = \"ftp://ftp.ensemblgenomes.org/pub/%s/current/fasta/\"\n        self._genome_dbs = [\"bacteria\", \"protists\", \"metazoa\", \"fungi\",\n                \"plants\"]\n        self._initialized = False\n        self._config = config\n\n    def _initialize(self):\n        if not self._initialized:\n            urls = [self._genome_ftp % d for d in self._genome_dbs] + \\\n                   [self._main_ftp]\n            self._org_to_urls = dict()\n            for url in urls:\n                orgs = self._files_at_url(url)\n                for org in orgs:\n                    self._org_to_urls[org] = url\n            self._initialized = True\n\n    def _files_at_url(self, url):\n        parts = url.replace(\"ftp://\", \"\").split(\"/\")\n        ftp = ftplib.FTP(parts[0])\n        ftp.login()\n        orgs = ftp.nlst(\"/\".join(parts[1:]))\n        return [o.split(\"/\")[-1] for o in orgs]\n\n    def retrieve_db(self, organism, db_dir):\n        self._initialize()\n        ftp_url = self._get_ftp_url(organism)\n        if ftp_url is None:\n            return \"\"\n        file_name = ftp_url.split(\"/\")[-1]\n        final_file = os.path.join(db_dir, file_name.replace(\".gz\", \"\"))\n        db_name = os.path.splitext(os.path.basename(final_file))[0]\n        if not os.path.exists(final_file):\n            with _chdir(db_dir):\n                cl = [\"wget\", ftp_url]\n                subprocess.check_call(cl)\n                cl = [\"gunzip\", file_name]\n                subprocess.check_call(cl)\n        self._make_blast_db(db_dir, final_file, db_name, organism)\n        return db_name\n\n    def _get_ftp_url(self, organism):\n        ftp_url = None\n        org_parts = organism.split()\n        for check_org in [organism.replace(\" \", \"_\").lower(),\n                \"_\".join([org_parts[0][0], org_parts[1]]).lower()]:\n            try:\n                ftp_url = self._org_to_urls[check_org]\n                break\n            except KeyError:\n                pass\n        if ftp_url:\n            ftp_url = ftp_url + check_org + \"/pep/\"\n            files = self._files_at_url(ftp_url)\n            for f in files:\n                if f.endswith(\"pep.all.fa.gz\"):\n                    ftp_url = ftp_url + f\n                    break\n        return ftp_url"}
{"Repository": "macosx-shell-scripts", "input": "Mount a DMG and install one or more toplevel .pkg files className SubcommandInstallPackage(ImageMountingSubcommand) Method process_image", "label": "class SubcommandInstallPackage(ImageMountingSubcommand):\n    def process_image(self, image):\n        packages = glob.glob('{}/*.pkg'.format(image.mount_point()))\n        if not packages:\n            return\n        \n        for package_path in packages:\n            cmd = ['/usr/sbin/installer', '-pkg', package_path, '-target', '/']\n            process = subprocess.Popen(cmd)\n            process.communicate()"}
{"Repository": "dropdav", "input": "OAuthClient is a worker to attempt to execute a request. className OAuthClient(object) Method __init__ Method get_consumer Method get_token Method fetch_request_token Method fetch_access_token Method access_resource Attribute consumer Attribute token", "label": "class OAuthClient(object):\n    consumer = None\n    token = None\n\n    def __init__(self, oauth_consumer, oauth_token):\n        self.consumer = oauth_consumer\n        self.token = oauth_token\n\n    def get_consumer(self):\n        return self.consumer\n\n    def get_token(self):\n        return self.token\n\n    def fetch_request_token(self, oauth_request):\n        raise NotImplementedError\n\n    def fetch_access_token(self, oauth_request):\n        raise NotImplementedError\n\n    def access_resource(self, oauth_request):\n        raise NotImplementedError"}
{"Repository": "open-aff", "input": "ResNetV1b BottleneckV1b className AFFBottleneck(HybridBlock) Method hybrid_forward", "label": "class AFFBottleneck(HybridBlock):\n    # pylint: disable=unused-argument\n    expansion = 4\n    def __init__(self, askc_type, planes, strides=1, dilation=1,\n                 downsample=None, previous_dilation=1, norm_layer=None,\n                 norm_kwargs=None, last_gamma=False, **kwargs):\n        super(AFFBottleneck, self).__init__()\n        norm_kwargs = norm_kwargs if norm_kwargs is not None else {}\n        self.conv1 = nn.Conv2D(channels=planes, kernel_size=1,\n                               use_bias=False)\n        self.bn1 = norm_layer(in_channels=planes, **norm_kwargs)\n        self.conv2 = nn.Conv2D(channels=planes, kernel_size=3, strides=strides,\n                               padding=dilation, dilation=dilation, use_bias=False)\n        self.bn2 = norm_layer(in_channels=planes, **norm_kwargs)\n\n        self.relu1 = nn.Activation('relu')\n        self.relu2 = nn.Activation('relu')\n        self.relu3 = nn.Activation('relu')\n\n        self.conv3 = nn.Conv2D(channels=planes * 4, kernel_size=1, use_bias=False)\n        if not last_gamma:\n            self.bn3 = norm_layer(in_channels=planes*4, **norm_kwargs)\n        else:\n            self.bn3 = norm_layer(in_channels=planes*4, gamma_initializer='zeros',\n                                  **norm_kwargs)\n\n        self.downsample = downsample\n        self.dilation = dilation\n        self.strides = strides\n\n        if askc_type == 'DirectAdd':\n            self.attention = DirectAddFuse()\n        elif askc_type == 'ResGlobLocaforGlobLocaCha':\n            self.attention = ResGlobLocaforGlobLocaChaFuse(channels=planes*4, r=16)\n        elif askc_type == 'ASKCFuse':\n            self.attention = ASKCFuse(channels=planes*4, r=16)\n        else:\n            raise ValueError('Unknown askc_type')\n\n    def hybrid_forward(self, F, x):\n\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu1(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu2(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out = self.attention(out, residual)\n        out = self.relu3(out)\n\n        return out"}
{"Repository": "OpenEats", "input": "used by users to add a new aisle className GroceryAisleForm(ModelForm) Method clean", "label": "class GroceryAisleForm(ModelForm):\n    class Meta:\n        model = GroceryAisle\n        widgets = {'author': HiddenInput()}\n\n    def clean(self):\n        cleaned_data = self.cleaned_data\n        try:\n            GroceryAisle.objects.get(aisle=cleaned_data['aisle'], author=cleaned_data['author'])\n        except:\n            pass\n        else:\n            raise forms.ValidationError(_('Aisle with this name already exists for your account'))\n        return cleaned_data"}
{"Repository": "Marzban", "input": "Missing associated documentation comment in .proto file. className LoggerServiceStub(object) Method __init__ Attribute RestartLogger", "label": "class LoggerServiceStub(object):\n    def __init__(self, channel):\n        self.RestartLogger = channel.unary_unary(\n                '/xray.app.log.command.LoggerService/RestartLogger',\n                request_serializer=app_dot_log_dot_command_dot_config__pb2.RestartLoggerRequest.SerializeToString,\n                response_deserializer=app_dot_log_dot_command_dot_config__pb2.RestartLoggerResponse.FromString,\n                )"}
{"Repository": "DeepTemporalClustering", "input": "Clustering layer converts input sample (feature) to soft label, i. className TSClusteringLayer(Layer) Method __init__ Method build Method call Method compute_output_shape Method get_config Attribute n_clusters Attribute alpha Attribute dist_metric Attribute initial_weights Attribute input_spec Attribute clusters Attribute built", "label": "class TSClusteringLayer(Layer):\n    def __init__(self, n_clusters, weights=None, alpha=1.0, dist_metric='eucl', **kwargs):\n        if 'input_shape' not in kwargs and 'input_dim' in kwargs:\n            kwargs['input_shape'] = (kwargs.pop('input_dim'),)\n        super(TSClusteringLayer, self).__init__(**kwargs)\n        self.n_clusters = n_clusters\n        self.alpha = alpha\n        self.dist_metric = dist_metric\n        self.initial_weights = weights\n        self.input_spec = InputSpec(ndim=3)\n        self.clusters = None\n        self.built = False\n\n    def build(self, input_shape):\n        assert len(input_shape) == 3\n        input_dim = input_shape[2]\n        input_steps = input_shape[1]\n        self.input_spec = InputSpec(dtype=K.floatx(), shape=(None, input_steps, input_dim))\n        self.clusters = self.add_weight(shape=(self.n_clusters, input_steps, input_dim), initializer='glorot_uniform', name='cluster_centers')\n        if self.initial_weights is not None:\n            self.set_weights(self.initial_weights)\n            del self.initial_weights\n        self.built = True\n\n    def call(self, inputs, **kwargs):\n        if self.dist_metric == 'eucl':\n            distance = K.sum(K.sqrt(K.sum(K.square(K.expand_dims(inputs, axis=1) - self.clusters), axis=2)), axis=-1)\n        elif self.dist_metric == 'cid':\n            ce_x = K.sqrt(K.sum(K.square(inputs[:, 1:, :] - inputs[:, :-1, :]), axis=1))  # shape (n_samples, n_features)\n            ce_w = K.sqrt(K.sum(K.square(self.clusters[:, 1:, :] - self.clusters[:, :-1, :]), axis=1))  # shape (n_clusters, n_features)\n            ce = K.maximum(K.expand_dims(ce_x, axis=1), ce_w) / K.minimum(K.expand_dims(ce_x, axis=1), ce_w)  # shape (n_samples, n_clusters, n_features)\n            ed = K.sqrt(K.sum(K.square(K.expand_dims(inputs, axis=1) - self.clusters), axis=2))  # shape (n_samples, n_clusters, n_features)\n            distance = K.sum(ed * ce, axis=-1)  # shape (n_samples, n_clusters)\n        elif self.dist_metric == 'cor':\n            inputs_norm = (inputs - K.expand_dims(K.mean(inputs, axis=1), axis=1)) / K.expand_dims(K.std(inputs, axis=1), axis=1)  # shape (n_samples, timesteps, n_features)\n            clusters_norm = (self.clusters - K.expand_dims(K.mean(self.clusters, axis=1), axis=1)) / K.expand_dims(K.std(self.clusters, axis=1), axis=1)  # shape (n_clusters, timesteps, n_features)\n            pcc = K.mean(K.expand_dims(inputs_norm, axis=1) * clusters_norm, axis=2)  # Pearson correlation coefficients\n            distance = K.sum(K.sqrt(2.0 * (1.0 - pcc)), axis=-1)  # correlation-based similarities, shape (n_samples, n_clusters)\n        elif self.dist_metric == 'acf':\n            raise NotImplementedError\n        else:\n            raise ValueError('Available distances are eucl, cid, cor and acf!')\n        q = 1.0 / (1.0 + K.square(distance) / self.alpha)\n        q **= (self.alpha + 1.0) / 2.0\n        q = K.transpose(K.transpose(q) / K.sum(q, axis=1))\n        return q\n\n    def compute_output_shape(self, input_shape):\n        assert input_shape and len(input_shape) == 3\n        return input_shape[0], self.n_clusters\n\n    def get_config(self):\n        config = {'n_clusters': self.n_clusters, 'dist_metric': self.dist_metric}\n        base_config = super(TSClusteringLayer, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))"}
{"Repository": "EVA3D", "input": "Dataset to create indexes from `Sampler`. className DatasetFromSampler(Dataset) Method __init__ Method __getitem__ Method __len__ Attribute sampler Attribute sampler_list", "label": "class DatasetFromSampler(Dataset):\n    def __init__(self, sampler: Sampler):\n        self.sampler = sampler\n        self.sampler_list = None\n\n    def __getitem__(self, index: int):\n        if self.sampler_list is None:\n            self.sampler_list = list(self.sampler)\n        return self.sampler_list[index]\n\n    def __len__(self) -> int:\n        return len(self.sampler)"}
{"Repository": "TileStache", "input": "A tile exists, but it shouldn't be returned to the client. className TheTileLeftANote(Exception) Method __init__ Attribute headers Attribute status_code Attribute content Attribute emit_content_type", "label": "class TheTileLeftANote(Exception):\n    def __init__(self, headers=None, status_code=200, content='', emit_content_type=True):\n        self.headers = headers or Headers([])\n        self.status_code = status_code\n        self.content = content\n        self.emit_content_type = bool(emit_content_type)\n\n        Exception.__init__(self, self.headers, self.status_code,\n                           self.content, self.emit_content_type)"}
{"Repository": "CapsLayer", "input": "Data Loader. className DataLoader(object) Method __call__", "label": "class DataLoader(object):\n    def __init__(self, path=None,\n                 num_works=1,\n                 splitting=\"TVT\",\n                 one_hot=False,\n                 name=\"create_inputs\"):\n        if path is None or not os.path.exists(path):\n            tfrecord_runner()\n\n        self.handle = tf.placeholder(tf.string, shape=[])\n        self.next_element = None\n        self.path = path\n        self.name = name\n\n    def __call__(self, batch_size, mode):\n        with tf.name_scope(self.name):\n            mode = mode.lower()\n            modes = [\"train\", \"test\", \"eval\"]\n            filenames = [os.path.join(self.path, '%s_cifar100.tfrecord' % mode)]\n\n            dataset = tf.data.TFRecordDataset(filenames)\n            dataset = dataset.map(parse_fun)\n            dataset = dataset.batch(batch_size)\n\n            if mode == \"train\":\n                dataset = dataset.shuffle(buffer_size=50000)\n                dataset = dataset.repeat()\n                iterator = dataset.make_one_shot_iterator()\n            elif mode == \"eval\":\n                dataset = dataset.repeat(1)\n                iterator = dataset.make_initializable_iterator()\n            elif mode == \"test\":\n                dataset = dataset.repeat(1)\n                iterator = dataset.make_one_shot_iterator()\n\n            if self.next_element is None:\n                self.next_element = tf.data.Iterator.from_string_handle(self.handle, iterator.output_types).get_next()\n\n            return(iterator)"}
{"Repository": "aiosnow", "input": "The application returned a parsable error in the response className RequestError(AiosnowException) Method __init__ Attribute message Attribute status", "label": "class RequestError(AiosnowException):\n    def __init__(self, message: str, status: int):\n        self.message = message\n        self.status = status"}
{"Repository": "py-umls", "input": "Handles documents for storage in sqlite3 className SQLiteDocHandler(DocHandler) Method __init__ Method addDocument Method finalize Method __str__ Attribute db_file Attribute handled Attribute sqlite", "label": "class SQLiteDocHandler(DocHandler):\n\tdef __init__(self):\n\t\tsuper().__init__()\n\t\tfrom sqlite import SQLite\n\t\tabsolute = os.path.dirname(os.path.realpath(__file__))\n\t\tdb_file = os.environ.get('SQLITE_FILE')\n\t\tdb_file = db_file if db_file else os.path.join(absolute, 'databases/rxnorm.db')\n\t\tself.db_file = db_file\n\t\tself.handled = 0\n\n\t\tself.sqlite = SQLite.get(self.db_file)\n\t\tself.sqlite.execute('DROP TABLE IF EXISTS drug_cache')\n\n\t\tself.sqlite.execute('''CREATE TABLE drug_cache\n\t\t\t\t\t\t(rxcui varchar, property text, value text)''')\n\n\t\tself.sqlite.execute('CREATE INDEX i_drug_cache ON drug_cache (rxcui, property)')\n\n\t\tself.sqlite.execute('DROP VIEW IF EXISTS drug_treatments_by_ndc')\n\t\tself.sqlite.execute('''CREATE VIEW drug_treatments_by_ndc as\n\t\t\t\tselect a.value as ndc, b.value as treatment_intent\n\t\t\t\tfrom drug_cache a join drug_cache b on a.rxcui=b.rxcui\n\t\t\t\twhere a.property='ndc' and b.property='treatment_intent'\n\t\t\t\t''')\n\n\t\tself.sqlite.execute('DROP VIEW IF EXISTS drug_classes_by_ndc')\n\t\tself.sqlite.execute('''CREATE VIEW drug_classes_by_ndc as\n\t\t\t\tselect a.value as ndc, b.value as drug_class\n\t\t\t\tfrom drug_cache a join drug_cache b on a.rxcui=b.rxcui\n\t\t\t\twhere a.property='ndc' and b.property='drug_class'\n\t\t\t\t''')\n\n\t\tself.sqlite.execute('DROP VIEW IF EXISTS drug_ingredients_by_ndc')\n\t\tself.sqlite.execute('''CREATE VIEW drug_ingredients_by_ndc as\n\t\t\t\tselect a.value as ndc, b.value as drug_ingredient, c.str as ingredient_name\n\t\t\t\tfrom drug_cache a join drug_cache b on a.rxcui=b.rxcui\n\t\t\t\tjoin RXNCONSO c on c.rxcui=b.value\n\t\t\t\twhere a.property='ndc' and b.property='ingredient'\n\t\t\t\tand c.sab='RXNORM' and c.tty='IN'\n                ''')\n\tdef addDocument(self, doc):\n\t\trxcui =  doc.get('rxcui', '0')\n\t\tfields = {\n\t\t\t'tty': doc.get('tty', None),\n\t\t\t'ndc': doc.get('ndc', None),\n\t\t\t'label': doc.get('label', None),\n\t\t\t'drug_class': doc.get('drugClasses', None),\n\t\t\t'treatment_intent': doc.get('treatmentIntents', None),\n\t\t\t'ingredient': doc.get('ingredients', None)\n\t\t\t}\n\t\tfor k, v in fields.items():\n\t\t\tif not v: continue\n\t\t\tv = v if isinstance(v, list) else [v]\n\t\t\tfor vv in v:\n\t\t\t\tself.sqlite.execute(\n\t\t\t\t\t'INSERT INTO drug_cache(rxcui, property, value) values(?, ?, ?)',\n\t\t\t\t\t(rxcui, k, vv))\n\t\tself.handled += 1\n\t\tif (self.handled % 50 == 0): self.sqlite.commit()\n\t\t\n\tdef finalize(self): \n\t\tself.sqlite.commit()\n\t\n\tdef __str__(self):\n\t\treturn \"SQLite import {}\".format(self.db_file)"}
{"Repository": "scaleformer", "input": "Day of year encoded as value between [-0.5, 0.5] className DayOfYear(TimeFeature) Method __call__", "label": "class DayOfYear(TimeFeature):\n    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n        return (index.dayofyear - 1) / 365.0 - 0.5"}
{"Repository": "avod", "input": "L2 localization loss function with anchorwise output support. className WeightedL2LocalizationLoss(Loss) Method _compute_loss", "label": "class WeightedL2LocalizationLoss(Loss):\n    def _compute_loss(self, prediction_tensor, target_tensor, weights):\n        weighted_diff = (prediction_tensor - target_tensor) * tf.expand_dims(\n            weights, 2)\n        square_diff = 0.5 * tf.square(weighted_diff)\n        return tf.reduce_sum(square_diff)"}
{"Repository": "cc_net", "input": "Split a files in several smaller files based on the value of a field. className split(Transformer) Method make_split_fn Method do Method summary Method close", "label": "class split(Transformer):\n    # Not parallelisable since we are writing to files.\n    parallelisable = False\n\n    def __init__(\n        self,\n        pattern: Union[Path, str] = None,\n        split_fn: Callable[[dict], str] = None,\n        mkdir: bool = False,\n    ):\n        super().__init__()\n        assert not (\n            pattern and split_fn\n        ), \"split can't have both a pattern and a split_fn\"\n        if split_fn is not None:\n            self.split_fn = split_fn\n        else:\n            assert pattern, \"split need either a pattern or a split_fn\"\n            self.split_fn = self.make_split_fn(str(pattern))\n        self.mkdir = mkdir\n        self.o: dict = {}\n\n    def make_split_fn(self, pattern: str) -> Callable[[dict], str]:\n        candidates = list(re.findall(r\"(?i:\\{([_a-z][_a-z0-9]*)\\})\", pattern))\n        return lambda doc: pattern.format(**{c: doc[c] for c in candidates})\n\n    def do(self, doc):\n        filename = self.split_fn(doc)\n        if not filename:\n            return\n        o = self.o.get(filename, None)\n        if o is None:\n            if self.mkdir:\n                Path(filename).parent.mkdir(parents=True, exist_ok=True)\n            self.o[filename] = open_write(filename)\n        print(json.dumps(doc, ensure_ascii=False), file=self.o[filename], flush=True)\n\n    def summary(self):\n        summ = super().summary()\n        summ.append(f\"Found {len(self.o)} splits.\")\n        return summ\n\n    def close(self):\n        for file in self.o.values():\n            file.close()"}
{"Repository": "pyescpos", "input": "Implements a simple USB connection. className USBConnection(object) Method create Method __repr__ Method __str__ Method _raise_with_details Method catch Method write Method read", "label": "class USBConnection(object):\n    SETTINGS_EXAMPLE = '0492:8760,interface=0,ep_out=3,ep_in=0'\n\n    RE_VENDOR_PRODUCT = re.compile(\n            r'((0x)?(?P<vendor>[0-9a-f]*)):((0x)?(?P<product>[0-9a-f]*))',\n            re.IGNORECASE\n        )\n\n    RE_KEY_VALUE = re.compile(\n            r'(?P<key>\\w+)\\ *={1}\\ *((0x)?(?P<value>[0-9a-f]*))',\n            re.IGNORECASE\n        )\n\n    @classmethod\n    def create(cls, setting):\n        defaults = dict(interface=0, ep_in=0, ep_out=0)\n\n        match = USBConnection.RE_VENDOR_PRODUCT.match(setting)\n        if match is None:\n            raise ValueError('Invalid settings string: {!r}'.format(setting))\n\n        vendor_id = int(match.group('vendor'), 16)\n        product_id = int(match.group('product'), 16)\n\n        for match in USBConnection.RE_KEY_VALUE.finditer(setting):\n            defaults[match.group('key')] = int(match.group('value'), 16)\n\n        return USBConnection(vendor_id, product_id, **defaults)\n\n    def __init__(\n            self,\n            vendor_id,\n            product_id,\n            interface=0,\n            ep_in=0,\n            ep_out=0,\n            timeout=2000):\n        super(USBConnection, self).__init__()\n        self.usbport = None\n        self.vendor_id = vendor_id\n        self.product_id = product_id\n        self.interface = interface\n        self.ep_in = ep_in\n        self.ep_out = ep_out\n        self.timeout = timeout\n\n    def __repr__(self):\n        content = (\n                '{}(0x{:04x}, 0x{:04x}, '\n                'interface=0x{:x}, ep_in=0x{:x}, ep_out=0x{:x}, '\n                'timeout={!r})'\n            ).format(\n                self.__class__.__name__,\n                self.vendor_id,\n                self.product_id,\n                self.interface,\n                self.ep_in,\n                self.ep_out,\n                self.timeout\n            )\n        return content\n\n    def __str__(self):\n        # eg: \"0x0492:0x8760,interface=0x0,ep_out=0x3,ep_in=0x0\"\n        content = (\n                '0x{:04x}:0x{:04x},interface=0x{:x},ep_in==0x{:x},'\n                'ep_out=0x{:x}'\n            ).format(\n                self.vendor_id,\n                self.product_id,\n                self.interface,\n                self.ep_in,\n                self.ep_out\n            )\n        return content\n\n    def _raise_with_details(self, message, exctype=RuntimeError):\n        raise exctype((\n                '{}: {!r} (idVendor={:04x}, idProduct={:04x}, '\n                'interface={:x}, ep_in={:x}, ep_out={:x})'\n            ).format(\n                message,\n                self.usbport,\n                self.vendor_id,\n                self.product_id,\n                self.interface,\n                self.ep_in,\n                self.ep_out\n            ))\n\n    @depends_on_pyusb_lib\n    def catch(self):\n        self.usbport = usb.core.find(\n                idVendor=self.vendor_id, idProduct=self.product_id)\n\n        if self.usbport is None:\n            self._raise_with_details(\n                    'cannot find specified printer',\n                    exctype=ValueError\n                )\n\n        if self.usbport.is_kernel_driver_active(self.interface):\n            try:\n                self.usbport.detach_kernel_driver(self.interface)\n            except usb.core.USBError as e:\n                msg = 'unable to detach kernel driver: {:s}'.format(e)\n                self._raise_with_details(msg, exctype=usb.core.USBError)\n\n        try:\n            self.usbport.set_configuration()\n            self.usbport.reset()\n        except usb.core.USBError as e:\n            msg = 'unable to set configuration: {:s}'.format(e)\n            self._raise_with_details(msg, exctype=usb.core.USBError)\n\n    def write(self, data):\n        if logger.isEnabledFor(logging.DEBUG):\n            logger.debug('writing to USB port %s:\\n%s', self, hexdump(data))\n        self.usbport.write(self.ep_out, data, timeout=self.timeout)\n\n    def read(self):\n        return ''"}
{"Repository": "wetectron", "input": "This class samples batches, ensuring that they contain a fixed proportion of positives className BalancedPositiveNegativeSampler(object) Method __init__ Method __call__ Attribute batch_size_per_image Attribute positive_fraction", "label": "class BalancedPositiveNegativeSampler(object):\n    def __init__(self, batch_size_per_image, positive_fraction):\n        self.batch_size_per_image = batch_size_per_image\n        self.positive_fraction = positive_fraction\n\n    def __call__(self, matched_idxs):\n        pos_idx = []\n        neg_idx = []\n        for matched_idxs_per_image in matched_idxs:\n            positive = torch.nonzero(matched_idxs_per_image >= 1, as_tuple=False).squeeze(1)\n            negative = torch.nonzero(matched_idxs_per_image == 0, as_tuple=False).squeeze(1)\n\n            num_pos = int(self.batch_size_per_image * self.positive_fraction)\n            # protect against not enough positive examples\n            num_pos = min(positive.numel(), num_pos)\n            num_neg = self.batch_size_per_image - num_pos\n            # protect against not enough negative examples\n            num_neg = min(negative.numel(), num_neg)\n\n            # randomly select positive and negative examples\n            perm1 = torch.randperm(positive.numel(), device=positive.device)[:num_pos]\n            perm2 = torch.randperm(negative.numel(), device=negative.device)[:num_neg]\n\n            pos_idx_per_image = positive[perm1]\n            neg_idx_per_image = negative[perm2]\n\n            # create binary mask from indices\n            pos_idx_per_image_mask = torch.zeros_like(\n                matched_idxs_per_image, dtype=torch.bool\n            )\n            neg_idx_per_image_mask = torch.zeros_like(\n                matched_idxs_per_image, dtype=torch.bool\n            )\n            pos_idx_per_image_mask[pos_idx_per_image] = 1\n            neg_idx_per_image_mask[neg_idx_per_image] = 1\n\n            pos_idx.append(pos_idx_per_image_mask)\n            neg_idx.append(neg_idx_per_image_mask)\n\n        return pos_idx, neg_idx"}
{"Repository": "mpltools", "input": "A line-based string reader. className Reader(object) Method __init__ Method __getitem__ Method reset Method read Method seek_next_non_empty_line Method eof Method read_to_condition Method read_to_next_empty_line Method is_empty Method read_to_next_unindented_line Method is_unindented Method peek Method is_empty Attribute _str Attribute _str", "label": "class Reader(object):\n    def __init__(self, data):\n        if isinstance(data,list):\n            self._str = data\n        else:\n            self._str = data.split('\\n') # store string as list of lines\n\n        self.reset()\n\n    def __getitem__(self, n):\n        return self._str[n]\n\n    def reset(self):\n        self._l = 0 # current line nr\n\n    def read(self):\n        if not self.eof():\n            out = self[self._l]\n            self._l += 1\n            return out\n        else:\n            return ''\n\n    def seek_next_non_empty_line(self):\n        for l in self[self._l:]:\n            if l.strip():\n                break\n            else:\n                self._l += 1\n\n    def eof(self):\n        return self._l >= len(self._str)\n\n    def read_to_condition(self, condition_func):\n        start = self._l\n        for line in self[start:]:\n            if condition_func(line):\n                return self[start:self._l]\n            self._l += 1\n            if self.eof():\n                return self[start:self._l+1]\n        return []\n\n    def read_to_next_empty_line(self):\n        self.seek_next_non_empty_line()\n        def is_empty(line):\n            return not line.strip()\n        return self.read_to_condition(is_empty)\n\n    def read_to_next_unindented_line(self):\n        def is_unindented(line):\n            return (line.strip() and (len(line.lstrip()) == len(line)))\n        return self.read_to_condition(is_unindented)\n\n    def peek(self,n=0):\n        if self._l + n < len(self._str):\n            return self[self._l + n]\n        else:\n            return ''\n\n    def is_empty(self):\n        return not ''.join(self._str).strip()"}
{"Repository": "bittorrent", "input": "Holds all the information about the piece of a file. className Piece(object) Method __init__ Method calculateLastSize Method addBlock Method reset Method checkHash Attribute pieceIndex Attribute pieceSize Attribute pieceHash Attribute finished Attribute num_blocks Attribute blockTracker Attribute blocks Attribute blocksSoFar", "label": "class Piece(object):\n    def __init__(self, pieceIndex, pieceSize, pieceHash):\n        self.pieceIndex = pieceIndex\n        self.pieceSize = pieceSize\n        self.pieceHash = pieceHash\n        self.finished = False\n        self.num_blocks = int(math.ceil(float(pieceSize)/BLOCK_SIZE))\n        self.blockTracker = BitArray(self.num_blocks)\n        self.blocks = [False]*self.num_blocks\n        self.blocksSoFar = 0\n\n    def calculateLastSize(self):\n        return self.pieceSize - ((self.num_blocks-1)*(BLOCK_SIZE))\n\n    def addBlock(self, offset, data):\n        if offset == 0:\n            index = 0\n        else:\n            index = offset/BLOCK_SIZE\n\n        if not self.blockTracker[index]:\n            self.blocks[index] = data\n            self.blockTracker[index] = True\n            self.blocksSoFar += 1\n\n        self.finished = all(self.blockTracker)\n\n        # Need to do something here where I send the piece itself    \n        if self.finished:\n            return self.checkHash()\n\n        return True\n\n    def reset(self):\n        self.blockTracker = BitArray(self.num_blocks)\n        self.finished = False\n\n    def checkHash(self):\n        allData = ''.join(self.blocks)\n\n        hashedData = hashlib.sha1(allData).digest()\n        if hashedData == self.pieceHash:\n            self.block = allData\n            return True\n        else:\n            self.piece.reset()\n            return False"}
{"Repository": "Feature-Selective-Anchor-Free-Module-for-Single-Shot-Object-Detection", "input": "A wrapper of concatenated dataset. className ConcatDataset(_ConcatDataset) Method __init__ Attribute CLASSES Attribute flag", "label": "class ConcatDataset(_ConcatDataset):\n    def __init__(self, datasets):\n        super(ConcatDataset, self).__init__(datasets)\n        self.CLASSES = datasets[0].CLASSES\n        if hasattr(datasets[0], 'flag'):\n            flags = []\n            for i in range(0, len(datasets)):\n                flags.append(datasets[i].flag)\n            self.flag = np.concatenate(flags)"}
{"Repository": "NeuralRecon", "input": "Convert intrinsics and extrinsics matrices to a single projection matrix className IntrinsicsPoseToProjection(object) Method __init__ Method rotate_view_to_align_xyplane Method __call__ Attribute nviews Attribute stride", "label": "class IntrinsicsPoseToProjection(object):\n    def __init__(self, n_views, stride=1):\n        self.nviews = n_views\n        self.stride = stride\n\n    def rotate_view_to_align_xyplane(self, Tr_camera_to_world):\n        # world space normal [0, 0, 1]  camera space normal [0, -1, 0]\n        z_c = np.dot(np.linalg.inv(Tr_camera_to_world), np.array([0, 0, 1, 0]))[: 3]\n        axis = np.cross(z_c, np.array([0, -1, 0]))\n        axis = axis / np.linalg.norm(axis)\n        theta = np.arccos(-z_c[1] / (np.linalg.norm(z_c)))\n        quat = transforms3d.quaternions.axangle2quat(axis, theta)\n        rotation_matrix = transforms3d.quaternions.quat2mat(quat)\n        return rotation_matrix\n\n    def __call__(self, data):\n        middle_pose = data['extrinsics'][self.nviews // 2]\n        rotation_matrix = self.rotate_view_to_align_xyplane(middle_pose)\n        rotation_matrix4x4 = np.eye(4)\n        rotation_matrix4x4[:3, :3] = rotation_matrix\n        data['world_to_aligned_camera'] = torch.from_numpy(rotation_matrix4x4).float() @ middle_pose.inverse()\n\n        proj_matrices = []\n        for intrinsics, extrinsics in zip(data['intrinsics'], data['extrinsics']):\n            view_proj_matrics = []\n            for i in range(3):\n                # from (camera to world) to (world to camera)\n                proj_mat = torch.inverse(extrinsics.data.cpu())\n                scale_intrinsics = intrinsics / self.stride / 2 ** i\n                scale_intrinsics[-1, -1] = 1\n                proj_mat[:3, :4] = scale_intrinsics @ proj_mat[:3, :4]\n                view_proj_matrics.append(proj_mat)\n            view_proj_matrics = torch.stack(view_proj_matrics)\n            proj_matrices.append(view_proj_matrics)\n        data['proj_matrices'] = torch.stack(proj_matrices)\n        data.pop('intrinsics')\n        data.pop('extrinsics')\n        return data"}
{"Repository": "ganbert", "input": "Runs end-to-end tokenziation. className FullTokenizer(object) Method __init__ Method tokenize Method convert_tokens_to_ids Method convert_ids_to_tokens Attribute vocab Attribute inv_vocab Attribute basic_tokenizer Attribute wordpiece_tokenizer", "label": "class FullTokenizer(object):\n  def __init__(self, vocab_file, do_lower_case=True):\n    self.vocab = load_vocab(vocab_file)\n    self.inv_vocab = {v: k for k, v in self.vocab.items()}\n    self.basic_tokenizer = BasicTokenizer(do_lower_case=do_lower_case)\n    self.wordpiece_tokenizer = WordpieceTokenizer(vocab=self.vocab)\n\n  def tokenize(self, text):\n    split_tokens = []\n    for token in self.basic_tokenizer.tokenize(text):\n      for sub_token in self.wordpiece_tokenizer.tokenize(token):\n        split_tokens.append(sub_token)\n\n    return split_tokens\n\n  def convert_tokens_to_ids(self, tokens):\n    return convert_by_vocab(self.vocab, tokens)\n\n  def convert_ids_to_tokens(self, ids):\n    return convert_by_vocab(self.inv_vocab, ids)"}
{"Repository": "Discord-Anti-Spam", "input": "A generic Protocol for any Cache to implement className Cache(Protocol) Method __init__", "label": "class Cache(Protocol):\n    def __init__(self, handler) -> None:\n        self.handler = handler\n\n    async def initialize(self, *args, **kwargs) -> None:\n        pass\n\n    async def get_guild(self, guild_id: int) -> Guild:\n        raise NotImplementedError\n\n    async def set_guild(self, guild: Guild) -> None:\n        raise NotImplementedError\n\n    async def delete_guild(self, guild_id: int) -> None:\n        raise NotImplementedError\n\n    async def get_member(self, member_id: int, guild_id: int) -> Member:\n        raise NotImplementedError\n\n    async def set_member(self, member: Member) -> None:\n        raise NotImplementedError\n\n    async def delete_member(self, member_id: int, guild_id: int) -> None:\n        raise NotImplementedError\n\n    async def add_message(self, message: Message) -> None:\n        raise NotImplementedError\n\n    async def reset_member_count(\n        self, member_id: int, guild_id: int, reset_type: ResetType\n    ) -> None:\n        raise NotImplementedError\n\n    async def get_all_guilds(self) -> AsyncIterable[Guild]:\n    async def get_all_members(self, guild_id: int) -> AsyncIterable[Member]:\n    async def drop(self) -> None:\n        raise NotImplementedError"}
{"Repository": "micom", "input": "A visualization object. className Visualization(object) Method __init__ Method view Method save Attribute filename Attribute data Attribute template", "label": "class Visualization(object):\n    def __init__(self, filename, data, template):\n        self.filename = filename\n        self.data = data\n        self.template = env.get_template(template)\n\n    def view(self):\n        webbrowser.open(\"file://%s\" % path.realpath(self.filename), new=2)\n\n    def save(self, **kwargs):\n        out = self.filename\n        files = {k: d.to_csv(index=False) for k, d in self.data.items()}\n        logger.info(\"Writing visualization to %s.\" % out)\n        self.template.stream(files=files, **kwargs).dump(out)"}
{"Repository": "cudaBERT", "input": "BERT model (\"Bidirectional Embedding Representations from a Transformer\"). className BertModel(PreTrainedBertModel) Method __init__ Method forward Attribute embeddings Attribute encoder Attribute pooler", "label": "class BertModel(PreTrainedBertModel):\n    def __init__(self, config):\n        super(BertModel, self).__init__(config)\n        self.embeddings = BertEmbeddings(config)\n        self.encoder = BertEncoder(config)\n        self.pooler = BertPooler(config)\n        self.apply(self.init_bert_weights)\n\n    def forward(self, input_ids, token_type_ids=None, attention_mask=None, output_all_encoded_layers=True):\n        if attention_mask is None:\n            attention_mask = torch.ones_like(input_ids)\n        if token_type_ids is None:\n            token_type_ids = torch.zeros_like(input_ids)\n\n        # We create a 3D attention mask from a 2D tensor mask.\n        # Sizes are [batch_size, 1, 1, to_seq_length]\n        # So we can broadcast to [batch_size, num_heads, from_seq_length, to_seq_length]\n        # this attention mask is more simple than the triangular masking of causal attention\n        # used in OpenAI GPT, we just need to prepare the broadcast dimension here.\n        extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)\n\n        # Since attention_mask is 1.0 for positions we want to attend and 0.0 for\n        # masked positions, this operation will create a tensor which is 0.0 for\n        # positions we want to attend and -10000.0 for masked positions.\n        # Since we are adding it to the raw scores before the softmax, this is\n        # effectively the same as removing these entirely.\n        extended_attention_mask = extended_attention_mask.to(dtype=next(self.parameters()).dtype) # fp16 compatibility\n        extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0\n\n        embedding_output = self.embeddings(input_ids, token_type_ids)\n        # np.save(\"debug/pytorch/embedding_out.npy\", embedding_output.cpu().detach().numpy().astype(np.float32))\n\n        encoded_layers = self.encoder(embedding_output,\n                                      extended_attention_mask,\n                                      output_all_encoded_layers=output_all_encoded_layers)\n        sequence_output = encoded_layers[-1]\n        pooled_output = self.pooler(sequence_output)\n        # np.save(\"debug/pytorch/pooler_output.npy\", pooled_output.cpu().detach().numpy().astype(np.float32))\n\n        if not output_all_encoded_layers:\n            encoded_layers = encoded_layers[-1]\n        return encoded_layers, pooled_output"}
{"Repository": "multimodal-vae-public", "input": "Computes and stores the average and current value className AverageMeter(object) Method __init__ Method reset Method update", "label": "class AverageMeter(object):\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count"}
{"Repository": "PST-table", "input": "Computes and stores the average and current value className AverageMeter(object) Method __init__ Method reset Method update", "label": "class AverageMeter(object):\n\tdef __init__(self):\n\t\tself.reset()\n\n\tdef reset(self):\n\t\tself.val = 0\n\t\tself.avg = 0\n\t\tself.sum = 0\n\t\tself.count = 0\n\n\tdef update(self, val, n=1):\n\t\tself.val = val\n\t\tself.sum += val * n\n\t\tself.count += n\n\t\tself.avg = self.sum / self.count"}
{"Repository": "django-crequest", "input": "Provides storage for the \"current\" request object, so that code anywhere in your project can access it, without it having to be passed to that code from the view. className CrequestMiddleware(MiddlewareMixin) Method process_request Method process_response Method get_request Method set_request Method del_request", "label": "class CrequestMiddleware(MiddlewareMixin):\n    _requests = {}\n\n    def process_request(self, request):\n        self.__class__.set_request(request)\n\n    def process_response(self, request, response):\n        self.__class__.del_request()\n        return response\n\n    @classmethod\n    def get_request(cls, default=None):\n        return cls._requests.get(threading.current_thread(), default)\n\n    @classmethod\n    def set_request(cls, request):\n        cls._requests[threading.current_thread()] = request\n\n    @classmethod\n    def del_request(cls):\n        cls._requests.pop(threading.current_thread(), None)"}
{"Repository": "Django-facebook", "input": "A backend compatible with Django Registration className FacebookRegistrationBackend(NooptRegistrationBackend) Method register Method authenticate", "label": "class FacebookRegistrationBackend(NooptRegistrationBackend):\n    def register(self, request, form=None, **kwargs):\n        username, email, password = kwargs['username'], kwargs[\n            'email'], kwargs['password1']\n        # Create user doesn't accept additional parameters,\n        new_user = get_user_model(\n        ).objects.create_user(username, email, password)\n\n        signals.user_registered.send(sender=self.__class__,\n                                     user=new_user,\n                                     request=request)\n        authenticated_user = self.authenticate(request, username, password)\n        return authenticated_user\n\n    def authenticate(self, request, username, password):\n        # authenticate() always has to be called before login(), and\n        # will return the user we just created.\n        authentication_details = dict(username=username, password=password)\n        user = authenticate(**authentication_details)\n        login(request, user)\n\n        if user is None or not user.is_authenticated():\n            backends = get_backends()\n            msg_format = 'Authentication using backends %s and data %s failed'\n            raise ValueError(msg_format % (backends, authentication_details))\n\n        return user"}
{"Repository": "fawkes", "input": "The configurations specific to Twitter. className TwitterReviewChannel(ReviewChannel) Method __init__ Attribute consumer_key Attribute consumer_secret Attribute access_token_key Attribute access_token_secret Attribute twitter_handle_list Attribute twitter_handle_filter_list Attribute timestamp_key Attribute timestamp_format Attribute message_key Attribute timezone", "label": "class TwitterReviewChannel(ReviewChannel):\n    def __init__(self, config):\n        super().__init__(config)\n        self.consumer_key = config[\"consumer_key\"]\n        self.consumer_secret = config[\"consumer_secret\"]\n        self.access_token_key = config[\"access_token_key\"]\n        self.access_token_secret = config[\"access_token_secret\"]\n        self.twitter_handle_list = config[\"twitter_handle_list\"]\n        self.twitter_handle_filter_list = config[\"twitter_handle_filter_list\"]\n\n        # Pre defined constants for Twitter\n        self.timestamp_key = \"created_at\"\n        self.timestamp_format = \"%a %b %d %H:%M:%S %z %Y\"\n        self.message_key = \"text\"\n        self.timezone = \"GMT\""}
{"Repository": "CircleNet", "input": "Computes and stores the average and current value className AverageMeter(object) Method __init__ Method reset Method update", "label": "class AverageMeter(object):\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        if self.count > 0:\n          self.avg = self.sum / self.count"}
{"Repository": "WACV23_TSNet", "input": "Computes and stores the average and current value className AverageMeter(object) Method __init__ Method reset Method update", "label": "class AverageMeter(object):\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count"}
{"Repository": "Imitator", "input": "Wraps an arbitrary object with __len__ and __getitem__ into a pytorch dataset className WrappedDataset(Dataset) Method __init__ Method __len__ Method __getitem__ Attribute data", "label": "class WrappedDataset(Dataset):\n    def __init__(self, dataset):\n        self.data = dataset\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        return self.data[idx]"}
{"Repository": "symbolic-pymc", "input": "A printer that indents and keeps track of already printed subgraphs. className TFlowPrinter(object) Method __init__ Method indented Method format Method print Method println Method subgraph_add Method __repr__ Attribute buffer Attribute formatter Attribute depth_count Attribute depth_upper_idx Attribute indentation Attribute indentation Attribute printed_subgraphs", "label": "class TFlowPrinter(object):\n    def __init__(self, formatter, buffer, depth_lower_idx=0, depth_upper_idx=sys.maxsize):\n        # The buffer to which results are printed\n        self.buffer = buffer\n        # A function used to pre-process printed results\n        self.formatter = formatter\n\n        self.depth_count = 0\n        self.depth_lower_idx, self.depth_upper_idx = depth_lower_idx, depth_upper_idx\n\n        # This is the current indentation string\n        if self.depth_lower_idx > 0:\n            self.indentation = \"... \"\n        else:\n            self.indentation = \"\"\n\n        # The set of graphs that have already been printed\n        self.printed_subgraphs = set()\n\n    @contextmanager\n    def indented(self, indent):\n        pre_indentation = self.indentation\n\n        self.depth_count += 1\n\n        if self.depth_lower_idx < self.depth_count <= self.depth_upper_idx:\n            self.indentation += indent\n\n        try:\n            yield\n        except DepthExceededException:\n            pass\n        finally:\n            self.indentation = pre_indentation\n            self.depth_count -= 1\n\n    def format(self, obj):\n        return self.indentation + self.formatter(obj)\n\n    def print(self, obj, suffix=\"\"):\n        if self.depth_lower_idx <= self.depth_count < self.depth_upper_idx:\n            self.buffer.write(self.format(obj) + suffix)\n            self.buffer.flush()\n        elif self.depth_count == self.depth_upper_idx:\n            # Only print the cut-off indicator at the first occurrence\n            self.buffer.write(self.format(f\"...{suffix}\"))\n            self.buffer.flush()\n\n            # Prevent the caller from traversing at this level or higher\n            raise DepthExceededException()\n\n    def println(self, obj):\n        self.print(obj, suffix=\"\\n\")\n\n    def subgraph_add(self, obj):\n        if self.depth_lower_idx <= self.depth_count < self.depth_upper_idx:\n            # Only track printed subgraphs when they're actually printed\n            self.printed_subgraphs.add(obj)\n\n    def __repr__(self):  # pragma: no cover\n        return (\n            \"TFlowPrinter\\n\"\n            f\"\\tdepth_lower_idx={self.depth_lower_idx},\\tdepth_upper_idx={self.depth_upper_idx}\\n\"\n            f\"\\tindentation='{self.indentation}',\\tdepth_count={self.depth_count}\"\n        )"}
{"Repository": "mailin", "input": "Base IPv4 object. className _BaseV4(object) Method __init__ Method _explode_shorthand_ip_string Method _ip_int_from_string Method _parse_octet Method _string_from_ip_int Method max_prefixlen Method packed Method version Method is_reserved Method is_private Method is_multicast Method is_unspecified Method is_loopback Method is_link_local Attribute _version Attribute _max_prefixlen", "label": "class _BaseV4(object):\n    # Equivalent to 255.255.255.255 or 32 bits of 1's.\n    _ALL_ONES = (2**IPV4LENGTH) - 1\n    _DECIMAL_DIGITS = frozenset('0123456789')\n\n    def __init__(self, address):\n        self._version = 4\n        self._max_prefixlen = IPV4LENGTH\n\n    def _explode_shorthand_ip_string(self):\n        return str(self)\n\n    def _ip_int_from_string(self, ip_str):\n        octets = ip_str.split('.')\n        if len(octets) != 4:\n            raise AddressValueError(ip_str)\n\n        packed_ip = 0\n        for oc in octets:\n            try:\n                packed_ip = (packed_ip << 8) | self._parse_octet(oc)\n            except ValueError:\n                raise AddressValueError(ip_str)\n        return packed_ip\n\n    def _parse_octet(self, octet_str):\n        # Whitelist the characters, since int() allows a lot of bizarre stuff.\n        if not self._DECIMAL_DIGITS.issuperset(octet_str):\n            raise ValueError\n        octet_int = int(octet_str, 10)\n        # Disallow leading zeroes, because no clear standard exists on\n        # whether these should be interpreted as decimal or octal.\n        if octet_int > 255 or (octet_str[0] == '0' and len(octet_str) > 1):\n            raise ValueError\n        return octet_int\n\n    def _string_from_ip_int(self, ip_int):\n        octets = []\n        for _ in xrange(4):\n            octets.insert(0, str(ip_int & 0xFF))\n            ip_int >>= 8\n        return '.'.join(octets)\n\n    @property\n    def max_prefixlen(self):\n        return self._max_prefixlen\n\n    @property\n    def packed(self):\n        return v4_int_to_packed(self._ip)\n\n    @property\n    def version(self):\n        return self._version\n\n    @property\n    def is_reserved(self):\n       return self in IPv4Network('240.0.0.0/4')\n\n    @property\n    def is_private(self):\n        return (self in IPv4Network('10.0.0.0/8') or\n                self in IPv4Network('172.16.0.0/12') or\n                self in IPv4Network('192.168.0.0/16'))\n\n    @property\n    def is_multicast(self):\n        return self in IPv4Network('224.0.0.0/4')\n\n    @property\n    def is_unspecified(self):\n        return self in IPv4Network('0.0.0.0')\n\n    @property\n    def is_loopback(self):\n        return self in IPv4Network('127.0.0.0/8')\n\n    @property\n    def is_link_local(self):\n        return self in IPv4Network('169.254.0.0/16')"}
{"Repository": "benchmark", "input": "A CPU monitor that uses psutil to monitor CPU usage className CPUMonitor(Monitor) Method __init__ Method _get_cpu_stats Method _monitoring_iteration Method _collect_records Attribute _cpu_records Attribute _monitored_pid Attribute _monitored_pid", "label": "class CPUMonitor(Monitor):\n    def __init__(self, frequency, metrics_needed=[], monitored_pid=None):\n        super().__init__(frequency, metrics_needed)\n        # It is a raw record list. [timestamp, cpu_memory_usage, cpu_available_memory]\n        self._cpu_records = []\n        if monitored_pid:\n            self._monitored_pid = monitored_pid\n        else:\n            self._monitored_pid = os.getpid()\n \n\n\n    def _get_cpu_stats(self):\n        server_process = psutil.Process(self._monitored_pid)\n        process_memory_info = server_process.memory_full_info()\n        system_memory_info = psutil.virtual_memory()\n        # Divide by 1024*1024 to convert from bytes to MB\n        a_raw_record = (time.time_ns(), process_memory_info.uss // 1048576, system_memory_info.available // 1048576)\n        return a_raw_record\n        \n    def _monitoring_iteration(self):\n        if CPUPeakMemory in self._metrics:\n            self._cpu_records.append(self._get_cpu_stats())\n\n    def _collect_records(self):\n        records = []\n        for record in self._cpu_records:\n            records.append(CPUPeakMemory(timestamp=record[0], value=record[1]))\n        return records"}
{"Repository": "ceilometer", "input": "Base class for managing configuration file refresh className ConfigManagerBase(object) Method __init__ Method load_config Attribute conf", "label": "class ConfigManagerBase(object):\n    def __init__(self, conf):\n        self.conf = conf\n\n    def load_config(self, cfg_file):\n        if os.path.exists(cfg_file):\n            cfg_loc = cfg_file\n        else:\n            cfg_loc = self.conf.find_file(cfg_file)\n            if not cfg_loc:\n                LOG.debug(\"No pipeline definitions configuration file found! \"\n                          \"Using default config.\")\n                cfg_loc = pkg_resources.resource_filename(\n                    __name__, 'pipeline/data/' + cfg_file)\n        with open(cfg_loc) as fap:\n            conf = yaml.safe_load(fap)\n        LOG.debug(\"Config file: %s\", conf)\n        return conf"}
{"Repository": "sacalon", "input": "Function object to store function information className Function(object) Method __init__ Attribute name Attribute params Attribute return_type", "label": "class Function(object):\n    def __init__(self, name, params, return_type):\n        self.name = name\n        self.params = params  # type : dict\n        self.return_type = return_type"}
{"Repository": "scattertext", "input": "Cliff's Delta from Cliff (1993). className CliffsDelta(CorpusBasedTermScorer) Method _set_scorer_args Method get_scores Method get_score_df Method get_name", "label": "class CliffsDelta(CorpusBasedTermScorer):\n    def _set_scorer_args(self, alpha: float = 0.05, *args, **kwargs):\n        self.alpha = alpha\n\n    def get_scores(self, *args) -> pd.Series:\n        '''\n        In this case, args aren't used, since this information is taken\n        directly from the corpus categories.\n\n        Returns\n        -------\n        np.array, scores\n        '''\n        score_df = self.get_score_df()\n        return score_df['Metric']\n\n    def get_score_df(self, label_append=''):\n        cat_X, ncat_X = self._get_cat_and_ncat(self._get_X())\n        catXnorm = cat_X / cat_X.sum(axis=1)\n        ncatXnorm = ncat_X / ncat_X.sum(axis=1)\n        m = catXnorm.shape[0]\n        n = ncatXnorm.shape[0]\n        data = []\n        for i, term in tqdm(enumerate(self._get_terms()), total=len(self._get_terms())):\n            cati = np.repeat(catXnorm[:, i], n, axis=1).ravel().A1\n            ncati = np.repeat(ncatXnorm[:, i], m, axis=1).T.ravel().A1\n            deltai = np.zeros(n * m)\n            deltai[cati > ncati] = 1\n            deltai[cati < ncati] = -1\n            delta = deltai.sum() / (m * n)\n            deltars = deltai.reshape((n, m))\n            delta_r = deltars.sum(axis=0) / n\n            delta_c = deltars.sum(axis=1) / m\n            sd = np.sqrt(\n                (\n                        m ** 2 * ((delta_r - delta) ** 2).sum()\n                        + n ** 2 * ((delta_c - delta) ** 2).sum()\n                        - ((deltai - delta) ** 2).sum()\n                ) / (m * n * (m - 1) * (n - 1))\n            )\n            zcrit = ndtri(self.alpha)\n            lo = (delta - delta ** 3 - zcrit * sd * np.sqrt(1 - 2 * delta ** 2 + delta ** 4 + (zcrit * delta) ** 2)) / (\n                    1 - delta ** 2 - (zcrit * delta) ** 2\n            )\n\n            hi = (delta - delta ** 3 + zcrit * sd * np.sqrt(1 - 2 * delta ** 2 + delta ** 4 + (zcrit * delta) ** 2)) / (\n                    1 - delta ** 2 - (zcrit * delta) ** 2\n            )\n            datum = {\n                'term': term,\n                'Metric': delta,\n                'Stddev': sd,\n                f'Low-{(1-self.alpha) * 100}% CI': lo,\n                f'High-{(1-self.alpha) * 100}% CI': hi,\n            }\n            data.append(datum)\n        return pd.DataFrame(data).set_index('term').assign(\n            TermCount1=cat_X.sum(axis=0).A1,\n            TermCount2=ncat_X.sum(axis=0).A1,\n            DocCount1=(cat_X > 0).sum(axis=0).A1,\n            DocCount2=(ncat_X > 0).sum(axis=0).A1\n        )\n\n    def get_name(self):\n        return \"Cliff's Delta\""}
{"Repository": "sgfmill", "input": "Variant of Tree_node used for a game root. className _Root_tree_node(Tree_node) Method __init__ Attribute owner Attribute parent Attribute _children", "label": "class _Root_tree_node(Tree_node):\n    def __init__(self, property_map, owner):\n        self.owner = owner\n        self.parent = None\n        self._children = []\n        Node.__init__(self, property_map, owner.presenter)"}
{"Repository": "back2future.pytorch", "input": "Scales images to a particular size className Scale(object) Method __init__ Method __call__ Attribute h Attribute w", "label": "class Scale(object):\n    def __init__(self, h, w):\n        self.h = h\n        self.w = w\n\n    def __call__(self, images):\n        in_h, in_w, _ = images[0].shape\n        scaled_h, scaled_w = self.h , self.w\n        scaled_images = [ToTensor()(imresize(im, (scaled_h, scaled_w))) for im in images]\n        return scaled_images"}
{"Repository": "Synchronized-BatchNorm-PyTorch", "input": "Data Parallel with a replication callback. className DataParallelWithCallback(DataParallel) Method replicate", "label": "class DataParallelWithCallback(DataParallel):\n    def replicate(self, module, device_ids):\n        modules = super(DataParallelWithCallback, self).replicate(module, device_ids)\n        execute_replication_callbacks(modules)\n        return modules"}
{"Repository": "nlp_xiaojiang", "input": "fix convolutional 1D can't receive masked input, detail: https://github. className NonMaskingLayer(Layer) Method __init__ Method build Method compute_mask Method call Method compute_output_shape Attribute supports_masking", "label": "class NonMaskingLayer(Layer):\n    def __init__(self, **kwargs):\n\n        self.supports_masking = True\n\n        super(NonMaskingLayer, self).__init__(**kwargs)\n\n\n\n    def build(self, input_shape):\n\n        pass\n\n\n\n    def compute_mask(self, input, input_mask=None):\n\n        # do not pass the mask to the next layers\n\n        return None\n\n\n\n    def call(self, x, mask=None):\n\n        return x\n\n\n\n    def compute_output_shape(self, input_shape):\n\n        return input_shape"}
{"Repository": "mcpipy", "input": "Methods for entities className CmdEntity(CmdPositioner) Method __init__", "label": "class CmdEntity(CmdPositioner):\n    def __init__(self, connection):\n\n        CmdPositioner.__init__(self, connection, \"entity\")"}
{"Repository": "Min-SNR-Diffusion-Training", "input": "Redirect stderr to stdout, optionally print stdout to a file, and optionally force flushing on both stdout and the file. className Logger(object) Method __init__ Method __enter__ Method __exit__ Method write Method flush Method close Attribute file Attribute file Attribute should_flush Attribute stdout Attribute stderr", "label": "class Logger(object):\n    def __init__(self, file_name: Optional[str] = None, file_mode: str = \"w\", should_flush: bool = True):\n        self.file = None\n\n        if file_name is not None:\n            self.file = open(file_name, file_mode)\n\n        self.should_flush = should_flush\n        self.stdout = sys.stdout\n        self.stderr = sys.stderr\n\n        sys.stdout = self\n        sys.stderr = self\n\n    def __enter__(self) -> \"Logger\":\n        return self\n\n    def __exit__(self, exc_type: Any, exc_value: Any, traceback: Any) -> None:\n        self.close()\n\n    def write(self, text: Union[str, bytes]) -> None:\n        if isinstance(text, bytes):\n            text = text.decode()\n        if len(text) == 0: # workaround for a bug in VSCode debugger: sys.stdout.write(''); sys.stdout.flush() => crash\n            return\n\n        if self.file is not None:\n            self.file.write(text)\n\n        self.stdout.write(text)\n\n        if self.should_flush:\n            self.flush()\n\n    def flush(self) -> None:\n        if self.file is not None:\n            self.file.flush()\n\n        self.stdout.flush()\n\n    def close(self) -> None:\n        self.flush()\n\n        # if using multiple loggers, prevent closing in wrong order\n        if sys.stdout is self:\n            sys.stdout = self.stdout\n        if sys.stderr is self:\n            sys.stderr = self.stderr\n\n        if self.file is not None:\n            self.file.close()\n            self.file = None"}
{"Repository": "jax_privacy", "input": "Configuration for restoring the model. className WithRestoreModelConfig(ModelConfig) Method make Method init", "label": "class WithRestoreModelConfig(ModelConfig):\n  model: ModelConfig\n  path: str | None = None\n  params_key: str | None = None\n  network_state_key: str | None = None\n  layer_to_ignore: str | None = None\n\n  def make(self, num_classes: int) -> Model:\n    model = self.model.make(num_classes=num_classes)\n    def init(rng_key: jax.Array, image: jax.Array, is_training: bool):\n      params, network_state = model.init(rng_key, image, is_training)\n      if self.path:\n        params, network_state = common.restore_from_path(\n            restore_path=self.path,\n            params_key=self.params_key,\n            network_state_key=self.network_state_key,\n            layer_to_reset=self.layer_to_ignore,\n            params_init=params,\n            network_state_init=network_state,\n        )\n\n      return params, network_state\n    return Model(init=init, apply=model.apply)"}
{"Repository": "MMD4Maya", "input": "MultiByteCharSetProber className MultiByteCharSetProber(CharSetProber) Method __init__ Method reset Method charset_name Method language Method feed Method get_confidence Attribute distribution_analyzer Attribute coding_sm Attribute _last_char", "label": "class MultiByteCharSetProber(CharSetProber):\n    def __init__(self, lang_filter=None):\n        super(MultiByteCharSetProber, self).__init__(lang_filter=lang_filter)\n        self.distribution_analyzer = None\n        self.coding_sm = None\n        self._last_char = [0, 0]\n\n    def reset(self):\n        super(MultiByteCharSetProber, self).reset()\n        if self.coding_sm:\n            self.coding_sm.reset()\n        if self.distribution_analyzer:\n            self.distribution_analyzer.reset()\n        self._last_char = [0, 0]\n\n    @property\n    def charset_name(self):\n        raise NotImplementedError\n\n    @property\n    def language(self):\n        raise NotImplementedError\n\n    def feed(self, byte_str):\n        for i in range(len(byte_str)):\n            coding_state = self.coding_sm.next_state(byte_str[i])\n            if coding_state == MachineState.ERROR:\n                self.logger.debug('%s %s prober hit error at byte %s',\n                                  self.charset_name, self.language, i)\n                self._state = ProbingState.NOT_ME\n                break\n            elif coding_state == MachineState.ITS_ME:\n                self._state = ProbingState.FOUND_IT\n                break\n            elif coding_state == MachineState.START:\n                char_len = self.coding_sm.get_current_charlen()\n                if i == 0:\n                    self._last_char[1] = byte_str[0]\n                    self.distribution_analyzer.feed(self._last_char, char_len)\n                else:\n                    self.distribution_analyzer.feed(byte_str[i - 1:i + 1],\n                                                    char_len)\n\n        self._last_char[0] = byte_str[-1]\n\n        if self.state == ProbingState.DETECTING:\n            if (self.distribution_analyzer.got_enough_data() and\n                    (self.get_confidence() > self.SHORTCUT_THRESHOLD)):\n                self._state = ProbingState.FOUND_IT\n\n        return self.state\n\n    def get_confidence(self):\n        return self.distribution_analyzer.get_confidence()"}
{"Repository": "scrapy-rotating-proxies", "input": "Downloader middleware for detecting bans. className BanDetectionMiddleware(object) Method response_is_ban Method exception_is_ban Method response_is_ban Method exception_is_ban Method __init__ Method from_crawler Method _load_policy Method process_response Method process_exception Attribute stats Attribute policy", "label": "class BanDetectionMiddleware(object):\n    def __init__(self, stats, policy):\n        self.stats = stats\n        self.policy = policy\n\n    @classmethod\n    def from_crawler(cls, crawler):\n        return cls(crawler.stats, cls._load_policy(crawler))\n\n    @classmethod\n    def _load_policy(cls, crawler):\n        policy_path = crawler.settings.get(\n            'ROTATING_PROXY_BAN_POLICY',\n            'rotating_proxies.policy.BanDetectionPolicy'\n        )\n        policy_cls = load_object(policy_path)\n        if hasattr(policy_cls, 'from_crawler'):\n            return policy_cls.from_crawler(crawler)\n        else:\n            return policy_cls()\n\n    def process_response(self, request, response, spider):\n        is_ban = getattr(spider, 'response_is_ban',\n                         self.policy.response_is_ban)\n        ban = is_ban(request, response)\n        request.meta['_ban'] = ban\n        if ban:\n            self.stats.inc_value(\"bans/status/%s\" % response.status)\n            if not len(response.body):\n                self.stats.inc_value(\"bans/empty\")\n        return response\n\n    def process_exception(self, request, exception, spider):\n        is_ban = getattr(spider, 'exception_is_ban',\n                         self.policy.exception_is_ban)\n        ban = is_ban(request, exception)\n        if ban:\n            ex_class = \"%s.%s\" % (exception.__class__.__module__,\n                                  exception.__class__.__name__)\n            self.stats.inc_value(\"bans/error/%s\" % ex_class)\n        request.meta['_ban'] = ban"}
{"Repository": "cc", "input": "Converts a list of numpy.ndarray (H x W x C) along with a intrinsics matrix to a list of torch.FloatTensor of shape (C x H x W) with a intrinsics tensor. className ArrayToTensor(object) Method __call__", "label": "class ArrayToTensor(object):\n    def __call__(self, images, intrinsics):\n        tensors = []\n        for im in images:\n            # put it from HWC to CHW format\n            im = np.transpose(im, (2, 0, 1))\n            # handle numpy array\n            tensors.append(torch.from_numpy(im).float()/255)\n        return tensors, intrinsics"}
{"Repository": "CardStock", "input": "Keep wxImages around and sorted by size, so we can avoid creating new ones every time we need to use one to build a region. className ImageFactory(object) Method shared Method __init__ Method ClearCache Method NormalizeSize Method RecycleImage Method GetImage Attribute map", "label": "class ImageFactory(object):\n    imgFactory = None\n\n    @classmethod\n    def shared(cls):\n        if not cls.imgFactory:\n            cls.imgFactory = ImageFactory()\n        return cls.imgFactory\n\n    def __init__(self):\n        self.map = None\n        self.ClearCache()\n\n    def ClearCache(self):\n        self.map = {}\n\n    @staticmethod\n    def NormalizeSize(w, h):\n        (w, h) = (int(w), int(h))\n        return (w+(63-((w-1)%64)), h+(63-((h-1)%64)))\n\n    def RecycleImage(self, img):\n        (w, h) = self.NormalizeSize(*img.GetSize())\n        if (w,h) in self.map:\n            self.map[(w,h)].append(img)\n        else:\n            self.map[(w, h)] = [img]\n\n    def GetImage(self, w, h):\n        (w, h) = self.NormalizeSize(w, h)\n        if (w,h) in self.map and len(self.map[(w,h)]):\n            img = self.map[(w,h)].pop()\n            img.Clear()\n            return img\n\n        img = wx.Image(w, h, clear=True)\n        return img"}
{"Repository": "django-djangui", "input": "This class takes an argument parser entry and assigns it to a Build spec className ArgParseNode(object) Method __init__ Method name Method __str__ Method to_django Attribute node_attrs", "label": "class ArgParseNode(object):\n    def __init__(self, action=None):\n        fields = ACTION_CLASS_TO_TYPE_FIELD.get(type(action), TYPE_FIELDS)\n        field_type = fields.get(action.type)\n        if field_type is None:\n            field_types = [i for i in fields.keys() if i is not None and issubclass(type(action.type), i)]\n            if len(field_types) > 1:\n                field_types = [i for i in fields.keys() if i is not None and isinstance(action.type, i)]\n            if len(field_types) == 1:\n                field_type = fields[field_types[0]]\n        self.node_attrs = dict([(i, field_type[i]) for i in GLOBAL_ATTRS])\n        null_check = field_type['nullcheck'](action)\n        for attr, attr_dict in six.iteritems(field_type['attr_kwargs']):\n            if attr_dict is None:\n                continue\n            if attr == 'value' and null_check:\n                continue\n            if 'action_name' in attr_dict:\n                self.node_attrs[attr] = getattr(action, attr_dict['action_name'])\n            elif 'callback' in attr_dict:\n                self.node_attrs[attr] = attr_dict['callback'](action)\n\n    @property\n    def name(self):\n        return self.node_attrs.get('name')\n\n    def __str__(self):\n        return json.dumps(self.node_attrs)\n\n    def to_django(self):\n        exclude = {'name', 'model'}\n        field_module = 'models'\n        django_kwargs = {}\n        if self.node_attrs['model'] == 'CharField':\n            django_kwargs['max_length'] = 255\n        django_kwargs['blank'] = not self.node_attrs['required']\n        try:\n            django_kwargs['default'] = self.node_attrs['value']\n        except KeyError:\n            pass\n        return u'{0} = {1}.{2}({3})'.format(self.node_attrs['name'], field_module, self.node_attrs['model'],\n                                           ', '.join(['{0}={1}'.format(i,v) for i,v in six.iteritems(django_kwargs)]),)"}
{"Repository": "MaskTextSpotter", "input": "Determine which FPN level each RoI in a set of RoIs should map to based on the heuristic in the FPN paper. className LevelMapper(object) Method __init__ Method __call__ Attribute k_min Attribute k_max Attribute s0 Attribute lvl0 Attribute eps", "label": "class LevelMapper(object):\n    def __init__(self, k_min, k_max, canonical_scale=224, canonical_level=4, eps=1e-6):\n        self.k_min = k_min\n        self.k_max = k_max\n        self.s0 = canonical_scale\n        self.lvl0 = canonical_level\n        self.eps = eps\n\n    def __call__(self, boxlists):\n        # Compute level ids\n        s = torch.sqrt(cat([boxlist.area() for boxlist in boxlists]))\n\n        # Eqn.(1) in FPN paper\n        target_lvls = torch.floor(self.lvl0 + torch.log2(s / self.s0 + self.eps))\n        target_lvls = torch.clamp(target_lvls, min=self.k_min, max=self.k_max)\n        return target_lvls.to(torch.int64) - self.k_min"}
{"Repository": "WeightWatcher", "input": "Test we can read 1 or more safetensors files and access the tensors as if they are stored in a dict className Test_SafeTensorsDict(Test_Base) Method setUpClass Method setUp Method test_SafeTensorsDict", "label": "class Test_SafeTensorsDict(Test_Base):\n    @classmethod\n    def setUpClass(cls):\n        ww.weightwatcher.torch = torch\n        return\n    \n    def setUp(self):\n        print(\"\\n-------------------------------------\\nIn Test_SafeTensorsDict:\", self._testMethodName)\n        logger = logging.getLogger(WW_NAME) \n        logger.setLevel(logging.INFO)\n        return\n    \n    \n    def test_SafeTensorsDict(self):\n        state_dict = models.resnet18().state_dict()\n        actual_keys = [k for k in state_dict.keys()]\n        \n        print(actual_keys)\n        model_name = 'resnet18'\n        weights_dir = None\n        \n        with TemporaryDirectory(dir=TEST_TMP_DIR, prefix=\"ww_\") as model_dir:\n            \n            print(f\"using {model_dir} as model_dir\")            \n            state_dict_filename = os.path.join(model_dir, \"model.0.safetensors\")\n            safe_save(state_dict, state_dict_filename)      \n            \n            fileglob = f\"{model_dir}/model*safetensors\"\n\n            safetensors_dict =  ww.weightwatcher.SafeTensorDict(fileglob)\n            \n            for key in actual_keys:\n                T = safetensors_dict[key]\n                self.assertIsNotNone(T)\n                \n        return"}
{"Repository": "ntplib", "input": "NTP packet class. className NTPPacket(object) Method __init__ Method to_data Method from_data Attribute leap Attribute version Attribute mode Attribute stratum Attribute poll Attribute precision Attribute root_delay Attribute root_dispersion Attribute ref_id Attribute ref_timestamp Attribute orig_timestamp Attribute recv_timestamp Attribute tx_timestamp", "label": "class NTPPacket(object):\n    _PACKET_FORMAT = \"!B B B b 11I\"\n    def __init__(self, version=2, mode=3, tx_timestamp=0):\n        self.leap = 0\n        self.version = version\n        self.mode = mode\n        self.stratum = 0\n        self.poll = 0\n        self.precision = 0\n        self.root_delay = 0\n        self.root_dispersion = 0\n        self.ref_id = 0\n        self.ref_timestamp = 0\n        self.orig_timestamp = 0\n        self.recv_timestamp = 0\n        self.tx_timestamp = tx_timestamp\n    def to_data(self):\n        try:\n            packed = struct.pack(\n                NTPPacket._PACKET_FORMAT,\n                (self.leap << 6 | self.version << 3 | self.mode),\n                self.stratum,\n                self.poll,\n                self.precision,\n                _to_int(self.root_delay) << 16 | _to_frac(self.root_delay, 16),\n                _to_int(self.root_dispersion) << 16 |\n                _to_frac(self.root_dispersion, 16),\n                self.ref_id,\n                _to_int(self.ref_timestamp),\n                _to_frac(self.ref_timestamp),\n                _to_int(self.orig_timestamp),\n                _to_frac(self.orig_timestamp),\n                _to_int(self.recv_timestamp),\n                _to_frac(self.recv_timestamp),\n                _to_int(self.tx_timestamp),\n                _to_frac(self.tx_timestamp))\n        except struct.error:\n            raise NTPException(\"Invalid NTP packet fields.\")\n        return packed\n\n    def from_data(self, data):\n        try:\n            unpacked = struct.unpack(\n                NTPPacket._PACKET_FORMAT,\n                data[0:struct.calcsize(NTPPacket._PACKET_FORMAT)]\n            )\n        except struct.error:\n            raise NTPException(\"Invalid NTP packet.\")\n\n        self.leap = unpacked[0] >> 6 & 0x3\n        self.version = unpacked[0] >> 3 & 0x7\n        self.mode = unpacked[0] & 0x7\n        self.stratum = unpacked[1]\n        self.poll = unpacked[2]\n        self.precision = unpacked[3]\n        self.root_delay = float(unpacked[4])/2**16\n        self.root_dispersion = float(unpacked[5])/2**16\n        self.ref_id = unpacked[6]\n        self.ref_timestamp = _to_time(unpacked[7], unpacked[8])\n        self.orig_timestamp = _to_time(unpacked[9], unpacked[10])\n        self.recv_timestamp = _to_time(unpacked[11], unpacked[12])\n        self.tx_timestamp = _to_time(unpacked[13], unpacked[14])"}
{"Repository": "UtterancePIT-Speech-Separation", "input": "Wrapper for short-time fourier transform of dataset className SpectrogramReader(object) Method __init__ Method __len__ Method __contains__ Method _load Method __iter__ Method __getitem__ Attribute stft_kwargs Attribute wave_dict Attribute wave_keys", "label": "class SpectrogramReader(object):\n    def __init__(self, wave_scp, **kwargs):\n        if not os.path.exists(wave_scp):\n            raise FileNotFoundError(\"Could not find file {}\".format(wave_scp))\n        self.stft_kwargs = kwargs\n        self.wave_dict = parse_scps(wave_scp)\n        self.wave_keys = [key for key in self.wave_dict.keys()]\n        logger.info(\n            \"Create SpectrogramReader for {} with {} utterances\".format(\n                wave_scp, len(self.wave_dict)))\n\n    def __len__(self):\n        return len(self.wave_dict)\n\n    def __contains__(self, key):\n        return key in self.wave_dict\n\n    # stft\n    def _load(self, key):\n        return stft(self.wave_dict[key], **self.stft_kwargs)\n\n    '''\n    # sequential index\n    def __iter__(self):\n        for key in self.wave_dict:\n            yield key, self._load(key)\n    '''\n    # random index\n\n    def __getitem__(self, key):\n        if key not in self.wave_dict:\n            raise KeyError(\"Could not find utterance {}\".format(key))\n        return self._load(key)"}
{"Repository": "FACEGOOD-Audio2Face", "input": "Formant Layer className FormantLayer(Model) Method __init__ Method call Attribute kernels_size Attribute outputs Attribute formant_layers", "label": "class FormantLayer(Model):\n    def __init__(self, kernels_size=None, outputs=None):\n        super(FormantLayer, self).__init__()\n\n        if kernels_size is None:\n            kernels_size = [[3, 1], [3, 1], [3, 1], [3, 1], [2, 1]]\n        if outputs is None:\n            outputs = [72, 108, 162, 243, 256]\n\n        self.kernels_size = kernels_size\n        self.outputs = outputs\n\n        self.formant_layers = models.Sequential()\n        for i in range(len(self.kernels_size)):\n            self.formant_layers.add(conv2d_layer(filters=self.outputs[i],\n                                                 kernel_size=self.kernels_size[i],\n                                                 strides=[2, 1]))\n\n    def call(self, x):\n        x = self.formant_layers(x)  \n        return x"}
{"Repository": "NSP-BERT", "input": "Data Generator className data_generator(DataGenerator) Method __init__ Method __iter__ Attribute is_pre Attribute is_soft_pos", "label": "class data_generator(DataGenerator):\n    def __init__(self, is_pre=True, is_soft_pos=False, *args, **kwargs):\n        super(data_generator, self).__init__(*args, **kwargs)\n        self.is_pre = is_pre\n        self.is_soft_pos = is_soft_pos\n\n    def __iter__(self, random=False):\n        batch_token_ids, batch_segment_ids, batch_output_ids = [], [], []\n        batch_position_ids = []\n        for is_end, (text, candi) in self.sample(random):\n            text_1, text_2 = text.split(\"#idiom#\")\n            if (self.is_soft_pos):\n                text = text.replace(\"#idiom#\", \"\")\n            if (self.is_pre):\n                token_ids, segment_ids = tokenizer.encode(first_text=candi, second_text=text, maxlen=maxlen)\n            else:\n                token_ids, segment_ids = tokenizer.encode(first_text=text, second_text=candi, maxlen=maxlen)\n            position_ids = []\n            if (self.is_soft_pos):\n                tokens_1 = tokenizer._tokenize(text_1)\n                tokens_2 = tokenizer._tokenize(text_2)\n                tokens_c = tokenizer._tokenize(candi)\n                position_ids = [0]  # \"[CSL]\n                if (self.is_pre):\n                    position_ids += [2 + len(tokens_1) + p_id for p_id in\n                                     range(len(tokens_c))]  # \"[CLS]\" + TEXT1 + TEXT2 + \"[SEP]\" + CANDI\n                    position_ids += [1]  # \"[CLS]\" + TEXT1 + TEXT2 + \"[SEP]\"\n                    position_ids += [position_ids[-1] + p_id + 1 for p_id in range(len(tokens_1))]  # \"[CLS]\" + TEXT1\n                    position_ids += [position_ids[-1] + len(tokens_c) + p_id + 1 for p_id in\n                                     range(len(tokens_2))]  # \"[CLS]\" + TEXT1 +TEXT2\n                    position_ids += [position_ids[-1] + 1]\n\n                else:\n                    position_ids += [position_ids[-1] + p_id + 1 for p_id in range(len(tokens_1))]  # \"[CLS]\" + TEXT1\n                    position_ids += [position_ids[-1] + len(tokens_c) + p_id + 1 for p_id in\n                                     range(len(tokens_2))]  # \"[CLS]\" + TEXT1 +TEXT2\n                    position_ids += [position_ids[-1] + 1]  # \"[CLS]\" + TEXT1 + TEXT2 + \"[SEP]\"\n                    position_ids += [1 + len(tokens_1) + p_id for p_id in\n                                     range(len(tokens_c))]  # \"[CLS]\" + TEXT1 + TEXT2 + \"[SEP]\" + CANDI\n                    position_ids += [len(tokens_1 + tokens_2 + tokens_c) + 2]\n                    # position_ids += [position_ids[-1] + 1]\n\n            source_ids, target_ids = token_ids[:], token_ids[:]\n            # label_ids = tokenizer.encode(label)[0][1:-1]\n\n            batch_token_ids.append(source_ids)\n            batch_segment_ids.append(segment_ids)\n            batch_position_ids.append(position_ids)\n\n            if len(batch_token_ids) == self.batch_size or is_end:\n                batch_token_ids = sequence_padding(batch_token_ids)\n                batch_segment_ids = sequence_padding(batch_segment_ids)\n                batch_position_ids = sequence_padding(batch_position_ids)\n                if (self.is_soft_pos):\n                    yield [batch_token_ids, batch_segment_ids, batch_position_ids], None\n                else:\n                    yield [batch_token_ids, batch_segment_ids], None\n                batch_token_ids, batch_segment_ids, batch_position_ids = [], [], []"}
{"Repository": "python-astar", "input": "sample use of the astar algorithm. className MazeSolver(AStar) Method __init__ Method heuristic_cost_estimate Method distance_between Method neighbors Attribute lines Attribute width Attribute height", "label": "class MazeSolver(AStar):\n    def __init__(self, maze):\n        self.lines = maze.strip().split('\\n')\n        self.width = len(self.lines[0])\n        self.height = len(self.lines)\n\n    def heuristic_cost_estimate(self, n1, n2):\n        (x1, y1) = n1\n        (x2, y2) = n2\n        return math.hypot(x2 - x1, y2 - y1)\n\n    def distance_between(self, n1, n2):\n        return 1\n\n    def neighbors(self, node):\n        x, y = node\n        return[(nx, ny) for nx, ny in[(x, y - 1), (x, y + 1), (x - 1, y), (x + 1, y)]if 0 <= nx < self.width and 0 <= ny < self.height and self.lines[ny][nx] == ' ']"}
{"Repository": "pyscript", "input": "Generic Pyscript Entity className PyscriptEntity(RestoreEntity) Method set_state Method set_attributes", "label": "class PyscriptEntity(RestoreEntity):\n    _attr_extra_state_attributes: dict\n    _attr_state: StateType = STATE_UNKNOWN\n\n    def set_state(self, state):\n        self._attr_state = state\n\n    def set_attributes(self, attributes):\n        self._attr_extra_state_attributes = attributes"}
{"Repository": "summarization-sing-pair-mix", "input": "Class to represent a hypothesis during beam search. Holds all the information needed for the hypothesis. className Hypothesis(object) Method __init__ Method extend Method does_trigram_exist Method latest_token Method log_prob Method avg_log_prob Attribute tokens Attribute log_probs Attribute state Attribute attn_dists Attribute p_gens Attribute coverage Attribute similarity Attribute summ_sent_idx Attribute already_added", "label": "class Hypothesis(object):\n    def __init__(self, tokens, log_probs, state, attn_dists, p_gens, coverage, summ_sent_idx, already_added):\n        self.tokens = tokens\n        self.log_probs = log_probs\n        self.state = state\n        self.attn_dists = attn_dists\n        self.p_gens = p_gens\n        self.coverage = coverage\n        self.similarity = 0.\n        self.summ_sent_idx = summ_sent_idx\n        self.already_added = already_added\n\n    def extend(self, token, log_prob, state, attn_dist, p_gen, coverage, summ_sent_idx):\n        if self.does_trigram_exist(token):\n            log_prob = -1000\n        return Hypothesis(tokens=self.tokens + [token],\n                          log_probs=self.log_probs + [log_prob],\n                          state=state,\n                          attn_dists=self.attn_dists + [attn_dist],\n                          p_gens=self.p_gens + [p_gen],\n                          coverage=coverage,\n                          summ_sent_idx=summ_sent_idx,\n                          already_added=self.already_added)\n\n    # Trigram trick\n    def does_trigram_exist(self, token):\n        if len(self.tokens) < 2:\n            return False\n        candidate_trigram = self.tokens[-2:] + [token]\n        for i in range(len(self.tokens)-2):\n            if self.tokens[i:i+3] == candidate_trigram:\n                return True\n        return False\n\n    @property\n    def latest_token(self):\n        return self.tokens[-1]\n\n    @property\n    def log_prob(self):\n        # the log probability of the hypothesis so far is the sum of the log probabilities of the tokens so far\n        return sum(self.log_probs)\n\n    @property\n    def avg_log_prob(self):\n        # normalize log probability by number of tokens (otherwise longer sequences always have lower probability)\n        return self.log_prob / len(self.tokens)"}
{"Repository": "covid19-sir", "input": "Access \"COVID-19 Data Hub\" server. className _COVID19dh(_DataBase) Method _country Method _province Method _city", "label": "class _COVID19dh(_DataBase):\n    # File title without extensions and suffix\n    TITLE = \"covid19dh\"\n    # Dictionary of column names\n    _OXCGRT_COLS_RAW = [\n        \"school_closing\",\n        \"workplace_closing\",\n        \"cancel_events\",\n        \"gatherings_restrictions\",\n        \"transport_closing\",\n        \"stay_home_restrictions\",\n        \"internal_movement_restrictions\",\n        \"international_movement_restrictions\",\n        \"information_campaigns\",\n        \"testing_policy\",\n        \"contact_tracing\",\n        \"stringency_index\",\n    ]\n    OXCGRT_VARS = [v.capitalize() for v in _OXCGRT_COLS_RAW]\n    COL_DICT = {\n        \"date\": Term.DATE,\n        \"iso_alpha_3\": Term.ISO3,\n        \"administrative_area_level_1\": Term.COUNTRY,\n        \"administrative_area_level_2\": Term.PROVINCE,\n        \"administrative_area_level_3\": Term.CITY,\n        \"tests\": Term.TESTS,\n        \"confirmed\": Term.C,\n        \"deaths\": Term.F,\n        \"recovered\": Term.R,\n        \"population\": Term.N,\n        **dict(zip(_OXCGRT_COLS_RAW, OXCGRT_VARS)),\n    }\n    # Stdout when downloading (shown at most one time)\n    STDOUT = \"Retrieving datasets from COVID-19 Data Hub https://covid19datahub.io/\"\n    # Citation\n    CITATION = 'Guidotti, E., Ardia, D., (2020), \"COVID-19 Data Hub\",' \\\n        ' Journal of Open Source Software 5(51):2376, doi: 10.21105/joss.02376.'\n\n    def _country(self):\n        url = \"https://storage.covid19datahub.io/level/1.csv.zip\"\n        df = self._provide(\n            url=url, suffix=\"_level1\", columns=list(self.COL_DICT.keys()), date=\"date\", date_format=\"%Y-%m-%d\")\n        # ships will be regarded as provinces of \"Others\" country\n        df[self.PROVINCE] = df[self.PROVINCE].astype(object)\n        ships = df.loc[df[self.ISO3].isna(), self.COUNTRY].unique()\n        for ship in ships:\n            df.loc[df[self.COUNTRY] == ship, [self.ISO3, self.PROVINCE]] = [self.OTHERS, ship]\n        return df\n\n    def _province(self, country):\n        iso3 = self._to_iso3(country)[0]\n        if iso3 == self.OTHERS:\n            df = self._country()\n            return df.loc[df[self.ISO3] == iso3]\n        url = f\"https://storage.covid19datahub.io/country/{iso3}.csv.zip\"\n        df = self._provide(\n            url=url, suffix=f\"_{iso3.lower()}\", columns=list(self.COL_DICT.keys()), date=\"date\", date_format=\"%Y-%m-%d\")\n        df = df.loc[(~df[self.PROVINCE].isna()) & (df[self.CITY].isna())]\n        df.loc[:, self.CITY] = df.loc[:, self.CITY].fillna(self.NA)\n        return df\n\n    def _city(self, country, province):\n        iso3 = self._to_iso3(country)[0]\n        url = f\"https://storage.covid19datahub.io/country/{iso3}.csv.zip\"\n        df = self._provide(\n            url=url, suffix=f\"_{iso3.lower()}\", columns=list(self.COL_DICT.keys()), date=\"date\", date_format=\"%Y-%m-%d\")\n        return df.loc[(df[self.PROVINCE] == province) & (~df[self.CITY].isna())]"}
{"Repository": "py-enumerable", "input": "Class to hold state for taking subset of consecutive elements in a collection className TakeEnumerable(Enumerable) Method __init__ Method __iter__ Attribute n", "label": "class TakeEnumerable(Enumerable):\n    def __init__(self, enumerable, n):\n        super(TakeEnumerable, self).__init__(enumerable)\n        self.n = n\n\n    def __iter__(self):\n        for index, element in enumerate(self._iterable):\n            if index < self.n:\n                yield element"}
{"Repository": "DRT", "input": "Computes and stores the average and current value className AverageMeter(object) Method __init__ Method reset Method update", "label": "class AverageMeter(object):\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count"}
{"Repository": "opendxl-client-python", "input": "Utility class used to track DXL Client-specific object instances className _ObjectTracker(object) Method __init__ Method enabled Method enabled Method obj_constructed Method obj_destructed Method obj_count Method get_instance Attribute _obj_count Attribute _enabled Attribute _lock Attribute _logger", "label": "class _ObjectTracker(object):\n    # The object tracker instance (singleton)\n    _instance = None\n\n    def __init__(self):\n        self._obj_count = 0\n        self._enabled = False\n        self._lock = RLock()\n        self._logger = logging.getLogger(__name__)\n\n    @property\n    def enabled(self):\n        return self._enabled\n\n    @enabled.setter\n    def enabled(self, val):\n        self._enabled = val\n\n    def obj_constructed(self, obj):\n        if self._enabled:\n            with self._lock:\n                self._obj_count += 1\n                self._logger.debug(\n                    \"Constructed: %s.%s objCount=%d\",\n                    obj.__module__, obj.__class__.__name__, self._obj_count)\n\n    def obj_destructed(self, obj):\n        if self._enabled:\n            with self._lock:\n                self._obj_count -= 1\n                self._logger.debug(\n                    \"Destructed: %s.%s objCount=%d\",\n                    obj.__module__, obj.__class__.__name__, self._obj_count)\n\n    @property\n    def obj_count(self):\n        with self._lock:\n            return self._obj_count\n\n    @staticmethod\n    def get_instance():\n        # Instance creation should be synchronized\n        if not _ObjectTracker._instance:\n            _ObjectTracker._instance = _ObjectTracker()\n\n        return _ObjectTracker._instance"}
{"Repository": "sen2like", "input": "Writer of S2H/F Product MTD file for product created from LS product className LandsatToS2LProductMtdWriter(S2LProductMtdWriter) Method __init__ Method _generate_tile_id Method _specific_replaces", "label": "class LandsatToS2LProductMtdWriter(S2LProductMtdWriter):\n    # Redefine constructor to DELIBERATELY Force\n    # - \"Landsat\" for sensor\n    # - \"None\" for input_xml_path\n    # And have a similar constructor contract\n    def __init__(self, sensor: str, input_xml_path: str, H_F='H', outfile: str = None):\n        super().__init__(\"Landsat\", None, H_F, outfile)\n\n    def _generate_tile_id(self, product: S2L_Product):\n        return _generate_landsat8_tile_id(product, self.H_F)\n\n    def _specific_replaces(self, product: S2L_Product):\n\n        # GENERAL_INFO\n        # ------------\n        acq_date = datetime.strftime(product.acqdate, ISO_DATE_TIME_FORMAT)\n        change_elm(self.root_out, rpath='./General_Info/Product_Info/PRODUCT_START_TIME', new_value=acq_date)\n        change_elm(self.root_out, rpath='./General_Info/Product_Info/PRODUCT_STOP_TIME', new_value=acq_date)\n        change_elm(self.root_out, rpath='./General_Info/Product_Info/PRODUCT_TYPE',\n                   new_value=f'{product.sensor}OLI2{self.H_F}')\n        change_elm(self.root_out, rpath='./General_Info/Product_Info/Datatake/SPACECRAFT_NAME',\n                   new_value=product.mtl.mission)\n        change_elm(self.root_out, rpath='./General_Info/Product_Info/Datatake/DATATAKE_SENSING_START',\n                   new_value=acq_date)\n        change_elm(self.root_out, rpath='./General_Info/Product_Info/Datatake/SENSING_ORBIT_NUMBER',\n                   new_value=product.mtl.relative_orbit)\n\n        if not config.getboolean('doSbaf'):\n            # FIXME : get product image characteristics from origin sensor (LS8 here),\n            #         copying from another template fro example\n            pass\n\n        U = _distance_variation_corr(product.acqdate)\n        change_elm(self.root_out, rpath='./General_Info/Product_Image_Characteristics/Reflectance_Conversion/U',\n                   new_value=str(U))\n\n        # Auxiliary_Data_Info\n        # -------------------\n        self.remove_children(GIPP_LIST_PATH)\n\n        config_fn = os.path.splitext(os.path.basename(config.parser.config_file))[0]\n        create_child(self.root_out, GIPP_LIST_PATH, tag=\"GIPP_FILENAME\", text=config_fn,\n                     attribs={\"version\": version.baseline, \"type\": \"GIP_S2LIKE\"})\n\n        # Quality_Indicators_Info\n        # -----------------------\n        self.remove_children(QUALITY_INDICATOR_PATH, exceptions=['Cloud_Coverage_Assessment'])\n        change_elm(self.root_out, './Quality_Indicators_Info/Cloud_Coverage_Assessment',\n                   new_value=product.mtl.cloud_cover)"}
{"Repository": "ganbert", "input": "Configuration for `BertModel`. className BertConfig(object) Method from_dict Method from_json_file Method to_dict Method to_json_string", "label": "class BertConfig(object):\n  def __init__(self,\n               vocab_size,\n               hidden_size=768,\n               num_hidden_layers=12,\n               num_attention_heads=12,\n               intermediate_size=3072,\n               hidden_act=\"gelu\",\n               hidden_dropout_prob=0.1,\n               attention_probs_dropout_prob=0.1,\n               max_position_embeddings=512,\n               type_vocab_size=16,\n               initializer_range=0.02):\n    self.vocab_size = vocab_size\n    self.hidden_size = hidden_size\n    self.num_hidden_layers = num_hidden_layers\n    self.num_attention_heads = num_attention_heads\n    self.hidden_act = hidden_act\n    self.intermediate_size = intermediate_size\n    self.hidden_dropout_prob = hidden_dropout_prob\n    self.attention_probs_dropout_prob = attention_probs_dropout_prob\n    self.max_position_embeddings = max_position_embeddings\n    self.type_vocab_size = type_vocab_size\n    self.initializer_range = initializer_range\n\n  @classmethod\n  def from_dict(cls, json_object):\n    config = BertConfig(vocab_size=None)\n    for (key, value) in six.iteritems(json_object):\n      config.__dict__[key] = value\n    return config\n\n  @classmethod\n  def from_json_file(cls, json_file):\n    with tf.gfile.GFile(json_file, \"r\") as reader:\n      text = reader.read()\n    return cls.from_dict(json.loads(text))\n\n  def to_dict(self):\n    output = copy.deepcopy(self.__dict__)\n    return output\n\n  def to_json_string(self):\n    return json.dumps(self.to_dict(), indent=2, sort_keys=True) + \"\\n\""}
{"Repository": "EvilTwinFramework", "input": "Minimalistic representation of a WPA handshake className WPAHandshake(object) Method __init__ Attribute id Attribute ssid Attribute client_mac Attribute location Attribute client_org Attribute client_org", "label": "class WPAHandshake(object):\n    def __init__(self, id=0, ssid=None, client_mac=None, location=None):\n        self.id = id\n        self.ssid = ssid\n        self.client_mac = client_mac\n        self.location = location\n        self.client_org = None\n        try:\n            self.client_org = EUI(self.client_mac).oui.registration().org   # OUI - Organizational Unique Identifier\n        except: pass                                                        # OUI not registered exception"}
{"Repository": "PicImageSearch", "input": "API client for the BaiDu image search engine. className BaiDu(HandOver) Method __init__ Method _extract_card_data", "label": "class BaiDu(HandOver):\n    def __init__(self, **request_kwargs: Any):\n        super().__init__(**request_kwargs)\n\n    @staticmethod\n    def _extract_card_data(data: PyQuery) -> List[Dict[str, Any]]:\n        for script in data(\"script\").items():\n            script_text = script.text()\n            if script_text and \"window.cardData\" in script_text:\n                start = script_text.find(\"[\")\n                end = script_text.rfind(\"]\") + 1\n                return json_loads(script_text[start:end])\n        return []\n\n    async def search(\n        self, url: Optional[str] = None, file: Union[str, bytes, Path, None] = None\n    ) -> BaiDuResponse:\n        params = {\"from\": \"pc\"}\n        files: Optional[Dict[str, Any]] = None\n        if url:\n            params[\"image\"] = url\n        elif file:\n            files = (\n                {\"image\": file}\n                if isinstance(file, bytes)\n                else {\"image\": open(file, \"rb\")}\n            )\n        else:\n            raise ValueError(\"Either 'url' or 'file' must be provided\")\n\n        resp = await self.post(\n            \"https://graph.baidu.com/upload\", params=params, files=files\n        )\n        next_url = (json_loads(resp.text))[\"data\"][\"url\"]\n        resp = await self.get(next_url)\n\n        utf8_parser = HTMLParser(encoding=\"utf-8\")\n        data = PyQuery(fromstring(resp.text, parser=utf8_parser))\n        card_data = self._extract_card_data(data)\n\n        for card in card_data:\n            if card.get(\"cardName\") == \"noresult\":\n                return BaiDuResponse({}, resp.url)\n            if card.get(\"cardName\") == \"simipic\":\n                next_url = card[\"tplData\"][\"firstUrl\"]\n                resp = await self.get(next_url)\n                return BaiDuResponse(json_loads(resp.text), resp.url)"}
{"Repository": "bibliothecula", "input": "DocumentHasTextMetadata class. Changes are not saved to database unless save() is called. className DocumentHasTextMetadata(DbObject) Method pk Method __str__ Method save Method delete", "label": "class DocumentHasTextMetadata(DbObject):\n    TABLE_NAME = \"DocumentHasTextMetadata\"\n    COLUMNS = [\n        \"id\",\n        \"name\",\n        \"document_uuid\",\n        \"metadata_uuid\",\n        \"created\",\n        \"last_modified\",\n    ]\n\n    def __init__(\n        self,\n        db: Database,\n        _id: int,\n        name: str,\n        document_uuid: uuid.UUID,\n        metadata_uuid: uuid.UUID,\n        *args,\n        **kwargs,\n    ):\n        self.db = db\n        self.name = name\n        if isinstance(document_uuid, uuid.UUID):\n            self.document_uuid = document_uuid\n        else:\n            raise TypeError(\"document_uuid must be a Document uuid.UUID.\")\n        if isinstance(metadata_uuid, uuid.UUID):\n            self.metadata_uuid = metadata_uuid\n        else:\n            raise TypeError(\"metadata_uuid must be a TextMetadata uuid.UUID.\")\n        self.id = _id\n        super(DocumentHasTextMetadata, self).__init__(*args, **kwargs)\n\n    def pk(self) -> int:\n        return self.id\n\n    def __str__(self):\n        return f\"<DocumentHasTextMetadata {repr(self.name)}>\"\n\n    def edit(\n        self,\n        program: Optional[str] = None,\n    ):\n        raise NotImplementedError(\"\")\n\n    def save(self):\n        # insert if not already existing\n        self.db.cur.execute(\n            f\"INSERT OR IGNORE INTO DocumentHasTextMetadata(id, name, document_uuid, metadata_uuid, last_modified) VALUES (?, ?, ?, ?, strftime('%Y-%m-%d %H:%M:%f', 'now'))\",\n            [self.id, self.name, self.document_uuid, self.metadata_uuid],\n        )\n        self.db.cur.execute(\n            f\"UPDATE DocumentHasTextMetadata SET name=?, document_uuid=?, metadata_uuid=?, last_modified = strftime('%Y-%m-%d %H:%M:%f', 'now') WHERE id = ?\",\n            [self.name, self.document_uuid.hex, self.metadata_uuid.hex, self.id],\n        )\n\n    def delete(self):\n        self.db.cur.execute(\n            f\"DELETE FROM DocumentHasTextMetadata WHERE id = ?\",\n            [self.id],\n        )\n        return True"}
{"Repository": "i3ipc-python", "input": "Sent when RandR issues a change notification (of either screens, outputs, CRTCs or output properties). className OutputEvent(IpcBaseEvent) Method __init__ Attribute ipc_data Attribute change", "label": "class OutputEvent(IpcBaseEvent):\n    def __init__(self, data):\n        self.ipc_data = data\n        self.change = data['change']"}
{"Repository": "apk_parse", "input": "Class to create ZIP archives with Python library files and packages. className PyZipFile(ZipFile) Method writepy Method _get_codename", "label": "class PyZipFile(ZipFile):\n    def writepy(self, pathname, basename = \"\"):\n        dir, name = os.path.split(pathname)\n        if os.path.isdir(pathname):\n            initname = os.path.join(pathname, \"__init__.py\")\n            if os.path.isfile(initname):\n                # This is a package directory, add it\n                if basename:\n                    basename = \"%s/%s\" % (basename, name)\n                else:\n                    basename = name\n                if self.debug:\n                    print \"Adding package in\", pathname, \"as\", basename\n                fname, arcname = self._get_codename(initname[0:-3], basename)\n                if self.debug:\n                    print \"Adding\", arcname\n                self.write(fname, arcname)\n                dirlist = os.listdir(pathname)\n                dirlist.remove(\"__init__.py\")\n                # Add all *.py files and package subdirectories\n                for filename in dirlist:\n                    path = os.path.join(pathname, filename)\n                    root, ext = os.path.splitext(filename)\n                    if os.path.isdir(path):\n                        if os.path.isfile(os.path.join(path, \"__init__.py\")):\n                            # This is a package directory, add it\n                            self.writepy(path, basename)  # Recursive call\n                    elif ext == \".py\":\n                        fname, arcname = self._get_codename(path[0:-3],\n                                         basename)\n                        if self.debug:\n                            print \"Adding\", arcname\n                        self.write(fname, arcname)\n            else:\n                # This is NOT a package directory, add its files at top level\n                if self.debug:\n                    print \"Adding files from directory\", pathname\n                for filename in os.listdir(pathname):\n                    path = os.path.join(pathname, filename)\n                    root, ext = os.path.splitext(filename)\n                    if ext == \".py\":\n                        fname, arcname = self._get_codename(path[0:-3],\n                                         basename)\n                        if self.debug:\n                            print \"Adding\", arcname\n                        self.write(fname, arcname)\n        else:\n            if pathname[-3:] != \".py\":\n                raise RuntimeError, \\\n                      'Files added with writepy() must end with \".py\"'\n            fname, arcname = self._get_codename(pathname[0:-3], basename)\n            if self.debug:\n                print \"Adding file\", arcname\n            self.write(fname, arcname)\n\n    def _get_codename(self, pathname, basename):\n        file_py  = pathname + \".py\"\n        file_pyc = pathname + \".pyc\"\n        file_pyo = pathname + \".pyo\"\n        if os.path.isfile(file_pyo) and \\\n                            os.stat(file_pyo).st_mtime >= os.stat(file_py).st_mtime:\n            fname = file_pyo    # Use .pyo file\n        elif not os.path.isfile(file_pyc) or \\\n             os.stat(file_pyc).st_mtime < os.stat(file_py).st_mtime:\n            import py_compile\n            if self.debug:\n                print \"Compiling\", file_py\n            try:\n                py_compile.compile(file_py, file_pyc, None, True)\n            except py_compile.PyCompileError,err:\n                print err.msg\n            fname = file_pyc\n        else:\n            fname = file_pyc\n        archivename = os.path.split(fname)[1]\n        if basename:\n            archivename = \"%s/%s\" % (basename, archivename)\n        return (fname, archivename)"}
{"Repository": "BERT4Rec", "input": "Configuration for `BertModel`. className BertConfig(object) Method from_dict Method from_json_file Method to_dict Method to_json_string", "label": "class BertConfig(object):\n    def __init__(self,\n                 vocab_size,\n                 hidden_size=768,\n                 num_hidden_layers=12,\n                 num_attention_heads=12,\n                 intermediate_size=3072,\n                 hidden_act=\"gelu\",\n                 hidden_dropout_prob=0.1,\n                 attention_probs_dropout_prob=0.1,\n                 max_position_embeddings=512,\n                 type_vocab_size=16,\n                 initializer_range=0.02):\n        self.vocab_size = vocab_size\n        self.hidden_size = hidden_size\n        self.num_hidden_layers = num_hidden_layers\n        self.num_attention_heads = num_attention_heads\n        self.hidden_act = hidden_act\n        self.intermediate_size = intermediate_size\n        self.hidden_dropout_prob = hidden_dropout_prob\n        self.attention_probs_dropout_prob = attention_probs_dropout_prob\n        self.max_position_embeddings = max_position_embeddings\n        self.type_vocab_size = type_vocab_size\n        self.initializer_range = initializer_range\n\n    @classmethod\n    def from_dict(cls, json_object):\n        config = BertConfig(vocab_size=None)\n        for (key, value) in six.iteritems(json_object):\n            config.__dict__[key] = value\n        return config\n\n    @classmethod\n    def from_json_file(cls, json_file):\n        with tf.gfile.GFile(json_file, \"r\") as reader:\n            text = reader.read()\n        return cls.from_dict(json.loads(text))\n\n    def to_dict(self):\n        output = copy.deepcopy(self.__dict__)\n        return output\n\n    def to_json_string(self):\n        return json.dumps(self.to_dict(), indent=2, sort_keys=True) + \"\\n\""}
{"Repository": "eman2", "input": "The base class of all clustering methods. className py_cluster_BaseClusterMethod(object) Method __init__ Method topo Method __get_data Method __get_raw_data Attribute distance Attribute _input Attribute _data", "label": "class py_cluster_BaseClusterMethod(object):\n   def __init__(self, input, distance_function):\n      self.distance = distance_function\n      self._input = input    # the original input\n      self._data  = input[:] # clone the input so we can work with it\n\n   def topo(self):\n      return self.data[0].topology()\n\n   def __get_data(self):\n      return self._data\n   data = property(__get_data)\n\n   def __get_raw_data(self):\n      return self._input\n   raw_data = property(__get_raw_data)"}
{"Repository": "BCL", "input": "Computes and stores the average and current value className AverageMeter(object) Method __init__ Method reset Method update", "label": "class AverageMeter(object):\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count"}
{"Repository": "rethinking-network-pruning", "input": "Computes and stores the average and current value className AverageMeter(object) Method __init__ Method reset Method update", "label": "class AverageMeter(object):\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count"}
{"Repository": "srt", "input": "Raised when part of an SRT block could not be parsed. className SRTParseError(Exception) Method __init__ Attribute expected_start Attribute actual_start Attribute unmatched_content", "label": "class SRTParseError(Exception):\n    def __init__(self, expected_start, actual_start, unmatched_content):\n        message = (\n            \"Expected contiguous start of match or end of input at char %d, \"\n            \"but started at char %d (unmatched content: %r)\"\n            % (expected_start, actual_start, unmatched_content)\n        )\n        super(SRTParseError, self).__init__(message)\n\n        self.expected_start = expected_start\n        self.actual_start = actual_start\n        self.unmatched_content = unmatched_content"}
{"Repository": "BicaVM", "input": "Postgres driver. className PostgresDB(DB) Method __init__ Method get_db_module Method _process_insert_query Method _connect Method _connect_with_pooling Attribute dbname Attribute paramstyle Attribute supports_multiple_insert", "label": "class PostgresDB(DB): \n    def __init__(self, **keywords):\n        if 'pw' in keywords:\n            keywords['password'] = keywords['pw']\n            del keywords['pw']\n            \n        db_module = self.get_db_module()\n        keywords['database'] = keywords.pop('db')\n        self.dbname = \"postgres\"\n        self.paramstyle = db_module.paramstyle\n        DB.__init__(self, db_module, keywords)\n        self.supports_multiple_insert = True\n        \n    def get_db_module(self):\n        try: \n            import psycopg2 as db\n            import psycopg2.extensions\n            psycopg2.extensions.register_type(psycopg2.extensions.UNICODE)\n        except ImportError: \n            try: \n                import psycopg as db\n            except ImportError: \n                import pgdb as db\n        return db\n\n    def _process_insert_query(self, query, tablename, seqname):\n        if seqname is None: \n            seqname = tablename + \"_id_seq\"\n        return query + \"; SELECT currval('%s')\" % seqname\n\n    def _connect(self, keywords):\n        conn = DB._connect(self, keywords)\n        conn.set_client_encoding('UTF8')\n        return conn\n        \n    def _connect_with_pooling(self, keywords):\n        conn = DB._connect_with_pooling(self, keywords)\n        conn._con._con.set_client_encoding('UTF8')\n        return conn"}
{"Repository": "adagan", "input": "Super-simple progress bar. className ProgressBar(object) Method __init__ Method bam Method __enter__ Method __exit__ Attribute _width Attribute verbose", "label": "class ProgressBar(object):\n    def __init__(self, verbose, iter_num):\n        self._width = iter_num\n        self.verbose = verbose\n        if self.verbose:\n            sys.stdout.write(\"[%s]\" % (\" \" * self._width))\n            sys.stdout.flush()\n            sys.stdout.write(\"\\b\" * (self._width + 1))\n\n    def bam(self):\n        if self.verbose:\n            sys.stdout.write(\"*\")\n            sys.stdout.flush()\n\n    def __enter__(self):\n        return self\n\n    def __exit__(self, exc_type, exc_value, traceback):\n        if self.verbose:\n            sys.stdout.write(\"\\n\")"}
{"Repository": "e2c", "input": "Represents a multivariate normal distribution parameterized by N(mu,Cov). className NormalDistribution(object) Method __init__ Attribute mu Attribute sigma Attribute logsigma Attribute v Attribute r", "label": "class NormalDistribution(object):\n  def __init__(self,mu,sigma,logsigma,v=None,r=None):\n    self.mu=mu\n    self.sigma=sigma # either stdev diagonal itself, or stdev diagonal from decomposition\n    self.logsigma=logsigma\n    dim=mu.get_shape()\n    if v is None:\n      v=tf.constant(0.,shape=dim)\n    if r is None:\n      r=tf.constant(0.,shape=dim)\n    self.v=v\n    self.r=r"}
{"Repository": "python-taiga", "input": "InstanceResource model Base class for methods that operates on a single resource (:py:meth:`update`, :py:meth:`patch`, :py:meth:`delete`). className InstanceResource(Resource) Method __init__ Method update Method patch Method delete Method to_dict Method parse Method __repr__ Method __str__ Method _rp Attribute requester", "label": "class InstanceResource(Resource):\n    endpoint = \"\"\n\n    parser = {}\n\n    allowed_params = []\n\n    repr_attribute = \"name\"\n\n    def __init__(self, requester, **params):\n        import dateutil.parser\n\n        self.requester = requester\n        for key, value in params.items():\n            if key in [\"created_date\", \"modified_date\"]:\n                if re.compile(r\"\\d+-\\d+-\\d+T\\d+:\\d+:\\d+\\+0000\").match(value):\n                    d = dateutil.parser.parse(value)\n                    value = d.astimezone(dateutil.tz.tzlocal())\n            setattr(self, key, value)\n\n    def update(self, **args):\n        self_dict = self.to_dict()\n        if args:\n            self_dict = dict(list(self_dict.items()) + list(args.items()))\n        response = self.requester.put(\"/{endpoint}/{id}\", endpoint=self.endpoint, id=self.id, payload=self_dict)\n        obj_json = response.json()\n        if \"version\" in obj_json:\n            self.__dict__[\"version\"] = obj_json[\"version\"]\n        return self\n\n    def patch(self, fields, **args):\n        self_dict = {key: value for (key, value) in self.to_dict().items() if key in fields}\n        if args:\n            self_dict = dict(list(self_dict.items()) + list(args.items()))\n        response = self.requester.patch(\"/{endpoint}/{id}\", endpoint=self.endpoint, id=self.id, payload=self_dict)\n        obj_json = response.json()\n        if \"version\" in obj_json:\n            self.__dict__[\"version\"] = obj_json[\"version\"]\n        return self\n\n    def delete(self, query=None):\n        self.requester.delete(\"/{endpoint}/{id}\", endpoint=self.endpoint, id=self.id, query=query)\n        return self\n\n    def to_dict(self):\n        self_dict = {}\n        for key, value in self.__dict__.items():\n            if self.allowed_params and key in self.allowed_params:\n                self_dict[key] = value\n        return self_dict\n\n    @classmethod\n    def parse(cls, requester, entry):\n        if not isinstance(entry, dict):\n            return entry\n        for key_to_parse, cls_to_parse in cls.parser.items():\n            if key_to_parse in entry:\n                entry[key_to_parse] = cls_to_parse.parse(requester, entry[key_to_parse])\n        return cls(requester, **entry)\n\n    def __repr__(self):\n        try:\n            return \"{}({})\".format(self.__class__.__name__, self.id)\n        except AttributeError:\n            return \"{}({})\".format(self.__class__.__name__, id(self))\n\n    def __str__(self):\n        return self._rp()\n\n    def _rp(self):\n        attr = getattr(self, self.repr_attribute, None)\n        if attr:\n            return \"{}\".format(attr)\n        else:\n            return repr(self)"}
{"Repository": "Emotional-Support-Conversation", "input": "`Memory` NamedTuple have a single field `bytes` and you can get a human readable str of the number of mega bytes by className Memory(NamedTuple) Method __repr__", "label": "class Memory(NamedTuple):\n    bytes: int\n\n    def __repr__(self) -> str:\n        return str(bytes_to_mega_bytes(self.bytes))"}
{"Repository": "pygerrit2", "input": "HTTP Digest Auth with netrc credentials. className HTTPDigestAuthFromNetrc(HTTPDigestAuth) Method __init__", "label": "class HTTPDigestAuthFromNetrc(HTTPDigestAuth):\n    def __init__(self, url):\n        auth = _get_netrc_auth(url)\n        if not auth:\n            raise ValueError(\"netrc missing or no credentials found in netrc\")\n        username, password = auth\n        super(HTTPDigestAuthFromNetrc, self).__init__(username, password)"}
{"Repository": "python-syncthing", "input": "HTTP REST endpoint for Statistic calls. className Statistics(BaseAPI) Method device Method folder", "label": "class Statistics(BaseAPI):\n    prefix = '/rest/stats/'\n\n    def device(self):\n        return self.get('device')\n\n    def folder(self):\n        return self.get('folder')"}
{"Repository": "alibi", "input": "A wrapper around `shap. className KernelExplainerWrapper(KernelExplainer) Method __init__ Method return_attribute", "label": "class KernelExplainerWrapper(KernelExplainer):\n    def __init__(self, *args, **kwargs):\n        if 'seed' in kwargs:\n            seed = kwargs.pop('seed')\n            np.random.seed(seed)\n        super().__init__(*args, **kwargs)\n\n    def get_explanation(self, X: Union[Tuple[int, np.ndarray], np.ndarray], **kwargs) -> \\\n            Union[Tuple[int, np.ndarray], Tuple[int, List[np.ndarray]], np.ndarray, List[np.ndarray]]:\n        # handle call from distributed context\n        if isinstance(X, tuple):\n            batch_idx, batch = X\n            shap_values = super().shap_values(batch, **kwargs)\n            return batch_idx, shap_values\n        else:\n            shap_values = super().shap_values(X, **kwargs)\n            return shap_values\n\n    def return_attribute(self, name: str) -> Any:\n        return self.__getattribute__(name)"}
{"Repository": "image_class", "input": "Sub-pixel convolutional upscaling layer based on the paper \"Real-Time Single Image and Video Super-Resolution Using an Efficient Sub-Pixel Convolutional Neural Network\" (https://arxiv. className SubPixelUpscaling(Layer) Method __init__ Method build Method call Method compute_output_shape Method get_config Attribute scale_factor Attribute data_format", "label": "class SubPixelUpscaling(Layer):\n    def __init__(self, scale_factor=2, data_format=None, **kwargs):\n        super(SubPixelUpscaling, self).__init__(**kwargs)\n\n        self.scale_factor = scale_factor\n        self.data_format = normalize_data_format(data_format)\n\n    def build(self, input_shape):\n        pass\n\n    def call(self, x, mask=None):\n        y = K_BACKEND.depth_to_space(x, self.scale_factor, self.data_format)\n        return y\n\n    def compute_output_shape(self, input_shape):\n        if self.data_format == 'channels_first':\n            b, k, r, c = input_shape\n            return (b, k // (self.scale_factor ** 2), r * self.scale_factor, c * self.scale_factor)\n        else:\n            b, r, c, k = input_shape\n            return (b, r * self.scale_factor, c * self.scale_factor, k // (self.scale_factor ** 2))\n\n    def get_config(self):\n        config = {'scale_factor': self.scale_factor,\n                  'data_format': self.data_format}\n        base_config = super(SubPixelUpscaling, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))"}
{"Repository": "bert-kbqa-NLPCC2017", "input": "Base class for data converters for sequence classification data sets. className DataProcessor(object) Method get_train_examples Method get_dev_examples Method get_labels Method _read_data", "label": "class DataProcessor(object):\n    def get_train_examples(self, data_dir):\n        raise NotImplementedError()\n\n    def get_dev_examples(self, data_dir):\n        raise NotImplementedError()\n\n    def get_labels(self):\n        raise NotImplementedError()\n\n    @classmethod\n    def _read_data(cls, input_file):\n        with codecs.open(input_file, 'r', encoding='utf-8') as f:\n            lines = []\n            words = []\n            labels = []\n            for line in f:\n                contends = line.strip()\n                tokens = contends.split(' ')\n                if len(tokens) == 2:\n                    word = line.strip().split(' ')[0]\n                    label = line.strip().split(' ')[-1]\n                else:\n                    if len(contends) == 0:\n                        l = ' '.join([label for label in labels if len(label) > 0])\n                        w = ' '.join([word for word in words if len(word) > 0])\n                        lines.append([l, w])\n                        words = []\n                        labels = []\n                        continue\n                if contends.startswith(\"-DOCSTART-\"):\n                    words.append('')\n                    continue\n                words.append(word)\n                labels.append(label)\n            return lines"}
{"Repository": "edX-6.00.2x-Introduction-to-Computational-Thinking-and-Data-Science", "input": "A Cluster is defines as a set of elements, all having className Cluster(object) Method __init__ Method singleLinkageDist Method maxLinkageDist Method averageLinkageDist Method members Method isIn Method toStr Method getNames Method __str__ Attribute points Attribute pointType", "label": "class Cluster(object):\n    def __init__(self, points, pointType):\n        self.points = points\n        self.pointType = pointType\n    def singleLinkageDist(self, other):\n        minDist = self.points[0].distance(other.points[0])\n        for p1 in self.points:\n            for p2 in other.points:\n                if p1.distance(p2) < minDist:\n                    minDist = p1.distance(p2)\n        return minDist\n    def maxLinkageDist(self, other):\n        maxDist = self.points[0].distance(other.points[0])\n        for p1 in self.points:\n            for p2 in other.points:\n                if p1.distance(p2) > maxDist:\n                    maxDist = p1.distance(p2)\n        return maxDist\n    def averageLinkageDist(self, other):\n        totDist = 0.0\n        for p1 in self.points:\n            for p2 in other.points:\n                totDist += p1.distance(p2)\n        return totDist/(len(self.points)*len(other.points))\n    def members(self):\n        for p in self.points:\n            yield p\n    def isIn(self, name):\n        for p in self.points:\n            if p.getName() == name:\n                return True\n        return False\n    def toStr(self):\n        result = ''\n        for p in self.points:\n            result = result + p.toStr() + ', '\n        return result[:-2]\n    def getNames(self):\n        names = []\n        for p in self.points:\n            names.append(p.getName())\n        return sorted(names)\n    def __str__(self):\n        names = self.getNames()\n        result = ''\n        for p in names:\n            result = result + p + ', '\n        return result[:-2]"}
{"Repository": "fast_abs_rl", "input": "get the article sentences only (for decoding use) className DecodeDataset(CnnDmDataset) Method __init__ Method __getitem__", "label": "class DecodeDataset(CnnDmDataset):\n    def __init__(self, split):\n        assert split in ['val', 'test']\n        super().__init__(split, DATASET_DIR)\n\n    def __getitem__(self, i):\n        js_data = super().__getitem__(i)\n        art_sents = js_data['article']\n        return art_sents"}
{"Repository": "CollMetric", "input": "A generator that, in parallel, generates tuples: user-positive-item pairs, negative-items className WarpSampler(object) Method __init__ Method next_batch Method close Attribute result_queue Attribute processors", "label": "class WarpSampler(object):\n    def __init__(self, user_item_matrix, batch_size=10000, n_negative=10, n_workers=5, check_negative=True):\n        self.result_queue = Queue(maxsize=n_workers*2)\n        self.processors = []\n        for i in range(n_workers):\n            self.processors.append(\n                Process(target=sample_function, args=(user_item_matrix,\n                                                      batch_size,\n                                                      n_negative,\n                                                      self.result_queue,\n                                                      check_negative)))\n            self.processors[-1].start()\n\n    def next_batch(self):\n        return self.result_queue.get()\n\n    def close(self):\n        for p in self.processors:  # type: Process\n            p.terminate()\n            p.join()"}
{"Repository": "MVIMP", "input": "Set the learning rate of each parameter group to the initial lr decayed by gamma once the number of epoch reaches one of the milestones. className MultiStepLR(_LRScheduler) Method __init__ Method get_lr Attribute milestones Attribute gamma", "label": "class MultiStepLR(_LRScheduler):\n    def __init__(self, optimizer, milestones, gamma=0.1, last_epoch=-1):\n\n        if not list(milestones) == sorted(milestones):\n\n            raise ValueError(\n\n                \"Milestones should be a list of\" \" increasing integers. Got {}\",\n\n                milestones,\n\n            )\n\n        self.milestones = milestones\n\n        self.gamma = gamma\n\n        super(MultiStepLR, self).__init__(optimizer, last_epoch)\n\n\n\n    def get_lr(self):\n\n        return [\n\n            base_lr * self.gamma ** bisect_right(self.milestones, self.last_epoch)\n\n            for base_lr in self.base_lrs\n\n        ]"}
{"Repository": "PitchExtractor", "input": "Args: adaptive_batch_size (bool): if true, decrease batch size when long data comes. className Collater(object) Method __init__ Method __call__ Attribute text_pad_index Attribute return_wave Attribute min_mel_length Attribute max_mel_length Attribute mel_length_step Attribute latent_dim", "label": "class Collater(object):\n    def __init__(self, return_wave=False):\n        self.text_pad_index = 0\n        self.return_wave = return_wave\n        self.min_mel_length = 192\n        self.max_mel_length = 192\n        self.mel_length_step = 16\n        self.latent_dim = 16\n\n    def __call__(self, batch):\n        # batch[0] = wave, mel, text, f0, speakerid\n        batch_size = len(batch)\n        nmels = batch[0][0].size(0)\n        mels = torch.zeros((batch_size, nmels, self.max_mel_length)).float()\n        f0s = torch.zeros((batch_size, self.max_mel_length)).float()\n        is_silences = torch.zeros((batch_size, self.max_mel_length)).float()\n\n        for bid, (mel, f0, is_silence) in enumerate(batch):\n            mel_size = mel.size(1)\n            mels[bid, :, :mel_size] = mel\n            f0s[bid, :mel_size] = f0\n            is_silences[bid, :mel_size] = is_silence\n\n        if self.max_mel_length > self.min_mel_length:\n            random_slice = np.random.randint(\n                self.min_mel_length//self.mel_length_step,\n                1+self.max_mel_length//self.mel_length_step) * self.mel_length_step + self.min_mel_length\n            mels = mels[:, :, :random_slice]\n            f0 = f0[:, :random_slice]\n\n        mels = mels.unsqueeze(1)\n        return mels, f0s, is_silences"}
{"Repository": "MONet-pytorch", "input": "This dataset class can load a set of images specified by the path --dataroot /path/to/data. className CLEVRDataset(BaseDataset) Method modify_commandline_options Method __init__ Method _transform Method __getitem__ Method __len__ Attribute A_paths", "label": "class CLEVRDataset(BaseDataset):\n    @staticmethod\n    def modify_commandline_options(parser, is_train):\n        parser.set_defaults(input_nc=3, output_nc=3,\n                            crop_size=192, # crop is done first\n                            load_size=64,  # before resize\n                            num_slots=11, display_ncols=11)\n        return parser\n\n    def __init__(self, opt):\n        BaseDataset.__init__(self, opt)\n        p = os.path.join(opt.dataroot, 'images', 'train' if opt.isTrain else 'test')\n        self.A_paths = sorted(make_dataset(p, opt.max_dataset_size))\n\n    def _transform(self, img):\n        img = TF.resized_crop(img, 64, 29, self.opt.crop_size, self.opt.crop_size, self.opt.load_size)\n        img = TF.to_tensor(img)\n        img = TF.normalize(img, [0.5] * self.opt.input_nc, [0.5] * self.opt.input_nc)\n        return img\n\n    def __getitem__(self, index):\n        A_path = self.A_paths[index]\n        A_img = Image.open(A_path).convert('RGB')\n        A = self._transform(A_img)\n        return {'A': A, 'A_paths': A_path}\n\n    def __len__(self):\n        return len(self.A_paths)"}
{"Repository": "kube-shell", "input": "Provide styles for the autocomplete menu and the toolbar. className StyleFactory(object) Method __init__ Method style_factory Attribute style", "label": "class StyleFactory(object):\n    def __init__(self, style_name):\n        self.style = self.style_factory(style_name)\n\n    def style_factory(self, style_name):\n        try:\n            style = get_style_by_name(style_name)\n        except ClassNotFound:\n            style = get_style_by_name('vim')\n\n        # Create a style dictionary.\n        styles = {}\n        styles.update(style.styles)\n        styles.update(default_style_extensions)\n        t = Token\n        styles.update({\n            t.Menu.Completions.Completion.Current: 'bg:#00aaaa #000000',\n            t.Menu.Completions.Completion: 'bg:#008888 #ffffff',\n            t.Menu.Completions.Meta.Current: 'bg:#00aaaa #000000',\n            t.Menu.Completions.Meta: 'bg:#00aaaa #ffffff',\n            t.Scrollbar.Button: 'bg:#003333',\n            t.Scrollbar: 'bg:#00aaaa',\n            t.Toolbar: 'bg:#222222 #cccccc',\n            t.Toolbar.Off: 'bg:#222222 #696969',\n            t.Toolbar.On: 'bg:#222222 #ffffff',\n            t.Toolbar.Search: 'noinherit bold',\n            t.Toolbar.Search.Text: 'nobold',\n            t.Toolbar.System: 'noinherit bold',\n            t.Toolbar.Arg: 'noinherit bold',\n            t.Toolbar.Arg.Text: 'nobold'\n        })\n\n        return style_from_dict(styles)"}
{"Repository": "ZODB", "input": "Class for a chat session. className ChatSession(Persistent) Method __init__ Method new_messages Method add_message Attribute name Attribute _messages", "label": "class ChatSession(Persistent):\n    def __init__(self, name):\n        self.name = name\n\n        # Internal attribute: _messages holds all the chat messages.\n        self._messages = OOBTree.OOBTree()\n\n\n    def new_messages(self):\n        \"Return new messages.\"\n\n        # self._v_last_time is the time of the most recent message\n        # returned to the user of this class.\n        if not hasattr(self, '_v_last_time'):\n            self._v_last_time = 0\n\n        new = []\n        T = self._v_last_time\n\n        for T2, message in self._messages.items():\n            if T2 > T:\n                new.append( message )\n                self._v_last_time = T2\n\n        return new\n\n    def add_message(self, message):\n        while 1:\n            try:\n                now = time.time()\n                self._messages[ now ] = message\n                transaction.commit()\n            except ConflictError:\n                # Conflict occurred; this process should abort,\n                # wait for a little bit, then try again.\n                transaction.abort()\n                time.sleep(.2)\n            else:\n                # No ConflictError exception raised, so break\n                # out of the enclosing while loop.\n                break\n        # end while"}
{"Repository": "sequitur-g2p", "input": "regenerate reference data: LanguageModel. className LanguageModelTestCase(TestCase) Method setUp Method testUnigram Method testTrigram", "label": "class LanguageModelTestCase(TestCase):\n    order = 3\n\n    def setUp(self):\n        self.vocabulary = loadVocabulary(\"tests/nab-5k-vocabulary.txt.gz\")\n        self.counts = loadCounts(\n            \"tests/nab-mini-corpus.mapped-counts.gz\", self.vocabulary\n        )\n        self.coc = eval(open(\"tests/nab-mini-corpus.raw-coc\").read())\n        self.builder = LanguageModelBuilder()\n        self.builder.setVocabulary(self.vocabulary)\n\n    def testUnigram(self):\n        self.builder.setHighestOrder(0)\n        self.builder.estimateDiscounts(self.coc)\n        f = EqualFile(\"tests/nab-mini-corpus.unigram.lm.gz\")\n        lm = LmArpaWriter(f, 0)\n        self.builder.build(self.counts, lm)\n        self.assertTrue(f)\n\n    def testTrigram(self):\n        self.builder.setHighestOrder(2)\n        self.builder.estimateDiscounts(self.coc)\n        f = EqualFile(\"tests/nab-mini-corpus.trigram.lm.gz\")\n        lm = LmArpaWriter(f, 2)\n        self.builder.build(self.counts, lm)\n        self.assertTrue(f)"}
{"Repository": "ly2video", "input": "Represents a location within a . className LySrcLocation(object) Method __init__ Method __str__ Method coords Method getAbsolutePitch Attribute filename Attribute lineNum Attribute columnNum Attribute octave Attribute notename Attribute alteration", "label": "class LySrcLocation(object):\n    __slots__ = ['filename', 'lineNum', 'columnNum', 'octave', 'notename', 'alteration']\n\n    def __init__(self, filename, lineNum, columnNum, octave, notename, alteration):\n        self.filename  = filename\n        self.lineNum   = lineNum\n        self.columnNum = columnNum\n        self.octave  = octave\n        self.notename  = notename\n        self.alteration  = alteration\n\n    def __str__(self):\n        return \"%s:%d:%d\" % (self.filename, self.lineNum + 1, self.columnNum)\n\n    def coords(self):\n        return (self.lineNum, self.columnNum)\n\n    def getAbsolutePitch(self):\n        accidentalSemitoneSteps = 2 * self.alteration\n\n        pitch = (self.octave + 5) * 12 + \\\n            C_MAJOR_SCALE_STEPS[self.notename] + \\\n            accidentalSemitoneSteps\n\n        token = noteToken(self.octave, self.notename, self.alteration)\n\n        return pitch, token"}
{"Repository": "hamilton", "input": "Base class housing the static function. className ResultMixin(object) Method build_result", "label": "class ResultMixin(object):\n    @staticmethod\n    @abc.abstractmethod\n    def build_result(**outputs: Dict[str, Any]) -> Any:\n        pass"}
{"Repository": "StarterCode", "input": "for validation className TvcValDataset(TvcTrainDataset) Method __init__ Method __getitem__ Method collate Attribute vid2clips", "label": "class TvcValDataset(TvcTrainDataset):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.vid2clips = self.caption_db.vid2clips\n\n    def __getitem__(self, i):\n        vid = self.vids[i]\n        clip_ids = self.vid2clips[vid]\n\n        video_inputs = self.video_db.__getitem__(vid)\n        nframes = len(video_inputs[4])  # clip_level_v_feats\n\n        clip_ranges = []\n        all_ts = []\n        gts = []\n        attn_masks = []\n        for clip_id in clip_ids:\n            ex = self.caption_db.get_clip(clip_id)\n            ts = ex['ts']\n            st, ed = self.get_st_ed_label(ts, nframes)\n            clip_ranges.append((st, ed))\n            attn_masks.append(torch.tensor([1]*(ed-st)))\n            all_ts.append(ts)\n            gts.append(ex['captions'][0]['text'])\n        return (video_inputs, clip_ranges, attn_masks,\n                (vid, clip_ids, all_ts, gts))\n\n    @staticmethod\n    def collate(inputs):\n        (video_inputs, all_clip_ranges, attn_masks_list, metas\n         ) = map(list, unzip(inputs))\n\n        all_attn_masks = list(concat(attn_masks_list))\n        attn_mask = pad_sequence(all_attn_masks,\n                                 batch_first=True, padding_value=0)\n        batch = {'cap_attn_mask': attn_mask,\n                 'clip_ranges': tuple(map(tuple, all_clip_ranges))}\n\n        vid_batch = video_collate(video_inputs)\n        batch.update(vid_batch)\n\n        # meta\n        vids, clip_ids, all_ts, all_gts = [], [], [], []\n        for vid, cids, tss, gts in metas:\n            for cid, ts, gt in zip(cids, tss, gts):\n                vids.append(vid)\n                clip_ids.append(int(cid))\n                all_ts.append(ts)\n                all_gts.append(gt)\n        batch['vid_names'] = vids\n        batch['clip_ids'] = clip_ids\n        batch['all_ts'] = all_ts\n        batch['gts'] = all_gts\n        return batch"}
{"Repository": "django-storages", "input": "File inherited class for libcloud storage objects read and write className LibCloudFile(File) Method __init__ Method _get_file Method _set_file Method size Method read Method write Method close Attribute name Attribute _storage Attribute _mode Attribute _is_dirty Attribute _file", "label": "class LibCloudFile(File):\n    def __init__(self, name, storage, mode):\n        self.name = name\n        self._storage = storage\n        self._mode = mode\n        self._is_dirty = False\n        self._file = None\n\n    def _get_file(self):\n        if self._file is None:\n            data = self._storage._read(self.name)\n            self._file = io.BytesIO(data)\n        return self._file\n\n    def _set_file(self, value):\n        self._file = value\n\n    file = property(_get_file, _set_file)\n\n    @property\n    def size(self):\n        if not hasattr(self, \"_size\"):\n            self._size = self._storage.size(self.name)\n        return self._size\n\n    def read(self, num_bytes=None):\n        return self.file.read(num_bytes)\n\n    def write(self, content):\n        if \"w\" not in self._mode:\n            raise AttributeError(\"File was opened for read-only access.\")\n        self.file = io.BytesIO(content)\n        self._is_dirty = True\n\n    def close(self):\n        if self._is_dirty:\n            self._storage._save(self.name, self.file)\n        self.file.close()"}
{"Repository": "SmokeDetector", "input": "YAML parser for ASN blacklists. className YAMLParserASN(YAMLParserCIDR) Method _validate", "label": "class YAMLParserASN(YAMLParserCIDR):\n    SCHEMA_VARIANT = 'yaml_asn'\n    SCHEMA_PRIKEY = 'asn'\n\n    def _validate(self, item):\n        if 'asn' not in item:\n            raise ValueError('Item must have member field \"asn\": {0!r}'.format(item))\n        asn = int(item['asn'])\n        if asn <= 0 or asn >= 4200000000 or 64496 <= asn <= 131071 or asn == 23456:\n            raise ValueError('Not a valid public AS number: {0}'.format(asn))"}
{"Repository": "sysreptor", "input": "SHA3 256-bit password hash. className UnsaltedSHA3_256PasswordHasher(BasePasswordHasher) Method salt Method encode Method decode Method verify Method safe_summary Method harden_runtime", "label": "class UnsaltedSHA3_256PasswordHasher(BasePasswordHasher):\n    algorithm = \"sha3_256\"\n\n    def salt(self):\n        return \"\"\n\n    def encode(self, password, salt):\n        if salt != \"\":\n            raise ValueError(\"salt must be empty.\")\n        hash = hashlib.sha3_256(password.encode()).hexdigest()\n        return \"sha3_256$$%s\" % hash\n\n    def decode(self, encoded):\n        assert encoded.startswith(\"sha3_256$$\")\n        return {\n            \"algorithm\": self.algorithm,\n            \"hash\": encoded[6:],\n            \"salt\": None,\n        }\n\n    def verify(self, password, encoded):\n        encoded_2 = self.encode(password, \"\")\n        return constant_time_compare(encoded, encoded_2)\n\n    def safe_summary(self, encoded):\n        decoded = self.decode(encoded)\n        return {\n            \"algorithm\": decoded[\"algorithm\"],\n            \"hash\": mask_hash(decoded[\"hash\"]),\n        }\n\n    def harden_runtime(self, password, encoded):\n        pass"}
{"Repository": "tinymongo", "input": "Base class for write result classes. className _WriteResult(object) Method __init__ Attribute acknowledged", "label": "class _WriteResult(object):\n    def __init__(self, acknowledged=True):\n        self.acknowledged = acknowledged  # here only to PyMongo compat"}
{"Repository": "py-enumerable", "input": "Class to hold state for zipping 2 collections together className ZipEnumerable(Enumerable) Method __init__ Method __iter__ Attribute enumerable Attribute result_func", "label": "class ZipEnumerable(Enumerable):\n    def __init__(self, enumerable1, enumerable2, result_func):\n        super(ZipEnumerable, self).__init__(enumerable1)\n        self.enumerable = enumerable2\n        self.result_func = result_func\n\n    def __iter__(self):\n        return map(\n            lambda r: self.result_func(r), zip(iter(self._iterable), self.enumerable)\n        )"}
{"Repository": "dramatiq", "input": "Meta for class-based actors. className generic_actor(type) Method __new__ Method __getattr__", "label": "class generic_actor(type):\n    def __new__(metacls, name, bases, attrs):\n        clazz = super().__new__(metacls, name, bases, attrs)\n        meta = getattr(clazz, \"Meta\", object())\n        if not getattr(meta, \"abstract\", False):\n            options = {name: getattr(meta, name) for name in vars(meta) if not name.startswith(\"_\")}\n            options.pop(\"abstract\", False)\n\n            clazz_instance = clazz()\n            actor_registry = options.pop(\"actor\", actor)\n            actor_instance = actor_registry(clazz_instance, **options)\n            setattr(clazz, \"__getattr__\", generic_actor.__getattr__)\n            setattr(clazz_instance, \"__actor__\", actor_instance)\n            return clazz_instance\n\n        setattr(meta, \"abstract\", False)\n        return clazz\n\n    def __getattr__(cls, name):\n        return getattr(cls.__actor__, name)"}
{"Repository": "migrate", "input": "Class that provides checkpoint utils of different object types. className CheckpointService() Method __init__ Method _get_checkpoint_file Method checkpoint_enabled Method checkpoint_file_exists Method get_checkpoint_key_set Method get_checkpoint_key_map Attribute _checkpoint_enabled Attribute _checkpoint_dir", "label": "class CheckpointService():\n    def __init__(self, configs):\n        self._checkpoint_enabled = configs['use_checkpoint']\n        self._checkpoint_dir = configs['export_dir'] + \"checkpoint/\"\n        os.makedirs(self._checkpoint_dir, exist_ok=True)\n\n    def _get_checkpoint_file(self, action_type, object_type):\n        return f\"{self._checkpoint_dir}/{action_type}_{object_type}.log\"\n\n    @property\n    def checkpoint_enabled(self):\n        return self._checkpoint_enabled\n\n    def checkpoint_file_exists(self, action_type, object_type):\n        return os.path.exists(self._get_checkpoint_file(action_type, object_type))\n\n    def get_checkpoint_key_set(self, action_type, object_type):\n        if self._checkpoint_enabled:\n            checkpoint_file = self._get_checkpoint_file(action_type, object_type)\n            return CheckpointKeySet(checkpoint_file)\n        else:\n            return DisabledCheckpointKeySet()\n\n    def get_checkpoint_key_map(self, action_type, object_type):\n        if self._checkpoint_enabled:\n            checkpoint_file = self._get_checkpoint_file(action_type, object_type)\n            return CheckpointKeyMap(checkpoint_file)\n        else:\n            return DisabledCheckpointKeyMap()"}
{"Repository": "qiskit-aqua", "input": "Cobyla Optimizer Tests. className TestCobylaOptimizer(QiskitOptimizationTestCase) Method test_cobyla_optimizer Method test_cobyla_optimizer_with_quadratic_constraint Method test_cobyla_optimizer_with_variable_bounds Method test_cobyla_optimizer_with_trials", "label": "class TestCobylaOptimizer(QiskitOptimizationTestCase):\n    def test_cobyla_optimizer(self):\n        # load optimization problem\n        problem = QuadraticProgram()\n        problem.continuous_var(upperbound=4)\n        problem.continuous_var(upperbound=4)\n        problem.linear_constraint(linear=[1, 1], sense='=', rhs=2)\n        problem.minimize(linear=[2, 2], quadratic=[[2, 0.25], [0.25, 0.5]])\n\n        # solve problem with cobyla\n        cobyla = CobylaOptimizer()\n        result = cobyla.solve(problem)\n\n        # analyze results\n        self.assertAlmostEqual(result.fval, 5.8750)\n\n    def test_cobyla_optimizer_with_quadratic_constraint(self):\n        # load optimization problem\n        problem = QuadraticProgram()\n        problem.continuous_var(upperbound=1)\n        problem.continuous_var(upperbound=1)\n\n        problem.minimize(linear=[1, 1])\n\n        linear = [-1, -1]\n        quadratic = [[1, 0], [0, 1]]\n        problem.quadratic_constraint(linear=linear, quadratic=quadratic, rhs=-1/2)\n\n        # solve problem with cobyla\n        cobyla = CobylaOptimizer()\n        result = cobyla.solve(problem)\n\n        # analyze results\n        self.assertAlmostEqual(result.fval, 1.0, places=2)\n\n    def test_cobyla_optimizer_with_variable_bounds(self):\n        # initialize optimizer\n        cobyla = CobylaOptimizer()\n\n        # initialize problem\n        problem = QuadraticProgram()\n\n        # set variables and bounds\n        problem.continuous_var(lowerbound=-1, upperbound=1)\n        problem.continuous_var(lowerbound=-2, upperbound=2)\n\n        # set objective and minimize\n        problem.minimize(linear=[1, 1])\n\n        # solve problem with cobyla\n        result = cobyla.solve(problem)\n\n        # analyze results\n        self.assertAlmostEqual(result.x[0], -1.0, places=6)\n        self.assertAlmostEqual(result.x[1], -2.0, places=6)\n\n        # set objective and minimize\n        problem.maximize(linear=[1, 1])\n\n        # solve problem with cobyla\n        result = cobyla.solve(problem)\n\n        # analyze results\n        self.assertAlmostEqual(result.x[0], 1.0, places=6)\n        self.assertAlmostEqual(result.x[1], 2.0, places=6)\n\n    def test_cobyla_optimizer_with_trials(self):\n        # load optimization problem\n        problem = QuadraticProgram()\n        problem.continuous_var(upperbound=4)\n        problem.continuous_var(upperbound=4)\n        problem.linear_constraint(linear=[1, 1], sense='=', rhs=2)\n        problem.minimize(linear=[2, 2], quadratic=[[2, 0.25], [0.25, 0.5]])\n\n        # solve problem with cobyla\n        cobyla = CobylaOptimizer(trials=3)\n        result = cobyla.solve(problem)\n\n        # analyze results\n        self.assertAlmostEqual(result.fval, 5.8750)"}
{"Repository": "pox", "input": "ICMP unreachable packet struct className unreach(packet_base) Method __init__ Method __str__ Method parse Method hdr Method srcip Method dstip Attribute prev Attribute unused Attribute next_mtu", "label": "class unreach(packet_base):\n    #NOTE: unreachable and time_exceeded are really similar.  If you\n    #      update one, please look at the other as well!\n\n    MIN_LEN = 4\n\n    def __init__(self, raw=None, prev=None, **kw):\n        packet_base.__init__(self)\n\n        self.prev = prev\n\n        self.unused = 0\n        self.next_mtu = 0\n\n        if raw is not None:\n            self.parse(raw)\n\n        self._init(kw)\n\n    def __str__(self):\n        s = ''.join(('[', 'm:', str(self.next_mtu), ']'))\n\n        return _str_rest(s, self)\n\n    def parse(self, raw):\n        assert isinstance(raw, bytes)\n        self.raw = raw\n        dlen = len(raw)\n        if dlen < self.MIN_LEN:\n            self.msg('(unreach parse) warning unreachable payload too short '\n                     'to parse header: data len %u' % dlen)\n            return\n\n        (self.unused, self.next_mtu) \\\n            = struct.unpack('!HH', raw[:self.MIN_LEN])\n\n        self.parsed = True\n\n        if dlen >= 28:\n            # xxx We're assuming this is IPv4!\n            from . import ipv4\n            self.next = ipv4.ipv4(raw=raw[unreach.MIN_LEN:],prev=self)\n        else:\n            self.next = raw[unreach.MIN_LEN:]\n\n    def hdr(self, payload):\n        return struct.pack('!HH', self.unused, self.next_mtu)\n\n    @property\n    def srcip (self):\n        try:\n          return self.payload.srcip\n        except Exception:\n          return None\n\n    @property\n    def dstip (self):\n        try:\n          return self.payload.dstip\n        except Exception:\n          return None"}
{"Repository": "FlowNetPytorch", "input": "Rescales the inputs and target arrays to the given 'size'. className Scale(object) Method __init__ Method __call__ Attribute size Attribute order", "label": "class Scale(object):\n    def __init__(self, size, order=2):\n        self.size = size\n        self.order = order\n\n    def __call__(self, inputs, target):\n        h, w, _ = inputs[0].shape\n        if (w <= h and w == self.size) or (h <= w and h == self.size):\n            return inputs, target\n        if w < h:\n            ratio = self.size / w\n        else:\n            ratio = self.size / h\n\n        inputs[0] = ndimage.interpolation.zoom(inputs[0], ratio, order=self.order)\n        inputs[1] = ndimage.interpolation.zoom(inputs[1], ratio, order=self.order)\n\n        target = ndimage.interpolation.zoom(target, ratio, order=self.order)\n        target *= ratio\n        return inputs, target"}
{"Repository": "DeepRFT", "input": "DOConv2d can be used as an alternative for torch. className DOConv2d_eval(Module) Method extra_repr Method __setstate__ Method _conv_forward Method forward", "label": "class DOConv2d_eval(Module):\n    __constants__ = ['stride', 'padding', 'dilation', 'groups',\n                     'padding_mode', 'output_padding', 'in_channels',\n                     'out_channels', 'kernel_size', 'D_mul']\n    __annotations__ = {'bias': Optional[torch.Tensor]}\n\n    def __init__(self, in_channels, out_channels, kernel_size=3, D_mul=None, stride=1,\n                 padding=1, dilation=1, groups=1, bias=False, padding_mode='zeros', simam=False):\n        super(DOConv2d_eval, self).__init__()\n\n        kernel_size = (kernel_size, kernel_size)\n        stride = (stride, stride)\n        padding = (padding, padding)\n        dilation = (dilation, dilation)\n\n        if in_channels % groups != 0:\n            raise ValueError('in_channels must be divisible by groups')\n        if out_channels % groups != 0:\n            raise ValueError('out_channels must be divisible by groups')\n        valid_padding_modes = {'zeros', 'reflect', 'replicate', 'circular'}\n        if padding_mode not in valid_padding_modes:\n            raise ValueError(\"padding_mode must be one of {}, but got padding_mode='{}'\".format(\n                valid_padding_modes, padding_mode))\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        self.kernel_size = kernel_size\n        self.stride = stride\n        self.padding = padding\n        self.dilation = dilation\n        self.groups = groups\n        self.padding_mode = padding_mode\n        self._padding_repeated_twice = tuple(x for x in self.padding for _ in range(2))\n        self.simam = simam\n        #################################### Initailization of D & W ###################################\n        M = self.kernel_size[0]\n        N = self.kernel_size[1]\n        self.W = Parameter(torch.Tensor(out_channels, in_channels // groups, M, N))\n        init.kaiming_uniform_(self.W, a=math.sqrt(5))\n\n        self.register_parameter('bias', None)\n    def extra_repr(self):\n        s = ('{in_channels}, {out_channels}, kernel_size={kernel_size}'\n             ', stride={stride}')\n        if self.padding != (0,) * len(self.padding):\n            s += ', padding={padding}'\n        if self.dilation != (1,) * len(self.dilation):\n            s += ', dilation={dilation}'\n        if self.groups != 1:\n            s += ', groups={groups}'\n        if self.bias is None:\n            s += ', bias=False'\n        if self.padding_mode != 'zeros':\n            s += ', padding_mode={padding_mode}'\n        return s.format(**self.__dict__)\n\n    def __setstate__(self, state):\n        super(DOConv2d, self).__setstate__(state)\n        if not hasattr(self, 'padding_mode'):\n            self.padding_mode = 'zeros'\n\n    def _conv_forward(self, input, weight):\n        if self.padding_mode != 'zeros':\n            return F.conv2d(F.pad(input, self._padding_repeated_twice, mode=self.padding_mode),\n                            weight, self.bias, self.stride,\n                            (0, 0), self.dilation, self.groups)\n        return F.conv2d(input, weight, self.bias, self.stride,\n                        self.padding, self.dilation, self.groups)\n\n    def forward(self, input):\n        return self._conv_forward(input, self.W)"}
{"Repository": "pyedflib", "input": "Ugly monkeypatching to get clib to build for development installs See coverage comment above for why we don't just let libraries be built via extensions. className develop_build_clib(develop) Method install_for_development", "label": "class develop_build_clib(develop):\n    def install_for_development(self):\n        self.run_command('egg_info')\n\n        # Build extensions in-place (the next 7 lines are the monkeypatch)\n        import glob\n        hitlist = glob.glob(os.path.join('build', '*', 'c_edf.*'))\n        if hitlist:\n            # Remove existing clib - running build_clib twice in a row fails\n            os.remove(hitlist[0])\n        self.reinitialize_command('build_clib', inplace=1)\n        self.run_command('build_clib')\n\n        self.reinitialize_command('build_ext', inplace=1)\n        self.run_command('build_ext')\n\n        try:\n            self.install_site_py()  # ensure that target dir is site-safe\n        except Exception as e:\n            print(e)\n\n        if setuptools.bootstrap_install_from:\n            self.easy_install(setuptools.bootstrap_install_from)\n            setuptools.bootstrap_install_from = None\n\n        # create an .egg-link in the installation dir, pointing to our egg\n        from distutils import log\n        log.info(\"Creating %s (link to %s)\", self.egg_link, self.egg_base)\n        if not self.dry_run:\n            with open(self.egg_link, \"w\") as f:\n                f.write(self.egg_path + \"\\n\" + self.setup_path)\n        # postprocess the installed distro, fixing up .pth, installing scripts,\n        # and handling requirements\n        self.process_distribution(None, self.dist, not self.no_deps)"}
{"Repository": "Obj2Seq", "input": "Sampler that restricts data loading to a subset of the dataset. className DistributedSampler(Sampler) Method __init__ Method __iter__ Method __len__ Method set_epoch Attribute dataset Attribute num_replicas Attribute rank Attribute epoch Attribute num_samples Attribute total_size Attribute shuffle Attribute split_length Attribute split_length", "label": "class DistributedSampler(Sampler):\n    def __init__(self, dataset, num_replicas=None, rank=None, local_rank=None, local_size=None, shuffle=True, fix_split=False):\n        if num_replicas is None:\n            if not dist.is_available():\n                raise RuntimeError(\"Requires distributed package to be available\")\n            num_replicas = dist.get_world_size()\n        if rank is None:\n            if not dist.is_available():\n                raise RuntimeError(\"Requires distributed package to be available\")\n            rank = dist.get_rank()\n        self.dataset = dataset\n        self.num_replicas = num_replicas\n        self.rank = rank\n        self.epoch = 0\n        self.num_samples = int(math.ceil(len(self.dataset) * 1.0 / self.num_replicas))\n        self.total_size = self.num_samples * self.num_replicas\n        self.shuffle = shuffle\n        if fix_split:\n            assert hasattr(self.dataset, \"split_length\"), \"The dataset does not have split_length attribute\"\n            assert shuffle, \"sampler_fix_split only works with shuffle\"\n            self.split_length = self.dataset.split_length\n        else:\n            self.split_length = None\n\n    def __iter__(self):\n        if self.shuffle:\n            # deterministically shuffle based on epoch\n            g = torch.Generator()\n            g.manual_seed(self.epoch)\n            if self.split_length is None:\n                indices = torch.randperm(len(self.dataset), generator=g).tolist()\n            else:\n                indices = []\n                sub_indices = [torch.randperm(l, generator=g).tolist() for l in self.split_length]\n                for sub in sub_indices:\n                    indices.extend(sub)\n        else:\n            indices = torch.arange(len(self.dataset)).tolist()\n\n        # add extra samples to make it evenly divisible\n        indices += indices[: (self.total_size - len(indices))]\n        assert len(indices) == self.total_size\n\n        # subsample\n        offset = self.num_samples * self.rank\n        indices = indices[offset : offset + self.num_samples]\n        assert len(indices) == self.num_samples\n\n        return iter(indices)\n\n    def __len__(self):\n        return self.num_samples\n\n    def set_epoch(self, epoch):\n        self.epoch = epoch"}
{"Repository": "scrap_engine", "input": "A datatype containing a string, that can be added to a map. className Text(ObjectGroup) Method width Method height Method __add__ Method __texter Method add Method remove Method rem_ob Method rechar", "label": "class Text(ObjectGroup):\n    def __init__(self, text, state=None, esccode=\"\", ob_class=Object,\n                 ob_args=None, ignore=\"\"):\n        super().__init__([], state)\n        if ob_args is None:\n            ob_args = {}\n        self.ob_class = ob_class\n        self.text = text\n        self.esccode = esccode\n        self.ignore = ignore\n        self.ob_args = ob_args\n        self.__texter(text)\n\n    @property\n    def width(self):\n        return sorted(len(i) for i in self.text.split(\"\\n\"))[-1]\n\n    @property\n    def height(self):\n        return len(self.text.split(\"\\n\"))\n\n    def __add__(self, other):\n        self.text += other.text\n        self.obs += other.obs\n        for obj in self.obs:\n            obj.group = self\n        if self.added:\n            self.remove()\n            self.add(self.map, self.x, self.y)\n        return self\n\n    def __texter(self, text):\n        for txt in text.split(\"\\n\"):\n            for char in txt:\n                if self.esccode != \"\":\n                    char = self.esccode + char + \"\\033[0m\"\n                obj = self.ob_class(\n                    char, self.state, arg_proto=self.ob_args\n                )\n                obj.group = self\n                self.obs.append(obj)\n        for obj in self.obs:\n            obj.group = self\n\n    def add(self, _map, x, y):\n        self.added = True\n        self.map = _map\n        self.x = x\n        self.y = y\n        count = 0\n        for l, text in enumerate(self.text.split(\"\\n\")):\n            for i, obj in enumerate(self.obs[count:count + len(text)]):\n                if obj.char != self.ignore:\n                    obj.add(self.map, x + i, y + l)\n            count += len(text)\n\n    def remove(self):\n        self.added = False\n        for obj in self.obs:\n            obj.remove()\n\n    def rem_ob(self, obj):\n        if obj in self.obs:\n            obj.group = None\n            index = self.obs.index(obj)\n            idx = 0\n            while idx < len(self.text):\n                if self.text[idx:idx+2] == \"\\n\":\n                    idx += 2\n                    continue\n                if idx == index:\n                    self.text = self.text[:idx] + self.text[idx + 1:]\n                    break\n                idx += 1\n            self.obs.pop(index)\n            return 0\n        return 1\n\n    def rechar(self, text, esccode=\"\"):\n        self.esccode = esccode\n        if self.added:\n            for obj in self.obs:\n                obj.remove()\n        self.obs = []\n        self.__texter(text)\n        self.text = text\n        if self.added:\n            self.add(self.map, self.x, self.y)"}
{"Repository": "ConvNeXt", "input": "Track a series of values and provide access to smoothed values over a window or the global series average. className SmoothedValue(object) Method __init__ Method update Method synchronize_between_processes Method median Method avg Method global_avg Method max Method value Method __str__ Attribute deque Attribute total Attribute count Attribute fmt", "label": "class SmoothedValue(object):\n    def __init__(self, window_size=20, fmt=None):\n        if fmt is None:\n            fmt = \"{median:.4f} ({global_avg:.4f})\"\n        self.deque = deque(maxlen=window_size)\n        self.total = 0.0\n        self.count = 0\n        self.fmt = fmt\n\n    def update(self, value, n=1):\n        self.deque.append(value)\n        self.count += n\n        self.total += value * n\n\n    def synchronize_between_processes(self):\n        if not is_dist_avail_and_initialized():\n            return\n        t = torch.tensor([self.count, self.total], dtype=torch.float64, device='cuda')\n        dist.barrier()\n        dist.all_reduce(t)\n        t = t.tolist()\n        self.count = int(t[0])\n        self.total = t[1]\n\n    @property\n    def median(self):\n        d = torch.tensor(list(self.deque))\n        return d.median().item()\n\n    @property\n    def avg(self):\n        d = torch.tensor(list(self.deque), dtype=torch.float32)\n        return d.mean().item()\n\n    @property\n    def global_avg(self):\n        return self.total / self.count\n\n    @property\n    def max(self):\n        return max(self.deque)\n\n    @property\n    def value(self):\n        return self.deque[-1]\n\n    def __str__(self):\n        return self.fmt.format(\n            median=self.median,\n            avg=self.avg,\n            global_avg=self.global_avg,\n            max=self.max,\n            value=self.value)"}
{"Repository": "ConfigArgParse", "input": "parses INI files using pythons configparser. className ConfigparserConfigFileParser(ConfigFileParser) Method get_syntax_description Method parse Method serialize", "label": "class ConfigparserConfigFileParser(ConfigFileParser):\n    def get_syntax_description(self):\n        msg = \"\"\"Uses configparser module to parse an INI file which allows multi-line\n        values.\n\n        Allowed syntax is that for a ConfigParser with the following options:\n\n            allow_no_value = False,\n            inline_comment_prefixes = (\"#\",)\n            strict = True\n            empty_lines_in_values = False\n\n        See https://docs.python.org/3/library/configparser.html for details.\n\n        Note: INI file sections names are still treated as comments.\n        \"\"\"\n        return msg\n\n    def parse(self, stream):\n        # see ConfigFileParser.parse docstring\n        import configparser\n        from ast import literal_eval\n        # parse with configparser to allow multi-line values\n        config = configparser.ConfigParser(\n            delimiters=(\"=\",\":\"),\n            allow_no_value=False,\n            comment_prefixes=(\"#\",\";\"),\n            inline_comment_prefixes=(\"#\",\";\"),\n            strict=True,\n            empty_lines_in_values=False,\n        )\n        try:\n            config.read_string(stream.read())\n        except Exception as e:\n            raise ConfigFileParserException(\"Couldn't parse config file: %s\" % e)\n\n        # convert to dict and remove INI section names\n        result = OrderedDict()\n        for section in config.sections():\n            for k,v in config[section].items():\n                multiLine2SingleLine = v.replace('\\n',' ').replace('\\r',' ')\n                # handle special case for lists\n                if '[' in multiLine2SingleLine and ']' in multiLine2SingleLine:\n                    # ensure not a dict with a list value\n                    prelist_string = multiLine2SingleLine.split('[')[0]\n                    if '{' not in prelist_string:\n                        result[k] = literal_eval(multiLine2SingleLine)\n                    else:\n                        result[k] = multiLine2SingleLine\n                else:\n                    result[k] = multiLine2SingleLine\n        return result\n\n    def serialize(self, items):\n        # see ConfigFileParser.serialize docstring\n        import configparser\n        import io\n        config = configparser.ConfigParser(\n            allow_no_value=False,\n            inline_comment_prefixes=(\"#\",),\n            strict=True,\n            empty_lines_in_values=False,\n        )\n        items = {\"DEFAULT\": items}\n        config.read_dict(items)\n        stream = io.StringIO()\n        config.write(stream)\n        stream.seek(0)\n        return stream.read()"}
{"Repository": "TransVOD", "input": "Inherit official COCO class in order to parse the annotations of bbox- related video tasks. className CocoVID(COCO) Method __init__ Method convert_img_to_vid Method createIndex Method get_vid_ids Method get_img_ids_from_vid Method get_ins_ids_from_vid Method get_img_ids_from_ins_id Method load_vids Attribute load_img_as_vid", "label": "class CocoVID(COCO):\n    def __init__(self, annotation_file=None, load_img_as_vid=False):\n\n        assert annotation_file, 'Annotation file must be provided.'\n\n        self.load_img_as_vid = load_img_as_vid\n\n        super(CocoVID, self).__init__(annotation_file=annotation_file)\n\n\n\n    def convert_img_to_vid(self, dataset):\n        if 'images' in self.dataset:\n\n            videos = []\n\n            for i, img in enumerate(self.dataset['images']):\n\n                videos.append(dict(id=img['id'], name=img['file_name']))\n\n                img['video_id'] = img['id']\n\n                img['frame_id'] = 0\n\n            dataset['videos'] = videos\n\n\n\n        if 'annotations' in self.dataset:\n\n            for i, ann in enumerate(self.dataset['annotations']):\n\n                ann['video_id'] = ann['image_id']\n\n                ann['instance_id'] = ann['id']\n\n        return dataset\n\n\n\n    def createIndex(self):\n        print('creating index...')\n\n        anns, cats, imgs, vids = {}, {}, {}, {}\n\n        (imgToAnns, catToImgs, vidToImgs, vidToInstances,\n\n         instancesToImgs) = defaultdict(list), defaultdict(list), defaultdict(\n\n             list), defaultdict(list), defaultdict(list)\n\n\n\n        if 'videos' not in self.dataset and self.load_img_as_vid:\n\n            self.dataset = self.convert_img_to_vid(self.dataset)\n\n\n\n        if 'videos' in self.dataset:\n\n            for video in self.dataset['videos']:\n\n                vids[video['id']] = video\n\n\n\n        if 'annotations' in self.dataset:\n\n            for ann in self.dataset['annotations']:\n\n                imgToAnns[ann['image_id']].append(ann)\n\n                anns[ann['id']] = ann\n\n                if 'instance_id' in ann:\n\n                    instancesToImgs[ann['instance_id']].append(ann['image_id'])\n\n                    if 'video_id' in ann and \\\n\n                        ann['instance_id'] not in \\\n\n                            vidToInstances[ann['video_id']]:\n\n                        vidToInstances[ann['video_id']].append(\n\n                            ann['instance_id'])\n\n\n\n        if 'images' in self.dataset:\n\n            for img in self.dataset['images']:\n\n                vidToImgs[img['video_id']].append(img)\n\n                imgs[img['id']] = img\n\n\n\n        if 'categories' in self.dataset:\n\n            for cat in self.dataset['categories']:\n\n                cats[cat['id']] = cat\n\n\n\n        if 'annotations' in self.dataset and 'categories' in self.dataset:\n\n            for ann in self.dataset['annotations']:\n\n                catToImgs[ann['category_id']].append(ann['image_id'])\n\n\n\n        print('index created!')\n\n\n\n        self.anns = anns\n\n        self.imgToAnns = imgToAnns\n\n        self.catToImgs = catToImgs\n\n        self.imgs = imgs\n\n        self.cats = cats\n\n        self.videos = vids\n\n        self.vidToImgs = vidToImgs\n\n        self.vidToInstances = vidToInstances\n\n        self.instancesToImgs = instancesToImgs\n\n\n\n    def get_vid_ids(self, vidIds=[]):\n        vidIds = vidIds if _isArrayLike(vidIds) else [vidIds]\n\n\n\n        if len(vidIds) == 0:\n\n            ids = self.videos.keys()\n\n        else:\n\n            ids = set(vidIds)\n\n\n\n        return list(ids)\n\n\n\n    def get_img_ids_from_vid(self, vidId):\n        img_infos = self.vidToImgs[vidId]\n\n        ids = list(np.zeros([len(img_infos)], dtype=np.int))\n\n\n\n        for i, img_info in enumerate(img_infos):\n\n            ids[i] = img_info[\"id\"]\n\n        # for img_info in img_infos:\n\n        #     ids[img_info['frame_id']] = img_info['id']\n\n            \n\n        return ids\n\n\n\n    def get_ins_ids_from_vid(self, vidId):\n        return self.vidToInstances[vidId]\n\n\n\n    def get_img_ids_from_ins_id(self, insId):\n        return self.instancesToImgs[insId]\n\n\n\n    def load_vids(self, ids=[]):\n        if _isArrayLike(ids):\n\n            return [self.videos[id] for id in ids]\n\n        elif type(ids) == int:\n\n            return [self.videos[ids]]"}
{"Repository": "ProjectQ", "input": "Extension raised if the build fails for any reason. className BuildFailed(Exception) Method __init__ Attribute cause", "label": "class BuildFailed(Exception):\n    def __init__(self):\n        super().__init__()\n        self.cause = sys.exc_info()[1]  # work around py 2/3 different syntax"}
{"Repository": "torshammer", "input": "socket_err contains original socket.error exception. className ProxyError(IOError) Method __init__ Method __str__ Attribute msg Attribute socket_err", "label": "class ProxyError(IOError):\n    def __init__(self, msg, socket_err=None):\n        self.msg = msg\n        self.socket_err = socket_err\n\n        if socket_err:\n            self.msg += \": {0}\".format(socket_err)\n\n    def __str__(self):\n        return self.msg"}
{"Repository": "bert_language_understanding", "input": "Base class for data converters for sequence classification data sets. className DataProcessor(object) Method get_train_examples Method get_dev_examples Method get_test_examples Method get_labels Method _read_tsv", "label": "class DataProcessor(object):\n  def get_train_examples(self, data_dir):\n    raise NotImplementedError()\n\n  def get_dev_examples(self, data_dir):\n    raise NotImplementedError()\n\n  def get_test_examples(self, data_dir):\n    raise NotImplementedError()\n\n  def get_labels(self):\n    raise NotImplementedError()\n\n  @classmethod\n  def _read_tsv(cls, input_file, quotechar=None):\n    with tf.gfile.Open(input_file, \"r\") as f:\n      reader = csv.reader(f, delimiter=\"\\t\", quotechar=quotechar)\n      lines = []\n      for line in reader:\n        lines.append(line)\n      return lines"}
{"Repository": "xgboost-survival-embeddings", "input": "Perform stacking of a XGBoost survival model with a Weibull AFT parametric model. className XGBSEStackedWeibull(XGBSEBaseEstimator) Method predict", "label": "class XGBSEStackedWeibull(XGBSEBaseEstimator):\n    def __init__(\n        self,\n        xgb_params=None,\n        weibull_params=None,\n    ):\n        if xgb_params is None:\n            xgb_params = DEFAULT_PARAMS\n        if weibull_params is None:\n            weibull_params = DEFAULT_PARAMS_WEIBULL\n\n        self.xgb_params = xgb_params\n        self.weibull_params = weibull_params\n        self.persist_train = False\n        self.feature_importances_ = None\n\n    def fit(\n        self,\n        X,\n        y,\n        num_boost_round=1000,\n        validation_data=None,\n        early_stopping_rounds=None,\n        verbose_eval=0,\n        persist_train=False,\n        index_id=None,\n        time_bins=None,\n    ):\n        E_train, T_train = convert_y(y)\n        if time_bins is None:\n            time_bins = get_time_bins(T_train, E_train)\n        self.time_bins = time_bins\n\n        # converting data to xgb format\n        dtrain = convert_data_to_xgb_format(X, y, self.xgb_params[\"objective\"])\n\n        # converting validation data to xgb format\n        evals = ()\n        if validation_data:\n            X_val, y_val = validation_data\n            dvalid = convert_data_to_xgb_format(\n                X_val, y_val, self.xgb_params[\"objective\"]\n            )\n            evals = [(dvalid, \"validation\")]\n\n        # training XGB\n        self.bst = xgb.train(\n            self.xgb_params,\n            dtrain,\n            num_boost_round=num_boost_round,\n            early_stopping_rounds=early_stopping_rounds,\n            evals=evals,\n            verbose_eval=verbose_eval,\n        )\n        self.feature_importances_ = self.bst.get_score()\n\n        # predicting risk from XGBoost\n        train_risk = self.bst.predict(\n            dtrain, iteration_range=(0, self.bst.best_iteration + 1)\n        )\n\n        # replacing 0 by minimum positive value in df\n        # so Weibull can be fitted\n        min_positive_value = T_train[T_train > 0].min()\n        T_train = np.clip(T_train, min_positive_value, None)\n\n        # creating df to use lifelines API\n        weibull_train_df = pd.DataFrame(\n            {\"risk\": train_risk, \"duration\": T_train, \"event\": E_train}\n        )\n\n        # fitting weibull aft\n        self.weibull_aft = WeibullAFTFitter(**self.weibull_params)\n        self.weibull_aft.fit(weibull_train_df, \"duration\", \"event\", ancillary=True)\n\n        if persist_train:\n            self.persist_train = True\n            if index_id is None:\n                index_id = X.index.copy()\n\n            index_leaves = self.bst.predict(\n                dtrain, pred_leaf=True, iteration_range=(0, self.bst.best_iteration + 1)\n            )\n            self.tree = BallTree(index_leaves, metric=\"hamming\")\n\n        self.index_id = index_id\n\n        return self\n\n    def predict(self, X, return_interval_probs=False):\n        # converting to xgb format\n        d_matrix = xgb.DMatrix(X)\n\n        # getting leaves and extracting neighbors\n        risk = self.bst.predict(\n            d_matrix, iteration_range=(0, self.bst.best_iteration + 1)\n        )\n        weibull_score_df = pd.DataFrame({\"risk\": risk})\n\n        # predicting from logistic regression artifacts\n\n        preds_df = self.weibull_aft.predict_survival_function(\n            weibull_score_df, self.time_bins\n        ).T\n\n        if return_interval_probs:\n            preds_df = calculate_interval_failures(preds_df)\n\n        return preds_df"}
{"Repository": "OpenFermion", "input": "Davidson algorithm applied to a QubitOperator. className QubitDavidson(Davidson) Method __init__", "label": "class QubitDavidson(Davidson):\n    def __init__(self, qubit_operator, n_qubits=None, options=None):\n        super(QubitDavidson, self).__init__(\n            generate_linear_qubit_operator(qubit_operator, n_qubits, options),\n            get_linear_qubit_operator_diagonal(qubit_operator, n_qubits),\n            options=options,\n        )"}
{"Repository": "tacotron2", "input": "A base AttentionMechanism class providing common functionality. className _BaseAttentionMechanism(AttentionMechanism) Method memory_layer Method query_layer Method values Method keys Method batch_size Method alignments_size Method initial_alignments", "label": "class _BaseAttentionMechanism(AttentionMechanism):\n\tdef __init__(self,\n\t\t\t\t\t\t\t query_layer,\n\t\t\t\t\t\t\t memory,\n\t\t\t\t\t\t\t probability_fn,\n\t\t\t\t\t\t\t memory_sequence_length=None,\n\t\t\t\t\t\t\t memory_layer=None,\n\t\t\t\t\t\t\t check_inner_dims_defined=True,\n\t\t\t\t\t\t\t score_mask_value=float(\"-inf\"),\n\t\t\t\t\t\t\t name=None):\n\t\tif (query_layer is not None\n\t\t\t\tand not isinstance(query_layer, layers_base.Layer)):\n\t\t\traise TypeError(\n\t\t\t\t\t\"query_layer is not a Layer: %s\" % type(query_layer).__name__)\n\t\tif (memory_layer is not None\n\t\t\t\tand not isinstance(memory_layer, layers_base.Layer)):\n\t\t\traise TypeError(\n\t\t\t\t\t\"memory_layer is not a Layer: %s\" % type(memory_layer).__name__)\n\t\tself._query_layer = query_layer\n\t\tself._memory_layer = memory_layer\n\t\tif not callable(probability_fn):\n\t\t\traise TypeError(\"probability_fn must be callable, saw type: %s\" %\n\t\t\t\t\t\t\t\t\t\t\ttype(probability_fn).__name__)\n\t\tself._probability_fn = lambda score, prev: (  # pylint:disable=g-long-lambda\n\t\t\t\tprobability_fn(\n\t\t\t\t\t\t_maybe_mask_score(score, memory_sequence_length, score_mask_value),\n\t\t\t\t\t\tprev))\n\t\twith ops.name_scope(\n\t\t\t\tname, \"BaseAttentionMechanismInit\", nest.flatten(memory)):\n\t\t\tself._values = _prepare_memory(\n\t\t\t\t\tmemory, memory_sequence_length,\n\t\t\t\t\tcheck_inner_dims_defined=check_inner_dims_defined)\n\t\t\tself._keys = (\n\t\t\t\t\tself.memory_layer(self._values) if self.memory_layer  # pylint: disable=not-callable\n\t\t\t\t\telse self._values)\n\t\t\tself._batch_size = (\n\t\t\t\t\tself._keys.shape[0].value or array_ops.shape(self._keys)[0])\n\t\t\tself._alignments_size = (self._keys.shape[1].value or\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t array_ops.shape(self._keys)[1])\n\n\t@property\n\tdef memory_layer(self):\n\t\treturn self._memory_layer\n\n\t@property\n\tdef query_layer(self):\n\t\treturn self._query_layer\n\n\t@property\n\tdef values(self):\n\t\treturn self._values\n\n\t@property\n\tdef keys(self):\n\t\treturn self._keys\n\n\t@property\n\tdef batch_size(self):\n\t\treturn self._batch_size\n\n\t@property\n\tdef alignments_size(self):\n\t\treturn self._alignments_size\n\n\tdef initial_alignments(self, batch_size, dtype):\n\t\tmax_time = self._alignments_size\n\t\treturn _zero_state_tensors(max_time, batch_size, dtype)"}
{"Repository": "bridgy-fed", "input": "A reply, like, repost, or other interaction that we've relayed. className Activity(StringIdModel) Method _get_kind Method source Method target Method to_as1", "label": "class Activity(StringIdModel):\n    STATUSES = ('new', 'complete', 'error', 'ignored')\n    PROTOCOLS = ('activitypub', 'ostatus')\n    DIRECTIONS = ('out', 'in')\n\n    # domains of the Bridgy Fed users this activity is to or from\n    domain = ndb.StringProperty(repeated=True)\n    status = ndb.StringProperty(choices=STATUSES, default='new')\n    protocol = ndb.StringProperty(choices=PROTOCOLS)\n    direction = ndb.StringProperty(choices=DIRECTIONS)\n\n    # usually only one of these at most will be populated.\n    source_mf2 = ndb.TextProperty()  # JSON\n    source_as2 = ndb.TextProperty()  # JSON\n    source_atom = ndb.TextProperty()\n    target_as2 = ndb.TextProperty()  # JSON\n\n    created = ndb.DateTimeProperty(auto_now_add=True)\n    updated = ndb.DateTimeProperty(auto_now=True)\n\n    @classmethod\n    def _get_kind(cls):\n        return 'Response'\n\n    def source(self):\n        return self.key.id().split()[0]\n\n    def target(self):\n        return self.key.id().split()[1]\n\n    def to_as1(self):\n        if self.source_mf2:\n            mf2 = json_loads(self.source_mf2)\n            items = mf2.get('items')\n            if items:\n                mf2 = items[0]\n            return microformats2.json_to_object(mf2)\n        if self.source_as2:\n            return as2.to_as1(json_loads(self.source_as2))\n        if self.source_atom:\n            return atom.atom_to_activity(self.source_atom)"}
{"Repository": "blinker", "input": "A signal receipt accumulator. className Sentinel(list) Method make_receiver Method receiver", "label": "class Sentinel(list):\n    def make_receiver(self, key):\n        def receiver(*sentby, **kw):\n            self.append((key, sentby[0], kw))\n\n        receiver.func_name = \"receiver_%s\" % key\n        return receiver"}
{"Repository": "pyquil", "input": "Encapsulate the QPU noise model containing information about the noisy gates. className NoiseModel(_NoiseModel) Method to_dict Method from_dict Method gates_by_name Method __eq__ Method __neq__", "label": "class NoiseModel(_NoiseModel):\n    def to_dict(self) -> Dict[str, Any]:\n        return {\n            \"gates\": [km.to_dict() for km in self.gates],\n            \"assignment_probs\": {str(qid): a.tolist() for qid, a in self.assignment_probs.items()},\n        }\n\n    @staticmethod\n    def from_dict(d: Dict[str, Any]) -> \"NoiseModel\":\n        return NoiseModel(\n            gates=[KrausModel.from_dict(t) for t in d[\"gates\"]],\n            assignment_probs={int(qid): np.array(a) for qid, a in d[\"assignment_probs\"].items()},\n        )\n\n    def gates_by_name(self, name: str) -> List[KrausModel]:\n        return [g for g in self.gates if g.gate == name]\n\n    def __eq__(self, other: object) -> bool:\n        return isinstance(other, NoiseModel) and self.to_dict() == other.to_dict()\n\n    def __neq__(self, other: object) -> bool:\n        return not self.__eq__(other)"}
{"Repository": "eyecite", "input": "Base class for a case, law, or journal citation. className ResourceCitation(CitationBase) Method __post_init__ Method __hash__ Method add_metadata Method dump Method corrected_reporter Method corrected_citation Method guess_edition", "label": "class ResourceCitation(CitationBase):\n    # Editions that might match this reporter string\n    exact_editions: Sequence[Edition] = field(default_factory=tuple)\n    variation_editions: Sequence[Edition] = field(default_factory=tuple)\n    all_editions: Sequence[Edition] = field(default_factory=tuple)\n    edition_guess: Optional[Edition] = None\n\n    # year extracted from metadata[\"year\"] and converted to int,\n    # if in a valid range\n    year: Optional[int] = None\n\n    def __post_init__(self):\n        self.exact_editions = tuple(self.exact_editions)\n        self.variation_editions = tuple(self.variation_editions)\n        self.all_editions = tuple(self.exact_editions) + tuple(\n            self.variation_editions\n        )\n        super().__post_init__()\n\n    def __hash__(self) -> int:\n        return hash(\n            hash_sha256(\n                {\n                    **dict(self.groups.items()),\n                    **{\n                        \"all_editions\": sorted(\n                            [asdict(e) for e in self.all_editions],\n                            key=lambda d: d[\"short_name\"],  # type: ignore\n                        ),\n                        \"class\": type(self).__name__,\n                    },\n                }\n            )\n        )\n\n    @dataclass(eq=True, unsafe_hash=True)\n    class Metadata(CitationBase.Metadata):\n        pin_cite: Optional[str] = None\n        year: Optional[str] = None\n\n    def add_metadata(self, words: \"Tokens\"):\n        self.guess_edition()\n\n    def dump(self) -> dict:\n        return {\n            **super().dump(),\n            \"year\": self.year,\n        }\n\n    def corrected_reporter(self):\n        return (\n            self.edition_guess.short_name\n            if self.edition_guess\n            else self.groups[\"reporter\"]\n        )\n\n    def corrected_citation(self):\n        if self.edition_guess:\n            return self.matched_text().replace(\n                self.groups[\"reporter\"], self.edition_guess.short_name\n            )\n        return self.matched_text()\n\n    def guess_edition(self):\n        # Use exact matches if possible, otherwise try variations\n        editions = self.exact_editions or self.variation_editions\n        if not editions:\n            return\n\n        # Attempt resolution by date\n        if len(editions) > 1 and self.year:\n            editions = [e for e in editions if e.includes_year(self.year)]\n\n        if len(editions) == 1:\n            self.edition_guess = editions[0]"}
{"Repository": "bayespy", "input": "Perform 1-p where p is a Beta node. className Complement(Deterministic) Method __init__ Method _compute_message_to_parent Method _compute_moments", "label": "class Complement(Deterministic):\n    _moments = BetaMoments()\n    _parent_moments = (BetaMoments(),)\n\n\n    def __init__(self, p, **kwargs):\n        super().__init__(p, dims=p.dims, **kwargs)\n\n\n    def _compute_message_to_parent(self, index, m, u_p):\n        if index != 0:\n            raise IndexError()\n        m0 = m[0][...,-1::-1]\n        return [m0]\n\n\n    def _compute_moments(self, u_p):\n        u0 = u_p[0][...,-1::-1]\n        return [u0]"}
{"Repository": "ExGAN", "input": "NWS Dataset className NWSDataset(Dataset) Method __len__ Method __getitem__", "label": "class NWSDataset(Dataset):\n    def __init__(\n            self, fake='DistShift/fake10.pt', c=0.75, k=10, n=2557\n    ):\n        val = int((c ** k) * n)\n        self.real = torch.load('data/real.pt').cuda(cudanum)\n        self.fake = torch.load(fake).cuda(cudanum)\n        self.realdata = torch.cat([self.real[:val], self.fake[:n - val]], 0)\n        indices = torch.randperm(n)\n        self.realdata = self.realdata[indices]\n\n    def __len__(self):\n        return self.realdata.shape[0]\n\n    def __getitem__(self, item):\n        img = self.realdata[item]\n        return img, img.sum() / 4096"}
{"Repository": "guacamol_baselines", "input": "map elites using two dimensions: molecular weight and log p className MWLogPMapElites(MapElites) Method __init__ Method compute_features Attribute mw_step_size Attribute logp_step_size", "label": "class MWLogPMapElites(MapElites):\n    def __init__(self, mw_step_size: float = 25., logp_step_size: float = 0.25, n_elites: int = 1):\n        self.mw_step_size = mw_step_size\n        self.logp_step_size = logp_step_size\n        super().__init__(n_elites)\n\n    def compute_features(self, m: Molecule) -> str:\n        mw = Descriptors.MolWt(m.mol)\n        log_p = Descriptors.MolLogP(m.mol)\n\n        # discretize\n        mw_cell_midpoint = round(mw / self.mw_step_size) * self.mw_step_size\n        log_p_cell_midpoint = round(log_p / self.logp_step_size) * self.logp_step_size\n\n        return f\"mw-midpoint={mw_cell_midpoint},logp-midpoint{log_p_cell_midpoint}\""}
{"Repository": "weechat-vimode", "input": "Wraps User Mapping Defined by :nmap Command className UserMapping(UMParser) Method __init__ Method __call__ Method lock Method process_count Method report_errors Attribute lhs Attribute rhs Attribute noremap", "label": "class UserMapping(UMParser):\n    count = 0\n    locked = False\n    noremap = False\n\n    def __init__(self, lhs, rhs, noremap=False):\n        self.lhs = lhs\n        self.rhs = rhs\n        self.noremap = noremap\n\n    def __call__(self, buf, input_line, cur, count):\n        if self.locked:\n            error_fmt = (\"Somthing's not right. The following user mapping \"\n                         \"is recursing on itself: (\\\"{}\\\", \\\"{}\\\").\")\n            print_warning(error_fmt.format(self.lhs, self.rhs))\n            return\n\n        rhs, count = self.process_count(count)\n        self.bad_sequence = \"\"\n        with self.lock():\n            for _ in range(count):\n                bad_seq_list = []\n                for action in self.parse(rhs):\n                    if self.bad_sequence:\n                        bad_seq_list.append(self.bad_sequence)\n\n                    action(buf, input_line, cur, self.count)\n\n                    # Reset count unless last key triggers\n                    # INSERT mode ('i', 'a', 'I', 'A', ...).\n                    if mode != 'INSERT':\n                        self.count = 0\n\n                    self.bad_sequence = \"\"\n\n                    buf = weechat.current_buffer()\n                    input_line = weechat.buffer_get_string(buf, \"input\")\n                    cur = weechat.buffer_get_integer(buf, \"input_pos\")\n\n                self.count = 0\n                if self.bad_sequence:\n                    bad_seq_list.append(self.bad_sequence)\n                    self.bad_sequence = \"\"\n\n                self.report_errors(bad_seq_list)\n\n    @contextmanager\n    def lock(self):\n        try:\n            self.locked = True\n            yield\n        finally:\n            self.locked = False\n\n    def process_count(self, count):\n        if re.search(r'#{\\d+}', self.rhs) is not None:\n            if count:\n                rhs = re.sub(r'#{\\d+}', str(count), self.rhs)\n            else:\n                rhs = re.sub(r'#{(\\d+)}', r'\\1', self.rhs)\n            new_count = 1\n        else:\n            rhs = self.rhs\n            new_count = max(count, 1)\n        return rhs, new_count\n\n    def report_errors(self, bad_seq_list):\n        for bad_seq in bad_seq_list:\n            error_msg = 'Failed to parse \"{}\" sequence ' \\\n                'in the following user mapping: ' \\\n                '(\"{}\", \"{}\").'.format(bad_seq, self.lhs, self.rhs)\n            print_warning(error_msg)"}
{"Repository": "KAIR", "input": "Train with four inputs (L, k, sf, sigma) and with pixel loss for USRNet className ModelPlain4(ModelPlain) Method feed_data Method netG_forward", "label": "class ModelPlain4(ModelPlain):\n    # ----------------------------------------\n    # feed L/H data\n    # ----------------------------------------\n    def feed_data(self, data, need_H=True):\n        self.L = data['L'].to(self.device)  # low-quality image\n        self.k = data['k'].to(self.device)  # blur kernel\n        self.sf = np.int(data['sf'][0,...].squeeze().cpu().numpy()) # scale factor\n        self.sigma = data['sigma'].to(self.device)  # noise level\n        if need_H:\n            self.H = data['H'].to(self.device)  # H\n\n    # ----------------------------------------\n    # feed (L, C) to netG and get E\n    # ----------------------------------------\n    def netG_forward(self):\n        self.E = self.netG(self.L, self.k, self.sf, self.sigma)"}
{"Repository": "cleanlab", "input": "Includes license file into `.egg-info` folder. className egg_info_ex(egg_info) Method run", "label": "class egg_info_ex(egg_info):\n    def run(self):\n        # don't duplicate license into `.egg-info` when building a distribution\n        if not self.distribution.have_run.get(\"install\", True):\n            # `install` command is in progress, copy license\n            self.mkpath(self.egg_info)\n            self.copy_file(\"LICENSE\", self.egg_info)\n\n        egg_info.run(self)"}
{"Repository": "alfred-mstodo-workflow", "input": "Extends Peewee's datetime field with timezone awareness className DateTimeUTCField(DateTimeField) Method python_value Method db_value Method _get_local_datetime_descriptor Method add_to_class", "label": "class DateTimeUTCField(DateTimeField):\n    def python_value(self, value):\n        value = super().python_value(value)\n\n        if isinstance(value, datetime):\n            value = value.replace(tzinfo=tzutc())\n        return value\n\n    def db_value(self, value):\n        if isinstance(value, datetime):\n            value = value.replace(tzinfo=None)\n\n        return super().db_value(value)\n\n    def _get_local_datetime_descriptor(self):\n        return LocalDateTimeDescriptor(self)\n\n    def add_to_class(self, model_class, name):\n        super().add_to_class(model_class, name)\n\n        setattr(model_class, name + '_local', self._get_local_datetime_descriptor())"}
{"Repository": "openseg.pytorch", "input": "Resize the given numpy. className Resize(RandomResize) Method __init__ Method __call__ Attribute target_size Attribute min_side_length Attribute max_side_length Attribute max_side_bound", "label": "class Resize(RandomResize):\n    def __init__(self, target_size=None, min_side_length=None, max_side_length=None, max_side_bound=None):\n        self.target_size = target_size\n        self.min_side_length = min_side_length\n        self.max_side_length = max_side_length\n        self.max_side_bound = max_side_bound\n\n    def __call__(self, img, **kwargs):\n        img, data_dict = super(RandomResize, self).__call__(img, **kwargs)\n\n        height, width, _ = img.shape\n        if self.target_size is not None:\n            target_size = self.target_size\n            w_scale_ratio = self.target_size[0] / width\n            h_scale_ratio = self.target_size[1] / height\n\n        elif self.min_side_length is not None:\n            scale_ratio = self.min_side_length / min(width, height)\n            w_scale_ratio, h_scale_ratio = scale_ratio, scale_ratio\n            target_size = [int(round(width * w_scale_ratio)),\n                           int(round(height * h_scale_ratio))]\n\n        else:\n            scale_ratio = self.max_side_length / max(width, height)\n            w_scale_ratio, h_scale_ratio = scale_ratio, scale_ratio\n            target_size = [int(round(width * w_scale_ratio)),\n                           int(round(height * h_scale_ratio))]\n\n        if self.max_side_bound is not None and max(target_size) > self.max_side_bound:\n            d_ratio = self.max_side_bound / max(target_size)\n            w_scale_ratio = d_ratio * w_scale_ratio\n            h_scale_ratio = d_ratio * h_scale_ratio\n            target_size = [int(round(width * w_scale_ratio)),\n                           int(round(height * h_scale_ratio))]\n\n        target_size = tuple(target_size)\n        return self._process(\n            img, data_dict,\n            False,\n            target_size, h_scale_ratio, w_scale_ratio\n        )"}
{"Repository": "HorNet", "input": "Track a series of values and provide access to smoothed values over a window or the global series average. className SmoothedValue(object) Method __init__ Method update Method synchronize_between_processes Method median Method avg Method global_avg Method max Method value Method __str__ Attribute deque Attribute total Attribute count Attribute fmt", "label": "class SmoothedValue(object):\n    def __init__(self, window_size=20, fmt=None):\n        if fmt is None:\n            fmt = \"{median:.4f} ({global_avg:.4f})\"\n        self.deque = deque(maxlen=window_size)\n        self.total = 0.0\n        self.count = 0\n        self.fmt = fmt\n\n    def update(self, value, n=1):\n        self.deque.append(value)\n        self.count += n\n        self.total += value * n\n\n    def synchronize_between_processes(self):\n        if not is_dist_avail_and_initialized():\n            return\n        t = torch.tensor([self.count, self.total], dtype=torch.float64, device='cuda')\n        dist.barrier()\n        dist.all_reduce(t)\n        t = t.tolist()\n        self.count = int(t[0])\n        self.total = t[1]\n\n    @property\n    def median(self):\n        d = torch.tensor(list(self.deque))\n        return d.median().item()\n\n    @property\n    def avg(self):\n        d = torch.tensor(list(self.deque), dtype=torch.float32)\n        return d.mean().item()\n\n    @property\n    def global_avg(self):\n        return self.total / self.count\n\n    @property\n    def max(self):\n        return max(self.deque)\n\n    @property\n    def value(self):\n        return self.deque[-1]\n\n    def __str__(self):\n        return self.fmt.format(\n            median=self.median,\n            avg=self.avg,\n            global_avg=self.global_avg,\n            max=self.max,\n            value=self.value)"}
{"Repository": "pytorch.cyclic.learning.rate", "input": "Sets the learning rate of each parameter group according to cyclical learning rate policy (CLR). className CyclicLR(object) Method batch_step Method _triangular_scale_fn Method _triangular2_scale_fn Method _exp_range_scale_fn Method get_lr", "label": "class CyclicLR(object):\n    def __init__(self, optimizer, base_lr=1e-3, max_lr=6e-3,\n                 step_size=2000, mode='triangular', gamma=1.,\n                 scale_fn=None, scale_mode='cycle', last_batch_iteration=-1):\n\n        if not isinstance(optimizer, Optimizer):\n            raise TypeError('{} is not an Optimizer'.format(\n                type(optimizer).__name__))\n        self.optimizer = optimizer\n\n        if isinstance(base_lr, list) or isinstance(base_lr, tuple):\n            if len(base_lr) != len(optimizer.param_groups):\n                raise ValueError(\"expected {} base_lr, got {}\".format(\n                    len(optimizer.param_groups), len(base_lr)))\n            self.base_lrs = list(base_lr)\n        else:\n            self.base_lrs = [base_lr] * len(optimizer.param_groups)\n\n        if isinstance(max_lr, list) or isinstance(max_lr, tuple):\n            if len(max_lr) != len(optimizer.param_groups):\n                raise ValueError(\"expected {} max_lr, got {}\".format(\n                    len(optimizer.param_groups), len(max_lr)))\n            self.max_lrs = list(max_lr)\n        else:\n            self.max_lrs = [max_lr] * len(optimizer.param_groups)\n\n        self.step_size = step_size\n\n        if mode not in ['triangular', 'triangular2', 'exp_range'] \\\n                and scale_fn is None:\n            raise ValueError('mode is invalid and scale_fn is None')\n\n        self.mode = mode\n        self.gamma = gamma\n\n        if scale_fn is None:\n            if self.mode == 'triangular':\n                self.scale_fn = self._triangular_scale_fn\n                self.scale_mode = 'cycle'\n            elif self.mode == 'triangular2':\n                self.scale_fn = self._triangular2_scale_fn\n                self.scale_mode = 'cycle'\n            elif self.mode == 'exp_range':\n                self.scale_fn = self._exp_range_scale_fn\n                self.scale_mode = 'iterations'\n        else:\n            self.scale_fn = scale_fn\n            self.scale_mode = scale_mode\n\n        self.batch_step(last_batch_iteration + 1)\n        self.last_batch_iteration = last_batch_iteration\n\n    def batch_step(self, batch_iteration=None):\n        if batch_iteration is None:\n            batch_iteration = self.last_batch_iteration + 1\n        self.last_batch_iteration = batch_iteration\n        for param_group, lr in zip(self.optimizer.param_groups, self.get_lr()):\n            param_group['lr'] = lr\n\n    def _triangular_scale_fn(self, x):\n        return 1.\n\n    def _triangular2_scale_fn(self, x):\n        return 1 / (2. ** (x - 1))\n\n    def _exp_range_scale_fn(self, x):\n        return self.gamma**(x)\n\n    def get_lr(self):\n        step_size = float(self.step_size)\n        cycle = np.floor(1 + self.last_batch_iteration / (2 * step_size))\n        x = np.abs(self.last_batch_iteration / step_size - 2 * cycle + 1)\n\n        lrs = []\n        param_lrs = zip(self.optimizer.param_groups, self.base_lrs, self.max_lrs)\n        for param_group, base_lr, max_lr in param_lrs:\n            base_height = (max_lr - base_lr) * np.maximum(0, (1 - x))\n            if self.scale_mode == 'cycle':\n                lr = base_lr + base_height * self.scale_fn(cycle)\n            else:\n                lr = base_lr + base_height * self.scale_fn(self.last_batch_iteration)\n            lrs.append(lr)\n        return lrs"}
{"Repository": "2d-fluid-simulator", "input": "Maker And Cell method className DyeMacSolver(MacSolver) Method update Method get_fields Method _update_dye", "label": "class DyeMacSolver(MacSolver):\n    def __init__(\n        self,\n        boundary_condition,\n        pressure_updater,\n        advect_function,\n        dt,\n        dx,\n        Re,\n        vorticity_confinement=None,\n    ):\n        super().__init__(\n            boundary_condition,\n            pressure_updater,\n            advect_function,\n            dt,\n            dx,\n            Re,\n            vorticity_confinement,\n        )\n\n        self.dye = DoubleBuffers(self._resolution, 3)  # dye\n\n    def update(self):\n        self._bc.set_velocity_boundary_condition(self.v.current)\n        self._update_velocities(self.v.next, self.v.current, self.p.current)\n        self.v.swap()\n\n        if self.vorticity_confinement is not None:\n            self.vorticity_confinement.apply(self.v)\n            self.v.swap()\n\n        self.pressure_updater.update(self.p, self.v.current)\n\n        limit_field(self.v.current, VELOCITY_LIMIT)\n\n        self._bc.set_dye_boundary_condition(self.dye.current)\n        self._update_dye(self.dye.next, self.dye.current, self.v.current)\n        self.dye.swap()\n        clamp_field(self.dye.current, 0.0, 1.0)\n\n    def get_fields(self):\n        return self.v.current, self.p.current, self.dye.current\n\n    @ti.kernel\n    def _update_dye(self, dn: ti.template(), dc: ti.template(), vc: ti.template()):\n        for i, j in dn:\n            if self.is_fluid_domain(i, j):\n                dn[i, j] = dc[i, j] - self.dt * self._advect(vc, dc, i, j, self.dx)"}
{"Repository": "PRIMAL", "input": "Configuration graph for gridded workspace with 8 connection This graph serves to generate the configuration graph for gridded workspace where each point in the grid has eight neighbors. className Grid_Graph_Conn_8(Grid_Graph) Method __init__ Attribute actions", "label": "class Grid_Graph_Conn_8(Grid_Graph):\n    def __init__(self, world_descriptor, diagonal_cost=False):\n        super(Grid_Graph_Conn_8, self).__init__(world_descriptor,\n                                                diagonal_cost=diagonal_cost)\n        self.actions = CONNECTED_8"}
{"Repository": "undercrawler", "input": "Consider same urls with and without www and using http or https as duplicates. className DupeFilter(SplashAwareDupeFilter) Method request_fingerprint", "label": "class DupeFilter(SplashAwareDupeFilter):\n    def request_fingerprint(self, request):\n        # It's only valid to do this normalization when using splash, which\n        # handles redirects \"inside\" splash. If scrapy sees these redirects,\n        # then http -> https and non-www -> www redirects will be dropped.\n        if 'splash' in request.meta and \\\n                not request.meta.get('_splash_processed'):\n            url = re.sub(r'^https?://(www\\.)?', 'http://', request.url)\n            meta = deepcopy(request.meta)\n            meta['splash'].setdefault('args', {})['url'] = url\n            request = request.replace(url=url, meta=meta)\n        return super().request_fingerprint(request)"}
{"Repository": "py-ipfs-http-client", "input": "Raised when encoding a Python object into a byte string has failed due to some problem with the input data. className EncodingError(EncoderError) Method __init__", "label": "class EncodingError(EncoderError):\n\t__slots__ = (\"original\",)\n\n\toriginal: Exception\n\t\n\tdef __init__(self, encoder_name: str, original: Exception) -> None:\n\t\tself.original = original\n\t\t\n\t\tsuper().__init__(\"Object encoding error: {}\".format(original), encoder_name)"}
{"Repository": "DeepCore", "input": "Computes and stores the average and current value className AverageMeter(object) Method __init__ Method reset Method update Method __str__ Attribute name Attribute fmt", "label": "class AverageMeter(object):\n    def __init__(self, name, fmt=':f'):\n        self.name = name\n        self.fmt = fmt\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count\n\n    def __str__(self):\n        fmtstr = '{name} {val' + self.fmt + '} ({avg' + self.fmt + '})'\n        return fmtstr.format(**self.__dict__)"}
{"Repository": "trading-server", "input": "Base class for strategy models. className Model(ABC) Method __init__ Method get_operating_timeframes Method get_lookback Method get_features Method get_name Method get_instruments Method run Method get_required_timeframes", "label": "class Model(ABC):\n    def __init__(self):\n        super().__init__()\n\n    def get_operating_timeframes(self):\n        return self.operating_timeframes\n\n    def get_lookback(self):\n        return self.lookback\n\n    def get_features(self):\n        return self.features\n\n    def get_name(self):\n        return self.name\n\n    def get_instruments(self):\n        return self.instruments\n\n    @abstractmethod\n    def run(self):\n    @abstractmethod\n    def get_required_timeframes(self, timeframes, result=False):"}
{"Repository": "kupfer", "input": "attribute-based color access, eg: colors.PINK className color_dict(object) Method __getattr__ Method __call__", "label": "class color_dict(object):\n\tdef __getattr__(self, a):\n\t\treturn get_color(a)\n\tdef __call__(self, a):\n\t\treturn get_color(a)"}
{"Repository": "YOWO", "input": "A class to evaluate detections using weighted PASCAL metrics. className WeightedPascalDetectionEvaluator(ObjectDetectionEvaluator) Method __init__", "label": "class WeightedPascalDetectionEvaluator(ObjectDetectionEvaluator):\n    def __init__(self, categories, matching_iou_threshold=0.5):\n        super(WeightedPascalDetectionEvaluator, self).__init__(\n            categories,\n            matching_iou_threshold=matching_iou_threshold,\n            evaluate_corlocs=False,\n            metric_prefix=\"WeightedPascalBoxes\",\n            use_weighted_mean_ap=True,\n        )"}
{"Repository": "django-rest-framework-guardian", "input": "Integration tests for the object level permissions assignment API. className ObjectPermissionsAssignmentIntegrationTests(TestCase) Method setUp Method create_object Method test_read_permissions Method test_change_permissions Method test_delete_permissions", "label": "class ObjectPermissionsAssignmentIntegrationTests(TestCase):\n    def setUp(self):\n        create = User.objects.create_user\n        self.writer = create('writer', password='password')\n        self.reader = create('reader', password='password')\n        self.no_perms = create('no_perms', password='password')\n\n        # Add readers to our reader group\n        reader_group = Group.objects.create(name='readers')\n        reader_group.user_set.add(self.reader)\n\n    def create_object(self):\n        request = factory.post('/')\n        request.user = self.writer\n\n        serializer = BasicSerializer(\n            data={'text': 'test'},\n            context={'request': request},\n        )\n        serializer.is_valid(raise_exception=True)\n        instance = serializer.save()\n\n        return instance\n\n    def test_read_permissions(self):\n        instance = self.create_object()\n\n        # Reader is indirectly assigned via its group permissions\n        self.assertTrue(self.writer.has_perm('view_basicmodel', instance))\n        self.assertTrue(self.reader.has_perm('view_basicmodel', instance))\n        self.assertFalse(self.no_perms.has_perm('view_basicmodel', instance))\n\n    def test_change_permissions(self):\n        instance = self.create_object()\n\n        self.assertTrue(self.writer.has_perm('change_basicmodel', instance))\n        self.assertFalse(self.reader.has_perm('change_basicmodel', instance))\n        self.assertFalse(self.no_perms.has_perm('change_basicmodel', instance))\n\n    def test_delete_permissions(self):\n        instance = self.create_object()\n\n        # No user should be assigned delete permissions\n        self.assertFalse(self.writer.has_perm('delete_basicmodel', instance))\n        self.assertFalse(self.reader.has_perm('delete_basicmodel', instance))\n        self.assertFalse(self.no_perms.has_perm('delete_basicmodel', instance))"}
{"Repository": "quanto", "input": "A standard affine quantizer. className AffineQuantizer(Function) Method backward", "label": "class AffineQuantizer(Function):\n    @staticmethod\n    def forward(\n        ctx, base: torch.Tensor, qtype: qtype, axis: int, group_size: int, scale: torch.Tensor, zeropoint: torch.Tensor\n    ):\n        if qtype not in (qint2, qint4):\n            raise ValueError(\"QBitsTensor can only be of qint2 or qint4 qtype\")\n        if axis not in (0, -1):\n            raise ValueError(\"QBitsTensor axis parameter must be 0 (first axis) or -1 (last axis)\")\n        size = base.size()\n        stride = base.stride()\n        if group_size is not None:\n            base = group(base, axis=axis, group_size=group_size)\n        bits = qtype.bits\n        data = torch.clamp(torch.round(base / scale) + zeropoint, min=0, max=2**bits - 1).to(torch.uint8)\n        packed = PackedTensor.pack(data, bits)\n\n        return QBitsTensor(qtype, axis, size, stride, packed, scale, zeropoint)\n\n    @staticmethod\n    def backward(ctx, gO):\n        # For autograd, quantization is a no-op\n        return gO, None, None, None, None, None"}
{"Repository": "youtube-8m", "input": "Calculate the hinge loss between the predictions and labels. className HingeLoss(BaseLoss) Method calculate_loss", "label": "class HingeLoss(BaseLoss):\n  def calculate_loss(self, predictions, labels, b=1.0, **unused_params):\n    with tf.name_scope(\"loss_hinge\"):\n      float_labels = tf.cast(labels, tf.float32)\n      all_zeros = tf.zeros(tf.shape(float_labels), dtype=tf.float32)\n      all_ones = tf.ones(tf.shape(float_labels), dtype=tf.float32)\n      sign_labels = tf.subtract(tf.scalar_mul(2, float_labels), all_ones)\n      hinge_loss = tf.maximum(\n          all_zeros,\n          tf.scalar_mul(b, all_ones) - sign_labels * predictions)\n      return tf.reduce_mean(tf.reduce_sum(hinge_loss, 1))"}
{"Repository": "ROLO", "input": "The most basic RNN cell. className BasicRNNCell(RNNCell) Method __init__ Method state_size Method output_size Method __call__ Attribute _num_units Attribute _activation", "label": "class BasicRNNCell(RNNCell):\n  def __init__(self, num_units, input_size=None, activation=tanh):\n    if input_size is not None:\n      logging.warn(\"%s: The input_size parameter is deprecated.\", self)\n    self._num_units = num_units\n    self._activation = activation\n\n  @property\n  def state_size(self):\n    return self._num_units\n\n  @property\n  def output_size(self):\n    return self._num_units\n\n  def __call__(self, inputs, state, scope=None):\n    with vs.variable_scope(scope or type(self).__name__):  # \"BasicRNNCell\"\n      output = self._activation(_linear([inputs, state], self._num_units, True))\n    return output, output"}
{"Repository": "gps", "input": "Exception thrown on timeouts. className TimeoutException(Exception) Method __init__", "label": "class TimeoutException(Exception):\n    def __init__(self, sec_waited):\n        Exception.__init__(self, \"Timed out after %f seconds\", sec_waited)"}
{"Repository": "rejected", "input": "Invoke when a message should be rejected and not re-queued, but not due to a processing error that should cause the consumer to stop. className ProcessingException(RejectedException) Method __init__", "label": "class ProcessingException(RejectedException):\n    def __init__(self, *args, **kwargs):\n        super(ProcessingException, self).__init__(*args, **kwargs)"}
{"Repository": "chinese-qa-with-bert", "input": "A single training/test example for the Squad dataset. className SquadExample(object) Method __str__ Method __repr__", "label": "class SquadExample(object):\n    def __init__(self,\n                 qas_id,\n                 question_text,\n                 doc_tokens,\n                 orig_answer_text=None,\n                 start_position=None,\n                 end_position=None,\n                 is_impossible=None):\n        self.qas_id = qas_id\n        self.question_text = question_text\n        self.doc_tokens = doc_tokens\n        self.orig_answer_text = orig_answer_text\n        self.start_position = start_position\n        self.end_position = end_position\n        self.is_impossible = is_impossible\n\n    def __str__(self):\n        return self.__repr__()\n\n    def __repr__(self):\n        s = \"\"\n        s += \"qas_id: %s\" % (self.qas_id)\n        s += \", question_text: %s\" % (self.question_text)\n        s += \", doc_tokens: [%s]\" % (\" \".join(self.doc_tokens))\n        if self.start_position:\n            s += \", start_position: %d\" % (self.start_position)\n        if self.end_position:\n            s += \", end_position: %d\" % (self.end_position)\n        if self.is_impossible:\n            s += \", is_impossible: %r\" % (self.is_impossible)\n        return s"}
{"Repository": "peepdf", "input": "Translates a stream of lzw codepoints into a variable width packed stream of bytes, for use by L{BitUnpacker}. className BitPacker(object) Method __init__ Method pack Attribute _initial_code_size", "label": "class BitPacker(object):\n    def __init__(self, initial_code_size):\n       self._initial_code_size = initial_code_size\n\n\n    def pack(self, codepoints):\n        tailbits = []\n        codesize = self._initial_code_size\n\n        minwidth = 8\n        while (1 << minwidth) < codesize:\n            minwidth = minwidth + 1\n\n        nextwidth = minwidth\n\n        for pt in codepoints:\n\n            newbits = inttobits(pt, nextwidth)\n            tailbits = tailbits + newbits\n\n            # PAY ATTENTION. This calculation should be driven by the\n            # size of the upstream codebook, right now we're just trusting\n            # that everybody intends to follow the TIFF spec.\n            codesize = codesize + 1\n            if pt == END_OF_INFO_CODE:\n               while len(tailbits) % 8:\n                  tailbits.append(0)\n                  \n            if pt in [ CLEAR_CODE, END_OF_INFO_CODE ]:\n                nextwidth = minwidth\n                codesize = self._initial_code_size\n            elif codesize >= (2 ** nextwidth):\n                nextwidth = nextwidth + 1\n\n            while len(tailbits) > 8:\n                nextbits = tailbits[:8]\n                nextbytes = bitstobytes(nextbits)\n                for bt in nextbytes:\n                    yield struct.pack(\"B\", bt)\n\n                tailbits = tailbits[8:]\n\n                       \n        if tailbits:\n            tail = bitstobytes(tailbits)\n            for bt in tail:\n                yield struct.pack(\"B\", bt)"}
{"Repository": "Differentially-Private-Deep-Learning", "input": "Implementation for the loss used in masked language model (MLM) training. className MaskedLmLoss(FairseqCriterion) Method __init__ Method forward Method aggregate_logging_outputs", "label": "class MaskedLmLoss(FairseqCriterion):\n    def __init__(self, args, task):\n        super().__init__(args, task)\n\n    def forward(self, model, sample, reduce=True):\n        # compute MLM loss\n        masked_tokens = sample['target'].ne(self.padding_idx)\n        sample_size = masked_tokens.int().sum().item()\n\n        # (Rare case) When all tokens are masked, the model results in empty\n        # tensor and gives CUDA error.\n        if sample_size == 0:\n            masked_tokens = None\n\n        logits = model(**sample['net_input'], masked_tokens=masked_tokens)[0]\n        targets = model.get_targets(sample, [logits])\n\n        if sample_size != 0:\n            targets = targets[masked_tokens]\n\n        loss = F.nll_loss(\n            F.log_softmax(\n                logits.view(-1, logits.size(-1)),\n                dim=-1,\n                dtype=torch.float32,\n            ),\n            targets.view(-1),\n            reduction='sum',\n            ignore_index=self.padding_idx,\n        )\n        logging_output = {\n            'loss': utils.item(loss.data) if reduce else loss.data,\n            'nll_loss': utils.item(loss.data) if reduce else loss.data,\n            'ntokens': sample['ntokens'],\n            'nsentences': sample['nsentences'],\n            'sample_size': sample_size,\n        }\n        return loss, sample_size, logging_output\n\n    @staticmethod\n    def aggregate_logging_outputs(logging_outputs):\n        loss = sum(log.get('loss', 0) for log in logging_outputs)\n        ntokens = sum(log.get('ntokens', 0) for log in logging_outputs)\n        nsentences = sum(log.get('nsentences', 0) for log in logging_outputs)\n        sample_size = sum(log.get('sample_size', 0) for log in logging_outputs)\n\n        agg_output = {\n            'loss': loss / sample_size / math.log(2),\n            'nll_loss': sum(log.get('nll_loss', 0) for log in logging_outputs) / sample_size / math.log(2) if ntokens > 0 else 0.,\n            'ntokens': ntokens,\n            'nsentences': nsentences,\n            'sample_size': sample_size,\n        }\n        return agg_output"}
{"Repository": "trackintel", "input": "Test helper class TrackintelGeoDataFrame. className TestTrackintelGeoDataFrame Method __init__ Method __init__ Method test_getitem Method test_copy Method test_merge Method test_constructor_fallback_class Method test_constructor_no_fallback_class Method test_constructor_calls_init", "label": "class TestTrackintelGeoDataFrame:\n    class A(TrackintelGeoDataFrame):\n        class AFallback(TrackintelDataFrame):\n            def __init__(self, *args, validate=True, **kwargs):\n                super().__init__(*args, **kwargs)\n\n        fallback_class = AFallback\n\n        def __init__(self, *args, validate=True, validate_geometry=True, **kwargs):\n            super().__init__(*args, **kwargs)\n\n    def test_getitem(self, example_positionfixes):\n        a = self.A(example_positionfixes)\n        b = a.loc[[True for _ in a.columns]]\n        assert type(b) is self.A\n\n    def test_copy(self, example_positionfixes):\n        a = self.A(example_positionfixes)\n        b = a.copy()\n        assert type(b) is self.A\n\n    def test_merge(self, example_positionfixes):\n        a = self.A(example_positionfixes)\n        b = a.merge(a, on=\"user_id\", suffixes=(\"\", \"_other\"))\n        assert type(b) is self.A\n\n    def test_constructor_fallback_class(self, example_positionfixes):\n        a = self.A(example_positionfixes)\n        a = a.drop(columns=a.geometry.name)\n        assert type(a) is self.A.fallback_class\n\n    def test_constructor_no_fallback_class(self, example_positionfixes):\n        a = self.A(example_positionfixes)\n        a.fallback_class = None  # unset it again\n        a = a.drop(columns=a.geometry.name)\n        assert type(a) is pd.DataFrame\n\n    def test_constructor_calls_init(self, example_positionfixes):\n        a = self.A(example_positionfixes)\n        assert type(a._constructor(a)) is self.A"}
{"Repository": "parallel-execute", "input": "Process Runner Wrapper className ProcessRunner(RunnerWrapper) Method start Method is_running", "label": "class ProcessRunner(RunnerWrapper):\n    def start(self):\n        if self.is_running():\n            raise RuntimeError(\"Can't start an already-running runner\")\n        self.runner = multiprocessing.Process(target=self.run)\n        self.runner.daemon = True\n        self.runner.start()\n\n    def is_running(self):\n        return self.runner and self.runner.is_alive()"}
{"Repository": "flatdata", "input": "Fields missing in provided dictionary object className MissingFieldError(RuntimeError) Method __init__", "label": "class MissingFieldError(RuntimeError):\n    def __init__(self, key, name):\n        super().__init__(f'Missing \"{key}\" is required for \"{name}\"')"}
{"Repository": "sublime-phpcs", "input": "Concrete class for PHP_CodeSniffer className Sniffer(ShellCommand) Method execute Method get_executable_args Method parse_report Method get_standards_available", "label": "class Sniffer(ShellCommand):\n    def execute(self, path):\n        if pref.get(\"phpcs_sniffer_run\") != True:\n            return\n\n        args = self.get_executable_args()\n        args.append(\"--report=checkstyle\")\n\n        # Add the additional arguments from the settings file to the command\n        for key, value in pref.get(\"phpcs_additional_args\").items():\n            arg = key\n            if key == \"--runtime-set\":\n                args.append(arg)\n                args.append(value)\n            elif value != \"\":\n                arg += \"=\" + value\n            args.append(arg)\n\n        target = os.path.normpath(path)\n\n        # Set the working directory for the command to the path of the target file, allowing\n        # phpcs the opportunity to find a default configuration file (phpcs.xml) in the file's path.\n        self.setWorkingDir(os.path.dirname(target))\n\n        args.append(target)\n        self.parse_report(args)\n\n    def get_executable_args(self):\n        args = []\n\n        if pref.get(\n            \"phpcs_php_prefix_path\"\n        ) != \"\" and self.__class__.__name__ in pref.get(\"phpcs_commands_to_php_prefix\"):\n            args = [pref.get(\"phpcs_php_prefix_path\")]\n\n        if pref.phpcs_executable_path != \"\":\n            application_path = pref.get(\"phpcs_executable_path\")\n        else:\n            application_path = \"phpcs\"\n\n        if len(args) > 0:\n            args.append(application_path)\n        else:\n            args = [application_path]\n\n        return args\n\n    def parse_report(self, args):\n        report = self.shell_out(args)\n        debug_message(report)\n        lines = re.finditer(\n            '.*line=\"(?P<line>\\d+)\" column=\"(?P<column>\\d+)\" severity=\"(?P<severity>\\w+)\" message=\"(?P<message>.*)\" source',\n            report,\n        )\n\n        for line in lines:\n            error = CheckstyleError(line.group(\"line\"), line.group(\"message\"))\n            self.error_list.append(error)\n\n    def get_standards_available(self):\n        args = self.get_executable_args()\n        args.append(\"-i\")\n\n        output = self.shell_out(args)\n        standards = output[35:].replace(\" and \", \", \").strip().split(\", \")\n        return standards"}
{"Repository": "sublimetext-stringutilities", "input": "Calculate the SHA1 hash of the selected text className ConvertSha1Command(ConvertSelection) Method convert", "label": "class ConvertSha1Command(ConvertSelection):\n    def convert(self, text):\n        return sha1(text).hexdigest()"}
{"Repository": "ablator", "input": "Is the specified user allowed to use the functionality? Returns `{ \"enabled\": true }` if the specified ClientUser is allowed to use the functionality group, ``{ \"enabled\": false }`` if the user is disallowed, or a 404 error if functionality group does not exist. className CanIUseSingleViewV1(APIView) Method get", "label": "class CanIUseSingleViewV1(APIView):\n    def get(self, request, client_user_string, functionality_id):\n        functionality = get_object_or_404(Functionality, id=functionality_id)\n        client_user = ClientUser.user_from_object(client_user_string, organization=functionality.app.organization)\n        return Response({\"enabled\": can_i_use(client_user, functionality)})"}
{"Repository": "mysqltools", "input": "1 --> yes className SlaveSQLRunning(ShowSlave) Method _get_value", "label": "class SlaveSQLRunning(ShowSlave):\n    show_slave_name=\"Slave_SQL_Running\"\n\n    def _get_value(self):\n        value = super()._get_value()\n        if value == 'this node is master':\n            return -1 #  -1 \n        if value.upper() == 'YES':\n            self._value = 1\n            return self._value\n        else:\n            self._value = 0\n            return self._value"}
{"Repository": "deep-image-retrieval", "input": "Rescale the input PIL. className RandomRotation(object) Method __init__ Method __call__ Attribute degrees Attribute interpolation", "label": "class RandomRotation(object):\n    def __init__(self, degrees, interpolation=Image.BILINEAR):\n        self.degrees = degrees\n        self.interpolation = interpolation\n\n    def __call__(self, inp):\n        img = F.grab_img(inp)\n        w, h = img.size\n\n        angle = np.random.uniform(-self.degrees, self.degrees)\n\n        img = img.rotate(angle, resample=self.interpolation)\n        w2, h2 = img.size\n\n        aff = F.aff_translate(-w/2,-h/2)\n        aff = F.aff_mul(aff, F.aff_rotate(-angle * np.pi/180))\n        aff = F.aff_mul(aff, F.aff_translate(w2/2,h2/2))\n        return F.update_img_and_labels(inp, img, aff=aff)"}
{"Repository": "modbus-tk", "input": "Base class to run command className CommandWrapper(object) Method _run_command", "label": "class CommandWrapper(object):\n    def _run_command(self, command_line):\n        try:\n            return subprocess.check_output(command_line.split(' ')), 0\n        except subprocess.CalledProcessError, ex:\n            return ex.output, ex.returncode"}
{"Repository": "glyph", "input": "Factory class for gp mutation functions. className MutateFactory(AFactory) Method _create Method add_options", "label": "class MutateFactory(AFactory):\n    _mapping = get_mapping(gp.all_mutations)\n\n    @staticmethod\n    def _create(args, IndividualClass):\n        args.mutation = args.mutation.lower()\n        mutate = MutateFactory.get_from_mapping(args.mutation)(IndividualClass.pset, **vars(args))\n        static_limit_decorator = deap.gp.staticLimit(\n            key=operator.attrgetter(\"height\"), max_value=args.mutation_max_height\n        )\n        mutate = static_limit_decorator(mutate)\n        return mutate\n\n    @staticmethod\n    def add_options(parser):\n        parser.add_argument(\n            \"--mutation\",\n            dest=\"mutation\",\n            type=str,\n            default=\"mutuniform\",\n            choices=list(MutateFactory._mapping.keys()),\n            help=\"the mutation method (default: mutuniform)\",\n        )\n        parser.add_argument(\n            \"--mutation-max-height\",\n            dest=\"mutation_max_height\",\n            metavar=\"n\",\n            type=utils.argparse.positive_int,\n            default=20,\n            help=\"limit for the expression tree height as a result of mutation (default: 20)\",\n        )\n        parser.add_argument(\n            \"--mutate_tree_min\",\n            dest=\"mutate_tree_min\",\n            default=0,\n            metavar=\"min_\",\n            type=utils.argparse.positive_int,\n            help=\"minimum value for tree based mutation methods (default: 0)\",\n        )\n        parser.add_argument(\n            \"--mutate_tree_max\",\n            dest=\"mutate_tree_max\",\n            default=2,\n            metavar=\"max_\",\n            type=utils.argparse.positive_int,\n            help=\"maximum value for tree based mutation methods (default: 2)\",\n        )"}
{"Repository": "CLCRN", "input": "IRDataset dataset. className Dataset(Dataset) Method __init__ Method __len__ Method __getitem__ Attribute file Attribute x Attribute y Attribute include_context Attribute context Attribute scaler", "label": "class Dataset(Dataset):\n    def __init__(self, dataset_dir, mode, batch_size, scaler=None, include_context=False):\n        self.file = dataset_dir\n        print('loading data of {} set...'.format(mode))\n        with open(dataset_dir + '/{}.pkl'.format(mode), 'rb') as f:\n            data = pickle.load(f)\n        self.x = data['x']\n        self.y = data['y']\n        self.include_context = include_context\n\n        if include_context == True:\n            self.context = data['context']\n\n        feature_len = data['x'].shape[-1]\n\n        if scaler != None:\n            self.scaler = scaler\n            for i in range(feature_len):\n                self.x[..., i] = scaler[i].transform(self.x[..., i])\n                self.y[..., i] = scaler[i].transform(self.y[..., i])\n    \n    def __len__(self):\n        return self.x.shape[0]\n\n    def __getitem__(self, idx):\n        data = self.x[idx]\n        labels = self.y[idx]\n\n        if self.include_context == True:\n            data_context = self.context[idx]\n            data = np.concatenate([data, data_context], axis=-1)\n\n        return data, labels"}
{"Repository": "Improved-Body-Parts", "input": "Computes and stores the average and current value className AverageMeter(object) Method __init__ Method update Attribute val Attribute avg Attribute sum Attribute count", "label": "class AverageMeter(object):\n    def __init__(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count"}
{"Repository": "rpforest", "input": "Compile the extension .pyx files. className Cythonize(Command) Method initialize_options Method finalize_options Method run", "label": "class Cythonize(Command):\n    user_options = []\n\n    def initialize_options(self):\n        pass\n\n    def finalize_options(self):\n        pass\n\n    def run(self):\n        if not CYTHON_AVAILABLE:\n            raise CythonNotInstalled\n        if not NUMPY_AVAILABLE:\n            raise NumpyNotInstalled\n        extensions = define_extensions(\"pyx\")\n        # language_level sets it py2 compatible\n        cythonize(extensions, compiler_directives={\"language_level\": \"2\"})"}
{"Repository": "truvari", "input": "Holds all relevant information for a set of collapsed calls className CollapsedCalls() Method calc_median_sizepos Method annotate_entry Method make_genotype_mask Method gt_conflict Method combine", "label": "class CollapsedCalls():\n    entry: None\n    match_id: str\n    matches: list = field(default_factory=list)\n    gt_consolidate_count: int = 0\n    genotype_mask: str = None  # not actually a str\n\n    def calc_median_sizepos(self):\n        st, en = truvari.entry_boundaries(self.entry)\n        sz = truvari.entry_size(self.entry)\n        starts = [st]\n        ends = [en]\n        sizes = [sz]\n        for i in self.matches:\n            st, en = truvari.entry_boundaries(i.comp)\n            sz = truvari.entry_size(i.comp)\n            starts.append(st)\n            ends.append(en)\n            sizes.append(sz)\n        return int(statistics.median(starts)), int(statistics.median(ends)), int(statistics.median(sizes))\n\n    def annotate_entry(self, header, med_info):\n        self.entry.translate(header)\n        self.entry.info[\"NumCollapsed\"] = len(self.matches)\n        self.entry.info[\"NumConsolidated\"] = self.gt_consolidate_count\n        self.entry.info[\"CollapseId\"] = self.match_id\n        if med_info:\n            self.entry.info[\"CollapseStart\"], self.entry.info[\"CollapseEnd\"], self.entry.info[\"CollapseSize\"] = self.calc_median_sizepos()\n\n    @staticmethod\n    def make_genotype_mask(entry, gtmode):\n        if gtmode == 'off':\n            return None\n        to_mask = (lambda x: 1 in x) if gtmode == 'all' else (\n            lambda x: x.count(1) == 1)\n        return np.array([to_mask(_.allele_indices) for _ in entry.samples.values()], dtype=bool)\n\n    def gt_conflict(self, other, which_gt):\n        if which_gt == 'off':\n            return False\n\n        o_mask = self.make_genotype_mask(other, which_gt)\n        if (self.genotype_mask & o_mask).any():\n            return True\n        self.genotype_mask |= o_mask\n        return False\n\n    def combine(self, other):\n        self.matches.extend(other.matches)\n        self.gt_consolidate_count += other.gt_consolidate_count\n        if self.genotype_mask is not None and other.genotype_mask is not None:\n            self.genotype_mask |= other.genotype_mask"}
{"Repository": "folders2flickr", "input": "Flickr Group Pool className Group(object) Method _general_getattr Method _load_properties Method __str__ Method getPhotos Method add Method remove", "label": "class Group(object):\n    def __init__(self, id, name=None, members=None, online=None,\\\n                 privacy=None, chatid=None, chatcount=None):\n        self.__loaded = False\n        self.__id = id\n        self.__name = name\n\n        self.__members = members\n        self.__online = online\n        self.__privacy = privacy\n        self.__chatid = chatid\n        self.__chatcount = chatcount\n        self.__url = None\n\n    id = property(lambda self: self._general_getattr('id'))\n    name = property(lambda self: self._general_getattr('name'))\n    members = property(lambda self: self._general_getattr('members'))\n    online = property(lambda self: self._general_getattr('online'))\n    privacy = property(lambda self: self._general_getattr('privacy'))\n    chatid = property(lambda self: self._general_getattr('chatid'))\n    chatcount = property(lambda self: self._general_getattr('chatcount'))\n\n    def _general_getattr(self, var):\n        if getattr(self, \"_%s__%s\" % (self.__class__.__name__, var)) is None \\\n           and not self.__loaded:\n            self._load_properties()\n        return getattr(self, \"_%s__%s\" % (self.__class__.__name__, var))\n\n    def _load_properties(self):\n        method = 'flickr.groups.getInfo'\n        data = _doget(method, group_id=self.id)\n\n        self.__loaded = True\n\n        group = data.rsp.group\n\n        self.__name = group.name.text\n        self.__description = group.description.text\n        self.__members = group.members.text\n        self.__privacy = group.privacy.text\n\n\n    def __str__(self):\n        return '<Flickr Group %s>' % self.id\n\n    def getPhotos(self, tags='', per_page='', page=''):\n        method = 'flickr.groups.pools.getPhotos'\n        data = _doget(method, group_id=self.id, tags=tags,\\\n                      per_page=per_page, page=page)\n        photos = []\n        for photo in data.rsp.photos.photo:\n            photos.append(_parse_photo(photo))\n        return photos\n\n    def add(self, photo):\n        method = 'flickr.groups.pools.add'\n        _dopost(method, auth=True, photo_id=photo.id, group_id=self.id)\n        return True\n\n    def remove(self, photo):\n        method = 'flickr.groups.pools.remove'\n        _dopost(method, auth=True, photo_id=photo.id, group_id=self.id)\n        return True"}
{"Repository": "theseus.dht", "input": "Interface for plugins that add support for new RPCs. className IKRPC(Interface) Method query_handler Method response_handler", "label": "class IKRPC(Interface):\n    # TODO will zope.interface.Interface let implementers leave these methods\n    # as None? if so, should document that as an option and add logic in\n    # KRPCProtocol to handle it.\n\n    name = Attribute(\n            \"The name of the RPC that this plugin adds support for. \"\n            \"KRPCProtocol enforces a 32-byte limit on KRPC names. \"\n            \"This limit may be changed through `KRPCProtocol.max_name_size`. \"\n            \"Type: bytes\"\n            )\n\n    def query_handler(args):\n    def response_handler(args):"}
{"Repository": "PTable", "input": "Some very basic tests. className BasicTests(CityDataTest) Method testNoBlankLines Method testAllLengthsEqual", "label": "class BasicTests(CityDataTest):\n    def testNoBlankLines(self):\n        string = self.x.get_string()\n        lines = string.split(\"\\n\")\n        self.assertTrue(\"\" not in lines)\n\n    def testAllLengthsEqual(self):\n        string = self.x.get_string()\n        lines = string.split(\"\\n\")\n        lengths = [len(line) for line in lines]\n        lengths = set(lengths)\n        self.assertEqual(len(lengths), 1)"}
{"Repository": "simplekv", "input": "Base class for store decorators. className StoreDecorator(object) Method __init__ Method __getattr__ Method __contains__ Method __iter__ Attribute _dstore", "label": "class StoreDecorator(object):\n    def __init__(self, store):\n        self._dstore = store\n\n    def __getattr__(self, attr):\n        store = object.__getattribute__(self, \"_dstore\")\n        return getattr(store, attr)\n\n    def __contains__(self, *args, **kwargs):\n        return self._dstore.__contains__(*args, **kwargs)\n\n    def __iter__(self, *args, **kwargs):\n        return self._dstore.__iter__(*args, **kwargs)"}
{"Repository": "CLUEPretrainedModels", "input": "A single training instance (sentence pair). className TrainingInstance(object) Method __str__ Method __repr__", "label": "class TrainingInstance(object):\n  def __init__(self, tokens, segment_ids, masked_lm_positions, masked_lm_labels,\n               is_random_next):\n    self.tokens = tokens\n    self.segment_ids = segment_ids\n    self.is_random_next = is_random_next\n    self.masked_lm_positions = masked_lm_positions\n    self.masked_lm_labels = masked_lm_labels\n\n  def __str__(self):\n    s = \"\"\n    s += \"tokens: %s\\n\" % (\" \".join(\n        [tokenization.printable_text(x) for x in self.tokens]))\n    s += \"segment_ids: %s\\n\" % (\" \".join([str(x) for x in self.segment_ids]))\n    s += \"is_random_next: %s\\n\" % self.is_random_next\n    s += \"masked_lm_positions: %s\\n\" % (\" \".join(\n        [str(x) for x in self.masked_lm_positions]))\n    s += \"masked_lm_labels: %s\\n\" % (\" \".join(\n        [tokenization.printable_text(x) for x in self.masked_lm_labels]))\n    s += \"\\n\"\n    return s\n\n  def __repr__(self):\n    return self.__str__()"}
{"Repository": "SublimeHaskell", "input": "Location in file at line className Location(object) Method __init__ Method __str__ Method to_string Method is_null Method get_id Method project_path Method as_tuple Method __eq__ Method __hash__ Method __lt__ Attribute project Attribute filename", "label": "class Location(object):\n    def __init__(self, filename, project=None):\n        self.project = project\n        self.filename = filename\n\n    def __str__(self):\n        return self.to_string()\n\n    def to_string(self):\n        return self.filename\n\n    def is_null(self):\n        return self.project is None and self.filename is None\n\n    def get_id(self):\n        return self.filename\n\n    def project_path(self):\n        return os.path.dirname(self.project) if self.project is not None else None\n\n    def as_tuple(self):\n        return (self.filename,)\n\n    def __eq__(self, other):\n        if isinstance(other, Location):\n            return self.as_tuple() == other.as_tuple()\n        return False\n\n    def __hash__(self):\n        return hash(self.as_tuple())\n\n    def __lt__(self, other):\n        return lt_impl(self, other, Location)"}
{"Repository": "simclr-converter", "input": "Computes and stores the average and current value className AverageMeter(object) Method __init__ Method reset Method update Method __str__ Attribute name Attribute fmt", "label": "class AverageMeter(object):\n    def __init__(self, name, fmt=':f'):\n        self.name = name\n        self.fmt = fmt\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count\n\n    def __str__(self):\n        fmtstr = '{name} {val' + self.fmt + '} ({avg' + self.fmt + '})'\n        return fmtstr.format(**self.__dict__)"}
{"Repository": "AFRCNN-For-Speech-Separation", "input": "Global Layer Normalization (globLN). className GlobLN(_LayerNorm) Method forward", "label": "class GlobLN(_LayerNorm):\n    def forward(self, x):\n        dims = list(range(1, len(x.shape)))\n        mean = x.mean(dim=dims, keepdim=True)\n        var = torch.pow(x - mean, 2).mean(dim=dims, keepdim=True)\n        return self.apply_gain_and_bias((x - mean) / (var + 1e-8).sqrt())"}
{"Repository": "pygame-examples", "input": "A simple class to keep track of vectors, including initializing from Cartesian and polar forms. className V(object) Method __init__ Method magnitude Method angle Method __add__ Method rotate Method __str__ Attribute x Attribute y Attribute x Attribute y", "label": "class V(object):\n    def __init__(self, x=0, y=0, angle=None, magnitude=None):\n        self.x = x\n        self.y = y\n\n        if (angle is not None and magnitude is not None):\n            self.x = magnitude * math.sin(math.radians(angle))\n            self.y = magnitude * math.cos(math.radians(angle))\n\n    @property\n    def magnitude(self):\n        return math.sqrt(self.x ** 2 + self.y ** 2)\n    \n    @property\n    def angle(self):\n        if self.y == 0:\n            if self.x > 0:\n                return 90.0\n            else:\n                return 270.0\n        if math.floor(self.x) == 0:\n            if self.y < 0:\n                return 180.0\n        return math.degrees(math.atan(self.x / float(self.y)))\n\n    def __add__(self, other):\n        return V(x=(self.x + other.x), y=(self.y + other.y))\n\n    def rotate(self, angle):\n        c = math.cos(math.radians(angle))\n        s = math.sin(math.radians(angle))\n        self.x = self.x * c - self.y * s\n        self.y = self.x * s + self.y * c\n\n    def __str__(self):\n        return \"X: %.3d Y: %.3d Angle: %.3d degrees Magnitude: %.3d\" % (self.x, self.y, self.angle, self.magnitude)"}
{"Repository": "few-shot-meta-baseline", "input": "A Relation Network. className RelationNetworkLearner(Learner) Method forward_pass Method compute_relations Method compute_loss Method compute_accuracy", "label": "class RelationNetworkLearner(Learner):\n  def __init__(self, is_training, transductive_batch_norm,\n               backprop_through_moments, ema_object, embedding_fn, reader,\n               weight_decay):\n    super(RelationNetworkLearner,\n          self).__init__(is_training, transductive_batch_norm,\n                         backprop_through_moments, ema_object, embedding_fn,\n                         reader)\n\n    # The data for the next episode.\n    self.episode = self.data\n    self.test_targets = self.episode.test_labels\n    self.way = compute_way(self.data)\n\n    # Hyperparameters.\n    self.weight_decay = weight_decay\n    tf.logging.info(\n        'RelationNetworkLearner: weight_decay {}'.format(weight_decay))\n\n    # Parameters for embedding function depending on meta-training or not.\n    self.forward_pass()\n\n  def forward_pass(self):\n    # Compute the support set's mean and var and use these as the moments for\n    # batch norm on the query set.\n    train_embeddings = self.embedding_fn(\n        self.episode.train_images, self.is_training, keep_spatial_dims=True)\n    self.train_embeddings = train_embeddings['embeddings']\n    support_set_moments = None\n    if not self.transductive_batch_norm:\n      support_set_moments = train_embeddings['moments']\n    test_embeddings = self.embedding_fn(\n        self.episode.test_images,\n        self.is_training,\n        moments=support_set_moments,\n        backprop_through_moments=self.backprop_through_moments,\n        keep_spatial_dims=True)\n    self.test_embeddings = test_embeddings['embeddings']\n\n  def compute_relations(self):\n    # [n_test, 21, 21, n_features].\n    test_embed_shape = self.test_embeddings.shape.as_list()\n    n_feature = test_embed_shape[3]\n    out_shape = test_embed_shape[1:3]\n    n_test = tf.shape(self.test_embeddings)[0]\n\n    # [n_test, num_clases, 21, 21, n_feature].\n    # It is okay one of the elements in the list to be tensor.\n    prototype_extended = tf.tile(\n        tf.expand_dims(self.prototypes, 0), [n_test, 1, 1, 1, 1])\n    # [num_clases, n_test, 21, 21, n_feature].\n    test_f_extended = tf.tile(\n        tf.expand_dims(self.test_embeddings, 1), [1, self.way, 1, 1, 1])\n    relation_pairs = tf.concat((prototype_extended, test_f_extended), 4)\n    # relation_pairs.shape.as_list()[-3:] == [-1] + out_shape + [n_feature*2]\n    relation_pairs = tf.reshape(relation_pairs,\n                                [-1] + out_shape + [n_feature * 2])\n    self.relationnet_dict = _relation_net(relation_pairs, 'relationnet')\n    relations = tf.reshape(self.relationnet_dict['output'], [-1, self.way])\n    return relations\n\n  def compute_loss(self):\n    onehot_train_labels = tf.one_hot(self.episode.train_labels, self.way)\n    self.prototypes = compute_prototypes(self.train_embeddings,\n                                         onehot_train_labels)\n    self.test_logits = self.compute_relations()\n    onehot_test_labels = tf.one_hot(self.episode.test_labels, self.way)\n    mse_loss = tf.losses.mean_squared_error(onehot_test_labels,\n                                            self.test_logits)\n    regularization = tf.reduce_sum(\n        tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES))\n    loss = mse_loss + self.weight_decay * regularization\n    return loss\n\n  def compute_accuracy(self):\n    self.test_predictions = tf.cast(tf.argmax(self.test_logits, 1), tf.int32)\n    correct = tf.equal(self.episode.test_labels, self.test_predictions)\n    return tf.reduce_mean(tf.cast(correct, tf.float32))"}
{"Repository": "prefect-dataplatform", "input": "Interact with a Snowflake schema using Pandas. className SnowflakePandas(Block) Method _get_connection_string Method read_sql Method load_raw_data", "label": "class SnowflakePandas(Block):\n\n    \"\"\"\n    Interact with a Snowflake schema using Pandas.\n    Requires pandas and snowflake-sqlalchemy packages to be installed.\n\n    Args:\n        snowflake_connector (SnowflakeConnector): Schema and credentials for a Snowflake schema.\n\n    Example:\n        Load stored block:\n        ```python\n        from dataplatform.blocks import SnowflakePandas\n        block = SnowflakePandas.load(\"BLOCK_NAME\")\n        ```\n    \"\"\"  # noqa\n\n    _block_type_name = \"Snowflake Pandas\"\n    _logo_url = \"https://images.ctfassets.net/gm98wzqotmnx/2DxzAeTM9eHLDcRQx1FR34/f858a501cdff918d398b39365ec2150f/snowflake.png?h=250\"  # noqa\n    _block_schema_capabilities = [\"load_raw_data\", \"read_sql\"]\n    snowflake_connector: SnowflakeConnector\n\n    def _get_connection_string(self) -> str:\n        acc_id = self.snowflake_connector.credentials.account\n        usr = self.snowflake_connector.credentials.user\n        role = self.snowflake_connector.credentials.role or \"SYSADMIN\"\n        pwd = self.snowflake_connector.credentials.password.get_secret_value()\n        db = self.snowflake_connector.database\n        schema = self.snowflake_connector.schema_\n        warehouse = self.snowflake_connector.warehouse\n        return f\"snowflake://{usr}:{pwd}@{acc_id}/{db}/{schema}?warehouse={warehouse}&role={role}\"\n\n    def read_sql(self, table_or_query: str) -> pd.DataFrame:\n        db = self._get_connection_string()\n        engine = create_engine(db)\n        return pd.read_sql(table_or_query, engine)\n\n    def load_raw_data(self, dataframe: pd.DataFrame, table_name: str) -> None:\n        conn_string = self._get_connection_string()\n        db_engine = create_engine(conn_string)\n        dataframe.to_sql(\n            table_name,\n            schema=self.snowflake_connector.schema_,\n            con=db_engine,\n            if_exists=\"replace\",\n            index=False,\n        )"}
{"Repository": "NMT_GAN", "input": "Dispatches to experts according to gates. className DistributedSingleDispatcher(object) Method __init__ Method _PartSizes Method _StitchIndices Method Dispatch Method Combine Attribute _gates Attribute _data_parallelism Attribute _model_parallelism", "label": "class DistributedSingleDispatcher(object):\n  def __init__(self, data_parallelism, model_parallelism, gates):\n    gates = data_parallelism(tf.to_int32, gates)\n    self._gates = gates\n    self._data_parallelism = data_parallelism\n    self._model_parallelism = model_parallelism\n\n    # Compute the sizes number of examples going from each datashard to each\n    # expert.\n    def _PartSizes(gates):\n      return tf.unsorted_segment_sum(\n          tf.ones_like(gates), gates, model_parallelism.n)\n\n    part_sizes_by_datashard = data_parallelism(_PartSizes, gates)\n    self._part_sizes_by_expert = tf.unstack(\n        tf.stack(part_sizes_by_datashard), num=model_parallelism.n, axis=1)\n\n    # These indices will be used to combine the output on the datashards.\n    def _StitchIndices(gates):\n      return tf.dynamic_partition(\n          tf.range(tf.size(gates)), gates, model_parallelism.n)\n\n    self._stitch_indices = data_parallelism(_StitchIndices, gates)\n\n  def Dispatch(self, d_tensors):\n    parts = self._data_parallelism(tf.dynamic_partition, d_tensors, self._gates,\n                                   self._model_parallelism.n)\n    parts_by_expert = TransposeListOfLists(parts)\n    x_tensors = self._model_parallelism(tf.concat, parts_by_expert, 0)\n    return x_tensors\n\n  def Combine(self, x_tensors):\n    parts = self._model_parallelism(tf.split, x_tensors,\n                                    self._part_sizes_by_expert)\n    d_tensors = self._data_parallelism(tf.dynamic_stitch, self._stitch_indices,\n                                       TransposeListOfLists(parts))\n    return d_tensors"}
{"Repository": "pytorch-lr-finder", "input": "This class does not inherit from `DataLoaderIter`, should be used to trigger exceptions related to type checking. className CustomLoaderIter(object) Method __init__ Method __iter__ Attribute loader", "label": "class CustomLoaderIter(object):\n    def __init__(self, loader):\n        self.loader = loader\n\n    def __iter__(self):\n        return iter(self.loader)"}
{"Repository": "LEAStereo", "input": "Learning Rate Scheduler Step mode: ``lr = baselr * 0. className LR_Scheduler(object) Method __call__ Method _adjust_learning_rate", "label": "class LR_Scheduler(object):\n    def __init__(self, mode, base_lr, num_epochs, iters_per_epoch=0,\n                 lr_step=0, warmup_epochs=0, min_lr=None):\n        self.mode = mode\n        print('Using {} LR Scheduler!'.format(self.mode))\n        self.lr = base_lr\n        if mode == 'step':\n            assert lr_step\n        self.lr_step = lr_step\n        self.iters_per_epoch = iters_per_epoch\n        self.N = num_epochs * iters_per_epoch\n        self.epoch = -1\n        self.warmup_iters = warmup_epochs * iters_per_epoch\n        self.min_lr = min_lr\n\n    def __call__(self, optimizer, i, epoch, best_pred):\n        T = epoch * self.iters_per_epoch + i\n        if self.mode == 'cos':\n            lr = 0.5 * self.lr * (1 + math.cos(1.0 * T / self.N * math.pi))\n        elif self.mode == 'poly':\n            lr = self.lr * pow((1 - 1.0 * T / self.N), 0.9)\n        elif self.mode == 'step':\n            lr = self.lr * (0.1 ** (epoch // self.lr_step))\n        else:\n            raise NotImplemented\n        # warm up lr schedule\n        if self.min_lr is not None:\n            if lr < self.min_lr:\n                lr = self.min_lr\n        if self.warmup_iters > 0 and T < self.warmup_iters:\n            lr = lr * 1.0 * T / self.warmup_iters\n        if epoch > self.epoch:\n            print('\\n=>Epoches %i, learning rate = %.4f, \\\n                previous best = %.4f' % (epoch, lr, best_pred))\n            self.epoch = epoch\n        assert lr >= 0\n        self._adjust_learning_rate(optimizer, lr)\n\n    def _adjust_learning_rate(self, optimizer, lr):\n        if len(optimizer.param_groups) == 1:\n            optimizer.param_groups[0]['lr'] = lr\n        else:\n            # enlarge the lr at the head\n            optimizer.param_groups[0]['lr'] = lr\n            for i in range(1, len(optimizer.param_groups)):\n                optimizer.param_groups[i]['lr'] = lr * 10"}
{"Repository": "libsass-python", "input": "The exception type that is raised by :func:`compile()`. className CompileError(ValueError) Method __init__", "label": "class CompileError(ValueError):\n    def __init__(self, msg):\n        super().__init__(to_native_s(msg))"}
{"Repository": "BERT-for-Chinese-Question-Answering", "input": "A single training/test example for simple sequence classification. className SquadExample(object) Method __str__ Method __repr__", "label": "class SquadExample(object):\n    def __init__(self,\n                 qas_id,\n                 question_text,\n                 doc_tokens,\n                 orig_answer_text=None,\n                 start_position=None,\n                 end_position=None):\n        self.qas_id = qas_id\n        self.question_text = question_text\n        self.doc_tokens = doc_tokens\n        self.orig_answer_text = orig_answer_text\n        self.start_position = start_position\n        self.end_position = end_position\n\n    def __str__(self):\n        return self.__repr__()\n\n    def __repr__(self):\n        s = \"\"\n        s += \"qas_id: %s\" % (tokenization.printable_text(self.qas_id))\n        s += \", question_text: %s\" % (\n            tokenization.printable_text(self.question_text))\n        s += \", doc_tokens: [%s]\" % (\" \".join(self.doc_tokens))\n        if self.start_position:\n            s += \", start_position: %d\" % (self.start_position)\n        if self.start_position:\n            s += \", end_position: %d\" % (self.end_position)\n        return s"}
{"Repository": "pytorch-noise2void", "input": "Convert ndarrays in sample to Tensors. className ToTensor(object) Method __call__", "label": "class ToTensor(object):\n    def __call__(self, data):\n        # Swap color axis because numpy image: H x W x C\n        #                         torch image: C x H x W\n\n        # for key, value in data:\n        #     data[key] = torch.from_numpy(value.transpose((2, 0, 1)))\n        #\n        # return data\n\n        input, label, mask = data['input'], data['label'], data['mask']\n\n        input = input.transpose((2, 0, 1)).astype(np.float32)\n        label = label.transpose((2, 0, 1)).astype(np.float32)\n        mask = mask.transpose((2, 0, 1)).astype(np.float32)\n        return {'input': torch.from_numpy(input), 'label': torch.from_numpy(label), 'mask': torch.from_numpy(mask)}"}
{"Repository": "micropython-ili9341", "input": "Bouncing Sprite. className BouncingSprite(object) Method update_pos Method draw", "label": "class BouncingSprite(object):\n    def __init__(self, path, w, h, screen_width, screen_height,\n                 speed, display):\n        self.buf = display.load_sprite(path, w, h)\n        self.w = w\n        self.h = h\n        self.screen_width = screen_width\n        self.screen_height = screen_height\n        self.display = display\n        self.x_speed = speed\n        self.y_speed = speed\n        self.x = self.screen_width // 2\n        self.y = self.screen_height // 2\n        self.prev_x = self.x\n        self.prev_y = self.y\n\n    def update_pos(self):\n        x = self.x\n        y = self.y\n        w = self.w\n        h = self.h\n        x_speed = abs(self.x_speed)\n        y_speed = abs(self.y_speed)\n\n        if x + w + x_speed >= self.screen_width:\n            self.x_speed = -x_speed\n        elif x - x_speed < 0:\n            self.x_speed = x_speed\n\n        if y + h + y_speed >= self.screen_height:\n            self.y_speed = -y_speed\n        elif y - y_speed <= 0:\n            self.y_speed = y_speed\n\n        self.prev_x = x\n        self.prev_y = y\n\n        self.x = x + self.x_speed\n        self.y = y + self.y_speed\n\n    def draw(self):\n        x = self.x\n        y = self.y\n        prev_x = self.prev_x\n        prev_y = self.prev_y\n        w = self.w\n        h = self.h\n        x_speed = abs(self.x_speed)\n        y_speed = abs(self.y_speed)\n\n        # Determine direction and remove previous portion of sprite\n        if prev_x > x:\n            # Left\n            self.display.fill_vrect(x + w, prev_y, x_speed, h, 0)\n        elif prev_x < x:\n            # Right\n            self.display.fill_vrect(x - x_speed, prev_y, x_speed, h, 0)\n        if prev_y > y:\n            # Upward\n            self.display.fill_vrect(prev_x, y + h, w, y_speed, 0)\n        elif prev_y < y:\n            # Downward\n            self.display.fill_vrect(prev_x, y - y_speed, w, y_speed, 0)\n\n        self.display.draw_sprite(self.buf, x, y, w, h)"}
{"Repository": "SimMatch", "input": "Computes and stores the average and current value className AverageMeter(object) Method __init__ Method reset Method update Method __str__ Attribute name Attribute fmt", "label": "class AverageMeter(object):\n    def __init__(self, name, fmt=':f'):\n        self.name = name\n        self.fmt = fmt\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count\n\n    def __str__(self):\n        fmtstr = '{name} {val' + self.fmt + '} ({avg' + self.fmt + '})'\n        return fmtstr.format(**self.__dict__)"}
{"Repository": "VIGOR", "input": "Creates a Soft Deep Bag-of-Features class. className SoftDBoW(PoolingBaseModel) Method forward", "label": "class SoftDBoW(PoolingBaseModel):\n    def __init__(self, feature_size, max_samples, cluster_size, output_dim,\n            gating=True,add_batch_norm=True, is_training=True):\n        super(self.__class__, self).__init__(\n            feature_size=feature_size,\n            max_samples=max_samples,\n            cluster_size=cluster_size,\n            output_dim=output_dim,\n            gating=gating,\n            add_batch_norm=add_batch_norm,\n            is_training=is_training)\n\n    def forward(self, reshaped_input):\n        cluster_weights = tf.get_variable(\"cluster_weights\",\n          [self.feature_size, self.cluster_size],\n          initializer = tf.random_normal_initializer(\n              stddev=1 / math.sqrt(self.feature_size)))\n        \n        activation = tf.matmul(reshaped_input, cluster_weights)\n        \n        if self.add_batch_norm:\n          activation = slim.batch_norm(\n              activation,\n              center=True,\n              scale=True,\n              is_training=self.is_training,\n              scope=\"cluster_bn\")\n        else:\n          cluster_biases = tf.get_variable(\"cluster_biases\",\n            [self.cluster_size],\n            initializer = tf.random_normal_initializer(\n                stddev=1 / math.sqrt(self.feature_size)))\n          activation += cluster_biases\n\n        activation = tf.nn.softmax(activation)\n\n        activation = tf.reshape(activation,\n                [-1, self.max_samples, self.cluster_size])\n\n        bof = tf.reduce_sum(activation,1)\n        bof = tf.nn.l2_normalize(bof,1)\n\n        hidden1_weights = tf.get_variable(\"hidden1_weights\",\n          [self.cluster_size, self.output_dim],\n          initializer=tf.random_normal_initializer(\n              stddev=1 / math.sqrt(self.cluster_size)))\n           \n        bof = tf.matmul(bof, hidden1_weights)\n\n        if self.gating:\n            bof = super(self.__class__, self).context_gating(bof)\n      \n        return bof"}
{"Repository": "Deep-SfM-Revisited", "input": "Converts a numpy.ndarray (H x W x C) to a torch.FloatTensor of shape (C x H x W). className ArrayToTensor(object) Method __call__", "label": "class ArrayToTensor(object):\n    def __call__(self, array):\n        assert(isinstance(array, np.ndarray))\n        array = np.transpose(array, (2, 0, 1))\n        # handle numpy array\n        tensor = torch.from_numpy(array)\n        # put it from HWC to CHW format\n        return tensor.float()"}
{"Repository": "reprozip", "input": "Gets a remote port from paramiko and forwards to the given connector. className SSHForwarder(BaseForwarder) Method __init__ Method __init__ Method sendall Method recv Method close Method _new_connection", "label": "class SSHForwarder(BaseForwarder):\n    def __init__(self, ssh_transport, remote_port, connector):\n        BaseForwarder.__init__(self, connector)\n        ssh_transport.request_port_forward('', remote_port,\n                                           self._new_connection)\n\n    class _ChannelWrapper(object):\n        def __init__(self, channel):\n            self.channel = channel\n\n        def sendall(self, data):\n            return self.channel.send(data)\n\n        def recv(self, data):\n            return self.channel.recv(data)\n\n        def close(self):\n            self.channel.close()\n\n    def _new_connection(self, channel, src_addr, dest_addr):\n        # Wraps the channel as a socket-like object that _forward() can use\n        socklike = self._ChannelWrapper(channel)\n        t = threading.Thread(target=self._forward,\n                             args=(socklike, src_addr))\n        t.setDaemon(True)\n        t.start()"}
{"Repository": "dbt-synapse", "input": "Custom command to verify that the git tag matches our version className VerifyVersionCommand(install) Method run", "label": "class VerifyVersionCommand(install):\n    description = \"Verify that the git tag matches our version\"\n\n    def run(self):\n        tag = os.getenv(\"GITHUB_REF_NAME\")\n        tag_without_prefix = tag[1:]\n\n        if tag_without_prefix != package_version:\n            info = \"Git tag: {0} does not match the version of this app: {1}\".format(\n                tag_without_prefix, package_version\n            )\n            sys.exit(info)"}
{"Repository": "circus", "input": "Provides information about a failed :func:`import_string` attempt. className ImportStringError(ImportError) Method __init__ Method __repr__ Attribute import_name Attribute exception", "label": "class ImportStringError(ImportError):\n    #: String in dotted notation that failed to be imported.\n    import_name = None\n    #: Wrapped exception.\n    exception = None\n\n    def __init__(self, import_name, exception):\n        self.import_name = import_name\n        self.exception = exception\n\n        msg = (\n            'import_string() failed for %r. Possible reasons are:\\n\\n'\n            '- missing __init__.py in a package;\\n'\n            '- package or module path not included in sys.path;\\n'\n            '- duplicated package or module name taking precedence in '\n            'sys.path;\\n'\n            '- missing module, class, function or variable;\\n\\n'\n            'Debugged import:\\n\\n%s\\n\\n'\n            'Original exception:\\n\\n%s: %s')\n\n        name = ''\n        tracked = []\n        for part in import_name.replace(':', '.').split('.'):\n            name += (name and '.') + part\n            imported = resolve_name(name, silent=True)\n            if imported:\n                tracked.append((name, getattr(imported, '__file__', None)))\n            else:\n                track = ['- %r found in %r.' % (n, i) for n, i in tracked]\n                track.append('- %r not found.' % name)\n                msg = msg % (import_name, '\\n'.join(track),\n                             exception.__class__.__name__, str(exception))\n                break\n\n        ImportError.__init__(self, msg)\n\n    def __repr__(self):\n        return '<%s(%r, %r)>' % (self.__class__.__name__, self.import_name,\n                                 self.exception)"}
{"Repository": "AutoLabelImg", "input": "Support setup.py upload. className UploadCommand(Command) Method status Method initialize_options Method finalize_options Method run", "label": "class UploadCommand(Command):\n    description=readme + '\\n\\n' + history,\n\n    user_options = []\n\n    @staticmethod\n    def status(s):\n        print('\\033[1m{0}\\033[0m'.format(s))\n\n    def initialize_options(self):\n        pass\n\n    def finalize_options(self):\n        pass\n\n    def run(self):\n        try:\n            self.status('Removing previous builds')\n            rmtree(os.path.join(here, 'dist'))\n        except OSError:\n            self.status('Fail to remove previous builds..')\n            pass\n\n        self.status('Building Source and Wheel (universal) distribution')\n        os.system(\n            '{0} setup.py sdist bdist_wheel --universal'.format(sys.executable))\n\n        self.status('Uploading the package to PyPI via Twine')\n        os.system('twine upload dist/*')\n\n        self.status('Pushing git tags')\n        os.system('git tag -d v{0}'.format(about['__version__']))\n        os.system('git tag v{0}'.format(about['__version__']))\n        # os.system('git push --tags')\n\n        sys.exit()"}
{"Repository": "tirg", "input": "The TIGR model with spatial modification over the last conv layer. className TIRGLastConv(ImgEncoderTextEncoderBase) Method __init__ Method compose_img_text Attribute a Attribute mod2d Attribute mod2d_gate", "label": "class TIRGLastConv(ImgEncoderTextEncoderBase):\n  def __init__(self, texts, embed_dim):\n    super(TIRGLastConv, self).__init__(texts, embed_dim)\n\n    self.a = torch.nn.Parameter(torch.tensor([1.0, 10.0, 1.0, 1.0]))\n    self.mod2d = torch.nn.Sequential(\n        torch.nn.BatchNorm2d(512 + embed_dim),\n        torch.nn.Conv2d(512 + embed_dim, 512 + embed_dim, [3, 3], padding=1),\n        torch.nn.ReLU(),\n        torch.nn.Conv2d(512 + embed_dim, 512, [3, 3], padding=1),\n    )\n\n    self.mod2d_gate = torch.nn.Sequential(\n        torch.nn.BatchNorm2d(512 + embed_dim),\n        torch.nn.Conv2d(512 + embed_dim, 512 + embed_dim, [3, 3], padding=1),\n        torch.nn.ReLU(),\n        torch.nn.Conv2d(512 + embed_dim, 512, [3, 3], padding=1),\n    )\n\n  def compose_img_text(self, imgs, texts):\n    text_features = self.extract_text_feature(texts)\n\n    x = imgs\n    x = self.img_model.conv1(x)\n    x = self.img_model.bn1(x)\n    x = self.img_model.relu(x)\n    x = self.img_model.maxpool(x)\n\n    x = self.img_model.layer1(x)\n    x = self.img_model.layer2(x)\n    x = self.img_model.layer3(x)\n    x = self.img_model.layer4(x)\n\n    # mod\n    y = text_features\n    y = y.reshape((y.shape[0], y.shape[1], 1, 1)).repeat(\n        1, 1, x.shape[2], x.shape[3])\n    z = torch.cat((x, y), dim=1)\n    t = self.mod2d(z)\n    tgate = self.mod2d_gate(z)\n    x = self.a[0] * F.sigmoid(tgate) * x + self.a[1] * t\n\n    x = self.img_model.avgpool(x)\n    x = x.view(x.size(0), -1)\n    x = self.img_model.fc(x)\n    return x"}
{"Repository": "cail2019_track2", "input": "A single training instance (sentence pair). className TrainingInstance(object) Method __str__ Method __repr__", "label": "class TrainingInstance(object):\n  def __init__(self, tokens, segment_ids, masked_lm_positions, masked_lm_labels,\n               is_random_next):\n    self.tokens = tokens\n    self.segment_ids = segment_ids\n    self.is_random_next = is_random_next\n    self.masked_lm_positions = masked_lm_positions\n    self.masked_lm_labels = masked_lm_labels\n\n  def __str__(self):\n    s = \"\"\n    s += \"tokens: %s\\n\" % (\" \".join(\n        [tokenization.printable_text(x) for x in self.tokens]))\n    s += \"segment_ids: %s\\n\" % (\" \".join([str(x) for x in self.segment_ids]))\n    s += \"is_random_next: %s\\n\" % self.is_random_next\n    s += \"masked_lm_positions: %s\\n\" % (\" \".join(\n        [str(x) for x in self.masked_lm_positions]))\n    s += \"masked_lm_labels: %s\\n\" % (\" \".join(\n        [tokenization.printable_text(x) for x in self.masked_lm_labels]))\n    s += \"\\n\"\n    return s\n\n  def __repr__(self):\n    return self.__str__()"}
{"Repository": "osrsbox-db", "input": "Modification of the Cerberus Validator class to remove specific fields. className MyValidator(Validator) Method _validate_description Method _validate_example", "label": "class MyValidator(Validator):\n    def _validate_description(self, description, field, value):\n        pass\n\n    def _validate_example(self, description, field, value):\n        pass"}
{"Repository": "PerfKitBenchmarker", "input": "Manages state for background tasks started in child threads. className _BackgroundThreadTaskManager(_BackgroundTaskManager) Method __init__ Method __exit__ Method StartTask Method AwaitAnyTask Method HandleKeyboardInterrupt Attribute _response_queue Attribute _task_queues Attribute _threads Attribute _available_worker_ids", "label": "class _BackgroundThreadTaskManager(_BackgroundTaskManager):\n  def __init__(self, *args, **kwargs):\n    super(_BackgroundThreadTaskManager, self).__init__(*args, **kwargs)\n    self._response_queue = _SingleReaderQueue()\n    self._task_queues = []\n    self._threads = []\n    self._available_worker_ids = list(range(self._max_concurrency))\n    uninitialized_worker_ids = set(self._available_worker_ids)\n    for worker_id in self._available_worker_ids:\n      task_queue = _NonPollingSingleReaderQueue()\n      self._task_queues.append(task_queue)\n      thread = threading.Thread(\n          target=_ExecuteBackgroundThreadTasks,\n          args=(worker_id, task_queue, self._response_queue),\n      )\n      thread.daemon = True\n      self._threads.append(thread)\n      thread.start()\n    # Wait for each Thread to finish its bootstrap code. Starting all the\n    # threads upfront like this and reusing them for later calls minimizes the\n    # risk of a KeyboardInterrupt interfering with any of the Lock interactions.\n    for _ in self._threads:\n      worker_id = self._response_queue.Get()\n      uninitialized_worker_ids.remove(worker_id)\n    assert not uninitialized_worker_ids, uninitialized_worker_ids\n\n  def __exit__(self, *unused_args, **unused_kwargs):\n    # Shut down worker threads.\n    for task_queue in self._task_queues:\n      task_queue.Put(_THREAD_STOP_PROCESSING)\n    for thread in self._threads:\n      _WaitForCondition(lambda: not thread.is_alive())\n\n  def StartTask(self, target, args, kwargs, thread_context):\n    assert (\n        self._available_worker_ids\n    ), 'StartTask called when no threads were available'\n    task = _BackgroundTask(target, args, kwargs, thread_context)\n    task_id = len(self.tasks)\n    self.tasks.append(task)\n    worker_id = self._available_worker_ids.pop()\n    self._task_queues[worker_id].Put((task_id, task))\n\n  def AwaitAnyTask(self):\n    worker_id, task_id = self._response_queue.Get()\n    self._available_worker_ids.append(worker_id)\n    return task_id\n\n  def HandleKeyboardInterrupt(self):\n    # Raise a KeyboardInterrupt in each child thread.\n    for thread in self._threads:\n      ctypes.pythonapi.PyThreadState_SetAsyncExc(\n          ctypes.c_long(thread.ident), ctypes.py_object(KeyboardInterrupt)\n      )\n    # Wake threads up from possible non-interruptable wait states so they can\n    # actually see the KeyboardInterrupt.\n    for task_queue, thread in zip(self._task_queues, self._threads):\n      task_queue.Put(_THREAD_WAIT_FOR_KEYBOARD_INTERRUPT)\n    for thread in self._threads:\n      _WaitForCondition(lambda: not thread.is_alive())"}
{"Repository": "cryptocurrency_arbitrage", "input": "An instance of this class will be returned by successful calls to TradeAPI.getOrderStatus and TradeAPI.placeOrder. className OrderItem(object) Method __init__ Attribute order_id Attribute status Attribute pair Attribute type Attribute rate Attribute amount Attribute initial_rate Attribute initial_amount Attribute status Attribute pair Attribute type Attribute rate Attribute amount Attribute initial_rate Attribute initial_amount Attribute initial_rate Attribute initial_amount Attribute date Attribute date", "label": "class OrderItem(object):\n    def __init__(self, order_id, info=None, initial_params=None, date=None):\n        self.order_id = int(order_id)\n        if info is not None:\n            order = info.get(u'order')\n            if type(order) is not dict:\n                raise Exception('The response is a %r, not a dict.' % type(order))\n            self.status = order[u'status']\n            self.pair = order[u'pair']\n            self.type = order[u'type']\n            self.rate = decimal.Decimal(order[u'rate'])\n            self.amount = decimal.Decimal(order[u'amount'])\n            self.initial_rate = decimal.Decimal(order[u'initial_rate'])\n            self.initial_amount = decimal.Decimal(order[u'initial_amount'])\n        else:\n            self.status = None\n            if initial_params is not None:\n                self.pair = initial_params['pair']\n                self.type = initial_params['type']\n                self.rate = initial_params['rate']\n                self.amount = initial_params['amount']\n                self.initial_rate = initial_params['rate']\n                self.initial_amount = initial_params['amount']\n            else:\n                self.initial_rate = None\n                self.initial_amount = None\n\n        if date is not None:\n            self.date = date\n        elif not hasattr(self, 'date'):\n            self.date = None"}
{"Repository": "DynamicGEM", "input": "Initialize the embedding class className TIMERS(StaticGraphEmbedding) Method __init__ Method get_method_name Method get_method_summary Method learn_embedding Method plotresults Method get_embedding Method get_edge_weight Method get_reconstructed_adj Method predict_next_adj", "label": "class TIMERS(StaticGraphEmbedding):\n    def __init__(self,  *hyper_dict, **kwargs):\n\n        hyper_params = {\n            'method_name': 'TIMERS',\n            'modelfile': None,\n            'weightfile': None,\n            'savefilesuffix': None\n\n        }\n        hyper_params.update(kwargs)\n        for key in hyper_params.keys():\n            self.__setattr__('_%s' % key, hyper_params[key])\n        for dictionary in hyper_dict:\n            for key in dictionary:\n                self.__setattr__('_%s' % key, dictionary[key])\n\n    def get_method_name(self):\n        return self._method_name\n\n    def get_method_summary(self):\n        return '%s' % (self._method_name)\n\n    def learn_embedding(self, graph=None):\n        timers = TIMERS_ALL.initialize()\n        timers.TIMERS(self._datafile, self._K / 2, self._Theta, self._datatype, nargout=0)\n\n    def plotresults(self, dynamic_sbm_series):\n\n        plt.figure()\n        plt.clf()\n        viz.plot_static_sbm_embedding(self._X[-4:], dynamic_sbm_series[-4:])\n\n        resultdir = self._resultdir + '/' + self._datatype\n        if not os.path.exists(resultdir):\n            os.mkdir(resultdir)\n\n        resultdir = resultdir + '/' + self._method\n        if not os.path.exists(resultdir):\n            os.mkdir(resultdir)\n\n        #         plt.savefig('./'+resultdir+'/V_'+self._method+'_nm'+str(self._nodemigration)+'_l'+str(self._length)+'_theta'+str(theta)+'_emb'+str(self._K*2)+'.pdf',bbox_inches='tight',dpi=600)\n        plt.show()\n        # plt.close()  \n\n    def get_embedding(self, outdir_tmp, method):\n        self._outdir_tmp = outdir_tmp\n        self._method = method\n        self._X = dataprep_util.getemb_TIMERS(self._outdir_tmp, int(self._length), int(self._K // 2), self._method)\n        return self._X\n\n    def get_edge_weight(self, t, i, j):\n        try:\n            return np.dot(self._X[t][i, :int(self._K // 2)], self._X[t][j, int(self._K // 2):])\n        except Exception as e:\n            print(e.message, e.args)\n            pdb.set_trace()\n\n    def get_reconstructed_adj(self, t, X=None, node_l=None):\n        if X is not None:\n            node_num = X.shape[0]\n            # self._X = X\n        else:\n            node_num = self._node_num\n        adj_mtx_r = np.zeros((node_num, node_num))\n        for v_i in range(node_num):\n            for v_j in range(node_num):\n                if v_i == v_j:\n                    continue\n                adj_mtx_r[v_i, v_j] = self.get_edge_weight(t, v_i, v_j)\n        return adj_mtx_r\n\n    def predict_next_adj(self, t, node_l=None):\n        if node_l is not None:\n            return self.get_reconstructed_adj(t, node_l)\n        else:\n            return self.get_reconstructed_adj(t)"}
{"Repository": "kaggle-heart", "input": "This layer turns (n_views * batch_size, num_channels, r, c) into (n_views * batch_size, n_views * num_channels, r, c) by rolling and concatenating feature maps. className CyclicConvRollLayer_c01b(CyclicConvRollLayer) Method __init__ Method get_output_shape_for Method get_output_for Attribute inv_tf_funcs", "label": "class CyclicConvRollLayer_c01b(CyclicConvRollLayer):\n    def __init__(self, input_layer):\n        super(CyclicConvRollLayer, self).__init__(input_layer)\n        self.inv_tf_funcs = [array_tf_0_c01b, array_tf_270_c01b, array_tf_180_c01b, array_tf_90_c01b]\n\n    def get_output_shape_for(self, input_shape):\n        return (4 * input_shape[0],) + input_shape[1:]\n\n    def get_output_for(self, input, *args, **kwargs):\n        s = input.shape\n        input_unfolded = input.reshape((s[0], s[1], s[2], 4, s[3] // 4))\n\n        permuted_inputs = []\n        for p, inv_tf in zip(self.perm_matrix, self.inv_tf_funcs):\n            input_permuted = inv_tf(input_unfolded[:, :, :, p, :].reshape(s))\n            permuted_inputs.append(input_permuted)\n\n        return T.concatenate(permuted_inputs, axis=0) # concatenate long the channel axis"}
{"Repository": "M-ADA", "input": "Computes and stores the average and current value className AverageMeter(object) Method __init__ Method reset Method update", "label": "class AverageMeter(object):\n    def __init__(self):\n\n        self.reset()\n\n\n\n    def reset(self):\n\n        self.val = 0\n\n        self.avg = 0\n\n        self.sum = 0\n\n        self.count = 0\n\n\n\n    def update(self, val, n=1):\n\n        self.val = val\n\n        self.sum += val * n\n\n        self.count += n\n\n        self.avg = self.sum / self.count"}
{"Repository": "python-microscopy", "input": "Client for the Acquire server. Offers a pythonic interface to the PYMEAcquire REST API. className AcquireClient(object) Method __init__ Method _poll_state Method _start_polling Method state Method _get_scope_state Method update_scope_state Method start_spooling Method spooling_info Method spooling_finished Attribute url Attribute port Attribute base_url Attribute _state", "label": "class AcquireClient(object):\n    def __init__(self, url='127.0.0.1', port=8999):\n        self.url = url\n        self.port = port\n        self.base_url = 'http://{}:{}'.format(self.url, self.port)\n        self._state = None\n\n\n    def _poll_state(self):\n        while True:\n            self._state = requests.get(self.base_url + '/scope_state_longpoll').json()\n\n    def _start_polling(self):\n        import threading\n        t = threading.Thread(target=self._poll_state)\n        t.daemon = True\n        t.start()\n\n    @property\n    def state(self):\n        if self._state is None:\n            self._state = self._get_scope_state() # get initiial state\n            self._start_polling()\n        \n        return self._state\n\n\n    def _get_scope_state(self):\n        return requests.get(self.base_url + '/get_scope_state').json()\n    \n    def update_scope_state(self, state:dict):\n        requests.post(self.base_url + '/update_scope_state', json=state)\n\n    def start_spooling(self, filename='', preflight_mode='abort', settings={}):\n        requests.post(self.base_url + f'/spool_controller/start_spooling?filename={filename}&preflight_mode={preflight_mode}', json=settings)\n\n        return self.spooling_finished\n\n    def spooling_info(self):\n        return requests.get(self.base_url + '/spool_controller/info').json()\n    \n    def spooling_finished(self):\n        return self.spooling_info()['status']['spool_complete']"}
{"Repository": "mixpanel-python", "input": "A consumer that maintains per-endpoint buffers of messages and then sends them in batches. className BufferedConsumer(object) Method send Method flush Method _flush_endpoint", "label": "class BufferedConsumer(object):\n    def __init__(self, max_size=50, events_url=None, people_url=None, import_url=None,\n            request_timeout=None, groups_url=None, api_host=\"api.mixpanel.com\",\n            retry_limit=4, retry_backoff_factor=0.25, verify_cert=True):\n        self._consumer = Consumer(events_url, people_url, import_url, request_timeout,\n            groups_url, api_host, retry_limit, retry_backoff_factor, verify_cert)\n        self._buffers = {\n            'events': [],\n            'people': [],\n            'groups': [],\n            'imports': [],\n        }\n        self._max_size = min(50, max_size)\n        self._api_key = None\n\n    def send(self, endpoint, json_message, api_key=None, api_secret=None):\n        if endpoint not in self._buffers:\n            raise MixpanelException('No such endpoint \"{0}\". Valid endpoints are one of {1}'.format(endpoint, self._buffers.keys()))\n\n        if not isinstance(api_key, tuple):\n            api_key = (api_key, api_secret)\n\n        buf = self._buffers[endpoint]\n        buf.append(json_message)\n        # Fixme: Don't stick these in the instance.\n        self._api_key = api_key\n        self._api_secret = api_secret\n        if len(buf) >= self._max_size:\n            self._flush_endpoint(endpoint)\n\n    def flush(self):\n        for endpoint in self._buffers.keys():\n            self._flush_endpoint(endpoint)\n\n    def _flush_endpoint(self, endpoint):\n        buf = self._buffers[endpoint]\n\n        while buf:\n            batch = buf[:self._max_size]\n            batch_json = '[{0}]'.format(','.join(batch))\n            try:\n                self._consumer.send(endpoint, batch_json, api_key=self._api_key)\n            except MixpanelException as orig_e:\n                mp_e = MixpanelException(orig_e)\n                mp_e.message = batch_json\n                mp_e.endpoint = endpoint\n                six.raise_from(mp_e, orig_e)\n            buf = buf[self._max_size:]\n        self._buffers[endpoint] = buf"}
{"Repository": "python-neo", "input": "Filter condition to check if target value is less than or equal to the control value. className LessThanOrEquals(FilterCondition) Method __init__ Method evaluate", "label": "class LessThanOrEquals(FilterCondition):\n    def __init__(self, control: Number) -> None:\n        self.control = control\n\n    def evaluate(self, compare: Number) -> bool:\n        return compare <= self.control"}
{"Repository": "text_data_enhancement_with_LaserTagger", "input": "Tag that corresponds to a token edit operation. className Tag(object) Method __init__ Method __str__ Attribute tag_type Attribute added_phrase", "label": "class Tag(object):\n  def __init__(self, tag):\n    if '|' in tag:\n      pos_pipe = tag.index('|') # split\n      tag_type, added_phrase = tag[:pos_pipe], tag[pos_pipe + 1:]\n    else:\n      tag_type, added_phrase = tag, ''\n    try:\n      self.tag_type = TagType[tag_type] # for example: tag_type:KEEP self.tag_type:TagType.KEEP\n    except KeyError:\n      raise ValueError(\n          'TagType should be KEEP, DELETE or SWAP, not {}'.format(tag_type))\n    self.added_phrase = added_phrase\n\n  def __str__(self):\n    if not self.added_phrase:\n      return self.tag_type.name\n    else:\n      return '{}|{}'.format(self.tag_type.name, self.added_phrase)"}
{"Repository": "FLASK", "input": "The model adapter for alpaca. className AlpacaAdapter(BaseAdapter) Method match Method get_default_conv_template", "label": "class AlpacaAdapter(BaseAdapter):\n    def match(self, model_path: str):\n        return \"alpaca\" in model_path.lower()\n\n    def get_default_conv_template(self, model_path: str) -> Conversation:\n        return get_conv_template(\"alpaca\")"}
{"Repository": "py-amqp", "input": "PLAIN SASL authentication mechanism. className PLAIN(SASL) Method __init__ Method start Attribute password", "label": "class PLAIN(SASL):\n    mechanism = b'PLAIN'\n\n    def __init__(self, username, password):\n        self.username, self.password = username, password\n\n    __slots__ = (\n        \"username\",\n        \"password\",\n        )\n\n    def start(self, connection):\n        if self.username is None or self.password is None:\n            return NotImplemented\n        login_response = BytesIO()\n        login_response.write(b'\\0')\n        login_response.write(self.username.encode('utf-8'))\n        login_response.write(b'\\0')\n        login_response.write(self.password.encode('utf-8'))\n        return login_response.getvalue()"}
{"Repository": "pynapple", "input": "Loader for data processed with matlab CNMF-E(https://github. className CNMF_E(BaseLoader) Method __init__ Method load_cnmfe_nwb Attribute basename", "label": "class CNMF_E(BaseLoader):\n    def __init__(self, path):\n        self.basename = os.path.basename(path)\n\n        super().__init__(path)\n\n        self.load_cnmfe_nwb(path)\n\n    def load_cnmfe_nwb(self, path):\n        self.nwb_path = os.path.join(path, \"pynapplenwb\")\n        if not os.path.exists(self.nwb_path):\n            raise RuntimeError(\"Path {} does not exist.\".format(self.nwb_path))\n\n        self.nwbfilename = [f for f in os.listdir(self.nwb_path) if \"nwb\" in f][0]\n        self.nwbfilepath = os.path.join(self.nwb_path, self.nwbfilename)\n\n        io = NWBHDF5IO(self.nwbfilepath, \"r\")\n        nwbfile = io.read()\n\n        if \"ophys\" in nwbfile.processing.keys():\n            data = nwbfile.processing[\"ophys\"][\"Fluorescence\"][\n                \"RoiResponseSeries\"\n            ].data[:]\n            t = nwbfile.processing[\"ophys\"][\"Fluorescence\"][\n                \"RoiResponseSeries\"\n            ].timestamps[:]\n            self.C = nap.TsdFrame(t=t, d=data)\n            self.A = nwbfile.processing[\"ophys\"][\"ImageSegmentation\"][\n                \"PlaneSegmentation\"\n            ][\"image_mask\"].data[:]\n\n            io.close()\n            return True\n        else:\n            io.close()\n            return False"}
{"Repository": "TextClassify_with_BERT", "input": "Runs end-to-end tokenziation. className FullTokenizer(object) Method __init__ Method tokenize Method convert_tokens_to_ids Method convert_ids_to_tokens Attribute vocab Attribute inv_vocab Attribute basic_tokenizer Attribute wordpiece_tokenizer", "label": "class FullTokenizer(object):\n  def __init__(self, vocab_file, do_lower_case=True):\n    self.vocab = load_vocab(vocab_file)\n    self.inv_vocab = {v: k for k, v in self.vocab.items()}\n    self.basic_tokenizer = BasicTokenizer(do_lower_case=do_lower_case)\n    self.wordpiece_tokenizer = WordpieceTokenizer(vocab=self.vocab)\n\n  def tokenize(self, text):\n    split_tokens = []\n    for token in self.basic_tokenizer.tokenize(text):\n      for sub_token in self.wordpiece_tokenizer.tokenize(token):\n        split_tokens.append(sub_token)\n\n    return split_tokens\n\n  def convert_tokens_to_ids(self, tokens):\n    return convert_by_vocab(self.vocab, tokens)\n\n  def convert_ids_to_tokens(self, ids):\n    return convert_by_vocab(self.inv_vocab, ids)"}
{"Repository": "FastFCN", "input": "Implements data parallelism at the module level. className DataParallelModel(DataParallel) Method gather Method replicate", "label": "class DataParallelModel(DataParallel):\n    def gather(self, outputs, output_device):\n        return outputs\n\n    def replicate(self, module, device_ids):\n        modules = super(DataParallelModel, self).replicate(module, device_ids)\n        execute_replication_callbacks(modules)\n        return modules"}
{"Repository": "ctf", "input": "Thin extension of ArgumentParser for custom error message. className CustomErrorArgumentParser(ArgumentParser) Method error", "label": "class CustomErrorArgumentParser(ArgumentParser):\n    def error(self, message):\n        print_err('Error when argument-parsing -- ', message)\n        sys.exit(1)"}
{"Repository": "FlowKit", "input": "Represents a divider for a single Dimension, used as part of a QuadrantGate definition. className QuadrantDivider(object) Method __repr__", "label": "class QuadrantDivider(object):\n    def __init__(\n            self,\n            divider_id,\n            dimension_ref,\n            compensation_ref,\n            values,\n            transformation_ref=None\n    ):\n        self.id = divider_id\n        self.dimension_ref = dimension_ref\n\n        # a compensation reference is required, although the value can be\n        # the string 'uncompensated' for non-compensated dimensions, or 'FCS'\n        # for using the embedded spill in the FCS file. Otherwise, it is a\n        # reference to a Matrix in the GatingStrategy\n        self.compensation_ref = compensation_ref\n\n        # transformation is optional\n        self.transformation_ref = transformation_ref\n\n        self.values = values\n\n    def __repr__(self):\n        return f'{self.__class__.__name__}(id: {self.id}, dim_ref: {self.dimension_ref})'"}
{"Repository": "CVE-2017-7494", "input": "Represents a VLAN header specified in IEEE 802. className EthernetTag(PacketBuffer) Method __init__ Method get_tpid Method set_tpid Method get_pcp Method set_pcp Method get_dei Method set_dei Method get_vid Method set_vid Method __str__", "label": "class EthernetTag(PacketBuffer):\n    def __init__(self, value=0x81000000):\n        PacketBuffer.__init__(self, 4)\n        self.set_long(0, value)\n\n    def get_tpid(self):\n        return self.get_word(0)\n\n    def set_tpid(self, value):\n        return self.set_word(0, value)\n\n    def get_pcp(self):\n        return (self.get_byte(2) & 0xE0) >> 5\n\n    def set_pcp(self, value):\n        orig_value = self.get_byte(2)\n        self.set_byte(2, (orig_value & 0x1F) | ((value & 0x07) << 5))\n\n    def get_dei(self):\n        return (self.get_byte(2) & 0x10) >> 4\n\n    def set_dei(self, value):\n        orig_value = self.get_byte(2)\n        self.set_byte(2, orig_value | 0x10 if value else orig_value & 0xEF)\n\n    def get_vid(self):\n        return self.get_word(2) & 0x0FFF\n\n    def set_vid(self, value):\n        orig_value = self.get_word(2)\n        self.set_word(2, (orig_value & 0xF000) | (value & 0x0FFF))\n\n    def __str__(self):\n        priorities = (\n            'Best Effort',\n            'Background',\n            'Excellent Effort',\n            'Critical Applications',\n            'Video, < 100 ms latency and jitter',\n            'Voice, < 10 ms latency and jitter',\n            'Internetwork Control',\n            'Network Control')\n\n        pcp = self.get_pcp()\n        return '\\n'.join((\n            '802.1Q header: 0x{0:08X}'.format(self.get_long(0)),\n            'Priority Code Point: {0} ({1})'.format(pcp, priorities[pcp]),\n            'Drop Eligible Indicator: {0}'.format(self.get_dei()),\n            'VLAN Identifier: {0}'.format(self.get_vid())))"}
{"Repository": "neo4j-python-driver", "input": "Implementation of a certificate provider that can rotate certificates. className AsyncRotatingClientCertificateProvider(AsyncClientCertificateProvider) Method __init__", "label": "class AsyncRotatingClientCertificateProvider(AsyncClientCertificateProvider):\n    def __init__(self, initial_cert: ClientCertificate) -> None:\n        self._cert: t.Optional[ClientCertificate] = initial_cert\n        self._lock = AsyncCooperativeLock()\n\n    async def get_certificate(self) -> t.Optional[ClientCertificate]:\n        async with self._lock:\n            cert, self._cert = self._cert, None\n            return cert\n\n    async def update_certificate(self, cert: ClientCertificate) -> None:\n        async with self._lock:\n            self._cert = cert"}
{"Repository": "rtclite", "input": "Base class for objects that are yielded by a task to the task manager and specify the condition(s) under which the task should be restarted. className YieldCondition(object) Method __init__ Method _expires Attribute task Attribute handle_expiration Attribute expiration Attribute expiration", "label": "class YieldCondition(object):\n    def __init__(self, timeout=None):\n        self.task = None\n        self.handle_expiration = None\n\n        if timeout is None:\n            self.expiration = None\n        else:\n            self.expiration = time.time() + float(timeout)\n\n    def _expires(self):\n        return (self.expiration is not None)"}
{"Repository": "LOUPE", "input": "Creates a NetFV class. className NetFV(PoolingBaseModel) Method forward", "label": "class NetFV(PoolingBaseModel):\n    def __init__(self, feature_size, max_samples, cluster_size, output_dim,\n            gating=True,add_batch_norm=True, is_training=True):\n        super(self.__class__, self).__init__(\n            feature_size=feature_size,\n            max_samples=max_samples,\n            cluster_size=cluster_size,\n            output_dim=output_dim,\n            gating=gating,\n            add_batch_norm=add_batch_norm,\n            is_training=is_training)\n\n    def forward(self, reshaped_input):\n        cluster_weights = tf.get_variable(\"cluster_weights\",\n          [self.feature_size, self.cluster_size],\n          initializer = tf.random_normal_initializer(\n              stddev=1 / math.sqrt(self.feature_size)))\n     \n        covar_weights = tf.get_variable(\"covar_weights\",\n          [self.feature_size, self.cluster_size],\n          initializer = tf.random_normal_initializer(\n              mean=1.0, stddev=1 /math.sqrt(self.feature_size)))\n      \n        covar_weights = tf.square(covar_weights)\n        eps = tf.constant([1e-6])\n        covar_weights = tf.add(covar_weights,eps)\n\n        activation = tf.matmul(reshaped_input, cluster_weights)\n        if self.add_batch_norm:\n          activation = slim.batch_norm(\n              activation,\n              center=True,\n              scale=True,\n              is_training=self.is_training,\n              scope=\"cluster_bn\")\n        else:\n          cluster_biases = tf.get_variable(\"cluster_biases\",\n            [self.cluster_size],\n            initializer = tf.random_normal_initializer(\n                stddev=1 / math.sqrt(self.feature_size)))\n          activation += cluster_biases\n        \n        activation = tf.nn.softmax(activation)\n\n        activation = tf.reshape(activation,\n                [-1, self.max_samples, self.cluster_size])\n\n        a_sum = tf.reduce_sum(activation,-2,keep_dims=True)\n\n        cluster_weights2 = tf.get_variable(\"cluster_weights2\",\n                [1,self.feature_size, self.cluster_size],\n                initializer = tf.random_normal_initializer(\n                    stddev=1 / math.sqrt(self.feature_size)))\n\n        a = tf.multiply(a_sum,cluster_weights2)\n        \n        activation = tf.transpose(activation,perm=[0,2,1])\n        \n        reshaped_input = tf.reshape(reshaped_input,\n                [-1,self.max_samples,self.feature_size])\n        fv1 = tf.matmul(activation,reshaped_input)\n        \n        fv1 = tf.transpose(fv1,perm=[0,2,1])\n\n        # computing second order FV\n        a2 = tf.multiply(a_sum,tf.square(cluster_weights2)) \n\n        b2 = tf.multiply(fv1,cluster_weights2) \n        fv2 = tf.matmul(activation,tf.square(reshaped_input)) \n     \n        fv2 = tf.transpose(fv2,perm=[0,2,1])\n        fv2 = tf.add_n([a2,fv2,tf.scalar_mul(-2,b2)])\n\n        fv2 = tf.divide(fv2,tf.square(covar_weights))\n        fv2 = tf.subtract(fv2,a_sum)\n\n        fv2 = tf.reshape(fv2,[-1,self.cluster_size*self.feature_size])\n      \n        fv2 = tf.nn.l2_normalize(fv2,1)\n        fv2 = tf.reshape(fv2,[-1,self.cluster_size*self.feature_size])\n        fv2 = tf.nn.l2_normalize(fv2,1)\n\n        fv1 = tf.subtract(fv1,a)\n        fv1 = tf.divide(fv1,covar_weights) \n\n        fv1 = tf.nn.l2_normalize(fv1,1)\n        fv1 = tf.reshape(fv1,[-1,self.cluster_size*self.feature_size])\n        fv1 = tf.nn.l2_normalize(fv1,1)\n\n        fv = tf.concat([fv1,fv2],1)\n \n        hidden1_weights = tf.get_variable(\"hidden1_weights\",\n          [2*self.cluster_size*self.feature_size, self.output_dim],\n          initializer=tf.random_normal_initializer(\n              stddev=1 / math.sqrt(self.cluster_size)))\n           \n        fv = tf.matmul(fv, hidden1_weights)\n\n        if self.gating:\n            fv = super(self.__class__, self).context_gating(fv)\n \n        return fv"}
{"Repository": "pants", "input": "Basic Proxy-Authorization className ProxyAuth(BasicAuth) Method __call__", "label": "class ProxyAuth(BasicAuth):\n    def __call__(self, request):\n        request.headers['Proxy-Authorization'] = _basic_auth(self.username,\n                                                             self.password)\n        return request"}
{"Repository": "scikit-kge", "input": "Run in-place build before Sphinx doc build className BuildDoc(SphinxBuildDoc) Method run", "label": "class BuildDoc(SphinxBuildDoc):\n        def run(self):\n            ret = subprocess.call(\n                [sys.executable, sys.argv[0], 'build_ext', '-i']\n            )\n            if ret != 0:\n                raise RuntimeError(\"Building Scipy failed!\")\n            SphinxBuildDoc.run(self)\n    cmdclass = {'build_sphinx': BuildDoc}"}
{"Repository": "Drive-WM", "input": "A custom distutils command that updates the dependency table. className DepsTableUpdateCommand(Command) Method initialize_options Method finalize_options Method run", "label": "class DepsTableUpdateCommand(Command):\n    description = \"build runtime dependency table\"\n    user_options = [\n        # format: (long option, short option, description).\n        (\n            \"dep-table-update\",\n            None,\n            \"updates src/diffusers/dependency_versions_table.py\",\n        ),\n    ]\n\n    def initialize_options(self):\n        pass\n\n    def finalize_options(self):\n        pass\n\n    def run(self):\n        entries = \"\\n\".join([f'    \"{k}\": \"{v}\",' for k, v in deps.items()])\n        content = [\n            \"# THIS FILE HAS BEEN AUTOGENERATED. To update:\",\n            \"# 1. modify the `_deps` dict in setup.py\",\n            \"# 2. run `make deps_table_update`\",\n            \"deps = {\",\n            entries,\n            \"}\",\n            \"\",\n        ]\n        target = \"src/diffusers/dependency_versions_table.py\"\n        print(f\"updating {target}\")\n        with open(target, \"w\", encoding=\"utf-8\", newline=\"\\n\") as f:\n            f.write(\"\\n\".join(content))"}
{"Repository": "hashdist", "input": "A non-comment block of code. className NonComment(object) Method __init__ Method add Method __repr__ Attribute start_lineno Attribute end_lineno", "label": "class NonComment(object):\n    is_comment = False\n    def __init__(self, start_lineno, end_lineno):\n        self.start_lineno = start_lineno\n        self.end_lineno = end_lineno\n\n    def add(self, string, start, end, line):\n        if string.strip():\n            # Only add if not entirely whitespace.\n            self.start_lineno = min(self.start_lineno, start[0])\n            self.end_lineno = max(self.end_lineno, end[0])\n\n    def __repr__(self):\n        return '%s(%r, %r)' % (self.__class__.__name__, self.start_lineno,\n            self.end_lineno)"}
{"Repository": "stuff", "input": "UNmap plugin for exporting unmap results in CSV format className CsvUNmapPlugin(UNmapPlugin) Method __init__ Method export", "label": "class CsvUNmapPlugin(UNmapPlugin):\n    def __init__(self, nmap_results, **kwargs):\n        UNmapPlugin.__init__(self, nmap_results, **kwargs)\n\n        if self.verbose :\n            self.logger.info(\"Output will be written in stdout\")\n\n        del self.filename\n        return\n\n\n    def export(self):\n        fmt = '\"{}\" ; \"{}\" ; \"{}\"\\n'\n\n        line =  fmt.format(\"IP\", \"Port\", \"Service\")\n        stdout.write(line)\n\n        for h in self.nmap_results.hosts:\n            for p in h.ports:\n                stdout.write(fmt.format(h.ip, p.port, p.banner))\n            stdout.flush()\n\n        return"}
{"Repository": "pyquarkchain", "input": "Used for making temporary objects in EvmState.ephemeral_clone() className OverlayDb(Db) Method __init__ Method get Method put Method delete Method commit Method _has_key Method __contains__ Attribute _db Attribute kv Attribute overlay", "label": "class OverlayDb(Db):\n    def __init__(self, db):\n        self._db = db\n        self.kv = None\n        self.overlay = {}\n\n    def get(self, key):\n        if key in self.overlay:\n            return self.overlay[key]\n        return self._db.get(key)\n\n    def put(self, key, value):\n        self.overlay[key] = value\n\n    def delete(self, key):\n        self.overlay[key] = None\n\n    def commit(self):\n        pass\n\n    def _has_key(self, key):\n        if key in self.overlay:\n            return self.overlay[key] is not None\n        return self._db.get(key) is not None\n\n    def __contains__(self, key):\n        return self._has_key(key)"}
{"Repository": "dqn-obstacle-avoidance", "input": "Obstacle. className Obstacle(MovingObject) Method __init__ Method _control Attribute _bounds Attribute _radius Attribute _height", "label": "class Obstacle(MovingObject):\n    def __init__(self, velocity, radius, bounds, height=1.0):\n        data = DebugData()\n        self._bounds = bounds\n        self._radius = radius\n        self._height = height\n        center = [0, 0, height / 2 - 0.5]\n        axis = [0, 0, 1]  # Upright cylinder.\n        data.addCylinder(center, axis, height, radius)\n        polydata = data.getPolyData()\n        super(Obstacle, self).__init__(velocity, polydata)\n\n    def _control(self, state, t):\n        x_min, x_max, y_min, y_max = self._bounds\n        x, y, theta = state\n        if x - self._radius <= x_min:\n            return np.pi\n        elif x + self._radius >= x_max:\n            return np.pi\n        elif y - self._radius <= y_min:\n            return np.pi\n        elif y + self._radius >= y_max:\n            return np.pi\n        return 0."}
{"Repository": "BERT-SQuAD", "input": "A single training/test example for the Squad dataset. className SquadExample(object) Method __str__ Method __repr__", "label": "class SquadExample(object):\n    def __init__(self,\n                 qas_id,\n                 question_text,\n                 doc_tokens,\n                 orig_answer_text=None,\n                 start_position=None,\n                 end_position=None):\n        self.qas_id = qas_id\n        self.question_text = question_text\n        self.doc_tokens = doc_tokens\n        self.orig_answer_text = orig_answer_text\n        self.start_position = start_position\n        self.end_position = end_position\n\n    def __str__(self):\n        return self.__repr__()\n\n    def __repr__(self):\n        s = \"\"\n        s += \"qas_id: %s\" % (self.qas_id)\n        s += \", question_text: %s\" % (\n            self.question_text)\n        s += \", doc_tokens: [%s]\" % (\" \".join(self.doc_tokens))\n        if self.start_position:\n            s += \", start_position: %d\" % (self.start_position)\n        if self.end_position:\n            s += \", end_position: %d\" % (self.end_position)\n        return s"}
{"Repository": "FormatFuzzer", "input": "The enum field class className Enum(IntBase) Method _pfp__parse Method __repr__ Method _pfp__cls_name", "label": "class Enum(IntBase):\n    enum_vals = None\n    enum_cls = None\n    enum_name = None\n\n    def __init__(\n        self,\n        stream=None,\n        enum_cls=None,\n        enum_vals=None,\n        bitsize=None,\n        metadata_processor=None,\n        bitfield_rw=None,\n        bitfield_padded=False,\n        bitfield_left_right=False,\n    ):\n        self.enum_name = None\n\n        if enum_vals is not None:\n            self.enum_vals = enum_vals\n\n        if enum_cls is not None:\n            self.enum_cls = enum_cls\n            self.endian = enum_cls.endian\n            self.width = enum_cls.width\n            self.format = enum_cls.format\n\n        super(Enum, self).__init__(\n            stream,\n            metadata_processor=metadata_processor,\n            bitfield_rw=bitfield_rw,\n            bitsize=bitsize,\n            bitfield_padded=bitfield_padded,\n            bitfield_left_right=bitfield_left_right,\n        )\n\n    def _pfp__parse(self, stream, save_offset=False, set_val=True):\n        res = super(Enum, self)._pfp__parse(stream, save_offset, set_val=set_val)\n\n        if self._pfp__value in self.enum_vals:\n            self.enum_name = self.enum_vals[self._pfp__value]\n        else:\n            self.enum_name = \"?? UNK_ENUM ??\"\n\n        return res\n\n    def __repr__(self):\n        res = super(Enum, self).__repr__()\n        res += \"({})\".format(self.enum_name)\n        return res\n\n    def _pfp__cls_name(self):\n        return \"Enum<{}>\".format(self.enum_cls.__name__)"}
{"Repository": "MAD-GANs", "input": "The most basic RNN cell. className BasicRNNCell(RNNCell) Method __init__ Method state_size Method output_size Method __call__ Attribute _num_units Attribute _activation Attribute _reuse", "label": "class BasicRNNCell(RNNCell):\n  def __init__(self, num_units, input_size=None, activation=tanh, reuse=None):\n    if input_size is not None:\n      logging.warn(\"%s: The input_size parameter is deprecated.\", self)\n    self._num_units = num_units\n    self._activation = activation\n    self._reuse = reuse\n\n  @property\n  def state_size(self):\n    return self._num_units\n\n  @property\n  def output_size(self):\n    return self._num_units\n\n  def __call__(self, inputs, state, scope=None):\n    with _checked_scope(self, scope or \"basic_rnn_cell\", reuse=self._reuse):\n      output = self._activation(\n          _linear([inputs, state], self._num_units, True))\n    return output, output"}
{"Repository": "hazm", "input": "       PerUDT . className UniversalDadeganReader(DadeganReader) Method __init__ Method _sentences", "label": "class UniversalDadeganReader(DadeganReader):\n    def __init__(self: DadeganReader, conllu_file: str) -> None:\n        self._conll_file = conllu_file\n        self._pos_map = lambda tags, _: \",\".join(tags)\n\n    def _sentences(self: DadeganReader) -> Iterator[str]:\n        text = conllu2conll(self._conll_file)\n\n        # refine text\n        text = text.replace(\"\", \"\").replace(\"\\t\", \"\\t\").replace(\"\\t\", \"\\t\").replace(\"\\t \", \"\\t\").replace(\" \\t\", \"\\t\").replace(\n            \"\\r\", \"\").replace(\"\\u2029\", \"\")\n\n        for item in text.replace(\" \", \"_\").split(\"\\n\\n\"):\n            if item.strip():\n                yield item"}
{"Repository": "webargs", "input": "A date adder endpoint. className DateAddHandler(BaseRequestHandler) Method post", "label": "class DateAddHandler(BaseRequestHandler):\n    dateadd_args = {\n        \"value\": fields.Date(required=False),\n        \"addend\": fields.Int(required=True, validate=validate.Range(min=1)),\n        \"unit\": fields.Str(\n            load_default=\"days\", validate=validate.OneOf([\"minutes\", \"days\"])\n        ),\n    }\n\n    @use_kwargs(dateadd_args)\n    def post(self, value, addend, unit):\n        value = value or dt.datetime.utcnow()\n        if unit == \"minutes\":\n            delta = dt.timedelta(minutes=addend)\n        else:\n            delta = dt.timedelta(days=addend)\n        result = value + delta\n        self.write({\"result\": result.isoformat()})"}
{"Repository": "appmetrics", "input": "A simple sliding-window reservoir that keeps the last N values className SlidingWindowReservoir(ReservoirBase) Method __init__ Method _do_add Method _get_values Method _same_parameters Method __repr__ Attribute size Attribute deque", "label": "class SlidingWindowReservoir(ReservoirBase):\n    def __init__(self, size=DEFAULT_UNIFORM_RESERVOIR_SIZE):\n        self.size = size\n        self.deque = collections.deque(maxlen=self.size)\n\n    def _do_add(self, value):\n        # No need for explicit lock - deques should be thread-safe:\n        # http://docs.python.org/2/library/collections.html#collections.deque\n        self.deque.append(value)\n\n    def _get_values(self):\n        return list(self.deque)\n\n    def _same_parameters(self, other):\n        return self.size == other.size\n\n    def __repr__(self):\n        return \"{}({})\".format(type(self).__name__, self.size)"}
{"Repository": "yolov3-tensorrt", "input": "Helper class to store the hyper parameters of a Conv layer, including its prefix name in the ONNX graph and the expected dimensions of weights for convolution, bias, and batch normalization. className ConvParams(object) Method __init__ Method generate_param_name Attribute node_name Attribute batch_normalize Attribute conv_weight_dims", "label": "class ConvParams(object):\n    def __init__(self, node_name, batch_normalize, conv_weight_dims):\n        self.node_name = node_name\n        self.batch_normalize = batch_normalize\n        assert len(conv_weight_dims) == 4\n        self.conv_weight_dims = conv_weight_dims\n\n    def generate_param_name(self, param_category, suffix):\n        assert suffix\n        assert param_category in ['bn', 'conv']\n        assert(suffix in ['scale', 'mean', 'var', 'weights', 'bias'])\n        if param_category == 'bn':\n            assert self.batch_normalize\n            assert suffix in ['scale', 'bias', 'mean', 'var']\n        elif param_category == 'conv':\n            assert suffix in ['weights', 'bias']\n            if suffix == 'bias':\n                assert not self.batch_normalize\n        param_name = self.node_name + '_' + param_category + '_' + suffix\n        return param_name"}
{"Repository": "embedding_study", "input": "BERT model (\"Bidirectional Embedding Representations from a Transformer\"). className BertModel(object) Method get_pooled_output Method get_sequence_output Method get_all_encoder_layers Method get_embedding_output Method get_embedding_table", "label": "class BertModel(object):\n    def __init__(self,\n                 config,\n                 is_training,\n                 input_ids,\n                 input_mask=None,\n                 token_type_ids=None,\n                 use_one_hot_embeddings=True,\n                 scope=None):\n        config = copy.deepcopy(config)\n        if not is_training:\n            config.hidden_dropout_prob = 0.0\n            config.attention_probs_dropout_prob = 0.0\n\n        input_shape = get_shape_list(input_ids, expected_rank=2)\n        batch_size = input_shape[0]\n        seq_length = input_shape[1]\n\n        if input_mask is None:\n            input_mask = tf.ones(shape=[batch_size, seq_length], dtype=tf.int32)\n\n        if token_type_ids is None:\n            token_type_ids = tf.zeros(shape=[batch_size, seq_length], dtype=tf.int32)\n\n        with tf.variable_scope(scope, default_name=\"bert\"):\n            with tf.variable_scope(\"embeddings\"):\n                # Perform embedding lookup on the word ids.\n                (self.embedding_output, self.embedding_table) = embedding_lookup(\n                    input_ids=input_ids,\n                    vocab_size=config.vocab_size,\n                    embedding_size=config.hidden_size,\n                    initializer_range=config.initializer_range,\n                    word_embedding_name=\"word_embeddings\",\n                    use_one_hot_embeddings=use_one_hot_embeddings)\n\n                # Add positional embeddings and token type embeddings, then layer\n                # normalize and perform dropout.\n                self.embedding_output = embedding_postprocessor(\n                    input_tensor=self.embedding_output,\n                    use_token_type=True,\n                    token_type_ids=token_type_ids,\n                    token_type_vocab_size=config.type_vocab_size,\n                    token_type_embedding_name=\"token_type_embeddings\",\n                    use_position_embeddings=True,\n                    position_embedding_name=\"position_embeddings\",\n                    initializer_range=config.initializer_range,\n                    max_position_embeddings=config.max_position_embeddings,\n                    dropout_prob=config.hidden_dropout_prob)\n\n            with tf.variable_scope(\"encoder\"):\n                # This converts a 2D mask of shape [batch_size, seq_length] to a 3D\n                # mask of shape [batch_size, seq_length, seq_length] which is used\n                # for the attention scores.\n                attention_mask = create_attention_mask_from_input_mask(\n                    input_ids, input_mask)\n\n                # Run the stacked transformer.\n                # `sequence_output` shape = [batch_size, seq_length, hidden_size].\n                self.all_encoder_layers = transformer_model(\n                    input_tensor=self.embedding_output,\n                    attention_mask=attention_mask,\n                    hidden_size=config.hidden_size,\n                    num_hidden_layers=config.num_hidden_layers,\n                    num_attention_heads=config.num_attention_heads,\n                    intermediate_size=config.intermediate_size,\n                    intermediate_act_fn=get_activation(config.hidden_act),\n                    hidden_dropout_prob=config.hidden_dropout_prob,\n                    attention_probs_dropout_prob=config.attention_probs_dropout_prob,\n                    initializer_range=config.initializer_range,\n                    do_return_all_layers=True)\n\n            self.sequence_output = self.all_encoder_layers[-1]\n            # The \"pooler\" converts the encoded sequence tensor of shape\n            # [batch_size, seq_length, hidden_size] to a tensor of shape\n            # [batch_size, hidden_size]. This is necessary for segment-level\n            # (or segment-pair-level) classification tasks where we need a fixed\n            # dimensional representation of the segment.\n            with tf.variable_scope(\"pooler\"):\n                # We \"pool\" the model by simply taking the hidden state corresponding\n                # to the first token. We assume that this has been pre-trained\n                first_token_tensor = tf.squeeze(self.sequence_output[:, 0:1, :], axis=1)\n                # https://github.com/google-research/bert/issues/43#issuecomment-435980269\n                self.pooled_output = tf.layers.dense(\n                    first_token_tensor,\n                    config.hidden_size,\n                    activation=tf.tanh,\n                    kernel_initializer=create_initializer(config.initializer_range))\n\n    def get_pooled_output(self):\n        return self.pooled_output\n\n    def get_sequence_output(self):\n        return self.sequence_output\n\n    def get_all_encoder_layers(self):\n        return self.all_encoder_layers\n\n    def get_embedding_output(self):\n        return self.embedding_output\n\n    def get_embedding_table(self):\n        return self.embedding_table"}
{"Repository": "baredroid", "input": "Class used to manage the swapping process. Each device has its own. className UpdateManagerRecovery(object) Method __init__ Method getProcess Method getPID Method update Method join Method startUpdate Attribute _deviceId Attribute _shell Attribute _state Attribute _process Attribute _pid Attribute _logger", "label": "class UpdateManagerRecovery(object):\n    def __init__(self, deviceId):\n        self._deviceId = deviceId\n        self._shell = adb.AndroidDebugBridge(deviceId)\n        self._state = 'ready' # ready to start the analysis\n        self._process = None\n        self._pid = 999999\n        self._logger = logging.getLogger('Manager.Update.Recovery - ' + self._deviceId)\n\n    def getProcess(self):\n        return self._process\n\n    def getPID(self):\n        self._logger.info('PID %d is associated to device %s' % (self._pid, self._deviceId))\n        return self._pid\n\n    def update(self):\n        sleep(2)\n        while True:\n            self._shell.cmd('sh /cache/recovery/swap')\n            break\n\n    def join(self):\n        sleep(2)\n        self._process.join()\n        self._logger.info('update_recovery process joined') \n\n    def startUpdate(self):\n        self._logger.info('Start update recovery process') \n        self._process = Process(target=self.update)\n        self._process.start()\n        self._pid = os.getpid()\n\n        self._logger.info('Start update device process with PID %d' % self._pid)\n        \n        return self._pid"}
{"Repository": "udiskie", "input": "Create machine specific translation files (for i18n via gettext). className build_mo(Command) Method initialize_options Method finalize_options Method run Method make_mo", "label": "class build_mo(Command):\n    description = 'Compile .po files into .mo files'\n\n    def initialize_options(self):\n        pass\n\n    def finalize_options(self):\n        pass\n\n    def run(self):\n        for lang in languages:\n            po_file = 'lang/{}.po'.format(lang)\n            mo_file = 'build/locale/{}/LC_MESSAGES/udiskie.mo'.format(lang)\n            self.mkpath(path.dirname(mo_file))\n            self.make_file(\n                po_file, mo_file, self.make_mo,\n                [po_file, mo_file])\n\n    def make_mo(self, po_filename, mo_filename):\n        try:\n            call(['msgfmt', po_filename, '-o', mo_filename])\n        except OSError as e:\n            # ignore failures since i18n support is optional:\n            logging.warning(e)"}
{"Repository": "trueskill", "input": "A model for the normal distribution. className Gaussian(object) Method __init__ Method mu Method sigma Method __mul__ Method __truediv__ Method __eq__ Method __lt__ Method __le__ Method __gt__ Method __ge__ Method __repr__ Method _repr_latex_ Attribute pi Attribute tau", "label": "class Gaussian(object):\n    #: Precision, the inverse of the variance.\n    pi = 0\n    #: Precision adjusted mean, the precision multiplied by the mean.\n    tau = 0\n\n    def __init__(self, mu=None, sigma=None, pi=0, tau=0):\n        if mu is not None:\n            if sigma is None:\n                raise TypeError('sigma argument is needed')\n            elif sigma == 0:\n                raise ValueError('sigma**2 should be greater than 0')\n            pi = sigma ** -2\n            tau = pi * mu\n        self.pi = pi\n        self.tau = tau\n\n    @property\n    def mu(self):\n        return self.pi and self.tau / self.pi\n\n    @property\n    def sigma(self):\n        return math.sqrt(1 / self.pi) if self.pi else inf\n\n    def __mul__(self, other):\n        pi, tau = self.pi + other.pi, self.tau + other.tau\n        return Gaussian(pi=pi, tau=tau)\n\n    def __truediv__(self, other):\n        pi, tau = self.pi - other.pi, self.tau - other.tau\n        return Gaussian(pi=pi, tau=tau)\n\n    __div__ = __truediv__  # for Python 2\n\n    def __eq__(self, other):\n        return self.pi == other.pi and self.tau == other.tau\n\n    def __lt__(self, other):\n        return self.mu < other.mu\n\n    def __le__(self, other):\n        return self.mu <= other.mu\n\n    def __gt__(self, other):\n        return self.mu > other.mu\n\n    def __ge__(self, other):\n        return self.mu >= other.mu\n\n    def __repr__(self):\n        return 'N(mu={:.3f}, sigma={:.3f})'.format(self.mu, self.sigma)\n\n    def _repr_latex_(self):\n        latex = r'\\mathcal{{ N }}( {:.3f}, {:.3f}^2 )'.format(self.mu, self.sigma)\n        return '$%s$' % latex"}
{"Repository": "HoroPCA", "input": "Stores k+1 reference points to define geodesic projections. className BSA(PCA) Method __init__ Method _project Method compute_loss Attribute hyperboloid Attribute auc", "label": "class BSA(PCA):\n    def __init__(self, dim, n_components, lr=1e-3, max_steps=100, hyperboloid=True, auc=True):\n        self.hyperboloid = hyperboloid\n        self.auc = auc\n        if self.hyperboloid:\n            super(BSA, self).__init__(dim, n_components + 1, lr, max_steps, keep_orthogonal=True)\n        else:\n            super(BSA, self).__init__(dim, n_components, lr, max_steps, keep_orthogonal=True)\n\n    def _project(self, x, Q):\n        # return poincare.orthogonal_projection(x, Q, normalized=self.keep_orthogonal)\n        proj = poincare.orthogonal_projection(x, Q, normalized=self.keep_orthogonal)\n        return proj\n\n    def compute_loss(self, x, Q):\n        if self.auc:\n            auc = []\n            if self.hyperboloid:\n                Q = hyperboloid.from_poincare(Q, ideal=True)\n                x = hyperboloid.from_poincare(x)\n\n                for i in range(1, self.n_components):\n                    Q_ = Q[:i + 1, :]\n                    proj = minkowski.orthogonal_projection(Q_, x)\n                    residual_variance = torch.sum(hyperboloid.distance(x, proj) ** 2)\n                    auc.append(residual_variance)\n            else:\n                for i in range(1, self.n_components):\n                    Q_ = Q[:i, :]\n                    proj = self._project(x, Q_)\n                    residual_variance = torch.sum(poincare.distance(x, proj) ** 2)\n                    auc.append(residual_variance)\n            return sum(auc)\n        else:\n            if self.hyperboloid:\n                Q = hyperboloid.from_poincare(Q, ideal=True)\n                x = hyperboloid.from_poincare(x)\n                proj = minkowski.orthogonal_projection(Q, x)\n                residual_variance = torch.sum(hyperboloid.distance(x, proj) ** 2)\n            else:\n                proj = self._project(x, Q)\n                residual_variance = torch.sum(poincare.distance(x, proj) ** 2)\n            return residual_variance"}
{"Repository": "SkeletonBridgeRecon", "input": "Computes and stores the average and current value className AverageValueMeter(object) Method __init__ Method reset Method update", "label": "class AverageValueMeter(object):\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count"}
{"Repository": "dynamics_sim", "input": "A class used to to represent the 2 player hawk-dove game. See U{http://www.life.umd.edu/classroom/zool360/L18-ESS/ess.html} className HawkDove(SymmetricNPlayerGame) Method __init__", "label": "class HawkDove(SymmetricNPlayerGame):\n    DEFAULT_PARAMS = dict(v=30, c=60)\n    STRATEGY_LABELS = ('Hawk', 'Dove')\n    PLAYER_LABELS = ('Player 1', 'Player 2')\n\n    def __init__(self, v, c):\n        payoff_matrix = (((v - c) / 2.0, v),\n                         (0, v / 2.0))\n\n        super(HawkDove, self).__init__(payoff_matrix, 2)"}
{"Repository": "pykafka", "input": "A response which may have a value at some point. className ResponseFuture(object) Method __init__ Method set_response Method set_error Method get Attribute handler Attribute error Attribute _ready", "label": "class ResponseFuture(object):\n    def __init__(self, handler):\n        self.handler = handler\n        self.error = False\n        self._ready = handler.Event()\n\n    def set_response(self, response):\n        self.response = response\n        self._ready.set()\n\n    def set_error(self, error):\n        self.error = error\n        self._ready.set()\n\n    def get(self, response_cls=None, timeout=None, **response_kwargs):\n        self._ready.wait(timeout)\n        if self.error:\n            raise self.error\n        if response_cls:\n            return response_cls(self.response, **response_kwargs)\n        else:\n            return self.response"}
{"Repository": "slowapi", "input": "represents a group of related limits either from a string or a callable that returns one className LimitGroup(object) Method __iter__ Method with_request", "label": "class LimitGroup(object):\n    def __init__(\n        self,\n        limit_provider: Union[str, Callable[..., str]],\n        key_function: Callable[..., str],\n        scope: Optional[Union[str, Callable[..., str]]],\n        per_method: bool,\n        methods: Optional[List[str]],\n        error_message: Optional[Union[str, Callable[..., str]]],\n        exempt_when: Optional[Callable[..., bool]],\n        cost: Union[int, Callable[..., int]],\n        override_defaults: bool,\n    ):\n        self.__limit_provider = limit_provider\n        self.__scope = scope\n        self.key_function = key_function\n        self.per_method = per_method\n        self.methods = methods and [m.lower() for m in methods] or methods\n        self.error_message = error_message\n        self.exempt_when = exempt_when\n        self.cost = cost\n        self.override_defaults = override_defaults\n        self.request = None\n\n    def __iter__(self) -> Iterator[Limit]:\n        if callable(self.__limit_provider):\n            if \"key\" in inspect.signature(self.__limit_provider).parameters.keys():\n                assert (\n                    \"request\" in inspect.signature(self.key_function).parameters.keys()\n                ), f\"Limit provider function {self.key_function.__name__} needs a `request` argument\"\n                if self.request is None:\n                    raise Exception(\"`request` object can't be None\")\n                limit_raw = self.__limit_provider(self.key_function(self.request))\n            else:\n                limit_raw = self.__limit_provider()\n        else:\n            limit_raw = self.__limit_provider\n        limit_items: List[RateLimitItem] = parse_many(limit_raw)\n        for limit in limit_items:\n            yield Limit(\n                limit,\n                self.key_function,\n                self.__scope,\n                self.per_method,\n                self.methods,\n                self.error_message,\n                self.exempt_when,\n                self.cost,\n                self.override_defaults,\n            )\n\n    def with_request(self, request):\n        self.request = request\n        return self"}
{"Repository": "RoseTTAFold2", "input": "Abstract DataModule. Children must define self.ds_{train | val | test}. className DataModule(ABC) Method __init__ Method prepare_data Method train_dataloader Method val_dataloader Method test_dataloader Attribute dataloader_kwargs Attribute ds_test", "label": "class DataModule(ABC):\n    def __init__(self, **dataloader_kwargs):\n        super().__init__()\n        if get_local_rank() == 0:\n            self.prepare_data()\n\n        # Wait until rank zero has prepared the data (download, preprocessing, ...)\n        if dist.is_initialized():\n            dist.barrier(device_ids=[get_local_rank()])\n\n        self.dataloader_kwargs = {'pin_memory': True, 'persistent_workers': dataloader_kwargs.get('num_workers', 0) > 0,\n                                  **dataloader_kwargs}\n        self.ds_train, self.ds_val, self.ds_test = None, None, None\n\n    def prepare_data(self):\n        pass\n\n    def train_dataloader(self) -> DataLoader:\n        return _get_dataloader(self.ds_train, shuffle=True, **self.dataloader_kwargs)\n\n    def val_dataloader(self) -> DataLoader:\n        return _get_dataloader(self.ds_val, shuffle=False, **self.dataloader_kwargs)\n\n    def test_dataloader(self) -> DataLoader:\n        return _get_dataloader(self.ds_test, shuffle=False, **self.dataloader_kwargs)"}
{"Repository": "FreeLB", "input": "Computes and stores the average and current value className AverageMeter(object) Method __init__ Method reset Method update", "label": "class AverageMeter(object):\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count"}
{"Repository": "layered-scene-inference", "input": "Loads a random image from the base_dir (or its subdirectories). className QueuedRandomTextureLoader(object) Method load", "label": "class QueuedRandomTextureLoader(object):\n  def __init__(self,\n               base_dir,\n               ext='.jpg',\n               batch_size=1,\n               h=None,\n               w=None,\n               nc=3,\n               split='all'):\n    self.batch_size = batch_size\n    if not base_dir.endswith('/'):\n      base_dir += '/'\n    self.img_list = []\n    self.base_dir = base_dir\n    for root, _, filenames in os.walk(base_dir):\n      for filename in fnmatch.filter(filenames, '*' + ext):\n        self.img_list.append(os.path.join(root, filename))\n\n    self.img_list.sort()\n    rng = np.random.RandomState(0)\n    rng.shuffle(self.img_list)\n    n_all = len(self.img_list)\n    n_train = int(round(0.7 * n_all))\n    n_val = int(round(0.15 * n_all))\n    if split == 'train':\n      self.img_list = self.img_list[0:n_train]\n    elif split == 'val':\n      self.img_list = self.img_list[n_train:(n_train + n_val)]\n    elif split == 'test':\n      self.img_list = self.img_list[(n_train + n_val):n_all]\n    print self.img_list[0]\n\n    # TensorFlow graph\n    log.info('Image directory : %s.', base_dir)\n    log.info('Number of Images : %d.', len(self.img_list))\n    log.info('Data Split : %s.', split)\n    print 'Image directory : %s.' % base_dir\n    print 'Number of Images : %d.' % len(self.img_list)\n    with tf.name_scope('queued_data_loader'):\n      filename_queue = tf.train.string_input_producer(\n          self.img_list, seed=0, shuffle=True)\n      image_reader = tf.WholeFileReader()\n      _, image_file = image_reader.read(filename_queue)\n      # image_file = tf.Print(image_file, [image_file_key])\n      image = tf.cast(tf.image.decode_image(image_file, channels=nc), 'float32')\n      image *= 1.0 / 255  # since images are loaded in [0, 255]\n      image = tf.slice(image, [0, 0, 0], [-1, -1, nc])\n\n      orig_shape = tf.shape(image)\n      orig_shape.set_shape((3))\n\n      image = tf.image.resize_images(\n          image, [h, w], method=tf.image.ResizeMethod.AREA)\n      image.set_shape((h, w, nc))\n\n      self.image_and_shape = tf.train.batch(\n          [image, orig_shape], batch_size=batch_size)\n\n      # Coordinate the loading of image files.\n      config = tf.ConfigProto()\n      config.gpu_options.allow_growth = True\n\n      self.tf_sess = tf.Session(config=config)\n\n      self.coord = tf.train.Coordinator()\n      self.threads = tf.train.start_queue_runners(\n          coord=self.coord, sess=self.tf_sess)\n\n    return\n\n  def load(self):\n    image_and_shape = self.tf_sess.run(self.image_and_shape)\n    return image_and_shape[0], image_and_shape[1]"}
{"Repository": "NeuralNLP-NeuralClassifier", "input": "Config load from json file className Config(object) Method __init__ Method __getitem__ Method __contains__ Method items Method add Method _update Attribute dict", "label": "class Config(object):\n    def __init__(self, config=None, config_file=None):\n        if config_file:\n            with open(config_file, 'r') as fin:\n                config = json.load(fin)\n\n        self.dict = config\n        if config:\n            self._update(config)\n\n    def __getitem__(self, key):\n        return self.dict[key]\n\n    def __contains__(self, item):\n        return item in self.dict\n\n    def items(self):\n        return self.dict.items()\n\n    def add(self, key, value):\n        self.__dict__[key] = value\n\n    def _update(self, config):\n        if not isinstance(config, dict):\n            return\n\n        for key in config:\n            if isinstance(config[key], dict):\n                config[key] = Config(config[key])\n\n            if isinstance(config[key], list):\n                config[key] = [Config(x) if isinstance(x, dict) else x for x in\n                               config[key]]\n\n        self.__dict__.update(config)"}
{"Repository": "DeepLab-v3-plus-cityscapes", "input": "InPlace Activated Batch Normalization with cross-GPU synchronization This assumes that it will be replicated across GPUs using the same mechanism as in `nn. className InPlaceABNSync(ABN) Method forward Method __repr__", "label": "class InPlaceABNSync(ABN):\n    def forward(self, x):\n        return inplace_abn_sync(x, self.weight, self.bias, self.running_mean, self.running_var,\n                                   self.training, self.momentum, self.eps, self.activation, self.slope)\n\n    def __repr__(self):\n        rep = '{name}({num_features}, eps={eps}, momentum={momentum},' \\\n              ' affine={affine}, activation={activation}'\n        if self.activation == \"leaky_relu\":\n            rep += ', slope={slope})'\n        else:\n            rep += ')'\n        return rep.format(name=self.__class__.__name__, **self.__dict__)"}
{"Repository": "fast-bert", "input": "A single training/test example for simple sequence classification. className InputExample(object) Method __init__ Attribute guid Attribute text_a Attribute text_b Attribute label Attribute label Attribute label", "label": "class InputExample(object):\n    def __init__(self, guid, text_a, text_b=None, label=None):\n        self.guid = guid\n        self.text_a = text_a\n        self.text_b = text_b\n        if isinstance(label, list):\n            self.label = label\n        elif label:\n            self.label = str(label)\n        else:\n            self.label = None"}
{"Repository": "syncthing-gtk", "input": "Little extension to install command; Allows --nostdownloader argument className BuildPyEx(build_py) Method run Method initialize_options Method _remove_module Method find_package_modules", "label": "class BuildPyEx(build_py):\n\tuser_options = build_py.user_options + [\n\t\t# Note to self: use\n\t\t# # ./setup.py build_py --nostdownloader install\n\t\t# to enable this option\n\t\t#\n\t\t('nostdownloader', None, 'prevents installing StDownloader module; disables autoupdate capability'),\n\t\t('nofinddaemon', None, 'prevents installing FindDaemonDialog module; always uses only default path to syncthig binary'),\n\t]\n\t\n\tdef run(self):\n\t\tbuild_py.run(self)\n\t\n\tdef initialize_options(self):\n\t\tbuild_py.initialize_options(self)\n\t\tself.nostdownloader = False\n\t\tself.nofinddaemon = False\n\t\n\t@staticmethod\n\tdef _remove_module(modules, to_remove):\n\t\tfor i in modules:\n\t\t\tif i[1] == to_remove:\n\t\t\t\tmodules.remove(i)\n\t\t\t\treturn\n\t\t\n\t\n\tdef find_package_modules(self, package, package_dir):\n\t\trv = build_py.find_package_modules(self, package, package_dir)\n\t\tif self.nostdownloader:\n\t\t\tBuildPyEx._remove_module(rv, \"stdownloader\")\n\t\tif self.nofinddaemon:\n\t\t\tBuildPyEx._remove_module(rv, \"finddaemondialog\")\n\t\treturn rv"}
{"Repository": "proto", "input": "Buffer to store environment transitions. className ReplayBuffer(object) Method __init__ Method __len__ Method add Method sample Attribute obs_shape Attribute action_shape Attribute capacity Attribute device Attribute obses Attribute next_obses Attribute actions Attribute rewards Attribute not_dones Attribute idx Attribute full Attribute last_save", "label": "class ReplayBuffer(object):\n    def __init__(self, obs_shape, action_shape, capacity, device):\n        self.obs_shape = obs_shape\n        self.action_shape = action_shape\n        self.capacity = capacity\n        self.device = device\n\n        self.obses = np.empty((capacity, *obs_shape), dtype=np.uint8)\n        self.next_obses = np.empty((capacity, *obs_shape), dtype=np.uint8)\n        self.actions = np.empty((capacity, *action_shape), dtype=np.float32)\n        self.rewards = np.empty((capacity, 1), dtype=np.float32)\n        self.not_dones = np.empty((self.capacity, 1), dtype=np.float32)\n\n        self.idx = 0\n        self.full = False\n        self.last_save = 0\n\n    def __len__(self):\n        return self.capacity if self.full else self.idx\n\n    def add(self, obs, action, reward, next_obs, done):\n        np.copyto(self.obses[self.idx], obs)\n        np.copyto(self.actions[self.idx], action)\n        np.copyto(self.rewards[self.idx], reward)\n        np.copyto(self.next_obses[self.idx], next_obs)\n        np.copyto(self.not_dones[self.idx], not done)\n\n        self.idx = (self.idx + 1) % self.capacity\n        self.full = self.full or self.idx == 0\n\n    def sample(self, batch_size, discount):\n        idxs = np.random.randint(0,\n                                 self.capacity if self.full else self.idx,\n                                 size=batch_size)\n\n        obses = torch.as_tensor(self.obses[idxs], device=self.device).float()\n        next_obses = torch.as_tensor(self.next_obses[idxs],\n                                     device=self.device).float()\n        actions = torch.as_tensor(self.actions[idxs], device=self.device)\n        rewards = torch.as_tensor(self.rewards[idxs], device=self.device)\n        discounts = np.ones((idxs.shape[0], 1), dtype=np.float32) * discount\n        discounts = torch.as_tensor(discounts, device=self.device)\n\n        return obses, actions, rewards, next_obses, discounts"}
{"Repository": "DensePhrases", "input": "Descriptor that mimics @property but caches output in member variable. className cached_property(property) Method __get__", "label": "class cached_property(property):\n    def __get__(self, obj, objtype=None):\n        # See docs.python.org/3/howto/descriptor.html#properties\n        if obj is None:\n            return self\n        if self.fget is None:\n            raise AttributeError(\"unreadable attribute\")\n        attr = \"__cached_\" + self.fget.__name__\n        cached = getattr(obj, attr, None)\n        if cached is None:\n            cached = self.fget(obj)\n            setattr(obj, attr, cached)\n        return cached"}
{"Repository": "rl-attack-detection", "input": "Objects that are pickled and unpickled via their constructor arguments. className EzPickle(object) Method __init__ Method __init__ Method __getstate__ Method __setstate__ Attribute _ezpickle_args Attribute _ezpickle_kwargs", "label": "class EzPickle(object):\n    def __init__(self, *args, **kwargs):\n        self._ezpickle_args = args\n        self._ezpickle_kwargs = kwargs\n\n    def __getstate__(self):\n        return {\"_ezpickle_args\": self._ezpickle_args, \"_ezpickle_kwargs\": self._ezpickle_kwargs}\n\n    def __setstate__(self, d):\n        out = type(self)(*d[\"_ezpickle_args\"], **d[\"_ezpickle_kwargs\"])\n        self.__dict__.update(out.__dict__)"}
{"Repository": "Quantsbin", "input": "This is the Garman Kohlhagen model. className GK(BSMFramework) Method __init__ Method risk_parameters Attribute instrument", "label": "class GK(BSMFramework):\n    def __init__(self, instrument, **market_kwargs):\n        self.instrument = instrument\n        super().__init__(instrument, **market_kwargs)\n\n    def risk_parameters(self):\n        risk_parameters = {\n            RiskParameter.DELTA.value: self.delta(),\n            RiskParameter.GAMMA.value: self.gamma(),\n            RiskParameter.THETA.value: self.theta(),\n            RiskParameter.VEGA.value: self.vega(),\n            RiskParameter.RHO.value: self.rho(),\n        }\n        if self.instrument.undl == UdlType.FX:\n            risk_parameters.update({RiskParameter.RHO_FOREIGN.value: self.phi()})\n        elif self.instrument.undl == UdlType.COMMODITY:\n            risk_parameters.update({RiskParameter.RHO_CONV.value: self.phi()})\n        return risk_parameters"}
{"Repository": "big-brother-bot", "input": "Represent an Alias. className Alias(Struct) Method _set_alias Method _get_alias Method _set_client_id Method _get_client_id Method _set_num_used Method _get_num_used Method _set_time_add Method _get_time_add Method _set_time_edit Method _get_time_edit Method save Method __str__", "label": "class Alias(Struct):\n    _alias = ''\n    _clientId = 0\n    _numUsed = 1\n    _timeAdd = 0\n    _timeEdit = 0\n\n    def _set_alias(self, v):\n        self._alias = v\n\n    def _get_alias(self):\n        return self._alias\n\n    alias = property(_get_alias, _set_alias)\n\n    def _set_client_id(self, v):\n        self._clientId = v\n\n    def _get_client_id(self):\n        return self._clientId\n\n    clientId = property(_get_client_id, _set_client_id)\n\n    def _set_num_used(self, v):\n        self._numUsed = v\n\n    def _get_num_used(self):\n        return self._numUsed\n\n    numUsed = property(_get_num_used, _set_num_used)\n\n    def _set_time_add(self, v):\n        self._timeAdd = int(v)\n\n    def _get_time_add(self):\n        return self._timeAdd\n\n    timeAdd = property(_get_time_add, _set_time_add)\n\n    def _set_time_edit(self, v):\n        self._timeEdit = int(v)\n\n    def _get_time_edit(self):\n        return self._timeEdit\n\n    timeEdit = property(_get_time_edit, _set_time_edit)\n\n    def save(self, console):\n        self.timeEdit = console.time()\n        if not self.id:\n            self.timeAdd = console.time()\n        return console.storage.setClientAlias(self)\n\n    def __str__(self):\n        return \"Alias(id=%s, alias=\\\"%s\\\", clientId=%s, numUsed=%s)\" % (self.id, self.alias, self.clientId, self.numUsed)"}
{"Repository": "sagemaker-run-notebook", "input": "Empty command because Command needs subclasses to override too much className BaseCommand(Command) Method initialize_options Method finalize_options Method get_inputs Method get_outputs", "label": "class BaseCommand(Command):\n    user_options = []\n\n    def initialize_options(self):\n        pass\n\n    def finalize_options(self):\n        pass\n\n    def get_inputs(self):\n        return []\n\n    def get_outputs(self):\n        return []"}
{"Repository": "SIP", "input": "Save controller values, return browser to home page. className change_values(ProtectedPage) Method GET Method change_values", "label": "class change_values(ProtectedPage):\n    def GET(self):\n        self.change_values()\n        \n    def change_values(self):    \n        qdict = web.input()\n        if \"rsn\" in qdict and qdict[\"rsn\"] == \"1\":\n            stop_stations()\n            raise web.seeother(\"/\")\n        elif \"en\" in qdict and qdict[\"en\"] == \"0\":\n            gv.srvals = [0] * (gv.sd[\"nst\"])  # turn off all stations\n            set_output()\n        if \"mm\" in qdict and qdict[\"mm\"] == \"0\":\n            clear_mm()            \n        if \"rd\" in qdict:        \n            if qdict[\"rd\"]:\n                gv.sd[\"rd\"] = int(float(qdict[\"rd\"]))\n                gv.sd[\"rdst\"] = round(gv.now + gv.sd[\"rd\"] * 3600)\n                stop_onrain()\n                report_rain_delay_change()\n            else:\n                gv.sd[\"rd\"] = 0\n                gv.sd[\"rdst\"] = 0\n                report_rain_delay_change()\n        for key in list(qdict.keys()):\n            try:\n                gv.sd[key] = int(qdict[key])\n            except Exception:\n                pass\n        jsave(gv.sd, \"sd\")\n        report_value_change()\n        raise web.seeother(\"/\")  # Send browser back to home page"}
{"Repository": "spkrepo", "input": "View for :class:`~spkrepo.models.Architecture` className ArchitectureView(ModelView) Method __init__ Method is_accessible", "label": "class ArchitectureView(ModelView):\n    def __init__(self, **kwargs):\n        super(ArchitectureView, self).__init__(Architecture, db.session, **kwargs)\n\n    # Permissions\n    def is_accessible(self):\n        return current_user.is_authenticated and current_user.has_role(\"package_admin\")\n\n    can_edit = False\n\n    can_delete = False\n\n    # Form\n    form_excluded_columns = \"builds\""}
{"Repository": "fondant", "input": "Component that transforms an incoming Dask DataFrame. className DaskTransformComponent(DaskComponent) Method transform", "label": "class DaskTransformComponent(DaskComponent):\n    @abstractmethod\n    def transform(self, dataframe: dd.DataFrame) -> dd.DataFrame:"}
{"Repository": "Sentiment-Analysis", "input": "Keeps track of most recent, average, sum, and count of a metric. className AverageMeter(object) Method __init__ Method reset Method update", "label": "class AverageMeter(object):\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count"}
{"Repository": "dotdrop", "input": "fake Options class className FakeOptions(Options) Method __init__ Attribute args Attribute log", "label": "class FakeOptions(Options):\n    def __init__(self, args):\n        self.args = args\n        self.log = Logger(debug=True)"}
{"Repository": "jiant", "input": "Multi-task Sampler with different task sampling probabilities over time We describe the individual unnormalized probabilities using numexpr expressions, using t as the variable, e. className TimeDependentProbMultiTaskSampler(BaseMultiTaskSampler) Method pop Method get_task_p Method reset_counter", "label": "class TimeDependentProbMultiTaskSampler(BaseMultiTaskSampler):\n    def __init__(\n        self,\n        task_dict: dict,\n        rng: Union[int, np.random.RandomState],\n        task_to_unnormalized_prob_funcs_dict: dict,\n        max_steps: Optional[int] = None,\n    ):\n        super().__init__(task_dict=task_dict, rng=rng)\n        assert task_dict.keys() == task_to_unnormalized_prob_funcs_dict.keys()\n        self.task_to_unnormalized_prob_funcs_dict = task_to_unnormalized_prob_funcs_dict\n        self.max_steps = max_steps\n\n        self.task_names = list(task_to_unnormalized_prob_funcs_dict.keys())\n        self.steps = 0\n\n    def pop(self):\n        if self.max_steps is not None and self.steps >= self.max_steps:\n            raise IndexError(f\"steps ({self.steps}) > max_steps ({self.max_steps})\")\n        task_name = self.rng.choice(self.task_names, p=self.get_task_p(self.steps))\n        self.steps += 1\n        return task_name, self.task_dict[task_name]\n\n    def get_task_p(self, steps=None) -> np.ndarray:\n        p_ls = np.empty(len(self.task_names))\n\n        # t is the variable in the numexpr expression\n        t = steps if steps is not None else self.steps\n\n        for i, task_name in enumerate(self.task_names):\n            p_ls[i] = numexpr.evaluate(\n                self.task_to_unnormalized_prob_funcs_dict[task_name],\n                local_dict={\"t\": t},\n            )\n        p_ls /= p_ls.sum()\n        return p_ls\n\n    def reset_counter(self):\n        self.steps = 0"}
{"Repository": "knowledge-distillation-pytorch", "input": "Tensorboard log utility className Board_Logger(object) Method __init__ Method scalar_summary Method image_summary Method histo_summary Attribute writer", "label": "class Board_Logger(object):\n    def __init__(self, log_dir):\n        self.writer = tf.summary.FileWriter(log_dir)\n\n    def scalar_summary(self, tag, value, step):\n        summary = tf.Summary(value=[tf.Summary.Value(tag=tag, simple_value=value)])\n        self.writer.add_summary(summary, step)\n\n    def image_summary(self, tag, images, step):\n        img_summaries = []\n        for i, img in enumerate(images):\n            # Write the image to a string\n            try:\n                s = StringIO()\n            except:\n                s = BytesIO()\n            scipy.misc.toimage(img).save(s, format=\"png\")\n\n            # Create an Image object\n            img_sum = tf.Summary.Image(encoded_image_string=s.getvalue(),\n                                       height=img.shape[0],\n                                       width=img.shape[1])\n            # Create a Summary value\n            img_summaries.append(tf.Summary.Value(tag='%s/%d' % (tag, i), image=img_sum))\n\n        # Create and write Summary\n        summary = tf.Summary(value=img_summaries)\n        self.writer.add_summary(summary, step)\n        \n    def histo_summary(self, tag, values, step, bins=1000):\n        # Create a histogram using numpy\n        counts, bin_edges = np.histogram(values, bins=bins)\n\n        # Fill the fields of the histogram proto\n        hist = tf.HistogramProto()\n        hist.min = float(np.min(values))\n        hist.max = float(np.max(values))\n        hist.num = int(np.prod(values.shape))\n        hist.sum = float(np.sum(values))\n        hist.sum_squares = float(np.sum(values**2))\n\n        # Drop the start of the first bin\n        bin_edges = bin_edges[1:]\n\n        # Add bin edges and counts\n        for edge in bin_edges:\n            hist.bucket_limit.append(edge)\n        for c in counts:\n            hist.bucket.append(c)\n\n        # Create and write Summary\n        summary = tf.Summary(value=[tf.Summary.Value(tag=tag, histo=hist)])\n        self.writer.add_summary(summary, step)\n        self.writer.flush()"}
{"Repository": "DeepFilterNet", "input": "Produces a csv parser that return a list of transformed elements. From python-decouple. className Csv(object) Method __call__ Method transform", "label": "class Csv(object):\n    def __init__(\n        self, cast: Type[T] = str, delimiter=\",\", strip=string.whitespace, post_process=list\n    ):\n        self.cast: Type[T] = cast\n        self.delimiter = delimiter\n        self.strip = strip\n        self.post_process = post_process\n\n    def __call__(self, value: Union[str, Tuple[T], List[T]]) -> List[T]:\n        if isinstance(value, (tuple, list)):\n            # if default value is a list\n            value = \"\".join(str(v) + self.delimiter for v in value)[:-1]\n\n        def transform(s):\n            return self.cast(s.strip(self.strip))\n\n        splitter = shlex(value, posix=True)\n        splitter.whitespace = self.delimiter\n        splitter.whitespace_split = True\n\n        return self.post_process(transform(s) for s in splitter)"}
{"Repository": "KBQA-BERT", "input": "A single set of features of data. className InputFeatures(object) Method __init__ Attribute input_ids Attribute input_mask Attribute segment_ids Attribute label_ids Attribute label_mask", "label": "class InputFeatures(object):\n    def __init__(self, input_ids, input_mask, segment_ids, label_ids, ):\n        self.input_ids = input_ids\n        self.input_mask = input_mask\n        self.segment_ids = segment_ids\n        self.label_ids = label_ids\n        # self.label_mask = label_mask"}
{"Repository": "RLcode", "input": "game server className Game(object) Method graphic Method start_play Method start_self_play Attribute board", "label": "class Game(object):\n    def __init__(self, board, **kwargs):\n        self.board = board\n\n    # \n    def graphic(self, board, player1, player2):\n        width = board.width\n        height = board.height\n\n        print(\"Player\", player1, \"with X\".rjust(3))\n        print(\"Player\", player2, \"with O\".rjust(3))\n        print()\n        for x in range(width):\n            print(\"{0:8}\".format(x), end='')\n        print('\\r\\n')\n        for i in range(height - 1, -1, -1):\n            print(\"{0:4d}\".format(i), end='')\n            for j in range(width):\n                loc = i * width + j\n                p = board.states.get(loc, -1)\n                if p == player1:\n                    print('X'.center(8), end='')\n                elif p == player2:\n                    print('O'.center(8), end='')\n                else:\n                    print('_'.center(8), end='')\n            print('\\r\\n\\r\\n')\n\n    # \n    def start_play(self, player1, player2, start_player=0, is_shown=1):\n        if start_player not in (0, 1):\n            raise Exception('start_player should be either 0 (player1 first) '\n                            'or 1 (player2 first)')\n        self.board.init_board(start_player) #\n\n        # board.playersP1=0 P2=1\n        # MCTSPlayerplayer1player2\n        p1, p2 = self.board.players\n        player1.set_player_ind(p1)\n        player2.set_player_ind(p2)\n        players = {p1: player1, p2: player2}\n\n        if is_shown:\n            self.graphic(self.board, player1.player, player2.player)\n\n        #=================================================\n        while True:\n            current_player = self.board.get_current_player()\n            player_in_turn = players[current_player] #players[]MCTSPlayer\n            move = player_in_turn.get_action(self.board) #MCTSPlayerboardgetactionmove\n            self.board.do_move(move) #\n            if is_shown:\n                self.graphic(self.board, player1.player, player2.player)\n            end, winner = self.board.game_end()\n            if end:\n                if is_shown:\n                    if winner != -1:\n                        print(\"Game end. Winner is\", players[winner])\n                    else:\n                        print(\"Game end. Tie\")\n                return winner\n\n    #self_play!!!selfplay\n    def start_self_play(self, player, is_shown=0, temp=1e-3):\n        self.board.init_board()\n        p1, p2 = self.board.players\n        states, mcts_probs, current_players = [], [], []\n\n        while True:\n            # ======get_actionstatemove\n            move, move_probs = player.get_action(self.board,\n                                                 temp=temp,\n                                                 return_prob=1)\n            \n            # ======smove_probsplayer\n            states.append(self.board.current_state())\n            mcts_probs.append(move_probs)\n            current_players.append(self.board.current_player)\n\n            # ======\n            self.board.do_move(move)\n            if is_shown:\n                self.graphic(self.board, p1, p2)\n            end, winner = self.board.game_end()\n            if end:\n                # winner from the perspective of the current player of each state\n                winners_z = np.zeros(len(current_players))\n                #winners_zwinner1-1\n                if winner != -1:\n                    winners_z[np.array(current_players) == winner] = 1.0\n                    winners_z[np.array(current_players) != winner] = -1.0\n                # reset MCTS root node\n                #\n                player.reset_player()\n\n                #show\n                if is_shown:\n                    if winner != -1:\n                        print(\"Game end. Winner is player:\", winner)\n                    else:\n                        print(\"Game end. Tie\")\n                return winner, zip(states, mcts_probs, winners_z)\n            # 1\n            # winner1P1 or P2\n            # Ndata\n            # states4xHxWstatenetworkv-preds()  p-preds()\n            # mcts_probsHxWmctsUCBp-preds\n            # winners_z s+1-1.0v-preds"}
{"Repository": "mysql-utilities", "input": "BDistDumb className BDistDumb(bdist_dumb) Method run", "label": "class BDistDumb(bdist_dumb):\n    description = bdist_dumb.description + ' (customized)'\n\n    def run(self):\n        if not self.skip_build:\n            self.run_command('build')\n\n        archive_basename = \"%s.%s\" % (self.distribution.get_fullname(),\n                                      self.plat_name)\n        pseudoinstall_root = os.path.join(self.dist_dir, archive_basename)\n\n        install = self.reinitialize_command('install', reinit_subcommands=1)\n        install.root = pseudoinstall_root\n        install.skip_build = self.skip_build\n        install.warn_dir = 0\n\n        log.info(\"installing to %s\" % self.bdist_dir)\n        self.run_command('install')\n\n        installman = self.get_finalized_command('install_man')\n        installman.root = pseudoinstall_root\n        installman.run()\n\n        # Make the archive\n        self.make_archive(pseudoinstall_root, self.format,\n                          root_dir=self.dist_dir, owner=self.owner,\n                          group=self.group, base_dir=archive_basename)\n\n        if not self.keep_temp:\n            cwd = os.getcwd()\n            os.chdir(self.dist_dir)\n            remove_tree(archive_basename, dry_run=self.dry_run)\n            os.chdir(cwd)"}
{"Repository": "game", "input": "allows for unclickable images className Images(object) Method __init__ Method draw Attribute surface Attribute imgtype Attribute img Attribute pos Attribute count Attribute imgheight Attribute rect", "label": "class Images(object):\n\tdef __init__(self, surface, image, pos=vec2d(0, 0),imgtype=\"\"):\n\t\tself.surface = surface\n\t\tself.imgtype = imgtype\n\t\tself.img = pygame.image.load(image).convert_alpha()\n\t\tself.pos = pos\n\t\tself.count = 0\n\t\tself.imgwidth, self.imgheight = self.img.get_size()\n\t\tprint \"Image dimensions are: \" + str(self.imgwidth) + \", \" + str(self.imgheight)\n\t\tself.rect = Rect(self.pos.x, self.pos.y, self.imgwidth, self.imgheight)  \n\t\t\n\tdef draw(self):\n\t\t#rotations didn't work well on diagonals. Could use a smoother method\n\t\t#but it's really only to test image draw and pause easier \n\t\tif self.imgtype == \"Spinner\":\n\t\t\tx, y, ix, iy = self.rect\n\t\t\tif self.count == 7:\n\t\t\t\tself.img= pygame.transform.rotate(self.img, -90)\n\t\t\t\tself.rect.y += self.imgheight\n\t\t\telif self.count == 15:\n\t\t\t\tself.img= pygame.transform.rotate(self.img, -90)\n\t\t\telif self.count == 22:\n\t\t\t\tself.img = pygame.transform.rotate(self.img, -90)\n\t\t\t\tself.rect.x -= self.imgheight\n\t\t\telif self.count == 30:\n\t\t\t\tself.img = pygame.transform.rotate(self.img, -90)\n\t\t\t\tself.count = 0\n\t\t\t\tself.rect.x += self.imgheight\n\t\t\t\tself.rect.y -= self.imgheight\n\t\t\tself.count += 1\n\t\tself.surface.blit(self.img, self.rect)"}
{"Repository": "ldap3", "input": "As per RFC 4512 (4.1.4) className MatchingRuleUseInfo(BaseObjectInfo) Method __repr__", "label": "class MatchingRuleUseInfo(BaseObjectInfo):\n    def __init__(self,\n                 oid=None,\n                 name=None,\n                 description=None,\n                 obsolete=False,\n                 apply_to=None,\n                 extensions=None,\n                 experimental=None,\n                 definition=None):\n        BaseObjectInfo.__init__(self,\n                                oid=oid,\n                                name=name,\n                                description=description,\n                                obsolete=obsolete,\n                                extensions=extensions,\n                                experimental=experimental,\n                                definition=definition)\n        self.apply_to = apply_to\n\n    def __repr__(self):\n        r = (linesep + '  Apply to: ' + list_to_string(self.apply_to)) if self.apply_to else ''\n        return 'Matching rule use' + BaseObjectInfo.__repr__(self).replace('<__desc__>', r)"}
{"Repository": "Cfd", "input": "QWidget for adding fluid boundary className VectorInputWidget(QWidget) Method __init__ Method valueTypeChanged Method vectorChanged Method updateUi Method vector Method setVector Attribute valueTypes Attribute componentWidget Attribute forms Attribute magNormalWidget Attribute currentValueType", "label": "class VectorInputWidget(QWidget):\n    def __init__(self, velocity, obj= None, parent=None):\n        super(VectorInputWidget, self).__init__(parent)  # for both py2 and py3\n\n        self.valueTypes = ['Cartisan components']\n\n        NumberOfComponents = 3\n        vector_config = [  ['vector'], ['vector'], ['vector'],\n                                ['Vx', 'Vy', 'Vz'],\n                                ['m/s']*NumberOfComponents,\n                                [[True]*NumberOfComponents]\n                                ]\n        self.componentWidget = InputWidget(None, vector_config)\n\n        self.forms = [self.componentWidget]\n        if within_FreeCADGui:\n            self.valueTypes += ['Magnitude and normal']\n            self.magNormalWidget = MagnitudeNormalWidget(velocity, obj, self)\n            self.forms += [self.magNormalWidget.form]\n        #create 2 widgets and select diff way\n        valueTypeTips = self.valueTypes\n        self.buttonGroupValueType, _buttonGroupLayout = _createChoiceGroup(self.valueTypes, valueTypeTips)\n        self.buttonGroupValueType.buttonClicked.connect(self.valueTypeChanged)  # diff conect syntax\n        self.currentValueType = self.valueTypes[0]\n\n        _layout = QVBoxLayout()\n        #_layout.addWidget(self.labelHelpText)\n        _layout.addLayout(_buttonGroupLayout)\n        for w in self.forms:\n            _layout.addWidget(w)\n        self.setLayout(_layout)\n\n        self.setVector(velocity)\n\n    def valueTypeChanged(self):\n        #print(self.buttonGroupValueType.checkedId())\n        self.currentValueType = self.valueTypes[self.buttonGroupValueType.checkedId()]\n        self.updateUi()\n\n    def vectorChanged(self):\n        #todo: update other form, preview vector\n        _vector = self.vector()\n        self.setVector(vector)  # will call each widget's self.updateUi()\n\n    def updateUi(self):\n        for i, form in enumerate(self.forms):\n            if i == self.buttonGroupValueType.checkedId():\n                form.setVisible(True)\n                form.updateUi()\n            else:\n                form.setVisible(False)\n\n    def vector(self):\n        if self.buttonGroupValueType.checkedId() == 0:\n            _inputs = self.componentWidget.inputSettings()\n            vector = [_inputs[k] for k in ['Vx', 'Vy', 'Vz']]\n        else:\n            if within_FreeCADGui:\n                vector = self.magNormalWidget.vector()\n        return vector\n\n    def setVector(self, vector):\n        #set vector for both widget\n        _inputs = self.componentWidget.inputSettings()\n        for i,k in enumerate(['Vx', 'Vy', 'Vz']):\n            _inputs[k] = vector[i]\n        self.componentWidget.setInputSettings(_inputs)\n        if within_FreeCADGui:\n            self.magNormalWidget.setVector(vector)\n        self.updateUi()"}
{"Repository": "envelopes", "input": "Local objects cannot manage themselves. className LocalManager(object) Method __init__ Method get_ident Method cleanup Method __repr__ Attribute locals Attribute locals Attribute locals Attribute ident_func Attribute ident_func", "label": "class LocalManager(object):\n    def __init__(self, locals=None, ident_func=None):\n        if locals is None:\n            self.locals = []\n        elif isinstance(locals, Local):\n            self.locals = [locals]\n        else:\n            self.locals = list(locals)\n        if ident_func is not None:\n            self.ident_func = ident_func\n            for local in self.locals:\n                object.__setattr__(local, '__ident_func__', ident_func)\n        else:\n            self.ident_func = get_ident\n\n    def get_ident(self):\n        return self.ident_func()\n\n    def cleanup(self):\n        for local in self.locals:\n            release_local(local)\n\n    def __repr__(self):\n        return '<%s storages: %d>' % (\n            self.__class__.__name__,\n            len(self.locals)\n        )"}
{"Repository": "bluelet", "input": "A waitable event is one encapsulating an action that can be waited for using a select() call. className WaitableEvent(Event) Method waitables Method fire", "label": "class WaitableEvent(Event):\n    def waitables(self):\n        return (), (), ()\n\n    def fire(self):\n        pass"}
{"Repository": "git-pandas", "input": "A special case where git flow is followed, so we know something about the branching scheme className GitFlowRepository(Repository) Method __init__", "label": "class GitFlowRepository(Repository):\n    def __init__(self):\n        super(GitFlowRepository, self).__init__()"}
{"Repository": "txdbus", "input": "Used to signal anticipated errors className DError(Exception) Method __init__ Method __str__ Attribute dbusErrorName Attribute errorName Attribute errorMessage", "label": "class DError(Exception):\n    def __init__(self, errorName, msg=None):\n        self.dbusErrorName = errorName\n        self.errorName = errorName\n        self.errorMessage = msg\n\n    def __str__(self):\n        return self.errorMessage"}
{"Repository": "electra", "input": "Writes pre-training examples to disk. className ExampleWriter(object) Method write_examples Method finish", "label": "class ExampleWriter(object):\n  def __init__(self, job_id, vocab_file, output_dir, max_seq_length,\n               num_jobs, blanks_separate_docs, do_lower_case,\n               num_out_files=1000, strip_accents=True):\n    self._blanks_separate_docs = blanks_separate_docs\n    tokenizer = tokenization.FullTokenizer(\n        vocab_file=vocab_file,\n        do_lower_case=do_lower_case,\n        strip_accents=strip_accents)\n    self._example_builder = ExampleBuilder(tokenizer, max_seq_length)\n    self._writers = []\n    for i in range(num_out_files):\n      if i % num_jobs == job_id:\n        output_fname = os.path.join(\n            output_dir, \"pretrain_data.tfrecord-{:}-of-{:}\".format(\n                i, num_out_files))\n        self._writers.append(tf.io.TFRecordWriter(output_fname))\n    self.n_written = 0\n\n  def write_examples(self, input_file):\n    with tf.io.gfile.GFile(input_file) as f:\n      for line in f:\n        line = line.strip()\n        if line or self._blanks_separate_docs:\n          example = self._example_builder.add_line(line)\n          if example:\n            self._writers[self.n_written % len(self._writers)].write(\n                example.SerializeToString())\n            self.n_written += 1\n      example = self._example_builder.add_line(\"\")\n      if example:\n        self._writers[self.n_written % len(self._writers)].write(\n            example.SerializeToString())\n        self.n_written += 1\n\n  def finish(self):\n    for writer in self._writers:\n      writer.close()"}
{"Repository": "sd-agent", "input": "A metric that tracks a value at particular points in time. className BucketGauge(Gauge) Method flush", "label": "class BucketGauge(Gauge):\n    def flush(self, timestamp, interval):\n        if self.value is not None:\n            res = [self.formatter(\n                metric=self.name,\n                timestamp=timestamp,\n                value=self.value,\n                tags=self.tags,\n                hostname=self.hostname,\n                device_name=self.device_name,\n                metric_type=MetricTypes.GAUGE,\n                interval=interval,\n            )]\n            self.value = None\n            return res\n\n        return []"}
{"Repository": "AutoAgents", "input": "Wrapper around SerpAPI. className SerpAPIWrapper(BaseModel) Method construct_url_and_params Method get_params Method _process_response", "label": "class SerpAPIWrapper(BaseModel):\n    search_engine: Any  #: :meta private:\n    params: dict = Field(\n        default={\n            \"engine\": \"google\",\n            \"google_domain\": \"google.com\",\n            \"gl\": \"us\",\n            \"hl\": \"en\",\n        }\n    )\n    config = Config()\n    serpapi_api_key: Optional[str] = config.serpapi_api_key\n    aiosession: Optional[aiohttp.ClientSession] = None\n\n    class Config:\n        arbitrary_types_allowed = True\n\n    async def run(self, query: str, **kwargs: Any) -> str:\n        return self._process_response(await self.results(query))\n\n    async def results(self, query: str) -> dict:\n        def construct_url_and_params() -> Tuple[str, Dict[str, str]]:\n            params = self.get_params(query)\n            params[\"source\"] = \"python\"\n            if self.serpapi_api_key:\n                params[\"serp_api_key\"] = self.serpapi_api_key\n            params[\"output\"] = \"json\"\n            url = \"https://serpapi.com/search\"\n            return url, params\n\n        url, params = construct_url_and_params()\n        if not self.aiosession:\n            async with aiohttp.ClientSession() as session:\n                async with session.get(url, params=params) as response:\n                    res = await response.json()\n        else:\n            async with self.aiosession.get(url, params=params) as response:\n                res = await response.json()\n\n        return res\n\n    def get_params(self, query: str) -> Dict[str, str]:\n        _params = {\n            \"api_key\": self.serpapi_api_key,\n            \"q\": query,\n        }\n        params = {**self.params, **_params}\n        return params\n\n    @staticmethod\n    def _process_response(res: dict) -> str:\n        # logger.debug(res)\n        focus = ['title', 'snippet', 'link']\n        get_focused = lambda x: {i: j for i, j in x.items() if i in focus}\n\n        if \"error\" in res.keys():\n            raise ValueError(f\"Got error from SerpAPI: {res['error']}\")\n        if \"answer_box\" in res.keys() and \"answer\" in res[\"answer_box\"].keys():\n            toret = res[\"answer_box\"][\"answer\"]\n        elif \"answer_box\" in res.keys() and \"snippet\" in res[\"answer_box\"].keys():\n            toret = res[\"answer_box\"][\"snippet\"]\n        elif (\n            \"answer_box\" in res.keys()\n            and \"snippet_highlighted_words\" in res[\"answer_box\"].keys()\n        ):\n            toret = res[\"answer_box\"][\"snippet_highlighted_words\"][0]\n        elif (\n            \"sports_results\" in res.keys()\n            and \"game_spotlight\" in res[\"sports_results\"].keys()\n        ):\n            toret = res[\"sports_results\"][\"game_spotlight\"]\n        elif (\n            \"knowledge_graph\" in res.keys()\n            and \"description\" in res[\"knowledge_graph\"].keys()\n        ):\n            toret = res[\"knowledge_graph\"][\"description\"]\n        elif \"snippet\" in res[\"organic_results\"][0].keys():\n            toret = res[\"organic_results\"][0][\"snippet\"]\n        else:\n            toret = \"No good search result found\"\n\n        toret_l = []\n        if \"answer_box\" in res.keys() and \"snippet\" in res[\"answer_box\"].keys():\n            toret_l += [get_focused(res[\"answer_box\"])]\n        if res.get(\"organic_results\"):\n            toret_l += [get_focused(i) for i in res.get(\"organic_results\")]\n\n        return str(toret) + '\\n' + str(toret_l)"}
{"Repository": "bert_serving", "input": "A single training/test example for simple sequence classification. className InputExample(object) Method __init__ Attribute guid Attribute text_a Attribute text_b Attribute label", "label": "class InputExample(object):\n    def __init__(self, guid, text_a, text_b=None, label=None):\n        self.guid = guid\n        self.text_a = text_a\n        self.text_b = text_b\n        self.label = label"}
{"Repository": "sqf", "input": "Raised by the parser and analyzer className SQFError(Exception) Method __init__ Attribute position Attribute message", "label": "class SQFError(Exception):\n    def __init__(self, position, message):\n        assert(isinstance(position, tuple))\n        self.position = position\n        self.message = message.replace(\"\\n\", \"\\\\n\").replace(\"\\t\", \"\\\\t\").replace(\"\\r\", \"\\\\r\")"}
{"Repository": "django-formset", "input": "Mixin class to be added to detached fields, if used outside a native Django Form. className RenderableDetachedFieldMixin(RenderableMixin) Method get_context Method as_widget Method render", "label": "class RenderableDetachedFieldMixin(RenderableMixin):\n    def get_context(self):\n        return {\n            'field': self,\n        }\n\n    def as_widget(self, widget=None, attrs=None, only_initial=False):\n        widget = widget or self.widget\n        attrs = attrs or {}\n        if self.disabled:\n            attrs['disabled'] = True\n        if '%s' in str(self.auto_id):\n            auto_id = self.auto_id % self._name\n        elif self.auto_id:\n            auto_id = self.auto_id\n        else:\n            auto_id = ''\n        if auto_id:\n            attrs['id'] = auto_id\n            if self.help_text:\n                attrs['aria-describedby'] = f'{auto_id}_help_text'\n        attrs['label'] = self._name.replace('_', ' ').title() if self.label is None else self.label\n        return widget.render(\n            name=self._name,\n            value=None,\n            attrs=attrs,\n            renderer=self.renderer,\n        )\n\n    def render(self, template_name=None, context=None, renderer=None):\n        renderer = renderer or self.renderer\n        template_name = template_name or 'formset/default/detached_field.html'\n        context = context or self.get_context()\n        return mark_safe(renderer.render(template_name, context))\n\n    __str__ = render\n    __html__ = render"}
{"Repository": "python-broqer", "input": "Filter object applied to publisher className Filter(Operator) Method get Method emit", "label": "class Filter(Operator):\n    def __init__(self, predicate: Callable[[Any], bool], *args,\n                 unpack: bool = False, **kwargs) -> None:\n        Operator.__init__(self)\n        self._predicate = partial(predicate, *args, **kwargs)  # type: Callable\n        self._unpack = unpack\n\n    def get(self) -> Any:\n        if self._originator is None:\n            raise ValueError('Operator is missing originator')\n\n        if self._subscriptions:\n            return self._state\n\n        value = self._originator.get()  # type: Any\n\n        if self._unpack:\n            # assert isinstance(value, (list, tuple))\n            if self._predicate(*value):\n                return value\n\n        elif self._predicate(value):\n            return value\n\n        return NONE\n\n    def emit(self, value: Any, who: Publisher) -> None:\n        if who is not self._originator:\n            raise ValueError('Emit from non assigned publisher')\n\n        if self._unpack:\n            if self._predicate(*value):\n                return Publisher.notify(self, value)\n        elif self._predicate(value):\n            return Publisher.notify(self, value)\n        return None"}
{"Repository": "kaggle-carvana-2017", "input": "Depthwise separable 2D convolution. className DepthwiseConv2D(Conv2D) Method build Method call Method compute_output_shape Method get_config", "label": "class DepthwiseConv2D(Conv2D):\n    def __init__(self,\n                 kernel_size,\n                 strides=(1, 1),\n                 padding='valid',\n                 depth_multiplier=1,\n                 data_format=None,\n                 activation=None,\n                 use_bias=True,\n                 depthwise_initializer='glorot_uniform',\n                 bias_initializer='zeros',\n                 depthwise_regularizer=None,\n                 bias_regularizer=None,\n                 activity_regularizer=None,\n                 depthwise_constraint=None,\n                 bias_constraint=None,\n                 **kwargs):\n        super(DepthwiseConv2D, self).__init__(\n            filters=None,\n            kernel_size=kernel_size,\n            strides=strides,\n            padding=padding,\n            data_format=data_format,\n            activation=activation,\n            use_bias=use_bias,\n            bias_regularizer=bias_regularizer,\n            activity_regularizer=activity_regularizer,\n            bias_constraint=bias_constraint,\n            **kwargs)\n        self.depth_multiplier = depth_multiplier\n        self.depthwise_initializer = initializers.get(depthwise_initializer)\n        self.depthwise_regularizer = regularizers.get(depthwise_regularizer)\n        self.depthwise_constraint = constraints.get(depthwise_constraint)\n        self.bias_initializer = initializers.get(bias_initializer)\n\n    def build(self, input_shape):\n        if len(input_shape) < 4:\n            raise ValueError('Inputs to `DepthwiseConv2D` should have rank 4. '\n                             'Received input shape:', str(input_shape))\n        if self.data_format == 'channels_first':\n            channel_axis = 1\n        else:\n            channel_axis = 3\n        if input_shape[channel_axis] is None:\n            raise ValueError('The channel dimension of the inputs to '\n                             '`DepthwiseConv2D` '\n                             'should be defined. Found `None`.')\n        input_dim = int(input_shape[channel_axis])\n        depthwise_kernel_shape = (self.kernel_size[0],\n                                  self.kernel_size[1],\n                                  input_dim,\n                                  self.depth_multiplier)\n\n        self.depthwise_kernel = self.add_weight(\n            shape=depthwise_kernel_shape,\n            initializer=self.depthwise_initializer,\n            name='depthwise_kernel',\n            regularizer=self.depthwise_regularizer,\n            constraint=self.depthwise_constraint)\n\n        if self.use_bias:\n            self.bias = self.add_weight(shape=(input_dim * self.depth_multiplier,),\n                                        initializer=self.bias_initializer,\n                                        name='bias',\n                                        regularizer=self.bias_regularizer,\n                                        constraint=self.bias_constraint)\n        else:\n            self.bias = None\n        # Set input spec.\n        self.input_spec = InputSpec(ndim=4, axes={channel_axis: input_dim})\n        self.built = True\n\n    def call(self, inputs, training=None):\n        outputs = K.depthwise_conv2d(\n            inputs,\n            self.depthwise_kernel,\n            strides=self.strides,\n            padding=self.padding,\n            dilation_rate=self.dilation_rate,\n            data_format=self.data_format)\n\n        if self.bias:\n            outputs = K.bias_add(\n                outputs,\n                self.bias,\n                data_format=self.data_format)\n\n        if self.activation is not None:\n            return self.activation(outputs)\n\n        return outputs\n\n    def compute_output_shape(self, input_shape):\n        if self.data_format == 'channels_first':\n            rows = input_shape[2]\n            cols = input_shape[3]\n            out_filters = input_shape[1] * self.depth_multiplier\n        elif self.data_format == 'channels_last':\n            rows = input_shape[1]\n            cols = input_shape[2]\n            out_filters = input_shape[3] * self.depth_multiplier\n\n        rows = conv_utils.conv_output_length(rows, self.kernel_size[0],\n                                             self.padding,\n                                             self.strides[0])\n        cols = conv_utils.conv_output_length(cols, self.kernel_size[1],\n                                             self.padding,\n                                             self.strides[1])\n\n        if self.data_format == 'channels_first':\n            return (input_shape[0], out_filters, rows, cols)\n        elif self.data_format == 'channels_last':\n            return (input_shape[0], rows, cols, out_filters)\n\n    def get_config(self):\n        config = super(DepthwiseConv2D, self).get_config()\n        config.pop('filters')\n        config.pop('kernel_initializer')\n        config.pop('kernel_regularizer')\n        config.pop('kernel_constraint')\n        config['depth_multiplier'] = self.depth_multiplier\n        config['depthwise_initializer'] = initializers.serialize(self.depthwise_initializer)\n        config['depthwise_regularizer'] = regularizers.serialize(self.depthwise_regularizer)\n        config['depthwise_constraint'] = constraints.serialize(self.depthwise_constraint)\n        return config"}
{"Repository": "react-flask-authentication", "input": "Login user by taking 'login_model' input and return JWT token className Login(Resource) Method post", "label": "class Login(Resource):\n    @rest_api.expect(login_model, validate=True)\n    def post(self):\n\n        req_data = request.get_json()\n\n        _email = req_data.get(\"email\")\n        _password = req_data.get(\"password\")\n\n        user_exists = Users.get_by_email(_email)\n\n        if not user_exists:\n            return {\"success\": False,\n                    \"msg\": \"This email does not exist.\"}, 400\n\n        if not user_exists.check_password(_password):\n            return {\"success\": False,\n                    \"msg\": \"Wrong credentials.\"}, 400\n\n        # create access token uwing JWT\n        token = jwt.encode({'email': _email, 'exp': datetime.utcnow() + timedelta(minutes=30)}, BaseConfig.SECRET_KEY)\n\n        user_exists.set_jwt_auth_active(True)\n        user_exists.save()\n\n        return {\"success\": True,\n                \"token\": token,\n                \"user\": user_exists.toJSON()}, 200"}
{"Repository": "requests3", "input": "The Base Transport Adapter className BaseAdapter(object) Method __init__ Method close", "label": "class BaseAdapter(object):\n    def __init__(self):\n        super(BaseAdapter, self).__init__()\n\n    def send(\n        self,\n        request,\n        stream=False,\n        timeout=None,\n        verify=True,\n        cert=None,\n        proxies=None,\n    ):\n        raise NotImplementedError\n\n    def close(self):\n        raise NotImplementedError"}
{"Repository": "gluino", "input": "Ram based caching This is implemented as global (per process, shared by all threads) dictionary. className CacheInRam(CacheAbstract) Method __init__ Method initialize Method clear Method increment Attribute initialized Attribute request Attribute storage", "label": "class CacheInRam(CacheAbstract):\n    locker = thread.allocate_lock()\n    meta_storage = {}\n\n    def __init__(self, request=None):\n        self.initialized = False\n        self.request = request\n        self.storage = {}\n\n    def initialize(self):\n        if self.initialized:\n            return\n        else:\n            self.initialized = True\n        self.locker.acquire()\n        request = self.request\n        if request:\n            app = request.application\n        else:\n            app = ''\n        if not app in self.meta_storage:\n            self.storage = self.meta_storage[app] = {\n                CacheAbstract.cache_stats_name: {'hit_total': 0, 'misses': 0}}\n        else:\n            self.storage = self.meta_storage[app]\n        self.locker.release()\n\n    def clear(self, regex=None):\n        self.initialize()\n        self.locker.acquire()\n        storage = self.storage\n        if regex is None:\n            storage.clear()\n        else:\n            self._clear(storage, regex)\n\n        if not CacheAbstract.cache_stats_name in storage.keys():\n            storage[CacheAbstract.cache_stats_name] = {\n                'hit_total': 0, 'misses': 0}\n\n        self.locker.release()\n\n    def __call__(self, key, f,\n                 time_expire=DEFAULT_TIME_EXPIRE,\n                 destroyer=None):\n        self.initialize()\n\n        dt = time_expire\n        now = time.time()\n\n        self.locker.acquire()\n        item = self.storage.get(key, None)\n        if item and f is None:\n            del self.storage[key]\n            if destroyer:\n                destroyer(item[1])\n        self.storage[CacheAbstract.cache_stats_name]['hit_total'] += 1\n        self.locker.release()\n\n        if f is None:\n            return None\n        if item and (dt is None or item[0] > now - dt):\n            return item[1]\n        elif item and (item[0] < now - dt) and destroyer:\n            destroyer(item[1])\n        value = f()\n\n        self.locker.acquire()\n        self.storage[key] = (now, value)\n        self.storage[CacheAbstract.cache_stats_name]['misses'] += 1\n        self.locker.release()\n        return value\n\n    def increment(self, key, value=1):\n        self.initialize()\n        self.locker.acquire()\n        try:\n            if key in self.storage:\n                value = self.storage[key][1] + value\n            self.storage[key] = (time.time(), value)\n        except BaseException, e:\n            self.locker.release()\n            raise e\n        self.locker.release()\n        return value"}
{"Repository": "pyflux", "input": "*** Adaptive Moment Estimation (ADAM) *** Computes adaptive learning rates for each parameter. className ADAM(object) Method __init__ Method update Attribute parameters Attribute f_gradient Attribute variance Attribute learning_rate Attribute ewma_1 Attribute ewma_2 Attribute epsilon Attribute t", "label": "class ADAM(object):\n    def __init__(self, starting_parameters, starting_variance, learning_rate, ewma_1, ewma_2):\n        self.parameters = starting_parameters\n        self.f_gradient = 0.0\n        self.variance = starting_variance\n        self.learning_rate = learning_rate\n        self.ewma_1 = ewma_1\n        self.ewma_2 = ewma_2\n\n        self.epsilon = np.power(10.0,-8)        \n        self.t = 1\n\n    def update(self, gradient):\n        self.f_gradient = self.ewma_1*self.f_gradient + (1-self.ewma_1)*gradient\n        f_gradient_hat = self.f_gradient / (1-np.power(self.ewma_1,self.t))\n        self.variance = self.ewma_2*self.variance + (1-self.ewma_2)*np.power(gradient,2)\n        variance_hat = self.variance / (1-np.power(self.ewma_2,self.t))\n        if self.t > 5:\n            self.parameters += (self.learning_rate+(self.learning_rate*15.0*(0.990**self.t)))*(f_gradient_hat/(np.sqrt(variance_hat)+self.epsilon))  \n        self.t += 1\n        return self.parameters"}
{"Repository": "blinkpy", "input": "Define a class for a Blink Doorbell camera. className BlinkDoorbell(BlinkCamera) Method __init__ Method arm Attribute camera_type", "label": "class BlinkDoorbell(BlinkCamera):\n    def __init__(self, sync):\n        super().__init__(sync)\n        self.camera_type = \"doorbell\"\n\n    @property\n    def arm(self):\n        return self.motion_enabled\n\n    async def async_arm(self, value):\n        url = (\n            f\"{self.sync.urls.base_url}/api/v1/accounts/\"\n            f\"{self.sync.blink.account_id}/networks/\"\n            f\"{self.sync.network_id}/doorbells/{self.camera_id}\"\n        )\n        if value:\n            url = f\"{url}/enable\"\n        else:\n            url = f\"{url}/disable\"\n\n        response = await api.http_post(self.sync.blink, url)\n        await api.wait_for_command(self.sync.blink, response)\n        return response\n\n    async def snap_picture(self):\n        url = (\n            f\"{self.sync.urls.base_url}/api/v1/accounts/\"\n            f\"{self.sync.blink.account_id}/networks/\"\n            f\"{self.sync.network_id}/doorbells/{self.camera_id}/thumbnail\"\n        )\n\n        response = await api.http_post(self.sync.blink, url)\n        await api.wait_for_command(self.sync.blink, response)\n        return response\n\n    async def get_sensor_info(self):\n    async def get_liveview(self):\n        url = (\n            f\"{self.sync.urls.base_url}/api/v1/accounts/\"\n            f\"{self.sync.blink.account_id}/networks/\"\n            f\"{self.sync.network_id}/doorbells/{self.camera_id}/liveview\"\n        )\n        response = await api.http_post(self.sync.blink, url)\n        await api.wait_for_command(self.sync.blink, response)\n        server = response[\"server\"]\n        link = server.replace(\"immis://\", \"rtsps://\")\n        return link"}
{"Repository": "lasertagger-chinese", "input": "Wrapper class that applies layer pre-processing and post-processing. className PrePostProcessingWrapper(object) Method __init__ Method __call__ Attribute layer Attribute postprocess_dropout Attribute train Attribute layer_norm", "label": "class PrePostProcessingWrapper(object):\n  def __init__(self, layer, params, train):\n    self.layer = layer\n    self.postprocess_dropout = params[\"layer_postprocess_dropout\"]\n    self.train = train\n\n    # Create normalization layer\n    self.layer_norm = LayerNormalization(params[\"hidden_size\"])\n\n  def __call__(self, x, *args, **kwargs):\n    # Preprocessing: apply layer normalization\n    y = self.layer_norm(x)\n\n    # Get layer output\n    y = self.layer(y, *args, **kwargs)\n\n    # Postprocessing: apply dropout and residual connection\n    if self.train:\n      y = tf.nn.dropout(y, 1 - self.postprocess_dropout)\n    return x + y"}
{"Repository": "concordia", "input": "This class contains the unit tests for the CompletedCampaignListView className CompletedCampaignListViewTests(TestCase) Method setUp Method test_queryset", "label": "class CompletedCampaignListViewTests(TestCase):\n    def setUp(self):\n        today = date.today()\n        self.campaign1 = create_campaign(\n            published=True, status=Campaign.Status.COMPLETED, completed_date=today\n        )\n        yesterday = today - timedelta(days=1)\n        self.campaign2 = create_campaign(\n            published=True,\n            status=Campaign.Status.COMPLETED,\n            slug=\"test-campaign-2\",\n            completed_date=yesterday,\n        )\n\n    def test_queryset(self):\n        view = CompletedCampaignListView()\n        self.assertGreater(\n            view.queryset.first().completed_date, view.queryset.last().completed_date\n        )"}
{"Repository": "psqlparse", "input": "Result target. className ResTarget(Node) Method __init__ Method tables Attribute name Attribute indirection Attribute val Attribute location", "label": "class ResTarget(Node):\n    def __init__(self, obj):\n        self.name = obj.get('name')\n        self.indirection = build_from_item(obj, 'indirection')\n        self.val = build_from_item(obj, 'val')\n        self.location = obj.get('location')\n\n    def tables(self):\n        _tables = set()\n        if isinstance(self.val, list):\n            for item in self.val:\n                _tables |= item.tables()\n        elif isinstance(self.val, Node):\n            _tables |= self.val.tables()\n\n        return _tables"}
{"Repository": "ner", "input": "BERT model (\"Bidirectional Encoder Representations from Transformers\"). className BertModel(object) Method get_pooled_output Method get_sequence_output Method get_all_encoder_layers Method get_embedding_output Method get_embedding_table", "label": "class BertModel(object):\n  def __init__(self,\n               config,\n               is_training,\n               input_ids,\n               input_mask=None,\n               token_type_ids=None,\n               use_one_hot_embeddings=False,\n               scope=None):\n    config = copy.deepcopy(config)\n    if not is_training:\n      config.hidden_dropout_prob = 0.0\n      config.attention_probs_dropout_prob = 0.0\n\n    input_shape = get_shape_list(input_ids, expected_rank=2)\n    batch_size = input_shape[0]\n    seq_length = input_shape[1]\n\n    if input_mask is None:\n      input_mask = tf.ones(shape=[batch_size, seq_length], dtype=tf.int32)\n\n    if token_type_ids is None:\n      token_type_ids = tf.zeros(shape=[batch_size, seq_length], dtype=tf.int32)\n\n    with tf.variable_scope(scope, default_name=\"bert\"):\n      with tf.variable_scope(\"embeddings\"):\n        # Perform embedding lookup on the word ids.\n        (self.embedding_output, self.embedding_table) = embedding_lookup(\n            input_ids=input_ids,\n            vocab_size=config.vocab_size,\n            embedding_size=config.hidden_size,\n            initializer_range=config.initializer_range,\n            word_embedding_name=\"word_embeddings\",\n            use_one_hot_embeddings=use_one_hot_embeddings)\n\n        # Add positional embeddings and token type embeddings, then layer\n        # normalize and perform dropout.\n        self.embedding_output = embedding_postprocessor(\n            input_tensor=self.embedding_output,\n            use_token_type=True,\n            token_type_ids=token_type_ids,\n            token_type_vocab_size=config.type_vocab_size,\n            token_type_embedding_name=\"token_type_embeddings\",\n            use_position_embeddings=True,\n            position_embedding_name=\"position_embeddings\",\n            initializer_range=config.initializer_range,\n            max_position_embeddings=config.max_position_embeddings,\n            dropout_prob=config.hidden_dropout_prob)\n\n      with tf.variable_scope(\"encoder\"):\n        # This converts a 2D mask of shape [batch_size, seq_length] to a 3D\n        # mask of shape [batch_size, seq_length, seq_length] which is used\n        # for the attention scores.\n        attention_mask = create_attention_mask_from_input_mask(\n            input_ids, input_mask)\n\n        # Run the stacked transformer.\n        # `sequence_output` shape = [batch_size, seq_length, hidden_size].\n        self.all_encoder_layers = transformer_model(\n            input_tensor=self.embedding_output,\n            attention_mask=attention_mask,\n            hidden_size=config.hidden_size,\n            num_hidden_layers=config.num_hidden_layers,\n            num_attention_heads=config.num_attention_heads,\n            intermediate_size=config.intermediate_size,\n            intermediate_act_fn=get_activation(config.hidden_act),\n            hidden_dropout_prob=config.hidden_dropout_prob,\n            attention_probs_dropout_prob=config.attention_probs_dropout_prob,\n            initializer_range=config.initializer_range,\n            do_return_all_layers=True)\n\n      self.sequence_output = self.all_encoder_layers[-1]\n      # The \"pooler\" converts the encoded sequence tensor of shape\n      # [batch_size, seq_length, hidden_size] to a tensor of shape\n      # [batch_size, hidden_size]. This is necessary for segment-level\n      # (or segment-pair-level) classification tasks where we need a fixed\n      # dimensional representation of the segment.\n      with tf.variable_scope(\"pooler\"):\n        # We \"pool\" the model by simply taking the hidden state corresponding\n        # to the first token. We assume that this has been pre-trained\n        first_token_tensor = tf.squeeze(self.sequence_output[:, 0:1, :], axis=1)\n        self.pooled_output = tf.layers.dense(\n            first_token_tensor,\n            config.hidden_size,\n            activation=tf.tanh,\n            kernel_initializer=create_initializer(config.initializer_range))\n\n  def get_pooled_output(self):\n    return self.pooled_output\n\n  def get_sequence_output(self):\n    return self.sequence_output\n\n  def get_all_encoder_layers(self):\n    return self.all_encoder_layers\n\n  def get_embedding_output(self):\n    return self.embedding_output\n\n  def get_embedding_table(self):\n    return self.embedding_table"}
{"Repository": "SyntheticTumors", "input": "Computes and stores the average and current value className AverageMeter(object) Method __init__ Method reset Method update", "label": "class AverageMeter(object):\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n\n        self.count += n\n        # self.avg = self.sum / self.count if self.count > 0 else self.sum\n        self.avg = np.where(self.count > 0, self.sum / self.count, self.sum)"}
{"Repository": "audiomate", "input": "The artist is the person/group who have produced a musical segment in a utterance. className Artist(Issuer) Method __init__ Method __str__ Method __copy__ Method __deepcopy__ Attribute name", "label": "class Artist(Issuer):\n    __slots__ = ['name']\n\n    def __init__(self, idx, name, info=None):\n        super(Artist, self).__init__(idx, info=info)\n\n        self.name = name\n\n    def __str__(self):\n        return 'Artist(idx={0}, info={1})'.format(self.idx, self.info)\n\n    def __copy__(self):\n        # self.utterances is ignored intentionally\n        # only a \"weak-ref\" when added to a corpus\n\n        cp = Artist(\n            self.idx,\n            self.name,\n            info=self.info\n        )\n\n        return cp\n\n    def __deepcopy__(self, memo):\n        # self.utterances is ignored intentionally\n        # only a \"weak-ref\" when added to a corpus\n\n        cp = Artist(\n            self.idx,\n            self.name,\n            info=copy.deepcopy(self.info, memo)\n        )\n\n        return cp"}
{"Repository": "weibo-scraper", "input": "Support setup.py upload. className UploadCommand(Command) Method status Method initialize_options Method finalize_options Method run", "label": "class UploadCommand(Command):\n    description = 'Build and publish the package.'\n    user_options = []\n\n    @staticmethod\n    def status(s):\n        print('\\033[1m{0}\\033[0m'.format(s))\n\n    def initialize_options(self):\n        pass\n\n    def finalize_options(self):\n        pass\n\n    def run(self):\n        try:\n            self.status('Removing previous builds')\n            rmtree(os.path.join(here, 'dist'))\n        except OSError:\n            pass\n\n        self.status('Building Source and Wheel (universal) distribution')\n        os.system('{0} setup.py sdist bdist_wheel --universal'.format(sys.executable))\n\n        self.status('Uploading the package to PyPi via Twine')\n        os.system('twine upload dist/*')\n\n        sys.exit()"}
{"Repository": "flowattack", "input": "Loss based on average endpoint error className EPELoss(Function) Method forward Method backward", "label": "class EPELoss(Function):\n    @staticmethod\n    def forward(ctx, input, target):\n        x = input-target\n        df = (x**2).sum(dim=1,keepdim=True)\n        ctx.saved_variable = (x,df)\n        return input.new( [ df.sqrt().sum() / (x.nelement()/2.0) ] )\n\n    @staticmethod\n    def backward(ctx, grad_output=None):\n        x,df = ctx.saved_variable\n        df_stacked = torch.cat((df,df),dim=1)\n        grad = Variable( x * df_stacked.rsqrt() / (x.nelement()/2.0) )\n        return grad*grad_output, None"}
{"Repository": "pywikibot", "input": "A Wikibase geo-shape representation. className WbGeoShape(WbDataPage) Method _get_data_site Method _get_type_specifics", "label": "class WbGeoShape(WbDataPage):\n    @classmethod\n    def _get_data_site(cls, site: DataSite) -> APISite:\n        return site.geo_shape_repository()\n\n    @classmethod\n    def _get_type_specifics(cls, site: DataSite) -> dict[str, Any]:\n        specifics = {\n            'ending': '.map',\n            'label': 'geo-shape',\n            'data_site': cls._get_data_site(site)\n        }\n        return specifics"}
{"Repository": "inql", "input": "A meta path importer to import six. className _SixMetaPathImporter(object) Method __init__ Method _add_module Method _get_module Method find_module Method find_spec Method __get_module Method load_module Method is_package Method get_code Method create_module Method exec_module Attribute name Attribute known_modules", "label": "class _SixMetaPathImporter(object):\n    def __init__(self, six_module_name):\n        self.name = six_module_name\n        self.known_modules = {}\n\n    def _add_module(self, mod, *fullnames):\n        for fullname in fullnames:\n            self.known_modules[self.name + \".\" + fullname] = mod\n\n    def _get_module(self, fullname):\n        return self.known_modules[self.name + \".\" + fullname]\n\n    def find_module(self, fullname, path=None):\n        if fullname in self.known_modules:\n            return self\n        return None\n\n    def find_spec(self, fullname, path, target=None):\n        if fullname in self.known_modules:\n            return spec_from_loader(fullname, self)\n        return None\n\n    def __get_module(self, fullname):\n        try:\n            return self.known_modules[fullname]\n        except KeyError:\n            raise ImportError(\"This loader does not know module \" + fullname)\n\n    def load_module(self, fullname):\n        try:\n            # in case of a reload\n            return sys.modules[fullname]\n        except KeyError:\n            pass\n        mod = self.__get_module(fullname)\n        if isinstance(mod, MovedModule):\n            mod = mod._resolve()\n        else:\n            mod.__loader__ = self\n        sys.modules[fullname] = mod\n        return mod\n\n    def is_package(self, fullname):\n        return hasattr(self.__get_module(fullname), \"__path__\")\n\n    def get_code(self, fullname):\n        self.__get_module(fullname)  # eventually raises ImportError\n        return None\n    get_source = get_code  # same as get_code\n\n    def create_module(self, spec):\n        return self.load_module(spec.name)\n\n    def exec_module(self, module):\n        pass"}
{"Repository": "junebug", "input": "Twisted log observer that logs to a rotated log file. className JunebugLogObserver(object) Method __init__ Method level_for_event Method logger_for_event Method _log_to_file Method __call__ Attribute worker_id Attribute log_context_sentinel Attribute log_context Attribute logfile", "label": "class JunebugLogObserver(object):\n    implements(ILogObserver)\n\n    DEFAULT_ERROR_LEVEL = logging.ERROR\n    DEFAULT_LOG_LEVEL = logging.INFO\n    LOG_LEVEL_THRESHOLD = logging.INFO\n    LOG_ENTRY = '%[(timestamp)s] '\n\n    def __init__(self, logfile, worker_id, log_context_sentinel=None):\n        '''\n        Create a new JunebugLogObserver.\n\n        :param logfile: File to write logs to.\n        :type logfile: :class:`twisted.python.logfile.LogFile`\n        :param str worker_id: ID of the worker that the log is for.\n        '''\n        if log_context_sentinel is None:\n            log_context_sentinel = DEFAULT_LOG_CONTEXT_SENTINEL\n        self.worker_id = worker_id\n        self.log_context_sentinel = log_context_sentinel\n        self.log_context = {self.log_context_sentinel: True}\n        self.logfile = logfile\n\n    def level_for_event(self, event):\n        '''Get the associated log level for an event.'''\n        level = event.get('logLevel')\n        if level is not None:\n            return level\n        if event.get('isError'):\n            return self.DEFAULT_ERROR_LEVEL\n        return self.DEFAULT_LOG_LEVEL\n\n    def logger_for_event(self, event):\n        '''Get the name of the logger for an event.'''\n        system = event.get('system')\n        logger = \".\".join(system.split(','))\n        return logger.lower()\n\n    def _log_to_file(self, event):\n        '''Logs the specified event to the log file.'''\n        level = self.level_for_event(event)\n        if level < self.LOG_LEVEL_THRESHOLD:\n            return\n\n        data = {\n            \"logger\": self.logger_for_event(event),\n            \"level\": level,\n            \"timestamp\": time.time(),\n            \"message\": log.textFromEventDict(event),\n        }\n\n        failure = event.get('failure')\n        if failure:\n            data['exception'] = {\n                'class': repr(failure.type),\n                'instance': repr(failure.value),\n                'stack': failure.stack,\n            }\n\n        self.logfile.write(json.dumps(data) + '\\n')\n\n    def __call__(self, event):\n        if self.log_context_sentinel in event:\n            return\n        if self.worker_id not in (event.get('system') or '').split(','):\n            return\n        log.callWithContext(self.log_context, self._log_to_file, event)"}
{"Repository": "cc-crawl-statistics", "input": "Dictionary with multiple counters for the same key className MultiCount(defaultdict) Method __init__ Method incr Method compress Method get_compressed Method get_count Method sum_values Attribute default_factory Attribute size", "label": "class MultiCount(defaultdict):\n    def __init__(self, size):\n        self.default_factory = lambda: [0]*size\n        self.size = size\n\n    def incr(self, key, *counts):\n        for i in range(0, self.size):\n            self[key][i] += counts[i]\n\n    @staticmethod\n    def compress(size, counts):\n        compress_from = size-1\n        last_val = counts[compress_from]\n        while compress_from > 0 and last_val == counts[compress_from-1]:\n            compress_from -= 1\n        if compress_from == 0:\n            return counts[0]\n        else:\n            return counts[0:compress_from+1]\n\n    def get_compressed(self, key):\n        return MultiCount.compress(self.size, self.get(key))\n\n    @staticmethod\n    def get_count(index, value):\n        if isinstance(value, int):\n            return value\n        if len(value) <= index:\n            return value[-1]\n        return value[index]\n\n    @staticmethod\n    def sum_values(values, compress=True):\n        counts = [0]\n        size = 1\n        for val in values:\n            if isinstance(val, int):\n                # compressed count, one unique count\n                for i in range(0, size):\n                    counts[i] += val\n            else:\n                if len(val) >= size:\n                    # enlarge counts array\n                    base_count = counts[-1]\n                    for j in range(size, len(val)):\n                        counts.append(base_count)\n                    size = len(val)\n                for i in range(0, len(val)):\n                    counts[i] += val[i]\n                if len(val) < size:\n                    for j in range(i+1, size):\n                        # add compressed counts\n                        counts[j] += val[i]\n        if compress:\n            return MultiCount.compress(size, counts)\n        else:\n            return counts"}
{"Repository": "LD-Net", "input": "evaluation class for LD-Net Parameters ---------- decoder : ``torch. className eval_wc(eval_batch) Method __init__ Method calc_score Attribute eval_b Attribute calc_s Attribute eval_b Attribute calc_s", "label": "class eval_wc(eval_batch):\n    def __init__(self, decoder, score_type):\n        eval_batch.__init__(self, decoder)\n\n        if 'f' in score_type:\n            self.eval_b = self.calc_f1_batch\n            self.calc_s = self.f1_score\n        else:\n            self.eval_b = self.calc_acc_batch\n            self.calc_s = self.acc_score\n\n    def calc_score(self, seq_model, dataset_loader):\n        seq_model.eval()\n        self.reset()\n\n        for f_c, f_p, b_c, b_p, flm_w, blm_w, blm_ind, f_w, _, f_y_m, g_y in dataset_loader:\n            scores = seq_model(f_c, f_p, b_c, b_p, flm_w, blm_w, blm_ind, f_w)\n            decoded = self.decoder.decode(scores.data, f_y_m)\n            self.eval_b(decoded, g_y)\n\n        return self.calc_s()"}
{"Repository": "canmatrix", "input": "Represents a PDU. className Pdu(object) Method add_signal Method add_signal_group Method get_signal_group_for_signal Method signal_by_name", "label": "class Pdu(object):\n    name = attr.ib(default=\"\")  # type: str\n    size = attr.ib(default=0)  # type: int\n    id = attr.ib(default=0)  # type: int\n    triggering_name = attr.ib(default=\"\")  # type: str\n    pdu_type = attr.ib(default=\"\")  # type: str\n    port_type = attr.ib(default=\"\")  # type: str\n    signals = attr.ib(factory=list)  # type: typing.MutableSequence[Signal]\n    signalGroups = attr.ib(factory=list)  # type: typing.MutableSequence[SignalGroup]\n    cycle_time = attr.ib(default=0)  # type: int\n\n    def add_signal(self, signal):\n        # type: (Signal) -> Signal\n        self.signals.append(signal)\n        return self.signals[len(self.signals) - 1]\n\n    def add_signal_group(self, Name, Id, signalNames, e2e_trans=None):\n        # type: (str, int, typing.Sequence[str]) -> None\n        newGroup = SignalGroup(Name, Id, e2e_trans=e2e_trans)\n        self.signalGroups.append(newGroup)\n        for signal in signalNames:\n            signal = signal.strip()\n            if signal.__len__() == 0:\n                continue\n            signalId = self.signal_by_name(signal)\n            if signalId is not None:\n                newGroup.add_signal(signalId)\n\n    def get_signal_group_for_signal(self, signal_to_find):\n        for signal_group in self.signalGroups:\n            for signal in signal_group:\n                if signal == signal_to_find:\n                    return signal_group\n        return None\n\n    def signal_by_name(self, name):\n        # type: (str) -> typing.Union[Signal, None]\n        for signal in self.signals:\n            if signal.name == name:\n                return signal\n        return None"}
{"Repository": "chainer-compiler", "input": "Single-GPU AlexNet without partition toward the channel axis. className AlexFp16(Alex) Method __init__ Method forward Attribute dtype Attribute conv1 Attribute conv2 Attribute conv3 Attribute conv4 Attribute conv5 Attribute fc6 Attribute fc7 Attribute fc8", "label": "class AlexFp16(Alex):\n    insize = 227\n\n    def __init__(self):\n        chainer.Chain.__init__(self)\n        self.dtype = np.float16\n        W = initializers.HeNormal(1 / np.sqrt(2), self.dtype)\n        bias = initializers.Zero(self.dtype)\n\n        with self.init_scope():\n            self.conv1 = L.Convolution2D(None, 96, 11, stride=4,\n                                         initialW=W, initial_bias=bias)\n            self.conv2 = L.Convolution2D(None, 256, 5, pad=2,\n                                         initialW=W, initial_bias=bias)\n            self.conv3 = L.Convolution2D(None, 384, 3, pad=1,\n                                         initialW=W, initial_bias=bias)\n            self.conv4 = L.Convolution2D(None, 384, 3, pad=1,\n                                         initialW=W, initial_bias=bias)\n            self.conv5 = L.Convolution2D(None, 256, 3, pad=1,\n                                         initialW=W, initial_bias=bias)\n            self.fc6 = L.Linear(None, 4096, initialW=W, initial_bias=bias)\n            self.fc7 = L.Linear(None, 4096, initialW=W, initial_bias=bias)\n            self.fc8 = L.Linear(None, 1000, initialW=W, initial_bias=bias)\n\n    def forward(self, x, t):\n        return Alex.forward(self, F.cast(x, self.dtype), t)"}
{"Repository": "Image-Background-Remover-Python", "input": "Convert ndarrays in sample to Tensors. className ToTensor(object) Method __call__", "label": "class ToTensor(object):\n\tdef __call__(self, sample):\n\n\t\timidx, image, label = sample['imidx'], sample['image'], sample['label']\n\n\t\ttmpImg = np.zeros((image.shape[0],image.shape[1],3))\n\t\ttmpLbl = np.zeros(label.shape)\n\n\t\timage = image/np.max(image)\n\t\tif(np.max(label)<1e-6):\n\t\t\tlabel = label\n\t\telse:\n\t\t\tlabel = label/np.max(label)\n\n\t\tif image.shape[2]==1:\n\t\t\ttmpImg[:,:,0] = (image[:,:,0]-0.485)/0.229\n\t\t\ttmpImg[:,:,1] = (image[:,:,0]-0.485)/0.229\n\t\t\ttmpImg[:,:,2] = (image[:,:,0]-0.485)/0.229\n\t\telse:\n\t\t\ttmpImg[:,:,0] = (image[:,:,0]-0.485)/0.229\n\t\t\ttmpImg[:,:,1] = (image[:,:,1]-0.456)/0.224\n\t\t\ttmpImg[:,:,2] = (image[:,:,2]-0.406)/0.225\n\n\t\ttmpLbl[:,:,0] = label[:,:,0]\n\n\n\t\ttmpImg = tmpImg.transpose((2, 0, 1))\n\t\ttmpLbl = label.transpose((2, 0, 1))\n\n\t\treturn {'imidx':torch.from_numpy(imidx), 'image': torch.from_numpy(tmpImg), 'label': torch.from_numpy(tmpLbl)}"}
{"Repository": "transductive-vos.pytorch", "input": "An abstract `SyncMaster` object. className SyncMaster(object) Method __init__ Method __getstate__ Method __setstate__ Method register_slave Method run_master Method nr_slaves Attribute _master_callback Attribute _queue Attribute _registry Attribute _activated", "label": "class SyncMaster(object):\n    def __init__(self, master_callback):\n        self._master_callback = master_callback\n        self._queue = queue.Queue()\n        self._registry = collections.OrderedDict()\n        self._activated = False\n\n    def __getstate__(self):\n        return {'master_callback': self._master_callback}\n\n    def __setstate__(self, state):\n        self.__init__(state['master_callback'])\n\n    def register_slave(self, identifier):\n        if self._activated:\n            assert self._queue.empty(), 'Queue is not clean before next initialization.'\n            self._activated = False\n            self._registry.clear()\n        future = FutureResult()\n        self._registry[identifier] = _MasterRegistry(future)\n        return SlavePipe(identifier, self._queue, future)\n\n    def run_master(self, master_msg):\n        self._activated = True\n\n        intermediates = [(0, master_msg)]\n        for i in range(self.nr_slaves):\n            intermediates.append(self._queue.get())\n\n        results = self._master_callback(intermediates)\n        assert results[0][0] == 0, 'The first result should belongs to the master.'\n\n        for i, res in results:\n            if i == 0:\n                continue\n            self._registry[i].result.put(res)\n\n        for i in range(self.nr_slaves):\n            assert self._queue.get() is True\n\n        return results[0][1]\n\n    @property\n    def nr_slaves(self):\n        return len(self._registry)"}
{"Repository": "dstat", "input": "Plugin for Condor queue stats className dstat_plugin(dstat) Method __init__ Method check Method extract Attribute name Attribute vars Attribute type Attribute width Attribute scale Attribute condor_config", "label": "class dstat_plugin(dstat):\n    global CONDOR_Q_STAT_PATTER\n    CONDOR_Q_STAT_PATTER = re.compile(r'(\\d+) jobs; (\\d+) idle, (\\d+) running, (\\d+) held')\n\n    def __init__(self):\n        self.name = 'condor queue'\n        self.vars = ('jobs', 'idle', 'running', 'held')\n        self.type = 'd'\n        self.width = 5\n        self.scale = 1\n        self.condor_config = None\n\n    def check(self):\n        config_file = os.environ['CONDOR_CONFIG']\n        if config_file == None:\n            raise Exception('Environment varibale CONDOR_CONFIG is missing')\n        self.condor_config = condor_classad(config_file)\n\n        bin_dir = self.condor_config['BIN']\n        if bin_dir == None:\n            raise Exception('Unable to find BIN directory in condor config file %s' % config_file)\n\n        self.condor_status_cmd = os.path.join(bin_dir, 'condor_q')\n\n        if not os.access(self.condor_status_cmd, os.X_OK):\n            raise Exception('Needs %s in the path' % self.condor_status_cmd)\n        else:\n            try:\n                p = os.popen(self.condor_status_cmd+' 2>&1 /dev/null')\n                ret = p.close()\n                if ret:\n                    raise Exception('Cannot interface with Condor - condor_q returned != 0?')\n            except IOError:\n                raise Exception('Unable to execute %s' % self.condor_status_cmd)\n            return True\n\n    def extract(self):\n        last_line = None\n\n        try:\n            for repeats in range(3):\n                for last_line in cmd_readlines(self.condor_status_cmd):\n                    pass\n\n                m = CONDOR_Q_STAT_PATTER.match(last_line)\n                if m == None:\n                    raise Exception('Invalid output from %s. Got: %s' % (cmd, last_line))\n\n                stats = [int(s.strip()) for s in m.groups()]\n                for i,j in enumerate(self.vars):\n                    self.val[j] = stats[i]\n        except Exception:\n            for name in self.vars:\n                self.val[name] = -1"}
{"Repository": "DeathSleep", "input": "UWOP_PUSH_NONVOL className PrologEpilogOpPushReg(PrologEpilogOp) Method _get_format Method __str__", "label": "class PrologEpilogOpPushReg(PrologEpilogOp):\n    def _get_format(self, unw_code):\n        return (\"UNWIND_CODE_PUSH_NONVOL\", (\"B,CodeOffset\", \"B:4,UnwindOp\", \"B:4,Reg\"))\n\n    def __str__(self):\n        return \".PUSHREG \" + REGISTERS[self.struct.Reg]"}
{"Repository": "game", "input": "A rectangular box. className Box(object) Method draw Method get_internal_rect", "label": "class Box(object):\n    def __init__(self, \n            surface,\n            rect,\n            bgcolor,\n            border_width=0,\n            border_color=Color('black')):\n        self.surface = surface\n        self.rect = rect\n        self.bgcolor = bgcolor\n        self.border_width = border_width\n        self.border_color = border_color\n        \n        # Internal rectangle\n        self.in_rect = Rect(\n            self.rect.left + self.border_width,\n            self.rect.top + self.border_width,\n            self.rect.width - self.border_width * 2,\n            self.rect.height - self.border_width * 2)\n        \n    def draw(self):\n        pygame.draw.rect(self.surface, self.border_color, self.rect)        \n        pygame.draw.rect(self.surface, self.bgcolor, self.in_rect)\n\n    def get_internal_rect(self):\n        return self.in_rect"}
{"Repository": "Improved-Body-Parts", "input": "Computes and stores the average and current value className AverageMeter(object) Method __init__ Method update Attribute val Attribute avg Attribute sum Attribute count", "label": "class AverageMeter(object):\n    def __init__(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count"}
{"Repository": "microsoft-authentication-library-for-python", "input": "It is used for handling the telemetry context for current OAuth2 \"exchange\". className _TelemetryContext(object) Method __init__ Method generate_headers Method hit_an_access_token Method update_telemetry Method _record_failure Attribute _buffer Attribute _lock Attribute _api_id Attribute _correlation_id Attribute _refresh_reason", "label": "class _TelemetryContext(object):\n    # https://identitydivision.visualstudio.com/DevEx/_git/AuthLibrariesApiReview?path=%2FTelemetry%2FMSALServerSideTelemetry.md&_a=preview\n    _SUCCEEDED = \"succeeded\"\n    _FAILED = \"failed\"\n    _FAILURE_SIZE = \"failure_size\"\n    _CURRENT_HEADER_SIZE_LIMIT = 100\n    _LAST_HEADER_SIZE_LIMIT = 350\n\n    def __init__(self, buffer, lock, api_id, correlation_id=None, refresh_reason=None):\n        self._buffer = buffer\n        self._lock = lock\n        self._api_id = api_id\n        self._correlation_id = correlation_id or _get_new_correlation_id()\n        self._refresh_reason = refresh_reason or NON_SILENT_CALL\n        logger.debug(\"Generate or reuse correlation_id: %s\", self._correlation_id)\n\n    def generate_headers(self):\n        with self._lock:\n            current = \"4|{api_id},{cache_refresh}|\".format(\n                api_id=self._api_id, cache_refresh=self._refresh_reason)\n            if len(current) > self._CURRENT_HEADER_SIZE_LIMIT:\n                logger.warning(\n                    \"Telemetry header greater than {} will be truncated by AAD\".format(\n                    self._CURRENT_HEADER_SIZE_LIMIT))\n            failures = self._buffer.get(self._FAILED, [])\n            return {\n                CLIENT_REQUEST_ID: self._correlation_id,\n                CLIENT_CURRENT_TELEMETRY: current,\n                CLIENT_LAST_TELEMETRY: \"4|{succeeded}|{failed_requests}|{errors}|\".format(\n                    succeeded=self._buffer.get(self._SUCCEEDED, 0),\n                    failed_requests=\",\".join(\"{a},{c}\".format(**f) for f in failures),\n                    errors=\",\".join(f[\"e\"] for f in failures),\n                    )\n                }\n\n    def hit_an_access_token(self):\n        with self._lock:\n            self._buffer[self._SUCCEEDED] = self._buffer.get(self._SUCCEEDED, 0) + 1\n\n    def update_telemetry(self, auth_result):\n        if auth_result:\n            with self._lock:\n                if \"error\" in auth_result:\n                    self._record_failure(auth_result[\"error\"])\n                else:  # Telemetry sent successfully. Reset buffer\n                    self._buffer.clear()  # This won't work: self._buffer = {}\n\n    def _record_failure(self, error):\n        simulation = len(\",{api_id},{correlation_id},{error}\".format(\n            api_id=self._api_id, correlation_id=self._correlation_id, error=error))\n        if self._buffer.get(self._FAILURE_SIZE, 0) + simulation < self._LAST_HEADER_SIZE_LIMIT:\n            self._buffer[self._FAILURE_SIZE] = self._buffer.get(\n                self._FAILURE_SIZE, 0) + simulation\n            self._buffer.setdefault(self._FAILED, []).append({\n                \"a\": self._api_id, \"c\": self._correlation_id, \"e\": error})"}
{"Repository": "Spatial-Temporal-Re-identification", "input": "Randomly selects a rectangle region in an image and erases its pixels. className RandomErasing(object) Method __init__ Method __call__ Attribute probability Attribute mean Attribute sl Attribute sh Attribute r1", "label": "class RandomErasing(object):\n    def __init__(self, probability = 0.5, sl = 0.02, sh = 0.4, r1 = 0.3, mean=[0.4914, 0.4822, 0.4465]):\n        self.probability = probability\n        self.mean = mean\n        self.sl = sl\n        self.sh = sh\n        self.r1 = r1\n       \n    def __call__(self, img):\n\n        if random.uniform(0, 1) > self.probability:\n            return img\n\n        for attempt in range(100):\n            area = img.size()[1] * img.size()[2]\n       \n            target_area = random.uniform(self.sl, self.sh) * area\n            aspect_ratio = random.uniform(self.r1, 1/self.r1)\n\n            h = int(round(math.sqrt(target_area * aspect_ratio)))\n            w = int(round(math.sqrt(target_area / aspect_ratio)))\n\n            if w < img.size()[2] and h < img.size()[1]:\n                x1 = random.randint(0, img.size()[1] - h)\n                y1 = random.randint(0, img.size()[2] - w)\n                if img.size()[0] == 3:\n                    img[0, x1:x1+h, y1:y1+w] = self.mean[0]\n                    img[1, x1:x1+h, y1:y1+w] = self.mean[1]\n                    img[2, x1:x1+h, y1:y1+w] = self.mean[2]\n                else:\n                    img[0, x1:x1+h, y1:y1+w] = self.mean[0]\n                return img\n\n        return img"}
{"Repository": "Deep-Reinforcement-Learning-Practice", "input": "Actor Critic Algorithms with sparse action. className AC(DRL) Method __init__ Method load Method _build_actor Method _build_critic Method _actor_loss Method discount_reward Method train Attribute actor Attribute critic Attribute gamma", "label": "class AC(DRL):\n    def __init__(self):\n        super(AC, self).__init__()\n\n        self.actor = self._build_actor()\n        self.critic = self._build_critic()\n\n        self.gamma = 0.9\n\n    def load(self):\n        if os.path.exists('model/actor_acs.h5') and os.path.exists('model/critic_acs.h5'):\n            self.actor.load_weights('model/actor_acs.h5')\n            self.critic.load_weights('model/critic_acs.h5')\n\n    def _build_actor(self):\n        inputs = Input(shape=(4,))\n        x = Dense(20, activation='relu')(inputs)\n        x = Dense(20, activation='relu')(x)\n        x = Dense(1, activation='sigmoid')(x)\n\n        model = Model(inputs=inputs, outputs=x)\n\n        return model\n\n    def _build_critic(self):\n        inputs = Input(shape=(4,))\n        x = Dense(20, activation='relu')(inputs)\n        x = Dense(20, activation='relu')(x)\n        x = Dense(1, activation='linear')(x)\n\n        model = Model(inputs=inputs, outputs=x)\n\n        return model\n\n    def _actor_loss(self, y_true, y_pred):\n        action_pred = y_pred\n        action_true, td_error = y_true[:, 0], y_true[:, 1]\n        action_true = K.reshape(action_true, (-1, 1))\n\n        loss = K.binary_crossentropy(action_true, action_pred)\n        loss = loss * K.flatten(td_error)\n\n        return loss\n\n    def discount_reward(self, next_states, reward, done):\n        q = self.critic.predict(next_states)[0][0]\n\n        target = reward\n        if not done:\n            target = reward + self.gamma * q\n\n        return target\n\n    def train(self, episode):\n        self.actor.compile(loss=self._actor_loss, optimizer=Adam(lr=0.001))\n        self.critic.compile(loss='mse', optimizer=Adam(lr=0.01))\n\n        history = {'episode': [], 'Episode_reward': [],\n                   'actor_loss': [], 'critic_loss': []}\n\n        for i in range(episode):\n            observation = self.env.reset()\n            rewards = []\n            alosses = []\n            closses = []\n\n            while True:\n                x = observation.reshape(-1, 4)\n                # choice action with prob.\n                prob = self.actor.predict(x)[0][0]\n                action = np.random.choice(np.array(range(2)), p=[1 - prob, prob])\n\n                next_observation, reward, done, _ = self.env.step(action)\n                next_observation = next_observation.reshape(-1, 4)\n                rewards.append(reward)\n\n                target = self.discount_reward(next_observation, reward, done)\n                y = np.array([target])\n\n                # TD_error = (r + gamma * next_q) - current_q\n                td_error = target - self.critic.predict(x)[0][0]\n                # loss1 = mse((r + gamma * next_q), current_q)\n                loss1 = self.critic.train_on_batch(x, y)\n\n                y = np.array([[action, td_error]])\n                loss2 = self.actor.train_on_batch(x, y)\n\n                observation = next_observation[0]\n\n                alosses.append(loss2)\n                closses.append(loss1)\n\n                if done:\n                    episode_reward = sum(rewards)\n                    aloss = np.mean(alosses)\n                    closs = np.mean(closses)\n\n                    history['episode'].append(i)\n                    history['Episode_reward'].append(episode_reward)\n                    history['actor_loss'].append(aloss)\n                    history['critic_loss'].append(closs)\n\n                    print('Episode: {} | Episode reward: {} | actor_loss: {:.3f} | critic_loss: {:.3f}'.format(i, episode_reward, aloss, closs))\n\n                    break\n\n        self.actor.save_weights('model/actor_acs.h5')\n        self.critic.save_weights('model/critic_acs.h5')\n\n        return history"}
{"Repository": "WebSocket-for-Python", "input": "Simple pool of bound websockets. className GEventWebSocketPool(Pool) Method track Method clear", "label": "class GEventWebSocketPool(Pool):\n    def track(self, websocket):\n        logger.info(\"Managing websocket %s\" % format_addresses(websocket))\n        return self.spawn(websocket.run)\n\n    def clear(self):\n        logger.info(\"Terminating server and all connected websockets\")\n        for greenlet in list(self):\n            try:\n                websocket = greenlet._run.im_self\n                if websocket:\n                    websocket.close(1001, 'Server is shutting down')\n            except:\n                pass\n            finally:\n                self.discard(greenlet)"}
{"Repository": "torchlayers", "input": "Applies a 2D global max pooling over the last dimension(s). className GlobalMaxPool2d(_GlobalPool) Method __init__ Attribute _pooling", "label": "class GlobalMaxPool2d(_GlobalPool):\n    def __init__(self):\n        super().__init__()\n        self._pooling = torch.nn.AdaptiveMaxPool2d(1)"}
{"Repository": "AdaFocusV2", "input": "Randomly horizontally flips the given PIL.Image with a probability of 0.5 className GroupRandomHorizontalFlip(object) Method __init__ Method __call__ Attribute is_flow", "label": "class GroupRandomHorizontalFlip(object):\n    def __init__(self, is_flow=False):\n        self.is_flow = is_flow\n\n    def __call__(self, img_group, is_flow=False):\n        v = random.random()\n        if v < 0.5:\n            ret = [img.transpose(Image.FLIP_LEFT_RIGHT) for img in img_group]\n            if self.is_flow:\n                for i in range(0, len(ret), 2):\n                    ret[i] = ImageOps.invert(ret[i])  # invert flow pixel values when flipping\n            return ret\n        else:\n            return img_group"}
{"Repository": "whitenoise", "input": "StaticFilesStorage subclass that compresses output files. className CompressedStaticFilesStorage(StaticFilesStorage) Method create_compressor", "label": "class CompressedStaticFilesStorage(StaticFilesStorage):\n    def post_process(\n        self, paths: dict[str, Any], dry_run: bool = False, **options: Any\n    ) -> _PostProcessT:\n        if dry_run:\n            return\n\n        extensions = getattr(settings, \"WHITENOISE_SKIP_COMPRESS_EXTENSIONS\", None)\n        compressor = self.create_compressor(extensions=extensions, quiet=True)\n\n        for path in paths:\n            if compressor.should_compress(path):\n                full_path = self.path(path)\n                prefix_len = len(full_path) - len(path)\n                for compressed_path in compressor.compress(full_path):\n                    compressed_name = compressed_path[prefix_len:]\n                    yield path, compressed_name, True\n\n    def create_compressor(self, **kwargs: Any) -> Compressor:\n        return Compressor(**kwargs)"}
{"Repository": "gamification-engine", "input": "A Reward value for an :class:`Achievement` className AchievementReward(ABase) Method get_achievement_reward", "label": "class AchievementReward(ABase):\n    @classmethod\n    def get_achievement_reward(cls, achievement_reward_id):\n        return DBSession.execute(t_achievements_rewards.select(t_achievements_rewards.c.id == achievement_reward_id)).fetchone()"}
{"Repository": "python-uurl", "input": "A dict that remembers old values for each key className MultiDict(DictMixin) Method __init__ Method __len__ Method __iter__ Method __contains__ Method __delitem__ Method keys Method __getitem__ Method __setitem__ Method append Method replace Method getall Method get Method iterallitems Attribute dict", "label": "class MultiDict(DictMixin):\n    # collections.MutableMapping would be better for Python >= 2.6\n    def __init__(self, *a, **k):\n        self.dict = dict()\n        for k, v in dict(*a, **k).iteritems():\n            self[k] = v\n\n    def __len__(self): return len(self.dict)\n    def __iter__(self): return iter(self.dict)\n    def __contains__(self, key): return key in self.dict\n    def __delitem__(self, key): del self.dict[key]\n    def keys(self): return self.dict.keys()\n    def __getitem__(self, key): return self.get(key, KeyError, -1)\n    def __setitem__(self, key, value): self.append(key, value)\n\n    def append(self, key, value): self.dict.setdefault(key, []).append(value)\n    def replace(self, key, value): self.dict[key] = [value]\n    def getall(self, key): return self.dict.get(key) or []\n\n    def get(self, key, default=None, index=-1):\n        if key not in self.dict and default != KeyError:\n            return [default][index]\n        return self.dict[key][index]\n\n    def iterallitems(self):\n        for key, values in self.dict.iteritems():\n            for value in values:\n                yield key, value"}
{"Repository": "Multitask-Emotion-Recognition-with-Incomplete-Labels", "input": "Randomly horizontally flips the given PIL.Image with a probability of 0.5 className RandomHorizontalFlip(object) Method __init__ Method __call__ Attribute v", "label": "class RandomHorizontalFlip(object):\n    def __init__(self, v):\n        self.v = v\n        return\n    def __call__(self, img):\n        if self.v < 0.5:\n            img = img.transpose(Image.FLIP_LEFT_RIGHT) \n        #print (\"horiontal flip: \",self.v)\n        return img"}
{"Repository": "E2E-TAD", "input": "Spatially rotate images. className GroupRotate(object) Method __call__ Method __repr__", "label": "class GroupRotate(object):\n    def __init__(self,\n                 limit,\n                 interpolation='bilinear',\n                 border_mode='constant',\n                 border_value=0,\n                 p=0.5):\n        if isinstance(limit, int):\n            limit = (-limit, limit)\n        self.limit = limit\n        self.interpolation = interpolation\n        self.border_mode = border_mode\n        self.border_value = border_value\n        self.p = p\n\n    def __call__(self, imgs):\n        if np.random.uniform(0, 1) <= self.p:\n            angle = np.random.uniform(*self.limit)\n            \n            imgs = [\n                image_utils.imrotate(\n                    img,\n                    angle=angle,\n                    interpolation=self.interpolation,\n                    border_mode=self.border_mode,\n                    border_value=self.border_value) for img in imgs\n            ]\n            imgs = np.array(imgs)\n\n        return imgs\n\n    def __repr__(self):\n        repr_str = self.__class__.__name__\n        repr_str += f'(limit={self.limit},'\n        repr_str += f'interpolation={self.interpolation},'\n        repr_str += f'border_mode={self.border_mode},'\n        repr_str += f'border_value={self.border_value},'\n        repr_str += f'p={self.p})'\n\n        return repr_str"}
{"Repository": "metagoofil", "input": "Unknown content with a size in bits. className RawBits(Field) Method __init__ Method hasValue Method createValue Method createDisplay", "label": "class RawBits(Field):\n    static_size = staticmethod(lambda *args, **kw: args[1])\n\n    def __init__(self, parent, name, size, description=None):\n        Field.__init__(self, parent, name, size, description)\n\n    def hasValue(self):\n        return True\n\n    def createValue(self):\n        return self._parent.stream.readBits(\n            self.absolute_address, self._size, self._parent.endian)\n\n    def createDisplay(self):\n        if self._size < config.max_bit_length:\n            return unicode(self.value)\n        else:\n            return _(\"<%s size=%u>\" %\n                (self.__class__.__name__, self._size))\n    createRawDisplay = createDisplay"}
{"Repository": "PyCDA", "input": "Given mean and std of each channel Will normalize each channel of the torch. className Normalize(object) Method __init__ Method __call__ Attribute mean Attribute std", "label": "class Normalize(object):\n    def __init__(self, mean, std=None):\n        if std is None:\n            assert len(mean) > 0\n        else:\n            assert len(mean) == len(std)\n        self.mean = mean\n        self.std = std\n\n    def __call__(self, image, label):\n        assert image.size(0) == len(self.mean)\n        if self.std is None:\n            for t, m in zip(image, self.mean):\n                t.sub_(m)\n        else:\n            for t, m, s in zip(image, self.mean, self.std):\n                t.sub_(m).div_(s)\n        return image, label"}
{"Repository": "sphinxcontrib-versioning", "input": "Similar to TemporaryDirectory in Python 3.x but with tuned weakref implementation. className TempDir(object) Method __init__ Method __enter__ Method __exit__ Method cleanup Attribute name", "label": "class TempDir(object):\n    def __init__(self, defer_atexit=False):\n        self.name = tempfile.mkdtemp('sphinxcontrib_versioning')\n        if defer_atexit:\n            atexit.register(shutil.rmtree, self.name, True)\n            return\n        try:\n            weakref.finalize(self, shutil.rmtree, self.name, True)\n        except AttributeError:\n            weakref.proxy(self, functools.partial(shutil.rmtree, self.name, True))\n\n    def __enter__(self):\n        return self.name\n\n    def __exit__(self, *_):\n        self.cleanup()\n\n    def cleanup(self):\n        shutil.rmtree(self.name, onerror=lambda *a: os.chmod(a[1], __import__('stat').S_IWRITE) or os.unlink(a[1]))\n        if os.path.exists(self.name):\n            raise IOError(17, \"File exists: '{}'\".format(self.name))"}
{"Repository": "SAN", "input": "CfgNode represents an internal node in the configuration tree. className CfgNode(dict) Method __init__ Method __getattr__ Method __setattr__ Method __str__ Method _indent Method __repr__", "label": "class CfgNode(dict):\n    def __init__(self, init_dict=None, key_list=None, new_allowed=False):\n        # Recursively convert nested dictionaries in init_dict into CfgNodes\n        init_dict = {} if init_dict is None else init_dict\n        key_list = [] if key_list is None else key_list\n        for k, v in init_dict.items():\n            if type(v) is dict:\n                # Convert dict to CfgNode\n                init_dict[k] = CfgNode(v, key_list=key_list + [k])\n        super(CfgNode, self).__init__(init_dict)\n\n    def __getattr__(self, name):\n        if name in self:\n            return self[name]\n        else:\n            raise AttributeError(name)\n\n    def __setattr__(self, name, value):\n        self[name] = value\n\n    def __str__(self):\n        def _indent(s_, num_spaces):\n            s = s_.split(\"\\n\")\n            if len(s) == 1:\n                return s_\n            first = s.pop(0)\n            s = [(num_spaces * \" \") + line for line in s]\n            s = \"\\n\".join(s)\n            s = first + \"\\n\" + s\n            return s\n\n        r = \"\"\n        s = []\n        for k, v in sorted(self.items()):\n            seperator = \"\\n\" if isinstance(v, CfgNode) else \" \"\n            attr_str = \"{}:{}{}\".format(str(k), seperator, str(v))\n            attr_str = _indent(attr_str, 2)\n            s.append(attr_str)\n        r += \"\\n\".join(s)\n        return r\n\n    def __repr__(self):\n        return \"{}({})\".format(self.__class__.__name__, super(CfgNode, self).__repr__())"}
{"Repository": "STR-TDSL", "input": "This class assigns to each predicted \"element\" (e. className Matcher(object) Method __init__ Method __call__ Method set_low_quality_matches_ Attribute high_threshold Attribute low_threshold Attribute allow_low_quality_matches", "label": "class Matcher(object):\n    BELOW_LOW_THRESHOLD = -1\n    BETWEEN_THRESHOLDS = -2\n\n    def __init__(self, high_threshold, low_threshold, allow_low_quality_matches=False):\n        assert low_threshold <= high_threshold\n        self.high_threshold = high_threshold\n        self.low_threshold = low_threshold\n        self.allow_low_quality_matches = allow_low_quality_matches\n\n    def __call__(self, match_quality_matrix):\n        if match_quality_matrix.numel() == 0:\n            # empty targets or proposals not supported during training\n            if match_quality_matrix.shape[0] == 0:\n                raise ValueError(\n                    \"No ground-truth boxes available for one of the images \"\n                    \"during training\")\n            else:\n                raise ValueError(\n                    \"No proposal boxes available for one of the images \"\n                    \"during training\")\n\n        # match_quality_matrix is M (gt) x N (predicted)\n        # Max over gt elements (dim 0) to find best gt candidate for each prediction\n        matched_vals, matches = match_quality_matrix.max(dim=0)\n        # print(matches)\n        if self.allow_low_quality_matches:\n            all_matches = matches.clone()\n\n        # Assign candidate matches with low quality to negative (unassigned) values\n        below_low_threshold = matched_vals < self.low_threshold\n        between_thresholds = (matched_vals >= self.low_threshold) & (\n            matched_vals < self.high_threshold\n        )\n        matches[below_low_threshold] = Matcher.BELOW_LOW_THRESHOLD\n        matches[between_thresholds] = Matcher.BETWEEN_THRESHOLDS\n\n        if self.allow_low_quality_matches:\n            self.set_low_quality_matches_(matches, all_matches, match_quality_matrix)\n\n        return matches\n\n    def set_low_quality_matches_(self, matches, all_matches, match_quality_matrix):\n        # For each gt, find the prediction with which it has highest quality\n        highest_quality_foreach_gt, _ = match_quality_matrix.max(dim=1)\n        # Find highest quality match available, even if it is low, including ties\n        gt_pred_pairs_of_highest_quality = torch.nonzero(\n            match_quality_matrix == highest_quality_foreach_gt[:, None]\n        )\n        # Example gt_pred_pairs_of_highest_quality:\n        #   tensor([[    0, 39796],\n        #           [    1, 32055],\n        #           [    1, 32070],\n        #           [    2, 39190],\n        #           [    2, 40255],\n        #           [    3, 40390],\n        #           [    3, 41455],\n        #           [    4, 45470],\n        #           [    5, 45325],\n        #           [    5, 46390]])\n        # Each row is a (gt index, prediction index)\n        # Note how gt items 1, 2, 3, and 5 each have two ties\n\n        pred_inds_to_update = gt_pred_pairs_of_highest_quality[:, 1]\n        matches[pred_inds_to_update] = all_matches[pred_inds_to_update]"}
{"Repository": "django-rest-framework-gis", "input": "unit tests for bbox support in restframework_gis className TestRestFrameworkGisBBox(TestCase) Method setUp Method _create_locations Method test_list Method test_post_location_list_geojson Method test_get_autogenerated_location_bbox_geojson Method test_bbox_improperly_configured", "label": "class TestRestFrameworkGisBBox(TestCase):\n    def setUp(self):\n        self.geojson_boxedlocation_list_url = reverse('api_geojson_boxedlocation_list')\n        self.geojson_location_bbox_list_url = reverse('api_geojson_location_bbox_list')\n\n    def _create_locations(self):\n        self.bl1 = BoxedLocation.objects.create(\n            id=1,\n            name='l1',\n            slug='l1',\n            geometry='POINT (13.007 42.423)',\n            bbox_geometry='POLYGON((12.997 42.413,12.997 42.433,13.017 42.433,13.017 42.413,12.997 42.413))',\n        )\n        self.bl2 = BoxedLocation.objects.create(\n            id=2,\n            name='l2',\n            slug='l2',\n            geometry='POINT (12.007 43.423)',\n            bbox_geometry='POLYGON((11.997 43.413,11.997 43.433,12.017 43.433,12.017 43.413,11.997 43.413))',\n        )\n        self.l1 = Location.objects.create(\n            id=1,\n            name='l1',\n            slug='l1',\n            geometry='POLYGON((12.997 42.413,12.997 42.433,13.017 42.433,13.017 42.413,12.997 42.413))',\n        )\n\n    def test_list(self):\n        self._create_locations()\n        response = self.client.get(self.geojson_boxedlocation_list_url)\n        self.assertEqual(response.status_code, 200)\n        self.assertEqual(len(response.data['features']), 2)\n        for feature in response.data['features']:\n            self.assertIn('bbox', feature)\n            fid = feature['id']\n            if fid == 1:\n                self.assertEqual(feature['bbox'], self.bl1.bbox_geometry.extent)\n            elif fid == 2:\n                self.assertEqual(feature['bbox'], self.bl2.bbox_geometry.extent)\n            else:\n                self.fail(\"Unexpected id: {0}\".format(fid))\n        BoxedLocation.objects.all().delete()\n\n    def test_post_location_list_geojson(self):\n        self.assertEqual(BoxedLocation.objects.count(), 0)\n        data = {\n            \"properties\": {\"name\": \"geojson input test\"},\n            \"geometry\": {\"type\": \"Point\", \"coordinates\": [12.49, 41.89]},\n            \"bbox\": [11.0, 40.0, 13.0, 42.0],\n        }\n        response = self.client.post(\n            self.geojson_boxedlocation_list_url,\n            data=json.dumps(data),\n            content_type='application/json',\n        )\n        self.assertEqual(response.status_code, 201)\n        self.assertEqual(BoxedLocation.objects.count(), 1)\n        self.assertEqual(\n            BoxedLocation.objects.all()[0].bbox_geometry.extent,\n            (11.0, 40.0, 13.0, 42.0),\n        )\n\n    def test_get_autogenerated_location_bbox_geojson(self):\n        self._create_locations()\n        response = self.client.get(self.geojson_location_bbox_list_url)\n        self.assertEqual(response.status_code, 200)\n        self.assertEqual(len(response.data['features']), 1)\n        self.assertEqual(response.data['features'][0]['bbox'], self.l1.geometry.extent)\n\n    def test_bbox_improperly_configured(self):\n        self._create_locations()\n\n        class LocationGeoFeatureSerializer(gis_serializers.GeoFeatureModelSerializer):\n            class Meta:\n                model = Location\n                geo_field = 'geometry'\n                bbox_geo_field = 'geometry'\n                auto_bbox = True\n\n        with self.assertRaises(ImproperlyConfigured):\n            LocationGeoFeatureSerializer(instance=self.l1)"}
{"Repository": "PynamoDB", "input": "Raised when an operation is attempted on a table that doesn't exist className TableDoesNotExist(PynamoDBException) Method __init__", "label": "class TableDoesNotExist(PynamoDBException):\n    def __init__(self, table_name: str) -> None:\n        msg = \"Table does not exist: `{}`\".format(table_name)\n        super(TableDoesNotExist, self).__init__(msg)"}
{"Repository": "anaconda", "input": "Metaclass for Tuple. className TupleMeta(TypingMeta) Method _has_type_var Method _eval_type Method __repr__ Method __getitem__ Method __eq__ Method __hash__ Method __instancecheck__ Method __subclasscheck__", "label": "class TupleMeta(TypingMeta):\n    def __new__(cls, name, bases, namespace, parameters=None,\n                use_ellipsis=False, _root=False):\n        self = super().__new__(cls, name, bases, namespace, _root=_root)\n        self.__tuple_params__ = parameters\n        self.__tuple_use_ellipsis__ = use_ellipsis\n        return self\n\n    def _has_type_var(self):\n        if self.__tuple_params__:\n            for t in self.__tuple_params__:\n                if _has_type_var(t):\n                    return True\n        return False\n\n    def _eval_type(self, globalns, localns):\n        tp = self.__tuple_params__\n        if tp is None:\n            return self\n        p = tuple(_eval_type(t, globalns, localns) for t in tp)\n        if p == self.__tuple_params__:\n            return self\n        else:\n            return self.__class__(self.__name__, self.__bases__, {},\n                                  p, _root=True)\n\n    def __repr__(self):\n        r = super().__repr__()\n        if self.__tuple_params__ is not None:\n            params = [_type_repr(p) for p in self.__tuple_params__]\n            if self.__tuple_use_ellipsis__:\n                params.append('...')\n            r += '[%s]' % (\n                ', '.join(params))\n        return r\n\n    def __getitem__(self, parameters):\n        if self.__tuple_params__ is not None:\n            raise TypeError(\"Cannot re-parameterize %r\" % (self,))\n        if not isinstance(parameters, tuple):\n            parameters = (parameters,)\n        if len(parameters) == 2 and parameters[1] == Ellipsis:\n            parameters = parameters[:1]\n            use_ellipsis = True\n            msg = \"Tuple[t, ...]: t must be a type.\"\n        else:\n            use_ellipsis = False\n            msg = \"Tuple[t0, t1, ...]: each t must be a type.\"\n        parameters = tuple(_type_check(p, msg) for p in parameters)\n        return self.__class__(self.__name__, self.__bases__,\n                              dict(self.__dict__), parameters,\n                              use_ellipsis=use_ellipsis, _root=True)\n\n    def __eq__(self, other):\n        if not isinstance(other, TupleMeta):\n            return NotImplemented\n        return self.__tuple_params__ == other.__tuple_params__\n\n    def __hash__(self):\n        return hash(self.__tuple_params__)\n\n    def __instancecheck__(self, obj):\n        raise TypeError(\"Tuples cannot be used with isinstance().\")\n\n    def __subclasscheck__(self, cls):\n        if cls is Any:\n            return True\n        if not isinstance(cls, type):\n            return super().__subclasscheck__(cls)  # To TypeError.\n        if issubclass(cls, tuple):\n            return True  # Special case.\n        if not isinstance(cls, TupleMeta):\n            return super().__subclasscheck__(cls)  # False.\n        if self.__tuple_params__ is None:\n            return True\n        if cls.__tuple_params__ is None:\n            return False  # ???\n        if cls.__tuple_use_ellipsis__ != self.__tuple_use_ellipsis__:\n            return False\n        # Covariance.\n        return (len(self.__tuple_params__) == len(cls.__tuple_params__) and\n                all(issubclass(x, p)\n                    for x, p in zip(cls.__tuple_params__,\n                                    self.__tuple_params__)))"}
{"Repository": "pyutil", "input": "Class for Opera className Opera(ChromiumBased) Method __init__", "label": "class Opera(ChromiumBased):\n    def __init__(self, cookie_file=None, domain_name=\"\", key_file=None):\n        args = {\n            'linux_cookies': ['~/.config/opera/Cookies'],\n            'windows_cookies': [\n                {'env': 'LOCALAPPDATA', 'path': 'Opera Software\\\\Opera Stable\\\\Cookies'},\n                {'env': 'APPDATA', 'path': 'Opera Software\\\\Opera Stable\\\\Cookies'},\n                {'env': 'CUSTOM_BROWSER_DIR', 'path': 'Opera Software\\\\Opera Stable\\\\Cookies'},\n                {'env': 'LOCALAPPDATA', 'path': 'Opera Software\\\\Opera Stable\\\\Network\\\\Cookies'},\n                {'env': 'CUSTOM_BROWSER_DIR', 'path': 'Opera Software\\\\Opera Stable\\\\Network\\\\Cookies'},\n                {'env': 'APPDATA', 'path': 'Opera Software\\\\Opera Stable\\\\Network\\\\Cookies'}\n            ],\n            'osx_cookies': ['~/Library/Application Support/com.operasoftware.Opera/Cookies'],\n            'windows_keys': [\n                {'env': 'LOCALAPPDATA', 'path': 'Opera Software\\\\Opera Stable\\\\Local State'},\n                {'env': 'CUSTOM_BROWSER_DIR', 'path': 'Opera Software\\\\Opera Stable\\\\Local State'},\n                {'env': 'APPDATA', 'path': 'Opera Software\\\\Opera Stable\\\\Local State'}\n            ],\n            'os_crypt_name': 'chromium',\n            'osx_key_service': 'Opera Safe Storage',\n            'osx_key_user': 'Opera'\n        }\n        super().__init__(browser='Opera', cookie_file=cookie_file, domain_name=domain_name, key_file=key_file, **args)"}
{"Repository": "pcl-adversarial-defense", "input": "Write console output to external text file. className Logger(object) Method __init__ Method __del__ Method __enter__ Method __exit__ Method write Method flush Method close Attribute console Attribute file Attribute file", "label": "class Logger(object):\n    def __init__(self, fpath=None):\n        self.console = sys.stdout\n        self.file = None\n        if fpath is not None:\n            mkdir_if_missing(os.path.dirname(fpath))\n            self.file = open(fpath, 'w')\n\n    def __del__(self):\n        self.close()\n\n    def __enter__(self):\n        pass\n\n    def __exit__(self, *args):\n        self.close()\n\n    def write(self, msg):\n        self.console.write(msg)\n        if self.file is not None:\n            self.file.write(msg)\n\n    def flush(self):\n        self.console.flush()\n        if self.file is not None:\n            self.file.flush()\n            os.fsync(self.file.fileno())\n\n    def close(self):\n        self.console.close()\n        if self.file is not None:\n            self.file.close()"}
{"Repository": "envelopes", "input": "Wrapper around :py:class:`smtplib.SMTP` class. className SMTP(object) Method is_connected Method _connect Method send", "label": "class SMTP(object):\n    def __init__(self, host=None, port=25, login=None, password=None,\n                 tls=False, timeout=None):\n        self._conn = None\n        self._host = host\n        self._port = port\n        self._login = login\n        self._password = password\n        self._tls = tls\n        self._timeout = timeout\n\n    @property\n    def is_connected(self):\n        try:\n            self._conn.noop()\n        except (AttributeError, smtplib.SMTPServerDisconnected):\n            return False\n        else:\n            return True\n\n    def _connect(self, replace_current=False):\n        if self._conn is None or replace_current:\n            try:\n                self._conn.quit()\n            except (AttributeError, smtplib.SMTPServerDisconnected):\n                pass\n\n            if self._timeout:\n                self._conn = smtplib.SMTP(self._host, self._port,\n                                          timeout=self._timeout)\n            else:\n                self._conn = smtplib.SMTP(self._host, self._port)\n\n        if self._tls:\n            self._conn.starttls()\n\n        if self._login:\n            self._conn.login(self._login, self._password or '')\n\n    def send(self, envelope):\n        if not self.is_connected:\n            self._connect()\n\n        msg = envelope.to_mime_message()\n        to_addrs = [envelope._addrs_to_header([addr]) for addr in envelope._to + envelope._cc + envelope._bcc]\n\n        return self._conn.sendmail(msg['From'], to_addrs, msg.as_string())"}
{"Repository": "DyCo3D", "input": "Sampler that restricts data loading to a subset of the dataset. className DistributedSampler(Sampler) Method __init__ Method __iter__ Method __len__ Method set_epoch Attribute dataset Attribute num_replicas Attribute rank Attribute epoch Attribute num_samples Attribute total_size Attribute shuffle", "label": "class DistributedSampler(Sampler):\n    def __init__(self, dataset, num_replicas=None, rank=None, shuffle=True):\n        if num_replicas is None:\n            if not dist.is_available():\n                raise RuntimeError(\"Requires distributed package to be available\")\n            num_replicas = dist.get_world_size()\n        if rank is None:\n            if not dist.is_available():\n                raise RuntimeError(\"Requires distributed package to be available\")\n            rank = dist.get_rank()\n        self.dataset = dataset\n        self.num_replicas = num_replicas\n        self.rank = rank\n        self.epoch = 0\n        self.num_samples = int(math.ceil(len(self.dataset) * 1.0 / self.num_replicas))\n        self.total_size = self.num_samples * self.num_replicas\n        self.shuffle = shuffle\n\n    def __iter__(self):\n        if self.shuffle:\n            # deterministically shuffle based on epoch\n            g = torch.Generator()\n            g.manual_seed(self.epoch)\n            indices = torch.randperm(len(self.dataset), generator=g).tolist()\n        else:\n            indices = torch.arange(len(self.dataset)).tolist()\n\n        # add extra samples to make it evenly divisible\n        indices += indices[: (self.total_size - len(indices))]\n        assert len(indices) == self.total_size\n\n        # subsample\n        offset = self.num_samples * self.rank\n        indices = indices[offset : offset + self.num_samples]\n        assert len(indices) == self.num_samples\n\n        return iter(indices)\n\n    def __len__(self):\n        return self.num_samples\n\n    def set_epoch(self, epoch):\n        self.epoch = epoch"}
{"Repository": "CapTipper", "input": "Holds imported symbol's information. className ImportData(DataContainer) Method __setattr__", "label": "class ImportData(DataContainer):\n    def __setattr__(self, name, val):\n\n        # If the instance doesn't yet have an ordinal attribute\n        # it's not fully initialized so can't do any of the\n        # following\n        #\n        if hasattr(self, 'ordinal') and hasattr(self, 'bound') and hasattr(self, 'name'):\n\n            if name == 'ordinal':\n\n                if self.pe.PE_TYPE == OPTIONAL_HEADER_MAGIC_PE:\n                    ordinal_flag = IMAGE_ORDINAL_FLAG\n                elif self.pe.PE_TYPE == OPTIONAL_HEADER_MAGIC_PE_PLUS:\n                    ordinal_flag = IMAGE_ORDINAL_FLAG64\n\n                # Set the ordinal and flag the entry as importing by ordinal\n                self.struct_table.Ordinal = ordinal_flag | (val & 0xffff)\n                self.struct_table.AddressOfData = self.struct_table.Ordinal\n                self.struct_table.Function = self.struct_table.Ordinal\n                self.struct_table.ForwarderString = self.struct_table.Ordinal\n            elif name == 'bound':\n                if self.struct_iat is not None:\n                    self.struct_iat.AddressOfData = val\n                    self.struct_iat.AddressOfData = self.struct_iat.AddressOfData\n                    self.struct_iat.Function = self.struct_iat.AddressOfData\n                    self.struct_iat.ForwarderString = self.struct_iat.AddressOfData\n            elif name == 'address':\n                self.struct_table.AddressOfData = val\n                self.struct_table.Ordinal = self.struct_table.AddressOfData\n                self.struct_table.Function = self.struct_table.AddressOfData\n                self.struct_table.ForwarderString = self.struct_table.AddressOfData\n            elif name == 'name':\n                # Make sure we reset the entry in case the import had been set to import by ordinal\n                if self.name_offset:\n\n                    name_rva = self.pe.get_rva_from_offset( self.name_offset )\n                    self.pe.set_dword_at_offset( self.ordinal_offset, (0<<31) | name_rva )\n\n                    # Complain if the length of the new name is longer than the existing one\n                    if len(val) > len(self.name):\n                        #raise Exception('The export name provided is longer than the existing one.')\n                        pass\n                    self.pe.set_bytes_at_offset( self.name_offset, val )\n\n        self.__dict__[name] = val"}
{"Repository": "antismash", "input": "Allow running tests via python setup.py test. className PyTest(TestCommand) Method finalize_options Method run_tests", "label": "class PyTest(TestCommand):\n    def finalize_options(self):\n        TestCommand.finalize_options(self)\n        self.test_args = []\n        self.test_suite = True  # pylint: disable=attribute-defined-outside-init\n\n    def run_tests(self):\n        import pytest  # pylint: disable=import-outside-toplevel\n        errcode = pytest.main(self.test_args)\n        sys.exit(errcode)"}
{"Repository": "appengine-opencv-sudoku-python", "input": "Handler to parse and solve the given sudoku puzzle image, and write the result to a GCS file. className SolveAsync(SolverBase) Method post", "label": "class SolveAsync(SolverBase):\n    def post(self):\n        image_url = self.request.get('image_url')\n        filename = self.request.get('filename')\n        image_data = None\n        try:\n            if image_url:\n                logging.info(\"image url: %s\", image_url)\n                result = urlfetch.fetch(image_url)\n                if result.status_code == 200:\n                    image_data = result.content\n            else:\n                logging.info('did not get image url...')\n        except:\n            logging.exception(\"issue fetching url data\")\n        if not image_data:\n            logging.info(\"no image data\")\n            utils.copy_error_image(filename)\n            return\n\n        self.parser = sudoku_image_parser.SudokuImageParser()\n        stringified_puzzle = ''\n        try:\n            stringified_puzzle = self.parser.parse(image_data)\n            logging.info(\"stringified puzzle: %s\", stringified_puzzle)\n        except (IndexError, sudoku_image_parser.ImageError) as e:\n            logging.debug(e)\n            utils.copy_error_image(filename)\n            return\n        try:\n            image_solution = self._solved_puzzle_image(stringified_puzzle)\n            gcs_file = utils.create_jpg_file(filename, image_solution.tostring())\n            logging.debug(\"url: %s%s\", self.api_url, gcs_file)\n            return\n        except (sudoku_solver.ContradictionError, ValueError) as e:\n            logging.debug(e)\n            utils.copy_error_image(filename)\n            return"}
{"Repository": "microbiome-tools", "input": "merge a bunch of samples into a single data frame called 'samples' [ fullTaxList, sample1Quantities, sample2Quantities, . className UbiomeMultiSample(object) Method __init__ Attribute fullTaxList Attribute samples", "label": "class UbiomeMultiSample(object):\n    def __init__(self,newSample = []):\n        self.fullTaxList = [[\"tax_name\",\"tax_rank\"]]\n        self.samples = []\n        if newSample:\n            self.fullTaxList +=newSample.taxnames()"}
{"Repository": "python36patterns", "input": "A radio.     It has a scan button, and an AM/FM toggle switch. className Radio(object) Method __init__ Method toggle_amfm Method scan Attribute amstate Attribute fmstate Attribute state", "label": "class Radio(object):\n    def __init__(self):\n        self.amstate = AmState(self)\n        self.fmstate = FmState(self)\n        self.state = self.amstate\n\n    def toggle_amfm(self):\n        self.state.toggle_amfm()\n\n    def scan(self):\n        self.state.scan()"}
{"Repository": "bt-speaker", "input": "Accepts one client unconditionally and hides the device once connected. className AutoAcceptSingleAudioAgent(BTAgent) Method __init__ Method update_discoverable Method auto_accept_one Method _watch_track Method _track_connection_state Attribute adapter Attribute allowed_uuids Attribute connected Attribute tracked_devices Attribute connect_callback Attribute disconnect_callback Attribute track_callback", "label": "class AutoAcceptSingleAudioAgent(BTAgent):\n    def __init__(self, connect_callback, disconnect_callback, track_callback):\n        BTAgent.__init__(self, default_pin_code=config.get('bluez', 'pin_code') or '0000', cb_notify_on_authorize=self.auto_accept_one)\n        self.adapter = BTAdapter(config.get('bluez', 'device_path'))\n        self.adapter.set_property('Discoverable', config.getboolean('bluez', 'discoverable'))\n        self.allowed_uuids = [ SERVICES[\"AdvancedAudioDistribution\"].uuid, SERVICES[\"AVRemoteControl\"].uuid ]\n        self.connected = None\n        self.tracked_devices =  []\n        self.connect_callback = connect_callback\n        self.disconnect_callback = disconnect_callback\n        self.track_callback = track_callback\n        self.update_discoverable()\n\n    def update_discoverable(self):\n        if not config.getboolean('bluez', 'discoverable'):\n            return\n\n        if bool(self.connected):\n            print(\"Hiding adapter from all devices.\")\n            self.adapter.set_property('Discoverable', False)\n        else:\n            print(\"Showing adapter to all devices.\")\n            self.adapter.set_property('Discoverable', True)\n\n    def auto_accept_one(self, method, device, uuid):\n        if not BTUUID(uuid).uuid in self.allowed_uuids: return False\n        if self.connected and self.connected != device:\n            print(\"Rejecting device, because another one is already connected. connected_device=%s, device=%s\" % (self.connected, device))\n            return False\n\n        # track connection state of the device (is there a better way?)\n        if not device in self.tracked_devices:\n            self.tracked_devices.append(device)\n            self.adapter._bus.add_signal_receiver(self._track_connection_state,\n                                                  path=device,\n                                                  signal_name='PropertiesChanged',\n                                                  dbus_interface='org.freedesktop.DBus.Properties',\n                                                  path_keyword='device')\n            self.adapter._bus.add_signal_receiver(self._watch_track,\n                                                  path=device + '/player0',\n                                                  signal_name='PropertiesChanged',\n                                                  dbus_interface='org.freedesktop.DBus.Properties',\n                                                  path_keyword='device')\n\n        return True\n\n    def _watch_track(self, addr, properties, signature, device):\n        if not 'Track' in properties: return\n        self.track_callback(properties['Track'])\n\n    def _track_connection_state(self, addr, properties, signature, device):\n        if self.connected and self.connected != device: return\n        if not 'Connected' in properties: return\n\n        if not self.connected and bool(properties['Connected']):\n            print(\"Device connected. device=%s\" % device)\n            self.connected = device\n            self.update_discoverable()\n            self.connect_callback()\n\n        elif self.connected and not bool(properties['Connected']):\n            print(\"Device disconnected. device=%s\" % device)\n            self.connected = None\n            self.update_discoverable()\n            self.disconnect_callback()"}
{"Repository": "PyCLUE", "input": "Runs basic tokenization (punctuation splitting, lower casing, etc.). className BasicTokenizer(object) Method __init__ Method tokenize Method _run_strip_accents Method _run_split_on_punc Method _tokenize_chars Method _clean_text Attribute do_lower_case", "label": "class BasicTokenizer(object):\n    def __init__(self, do_lower_case=True):\n        self.do_lower_case = do_lower_case\n\n\n\n    def tokenize(self, text):\n        text = convert_to_unicode(text)\n\n        text = self._clean_text(text)\n\n        text = self._tokenize_chars(text)\n\n\n\n        orig_tokens = whitespace_tokenize(text)\n\n        split_tokens = []\n\n        for token in orig_tokens:\n\n            if self.do_lower_case:\n\n                token = token.lower()\n\n                token = self._run_strip_accents(token)\n\n            split_tokens.extend(self._run_split_on_punc(token))\n\n\n\n        output_tokens = whitespace_tokenize(\" \".join(split_tokens))\n\n        return output_tokens\n\n\n\n    def _run_strip_accents(self, text):\n        text = unicodedata.normalize(\"NFD\", text)\n\n        output = []\n\n        for char in text:\n\n            cat = unicodedata.category(char)\n\n            if cat == \"Mn\":\n\n                continue\n\n            output.append(char)\n\n        return \"\".join(output)\n\n\n\n    def _run_split_on_punc(self, text):\n        chars = list(text)\n\n        i = 0\n\n        start_new_word = True\n\n        output = []\n\n        while i < len(chars):\n\n            char = chars[i]\n\n            if _is_punctuation(char):\n\n                output.append([char])\n\n                start_new_word = True\n\n            else:\n\n                if start_new_word:\n\n                    output.append([])\n\n                start_new_word = False\n\n                output[-1].append(char)\n\n            i += 1\n\n\n\n        return [\"\".join(x) for x in output]\n\n\n\n    def _tokenize_chars(self, text):\n\n        output = []\n\n        for char in text:\n\n            output.append(\" \")\n\n            output.append(char)\n\n            output.append(\" \")\n\n        return \"\".join(output)\n\n\n\n    def _clean_text(self, text):\n        output = []\n\n        for char in text:\n\n            cp = ord(char)\n\n            if cp == 0 or cp == 0xfffd or _is_control(char):\n\n                continue\n\n            if _is_whitespace(char):\n\n                output.append(\" \")\n\n            else:\n\n                output.append(char)\n\n        return \"\".join(output)"}
{"Repository": "TERAN", "input": "From a list of samples from the dataset, returns the batched captions. className TextCollator(object) Method __call__", "label": "class TextCollator(object):\n    def __call__(self, batch):\n        transposed_batch = list(zip(*batch))\n        # images = transposed_batch[0]\n        captions = transposed_batch[1]\n        return captions"}
{"Repository": "Show_and_Tell", "input": "Wrapper class for various (hyper)parameters. className Config(object) Method __init__ Attribute cnn Attribute max_caption_length Attribute dim_embedding Attribute num_lstm_units Attribute num_initalize_layers Attribute dim_initalize_layer Attribute num_attend_layers Attribute dim_attend_layer Attribute num_decode_layers Attribute dim_decode_layer Attribute fc_kernel_initializer_scale Attribute fc_kernel_regularizer_scale Attribute fc_activity_regularizer_scale Attribute conv_kernel_regularizer_scale Attribute conv_activity_regularizer_scale Attribute fc_drop_rate Attribute lstm_drop_rate Attribute attention_loss_factor Attribute num_epochs Attribute batch_size Attribute optimizer Attribute initial_learning_rate Attribute learning_rate_decay_factor Attribute num_steps_per_decay Attribute clip_gradients Attribute momentum Attribute use_nesterov Attribute decay Attribute centered Attribute beta1 Attribute beta2 Attribute epsilon Attribute save_period Attribute save_dir Attribute summary_dir Attribute vocabulary_file Attribute vocabulary_size Attribute train_image_dir Attribute train_caption_file Attribute temp_annotation_file Attribute temp_data_file Attribute eval_image_dir Attribute eval_caption_file Attribute eval_result_dir Attribute eval_result_file Attribute save_eval_result_as_image Attribute test_image_dir Attribute test_result_dir Attribute test_result_file Attribute trainable_variable", "label": "class Config(object):\n    def __init__(self):\n        # about the model architecture\n        self.cnn = 'vgg16'               # 'vgg16' or 'resnet50'\n        self.max_caption_length = 20\n        self.dim_embedding = 512\n        self.num_lstm_units = 512\n        self.num_initalize_layers = 1 ## Changed from 2 to 1    # 1 or 2\n        self.dim_initalize_layer = 512\n        self.num_attend_layers = 2       # 1 or 2\n        self.dim_attend_layer = 512\n        self.num_decode_layers = 1    ## Changed from 2 to 1   # 1 or 2\n        self.dim_decode_layer = 1024\n\n        # about the weight initialization and regularization\n        self.fc_kernel_initializer_scale = 0.08\n        self.fc_kernel_regularizer_scale = 1e-4\n        self.fc_activity_regularizer_scale = 0.0\n        self.conv_kernel_regularizer_scale = 1e-4\n        self.conv_activity_regularizer_scale = 0.0\n        self.fc_drop_rate = 0.5\n        self.lstm_drop_rate = 0.3\n        self.attention_loss_factor = 0.01\n\n        # about the optimization\n        self.num_epochs = 100\n        self.batch_size = 32\n        self.optimizer = 'Adam'    # 'Adam', 'RMSProp', 'Momentum' or 'SGD'\n        self.initial_learning_rate = 0.0001\n        self.learning_rate_decay_factor = 1.0\n        self.num_steps_per_decay = 100000\n        self.clip_gradients = 5.0\n        self.momentum = 0.0\n        self.use_nesterov = True\n        self.decay = 0.9\n        self.centered = True\n        self.beta1 = 0.9\n        self.beta2 = 0.999\n        self.epsilon = 1e-6\n\n        # about the saver\n        self.save_period = 1000\n        self.save_dir = './models/'\n        self.summary_dir = './summary/'\n\n        # about the vocabulary\n        self.vocabulary_file = './vocabulary.csv'\n        self.vocabulary_size = 5000\n\n        # about the training\n        self.train_image_dir = './train/images/'\n        self.train_caption_file = './train/captions_train2014.json'\n        self.temp_annotation_file = './train/anns.csv'\n        self.temp_data_file = './train/data.npy'\n\n        # about the evaluation\n        self.eval_image_dir = './val/images/'\n        self.eval_caption_file = './val/captions_val2014.json'\n        self.eval_result_dir = './val/results/'\n        self.eval_result_file = './val/results.json'\n        self.save_eval_result_as_image = False\n\n        # about the testing\n        self.test_image_dir = './test/images/'\n        self.test_result_dir = './test/results/'\n        self.test_result_file = './test/results.csv'\n\n        self.trainable_variable = False"}
{"Repository": "chameleon", "input": "Generate code from AST tree. className TemplateCodeGenerator(NodeTransformer) Method __init__ Method define Method require Method visit_Module Method visit_Comment Method visit_Builtin Method visit_Symbol Method visit_Static Attribute comments Attribute defines Attribute imports Attribute code", "label": "class TemplateCodeGenerator(NodeTransformer):\n    names = ()\n\n    imports: dict[type[Any] | Hashable, ast.Name]\n\n    def __init__(self, tree):\n        self.comments = []\n        self.defines = {}\n        self.imports = {}\n\n        # Run transform.\n        tree = self.visit(tree)\n\n        # Generate code.\n        code = unparse(tree)\n\n        # Fix-up comments.\n        comments = iter(self.comments)\n        code = re.sub(\n            r'^(\\s*)\\.\\.\\.$',\n            lambda m: \"\\n\".join(\n                (m.group(1) + \"#\" + line)\n                for line in next(comments).replace(\"\\r\", \"\\n\").split(\"\\n\")\n            ),\n            code,\n            flags=re.MULTILINE\n        )\n\n        self.code = code\n\n    def define(self, name, node):\n        assert node is not None\n        value = self.defines.get(name)\n\n        if value is node:\n            pass\n        elif value is None:\n            self.defines[name] = node\n        else:\n            raise CompilationError(\n                \"Duplicate symbol name for define.\", name)\n\n        return load(name)\n\n    def require(self, value: type[Any] | Hashable) -> ast.Name:\n        node = self.imports.get(value)\n        if node is None:\n            # we come up with a unique symbol based on the class name\n            name = (\n                \"_%s\"\n                % getattr(value, '__name__', str(value)).rsplit('.', 1)[-1]\n            )\n            node = load(name)\n            self.imports[value] = store(node.id)\n\n        return node\n\n    def visit_Module(self, module: Module) -> AST:\n        assert isinstance(module, Module)\n        module = super().generic_visit(module)  # type: ignore[assignment]\n        preamble: list[AST] = []\n\n        for name, node in self.defines.items():\n            assignment = Assign(targets=[store(name)], value=node, lineno=None)\n            preamble.append(self.visit(assignment))\n\n        imports: list[AST] = []\n        for value, node in self.imports.items():\n            stmt: AST\n\n            if isinstance(value, types.ModuleType):\n                stmt = Import(\n                    names=[alias(name=value.__name__, asname=node.id)])\n            elif hasattr(value, '__name__'):\n                path = reverse_builtin_map.get(value)\n                if path is None:\n                    path = value.__module__\n                    name = value.__name__\n                stmt = ImportFrom(\n                    module=path,\n                    names=[alias(name=name, asname=node.id)],\n                    level=0,\n                )\n            else:\n                raise TypeError(value)\n\n            imports.append(stmt)\n\n        return Module(imports + preamble + module.body, ())\n\n    def visit_Comment(self, node: Comment) -> AST:\n        self.comments.append(node.text)\n        return Expr(Constant(...))\n\n    def visit_Builtin(self, node: Builtin) -> AST:\n        name = load(node.id)\n        return self.visit(name)  # type: ignore[no-any-return]\n\n    def visit_Symbol(self, node: Symbol) -> AST:\n        return self.require(node.value)\n\n    def visit_Static(self, node: Static) -> AST:\n        if node.name is None:\n            name = \"_static_%s\" % str(id(node.value)).replace('-', '_')\n        else:\n            name = node.name\n        node = self.define(name, node.value)\n        return self.visit(node)  # type: ignore[no-any-return]"}
{"Repository": "teach", "input": "vocab.Vocab with a lock for parallel computations. className VocabWithLock(VocabBase) Method __init__ Method word2index Attribute lock", "label": "class VocabWithLock(VocabBase):\n    def __init__(self, words=(), lock=None):\n        self.lock = lock\n        super().__init__(words)\n\n    def word2index(self, word, train=False):\n        if isinstance(word, (list, tuple)):\n            return [self.word2index(w, train=train) for w in word]\n        with self.lock:\n            self.counts[word] += train\n            if word in self._word2index:\n                return self._word2index[word]\n            else:\n                if train:\n                    self._index2word += [word]\n                    self._word2index[word] = len(self._word2index)\n                else:\n                    return self._handle_oov_word(word)\n            index = self._word2index[word]\n        return index"}
{"Repository": "cstar", "input": "Custom command to verify that the git tag matches our version className VerifyVersionCommand(install) Method run", "label": "class VerifyVersionCommand(install):\n    description = 'verify that the git tag matches our version'\n\n    def run(self):\n        tag = os.getenv('CIRCLE_TAG')\n\n        if tag != VERSION:\n            info = \"Git tag: {0} does not match the version of this app: {1}\".format(\n                tag, VERSION\n            )\n            sys.exit(info)"}
{"Repository": "ecm", "input": "Basic LSTM recurrent network cell. className BasicLSTMCell(RNNCell) Method state_size Method output_size Method __call__", "label": "class BasicLSTMCell(RNNCell):\n    def __init__(self, num_units, forget_bias=1.0, input_size=None,\n                             state_is_tuple=True, activation=tanh):\n        if not state_is_tuple:\n            logging.warn(\"%s: Using a concatenated state is slower and will soon be \"\n                                     \"deprecated.    Use state_is_tuple=True.\", self)\n        if input_size is not None:\n            logging.warn(\"%s: The input_size parameter is deprecated.\", self)\n        self._num_units = num_units\n        self._forget_bias = forget_bias\n        self._state_is_tuple = state_is_tuple\n        self._activation = activation\n\n    @property\n    def state_size(self):\n        return (LSTMStateTuple(self._num_units, self._num_units)\n                        if self._state_is_tuple else 2 * self._num_units)\n\n    @property\n    def output_size(self):\n        return self._num_units\n\n    def __call__(self, inputs, state, scope=None):\n        with vs.variable_scope(scope or type(self).__name__):    # \"BasicLSTMCell\"\n            # Parameters of gates are concatenated into one multiply for efficiency.\n            if self._state_is_tuple:\n                c, h = state\n            else:\n                c, h = array_ops.split(1, 2, state)\n            concat = _linear([inputs, h], 4 * self._num_units, True)\n\n            # i = input_gate, j = new_input, f = forget_gate, o = output_gate\n            i, j, f, o = array_ops.split(1, 4, concat)\n\n            new_c = (c * sigmoid(f + self._forget_bias) + sigmoid(i) *\n                             self._activation(j))\n            new_h = self._activation(new_c) * sigmoid(o)\n\n            if self._state_is_tuple:\n                new_state = LSTMStateTuple(new_c, new_h)\n            else:\n                new_state = array_ops.concat(1, [new_c, new_h])\n            return new_h, new_state"}
{"Repository": "fast-depth", "input": "Normalize a ``numpy. className NormalizeNumpyArray(object) Method __init__ Method __call__ Attribute mean Attribute std", "label": "class NormalizeNumpyArray(object):\n    def __init__(self, mean, std):\n        self.mean = mean\n        self.std = std\n\n    def __call__(self, img):\n        if not(_is_numpy_image(img)):\n            raise TypeError('img should be ndarray. Got {}'.format(type(img)))\n        # TODO: make efficient\n        print(img.shape)\n        for i in range(3):\n            img[:,:,i] = (img[:,:,i] - self.mean[i]) / self.std[i]\n        return img"}
{"Repository": "fusee-interfacee-tk", "input": "Simple vulnerability trigger for macOS: we simply ask libusb to issue the broken control request, and it'll do it for us. className MacOSBackend(HaxBackend) Method trigger_vulnerability", "label": "class MacOSBackend(HaxBackend):\n    BACKEND_NAME = \"macOS\"\n    SUPPORTED_SYSTEMS = ['Darwin', 'libusbhax', 'macos', 'FreeBSD']\n\n    def trigger_vulnerability(self, length):\n\n        # Triggering the vulnerability is simplest on macOS; we simply issue the control request as-is.\n        return self.dev.ctrl_transfer(self.STANDARD_REQUEST_DEVICE_TO_HOST_TO_ENDPOINT, self.GET_STATUS, 0, 0, length)"}
{"Repository": "termtyper", "input": "Base Widget for Header NavItems className NavItemBase(Widget) Method __init__ Method on_click Method render", "label": "class NavItemBase(Widget):\n    DEFAULT_CSS = \"\"\"\n    NavItemBase {\n        content-align: center middle;\n        height: auto;\n        width: auto;\n        padding: 0 1;\n    }\n    \"\"\"\n\n    def __init__(self, text: str, screen_name: Optional[str] = None) -> None:\n        super().__init__()\n        self.text = text\n        self.screen_name = screen_name\n\n    def on_click(self) -> None:\n        if self.screen_name:\n            self.post_message(SetScreen(self.screen_name))\n\n    def render(self) -> RenderableType:\n        return self.text"}
{"Repository": "amazoncaptcha", "input": "Given path, which was supposed to be the path to the folder where script can store images, is not a folder. className NotFolderError(Exception) Method __init__ Method __str__ Attribute path Attribute message", "label": "class NotFolderError(Exception):\n    def __init__(self, path, message='is not a folder. Cannot store images there.'):\n\n        self.path = path\n\n        self.message = message\n\n\n\n    def __str__(self):\n\n        return f'\"{self.path}\" {self.message}'"}
{"Repository": "classix", "input": "build_ext command for use when numpy headers are needed. className CustomBuildExtCommand(build_ext) Method run", "label": "class CustomBuildExtCommand(build_ext):\n    def run(self):\n\n        import numpy\n\n        self.include_dirs.append(numpy.get_include())\n\n        build_ext.run(self)"}
{"Repository": "ropa", "input": "Post-installation for installation mode. className PostInstallCommand(install) Method run", "label": "class PostInstallCommand(install):\n    def run(self):\n        subprocess.call('scripts/post_install.sh')\n        install.run(self)"}
